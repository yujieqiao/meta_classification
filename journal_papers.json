{"30949800": {"mesh": ["Algorithms", "Computer Simulation", "Humans", "Models, Neurological", "Neuronal Plasticity", "Sensation", "Synapses"], "AbstractText": [{"section": null, "text": "During neural development sensory stimulation induces long-term changes in the receptive field of the neurons that encode the stimuli. The Bienenstock-Cooper-Munro (BCM) model was introduced to model and analyze this process computationally, and it remains one of the major models of unsupervised plasticity to this day. Here we show that for some stimulus types, the convergence of the synaptic weights under the BCM rule slows down exponentially as the number of synapses per neuron increases. We present a mathematical analysis of the slowdown that shows also how the slowdown can be avoided."}], "ArticleTitle": "Slowdown of BCM plasticity with many synapses."}, "32862338": {"mesh": ["Action Potentials", "Animals", "Ganglia, Spinal", "Membrane Potentials", "Models, Neurological", "Neurons", "Sodium Channels"], "AbstractText": [{"section": null, "text": "Small dorsal root ganglion (DRG) neurons are primary nociceptors which are responsible for sensing pain. Elucidation of their dynamics is essential for understanding and controlling pain. To this end, we present a numerical bifurcation analysis of a small DRG neuron model in this paper. The model is of Hodgkin-Huxley type and has 9 state variables. It consists of a Nav1.7 and a Nav1.8 sodium channel, a leak channel, a delayed rectifier potassium, and an A-type transient potassium channel. The dynamics of this model strongly depend on the maximal conductances of the voltage-gated ion channels and the external current, which can be adjusted experimentally. We show that the neuron dynamics are most sensitive to the Nav1.8 channel maximal conductance ([Formula: see text]). Numerical bifurcation analysis shows that depending on [Formula: see text] and the external current, different parameter regions can be identified with stable steady states, periodic firing of action potentials, mixed-mode oscillations (MMOs), and bistability between stable steady states and stable periodic firing of action potentials. We illustrate and discuss the transitions between these different regimes. We further analyze the behavior of MMOs. As the external current is decreased, we find that MMOs appear after a cyclic limit point. Within this region, bifurcation analysis shows a sequence of isolated periodic solution branches with one large action potential and a number of small amplitude peaks per period. For decreasing external current, the number of small amplitude peaks is increasing and the distance between the large amplitude action potentials is growing, finally tending to infinity and thereby leading to a stable steady state. A closer inspection reveals more complex concatenated MMOs in between these periodic MMO branches, forming Farey sequences. Lastly, we also find small solution windows with aperiodic oscillations which seem to be chaotic. The dynamical patterns found here-as consequences of bifurcation points regulated by different parameters-have potential translational significance as repetitive firing of action potentials imply pain of some form and intensity; manipulating these patterns by regulating the different parameters could aid in investigating pain dynamics."}], "ArticleTitle": "Computational analysis of a 9D model for a small DRG neuron."}, "32892300": {"mesh": ["Action Potentials", "Animals", "Cerebral Cortex", "Gap Junctions", "Models, Neurological", "Nerve Net", "Neurons", "Pyramidal Cells"], "AbstractText": [{"section": null, "text": "The existence of electrical communication among pyramidal cells (PCs) in the adult cortex has been debated by neuroscientists for several decades. Gap junctions (GJs) among cortical interneurons have been well documented experimentally and their functional roles have been proposed by both computational neuroscientists and experimentalists alike. Experimental evidence for similar junctions among pyramidal cells in the cortex, however, has remained elusive due to the apparent rarity of these couplings among neurons. In this work, we develop a neuronal network model that includes observed probabilities and strengths of electrotonic coupling between PCs and gap-junction coupling among interneurons, in addition to realistic synaptic connectivity among both populations. We use this network model to investigate the effect of electrotonic coupling between PCs on network behavior with the goal of theoretically addressing this controversy of existence and purpose of electrotonically coupled PCs in the cortex."}], "ArticleTitle": "A computational investigation of electrotonic coupling between pyramidal cells in the cortex."}, "31659570": {"mesh": ["Animals", "Axons", "Brain", "Computer Simulation", "Models, Neurological", "Myelin Sheath", "Nerve Net", "Neurons", "Ranvier's Nodes"], "AbstractText": [{"section": null, "text": "The paper presents a hierarchical series of computational models for myelinated axonal compartments. Three classes of models are considered, either with distributed parameters (2.5D EQS-ElectroQuasi Static, 1D TL-Transmission Lines) or with lumped parameters (0D). They are systematically analyzed with both analytical and numerical approaches, the main goal being to identify the best procedure for order reduction of each case. An appropriate error estimator is proposed in order to assess the accuracy of the models. This is the foundation of a procedure able to find the simplest reduced model having an imposed precision. The most computationally efficient model from the three geometries proved to be the analytical 1D one, which is able to have accuracy less than 0.1%. By order reduction with vector fitting, a finite model is generated with a relative difference of 10-&#8201;4 for order 5. The dynamical models thus extracted allow an efficient simulation of neurons and, consequently, of neuronal circuits. In such situations, the linear models of the myelinated compartments coupled with the dynamical, non-linear models of the Ranvier nodes, neuronal body (soma) and dendritic tree give global reduced models. In order to ease the simulation of large-scale neuronal systems, the sub-models at each level, including those of myelinated compartments should have the lowest possible order. The presented procedure is a first step in achieving simulations of neural systems with accuracy control."}], "ArticleTitle": "Reduced order models of myelinated axonal compartments."}, "32125562": {"mesh": ["Action Potentials", "Algorithms", "Animals", "Brain Mapping", "Cues", "Entorhinal Cortex", "Machine Learning", "Models, Neurological", "Neurons", "Space Perception", "Synapses"], "AbstractText": [{"section": null, "text": "Grid cells in the entorhinal cortex, together with head direction, place, speed and border cells, are major contributors to the organization of spatial representations in the brain. In this work we introduce a novel theoretical and algorithmic framework able to explain the optimality of hexagonal grid-like response patterns. We show that this pattern is a result of minimal variance encoding of neurons together with maximal robustness to neurons' noise and minimal number of encoding neurons. The novelty lies in the formulation of the encoding problem considering neurons as an overcomplete basis (a frame) where the position information is encoded. Through the modern Frame Theory language, specifically that of tight and equiangular frames, we provide new insights about the optimality of hexagonal grid receptive fields. The proposed model is based on the well-accepted and tested hypothesis of Hebbian learning, providing a simplified cortical-based framework that does not require the presence of velocity-driven oscillations (oscillatory model) or translational symmetries in the synaptic connections (attractor model). We moreover demonstrate that the proposed encoding mechanism naturally explains axis alignment of neighbor grid cells and maps shifts, rotations and scaling of the stimuli onto the shape of grid cells' receptive fields, giving a straightforward explanation of the experimental evidence of grid cells remapping under transformations of environmental cues."}], "ArticleTitle": "A computational model for grid maps in neural populations."}, "30980214": {"mesh": ["Algorithms", "Animals", "Computer Simulation", "Humans", "Machine Learning", "Models, Neurological", "Neurons", "Visual Cortex", "Visual Fields", "Visual Pathways"], "AbstractText": [{"section": null, "text": "In this work we show how to construct connectivity kernels induced by the receptive profiles of simple cells of the primary visual cortex (V1). These kernels are directly defined by the shape of such profiles: this provides a metric model for the functional architecture of V1, whose global geometry is determined by the reciprocal interactions between local elements. Our construction adapts to any bank of filters chosen to represent a set of receptive profiles, since it does not require any structure on the parameterization of the family. The connectivity kernel that we define carries a geometrical structure consistent with the well-known properties of long-range horizontal connections in V1, and it is compatible with the perceptual rules synthesized by the concept of association field. These characteristics are still present when the kernel is constructed from a bank of filters arising from an unsupervised learning algorithm."}], "ArticleTitle": "From receptive profiles to a metric model of V1."}, "32627092": {"mesh": ["Animals", "Axons", "Humans", "Membrane Potentials", "Models, Neurological", "Neural Conduction", "Neurons"], "AbstractText": [{"section": null, "text": "The derivation by Alan Hodgkin and Andrew Huxley of their famous neuronal conductance model relied on experimental data gathered using the squid giant axon. However, the experimental determination of conductances of neurons is difficult, in particular under the presence of spatial and temporal heterogeneities, and it is also reasonable to expect variations between species or even between different types of neurons of the same species.We tackle the inverse problem of determining, given voltage data, conductances with non-uniform distribution in the simpler setting of a passive cable equation, both in a single or branched neurons. To do so, we consider the minimal error iteration, a computational technique used to solve inverse problems. We provide several numerical results showing that the method is able to provide reasonable approximations for the conductances, given enough information on the voltages, even for noisy data."}], "ArticleTitle": "A computational approach for the inverse problem of neuronal conductances determination."}, "31134433": {"mesh": ["Algorithms", "Animals", "Cognition", "Computer Simulation", "Decision Making", "Electrophysiological Phenomena", "Humans", "Long-Term Synaptic Depression", "Machine Learning", "Mental Recall", "Neural Networks, Computer", "Neural Pathways", "Neuronal Plasticity", "Neurons", "Synapses"], "AbstractText": [{"section": null, "text": "We demonstrate that a randomly connected attractor network with dynamic synapses can discriminate between similar sequences containing multiple stimuli suggesting such networks provide a general basis for neural computations in the brain. The network contains units representing assemblies of pools of neurons, with preferentially strong recurrent excitatory connections rendering each unit bi-stable. Weak interactions between units leads to a multiplicity of attractor states, within which information can persist beyond stimulus offset. When a new stimulus arrives, the prior state of the network impacts the encoding of the incoming information, with short-term synaptic depression ensuring an itinerancy between sets of active units. We assess the ability of such a network to encode the identity of sequences of stimuli, so as to provide a template for sequence recall, or decisions based on accumulation of evidence. Across a range of parameters, such networks produce the primacy (better final encoding of the earliest stimuli) and recency (better final encoding of the latest stimuli) observed in human recall data and can retain the information needed to make a binary choice based on total number of presentations of a specific stimulus. Similarities and differences in the final states of the network produced by different sequences lead to predictions of specific errors that could arise when an animal or human subject generalizes from training data, when the training data comprises a subset of the entire stimulus repertoire. We suggest that such networks can provide the general purpose computational engines needed for us to solve many cognitive tasks."}], "ArticleTitle": "Spatiotemporal discrimination in attractor networks with short-term synaptic plasticity."}, "31620945": {"mesh": ["Action Potentials", "Animals", "Brain", "Computer Simulation", "Cortical Spreading Depression", "Interneurons", "Models, Neurological", "Pyramidal Cells", "Synapses"], "AbstractText": [{"section": null, "text": "Cortical spreading depression (CSD) is a wave of transient intense neuronal firing leading to a long lasting depolarizing block of neuronal activity. It is a proposed pathological mechanism of migraine with aura. Some forms of migraine are associated with a genetic mutation of the Nav1.1 channel, resulting in its gain of function and implying hyperexcitability of interneurons. This leads to the counterintuitive hypothesis that intense firing of interneurons can cause CSD ignition. To test this hypothesis in silico, we developed a computational model of an E-I pair (a pyramidal cell and an interneuron), in which the coupling between the cells in not just synaptic, but takes into account also the effects of the accumulation of extracellular potassium caused by the activity of the neurons and of the synapses. In the context of this model, we show that the intense firing of the interneuron can lead to CSD. We have investigated the effect of various biophysical parameters on the transition to CSD, including the levels of glutamate or GABA, frequency of the interneuron firing and the efficacy of the KCC2 co-transporter. The key element for CSD ignition in our model was the frequency of interneuron firing and the related accumulation of extracellular potassium, which induced a depolarizing block of the pyramidal cell. This constitutes a new mechanism of CSD ignition."}], "ArticleTitle": "Modeling cortical spreading depression induced by the hyperactivity of interneurons."}, "31468241": {"mesh": ["Action Potentials", "Algorithms", "Ion Channel Gating", "Ion Channels", "Membrane Potentials", "Models, Neurological", "Monte Carlo Method", "Neurons", "Patch-Clamp Techniques"], "AbstractText": [{"section": null, "text": "Identification of key ionic channel contributors to the overall dynamics of a neuron is an important problem in experimental neuroscience. Such a problem is challenging since even in the best cases, identification relies on noisy recordings of membrane potential only, and strict inversion to the constituent channel dynamics is mathematically ill-posed. In this work, we develop a biophysically interpretable, learning-based strategy for data-driven inference of neuronal dynamics. In particular, we propose two optimization frameworks to learn and approximate neural dynamics from an observed voltage trajectory. In both the proposed strategies, the membrane potential dynamics are approximated as a weighted sum of ionic currents. In the first strategy, the ionic currents are represented using voltage dependent channel conductances and membrane potential in a parametric form, while in the second strategy, the currents are represented as a linear combination of generic basis functions. A library of channel activation/inactivation and time-constant curves describing prototypical channel kinetics are used to provide estimates of the channel variables to approximate the ionic currents. Finally, a linear optimization problem is solved to infer the weights/scaling variables in the membrane-potential dynamics. In the first strategy, the weights can be used to recover the channel conductances, and the reversal potentials while in the second strategy, using the estimated weights, active channels can be inferred and the trajectory of the gating variables are recovered, allowing for biophysically salient inference. Our results suggest that the complex nonlinear behavior of the neural dynamics over a range of temporal scales can be efficiently inferred in a data-driven manner from noisy membrane potential recordings."}], "ArticleTitle": "Biophysically interpretable inference of single neuron dynamics."}, "32683665": {"mesh": ["Adaptation, Physiological", "Humans", "Models, Neurological", "Myoclonus", "Nystagmus, Pathologic", "Tremor"], "AbstractText": [{"section": null, "text": "The syndrome of oculopalatal tremor (OPT) featuring the olivo-cerebellar hypersychrony leads to disabling pendular nystagmus and palatal myoclonus. This rare disorder provides valuable information about the motor physiology and offers insights into the mechanistic underpinning of common movement disorders. This focused review summarizes the last decade of OPT research from our laboratory and addresses three critical questions: 1) How the disease of inferior olive affects the physiology of motor learning? We discovered that our brain's ability to compensate for the impaired motor command and implement errors to correct future movements could be affected if the cerebellum is occupied in receiving and transmitting the meaningless signal. A complete failure of OPT patients to adapt to change in rapid eye movements (saccades) provided proof of this principle. 2) Whether maladaptive olivo-cerebellar circuit offers insight into the mechanistic underpinning of the common movement disorder, dystonia, characterized by abnormal twisting and turning of the body part. We discovered that the subgroup of patients who had OPT also had dystonia affecting the neck, trunk, limbs, and face. We also found that the subjects who had tremor predominant neck dystonia (without OPT) also had impaired motor learning on a long and short timescale, just like those with OPT. Altogether, our studies focused on dystonia suggested the evidence for the maladaptive olive-cerebellar system. 3) We discovered that the OPT subjects had difficulty in perceiving the direction of their linear forward motion, i.e., heading, suggesting that olivo-cerebellar hypersynchrony also affects perception."}], "ArticleTitle": "Lessons learned from the syndrome of oculopalatal tremor."}, "31506807": {"mesh": ["Brain", "Computer Simulation", "Electroencephalography", "Hepatic Encephalopathy", "Humans", "Models, Neurological"], "AbstractText": [{"section": null, "text": "Acute hepatic encephalopathy (AHE) due to acute liver failure is a common form of delirium, a state of confusion, impaired attention, and decreased arousal. The electroencephalogram (EEG) in AHE often exhibits a striking abnormal pattern of brain activity, which epileptiform discharges repeat in a regular repeating pattern. This pattern is known as generalized periodic discharges, or triphasic-waves (TPWs). While much is known about the neurophysiological mechanisms underlying AHE, how these mechanisms relate to TPWs is poorly understood. In order to develop hypotheses how TPWs arise, our work builds a computational model of AHE (AHE-CM), based on three modifications of the well-studied Liley model which emulate mechanisms believed central to brain dysfunction in AHE: increased neuronal excitability, impaired synaptic transmission, and enhanced postsynaptic inhibition. To relate our AHE-CM to clinical EEG data from patients with AHE, we design a model parameter optimization method based on particle filtering (PF-POM). Based on results from 7 AHE patients, we find that the proposed AHE-CM not only performs well in reproducing important aspects of the EEG, namely the periodicity of triphasic waves (TPWs), but is also helpful in suggesting mechanisms underlying variation in EEG patterns seen in AHE. In particular, our model helps explain what conditions lead to increased frequency of TPWs. In this way, our model represents a starting point for exploring the underlying mechanisms of brain dynamics in delirium by relating microscopic mechanisms to EEG patterns."}], "ArticleTitle": "A novel neural computational model of generalized periodic discharges in acute hepatic encephalopathy."}, "32307640": {"mesh": ["Adult", "Algorithms", "Brain", "Cerebral Cortex", "Computer Simulation", "Electrophysiological Phenomena", "Epilepsy", "Humans", "Interneurons", "Models, Neurological", "Neural Pathways", "Neurites", "Neuronal Plasticity", "Neurons", "Presynaptic Terminals", "Pyramidal Cells", "Thalamus", "Transcranial Direct Current Stimulation", "gamma-Aminobutyric Acid"], "AbstractText": [{"section": null, "text": "Transcranial Direct brain stimulation (tDCS) is commonly used in order to modulate cortical networks activity during physiological processes through the application of weak electrical fields with scalp electrodes. Cathodal stimulation has been shown to decrease brain excitability in the context of epilepsy, with variable success. However, the cellular mechanisms responsible for the acute and the long-lasting effect of tDCS remain elusive. Using a novel approach of computational modeling that combines detailed but functionally integrated neurons we built a physiologically-based thalamocortical column. This model comprises 10,000 individual neurons made of pyramidal cells, and 3 types of gamma-aminobutyric acid (GABA) -ergic cells (VIP, PV, and SST) respecting the anatomy, layers, projection, connectivity and neurites orientation. Simulating realistic electric fields in term of intensity, main results showed that 1) tDCS effects are best explained by modulation of the presynaptic probability of release 2) tDCS affects the dynamic of cortical network only if a sufficient number of neurons are modulated 3)VIP GABAergic interneurons of the superficial layer of the cortex are especially affected by tDCS 4) Long lasting effect depends on glutamatergic synaptic plasticity."}], "ArticleTitle": "Modelling acute and lasting effects of tDCS on epileptic activity."}, "32519227": {"mesh": ["Computer Simulation", "Models, Neurological", "Neural Networks, Computer", "Neurons", "Reproducibility of Results"], "AbstractText": [{"section": null, "text": "Building upon previous experiments can be used to accomplish new goals. In computing, it is imperative to reuse computer code to continue development on specific projects. Reproducibility is a fundamental building block in science, and experimental reproducibility issues have recently been of great concern. It may be surprising that reproducibility is also of concern in computational science. In this study, we used a previously published code to investigate neural network activity and we were unable to replicate our original results. This led us to investigate the code in question, and we found that several different aspects, attributable to floating-point arithmetic, were the cause of these replicability issues. Furthermore, we uncovered other manifestations of this lack of replicability in other parts of the computation with this model. The simulated model is a standard system of ordinary differential equations, very much like those commonly used in computational neuroscience. Thus, we believe that other researchers in the field should be vigilant when using such models and avoid drawing conclusions from calculations if their qualitative results can be substantially modified through non-reproducible circumstances."}], "ArticleTitle": "Non-replicability circumstances in a neural network model with Hodgkin-Huxley-type neurons."}, "31104206": {"mesh": ["Algorithms", "Animals", "Cerebral Cortex", "Computer Simulation", "Humans", "Memory, Short-Term", "Nerve Net", "Neural Networks, Computer", "Psychomotor Performance", "Sensory Receptor Cells"], "AbstractText": [{"section": null, "text": "A functional role of the cerebral cortex is to form and hold representations of the sensory world for behavioral purposes. This is achieved by a sheet of neurons, organized in modules called cortical columns, that receives inputs in a peculiar manner, with only a few neurons driven by sensory inputs through thalamic projections, and a vast majority of neurons receiving mainly cortical inputs. How should cortical modules be organized, with respect to sensory inputs, in order for the cortex to efficiently hold sensory representations in memory? To address this question we investigate the memory performance of trees of recurrent networks (TRN) that are composed of recurrent networks, modeling cortical columns, connected with each others through a tree-shaped feed-forward backbone of connections, with sensory stimuli injected at the root of the tree. On these sensory architectures two types of short-term memory (STM) mechanisms can be implemented, STM via transient dynamics on the feed-forward tree, and STM via reverberating activity on the recurrent connectivity inside modules. We derive equations describing the dynamics of such networks, which allow us to thoroughly explore the space of possible architectures and quantify their memory performance. By varying the divergence ratio of the tree, we show that serial architectures, where sensory inputs are successively processed in different modules, are better suited to implement STM via transient dynamics, while parallel architectures, where sensory inputs are simultaneously processed by all modules, are better suited to implement STM via reverberating dynamics."}], "ArticleTitle": "Short term memory properties of sensory neural architectures."}, "32363561": {"mesh": ["Algorithms", "Computer Simulation", "Dominance, Ocular", "Electrophysiological Phenomena", "Feedback, Sensory", "Humans", "Models, Neurological", "N-Methylaspartate", "Neural Networks, Computer", "Neurons", "Neurotransmitter Agents", "Stochastic Processes", "Vision Disparity", "Vision, Binocular", "Visual Perception"], "AbstractText": [{"section": null, "text": "When similar visual stimuli are presented binocularly to both eyes, one perceives a fused single image. However, when the two stimuli are distinct, one does not perceive a single image; instead, one perceives binocular rivalry. That is, one perceives one of the stimulated patterns for a few seconds, then the other for few seconds, and so on - with random transitions between the two percepts. Most theoretical studies focus on rivalry, with few considering the coexistence of fusion and rivalry. Here we develop three distinct computational neuronal network models which capture binocular rivalry with realistic stochastic properties, fusion, and the hysteretic transition between. Each is a conductance-based point neuron model, which is multi-layer with two ocular dominance columns (L & R) and with an idealized \"ring\" architecture where the orientation preference of each neuron labels its location on a ring. In each model, the primary mechanism initiating binocular rivalry is cross-column inhibition, with firing rate adaptation governing the temporal properties of the transitions between percepts. Under stimulation by similar visual patterns, each of three models uses its own mechanism to overcome cross-column inhibition, and thus to prevent rivalry and allow the fusion of similar images: The first model uses cross-column feedforward inhibition from the opposite eye to \"shut off\" the cross-column feedback inhibition; the second model \"turns on\" a second layer of monocular neurons as a parallel pathway to the binocular neurons, rivaling out of phase with the first layer, and together these two pathways represent fusion; and the third model uses cross-column excitation to overcome the cross-column inhibition and enable fusion. Thus, each of the idealized ring models depends upon a different mechanism for fusion that might emerge as an underlying mechanism present in real visual cortex."}], "ArticleTitle": "Ring models of binocular rivalry and fusion."}, "31989403": {"mesh": ["Algorithms", "Beta Rhythm", "Computer Simulation", "Electroencephalography", "Electrophysiological Phenomena", "Finite Element Analysis", "Gamma Rhythm", "Humans", "Models, Neurological", "Neocortex", "Neurons"], "AbstractText": [{"section": null, "text": "In this paper a mean field model of spatio-temporal electroencephalographic activity in the neocortex is used to computationally study the emergence of neocortical gamma oscillations as a result of neuronal response modulation. It is shown using a numerical bifurcation analysis that gamma oscillations emerge robustly in the solutions of the model and transition to beta oscillations through coordinated modulation of the responsiveness of inhibitory and excitatory neuronal populations. The spatio-temporal pattern of the propagation of these oscillations across the neocortex is illustrated by solving the equations of the model using a finite element software package. Thereby, it is shown that the gamma oscillations remain localized to the regions of neuronal modulation. Moreover, it is discussed that the inherent spatial averaging effect of commonly used electrocortical measurement techniques can significantly alter the amplitude and pattern of fast oscillations in neocortical recordings, and hence can potentially affect physiological interpretations of these recordings."}], "ArticleTitle": "Transient neocortical gamma oscillations induced by neuronal response modulation."}, "31119525": {"mesh": ["Acidosis", "Amphibians", "Animals", "Biological Clocks", "Central Pattern Generators", "Cheek", "Electrophysiological Phenomena", "Ganglia, Invertebrate", "Lung", "Metamorphosis, Biological", "Motor Neurons", "Neural Inhibition", "Neural Networks, Computer", "Neurons", "Ranidae", "Synapses"], "AbstractText": [{"section": null, "text": "The neuronal multiunit model presented here is a formal model of the central pattern generator (CPG) of the amphibian ventilatory neural network, inspired by experimental data from Pelophylax ridibundus. The kernel of the CPG consists of three pacemakers and two follower neurons (buccal and lung respectively). This kernel is connected to a chain of excitatory and inhibitory neurons organized in loops. Simulations are performed with Izhikevich-type neurons. When driven by the buccal follower, the excitatory neurons transmit and reorganize the follower activity pattern along the chain, and when driven by the lung follower, the excitatory and inhibitory neurons of the chain fire in synchrony. The additive effects of synaptic inputs from the pacemakers on the buccal follower account for (1) the low frequency buccal rhythm, (2) the intra-burst high frequency oscillations, and (3) the episodic lung activity. Chemosensitivity to acidosis is implemented by an increase in the firing frequency of one of the pacemakers. This frequency increase leads to both a decrease in the buccal burst frequency and an increase in the lung episode frequency. The rhythmogenic properties of the model are robust against synaptic noise and pacemaker jitter. To validate the rhythm and pattern genesis of this formal CPG, neurograms were built from simulated motoneuron activity, and compared with experimental neurograms. The basic principles of our model account for several experimental observations, and we suggest that these principles may be generic for amphibian ventilation."}], "ArticleTitle": "Neural network model of an amphibian ventilatory central pattern generator."}, "33165797": {"mesh": ["Basal Ganglia", "Dopamine", "Models, Neurological", "Nucleus Accumbens", "Reward"], "AbstractText": [{"section": null, "text": "Nucleus accumbens is part of the neural structures required for reward based learning and cognitive processing of motivation. Understanding its cellular dynamics and its role in basal ganglia circuits is important not only in diagnosing behavioral disorders and psychiatric problems as addiction and depression but also for developing therapeutic treatments for them. Building a computational model would expand our comprehension of nucleus accumbens. In this work, we are focusing on establishing a model of nucleus accumbens which has not been considered as much as dorsal striatum in computational neuroscience. We will begin by modeling the behavior of single cells and then build a holistic model of nucleus accumbens considering the effect of synaptic currents. We will verify the validity of the model by showing the consistency of simulation results with the empirical data. Furthermore, the simulation results reveal the joint effect of cortical stimulation and dopaminergic modulation on the activity of medium spiny neurons. This effect differentiates with the type of dopamine receptors."}], "ArticleTitle": "Modeling nucleus accumbens : A Computational Model from Single Cell to Circuit Level."}, "32080777": {"mesh": ["Algorithms", "Calcium Signaling", "Computer Simulation", "Electrophysiological Phenomena", "Extracellular Space", "Humans", "Models, Neurological", "Nerve Net", "Neurons", "ROC Curve", "Synapses"], "AbstractText": [{"section": null, "text": "A major goal in neuroscience is to estimate neural connectivity from large scale extracellular recordings of neural activity in vivo. This is challenging in part because any such activity is modulated by the unmeasured external synaptic input to the network, known as the common input problem. Many different measures of functional connectivity have been proposed in the literature, but their direct relationship to synaptic connectivity is often assumed or ignored. For in vivo data, measurements of this relationship would require a knowledge of ground truth connectivity, which is nearly always unavailable. Instead, many studies use in silico simulations as benchmarks for investigation, but such approaches necessarily rely upon a variety of simplifying assumptions about the simulated network and can depend on numerous simulation parameters. We combine neuronal network simulations, mathematical analysis, and calcium imaging data to address the question of when and how functional connectivity, synaptic connectivity, and latent external input variability can be untangled. We show numerically and analytically that, even though the precision matrix of recorded spiking activity does not uniquely determine synaptic connectivity, it is in practice often closely related to synaptic connectivity. This relation becomes more pronounced when the spatial structure of neuronal variability is jointly considered."}], "ArticleTitle": "Inference of synaptic connectivity and external variability in neural microcircuits."}, "31980990": {"mesh": ["Algorithms", "Calcium Signaling", "Computer Simulation", "Electrophysiological Phenomena", "Glutamates", "Humans", "Long-Term Potentiation", "Membrane Potentials", "Models, Neurological", "Neuronal Plasticity", "Neurons", "Neurotransmitter Agents", "Receptors, AMPA", "Receptors, N-Methyl-D-Aspartate"], "AbstractText": [{"section": null, "text": "Hebbian plasticity means that if the firing of two neurons is correlated, then their connection is strengthened. Conversely, uncorrelated firing causes a decrease in synaptic strength. Spike-timing-dependent plasticity (STDP) represents one instantiation of Hebbian plasticity. Under STDP, synaptic changes depend on the relative timing of the pre- and post-synaptic firing. By inducing pre- and post-synaptic firing at different relative times the STDP curves of many neurons have been determined, and it has been found that there are different curves for different neuron types or synaptic sites. Biophysically, strengthening (long-term potentiation, LTP) or weakening (long-term depression, LTD) of glutamatergic synapses depends on the post-synaptic influx of calcium (Ca2+): weak influx leads to LTD, while strong, transient influx causes LTP. The voltage-dependent NMDA receptors are the main source of Ca2+ influx, but they will only open if a post-synaptic depolarisation coincides with pre-synaptic neurotransmitter release. Here we present a computational mechanism for Ca2+-dependent plasticity in which the interplay between the pre-synaptic neurotransmitter release and the post-synaptic membrane potential leads to distinct Ca2+ time-courses, which in turn lead to the change in synaptic strength. It is shown that the model complies with classic STDP results, as well as with results obtained with triplets of spikes. Furthermore, the model is capable of displaying different shapes of STDP curves, as observed in different experimental studies."}], "ArticleTitle": "A calcium-influx-dependent plasticity model exhibiting multiple STDP curves."}, "31506806": {"mesh": ["Animals", "Brain", "Mitochondria", "Models, Neurological", "Neurons", "Seizures"], "AbstractText": [{"section": null, "text": "The effect of pathological phenomena such as epileptic seizures and spreading depolarization (SD) on mitochondria and the potential feedback of mitochondrial dysfunction into the dynamics of those phenomena are complex and difficult to study experimentally due to the simultaneous changes in many variables governing neuronal behavior. By combining a model that accounts for a wide range of neuronal behaviors including seizures, normoxic SD, and hypoxic SD (HSD), together with a detailed model of mitochondrial function and intracellular Ca2+ dynamics, we investigate mitochondrial dysfunction and its potential role in recovery of the neuron from seizures, HSD, and SD. Our results demonstrate that HSD leads to the collapse of mitochondrial membrane potential and cellular ATP levels that recover only when normal oxygen supply is restored. Mitochondrial organic phosphate and pH gradients determine the strength of the depolarization block during HSD and SD, how quickly the cell enters the depolarization block when the oxygen supply is disrupted or potassium in the bath solution is raised beyond the physiological value, and how fast the cell recovers from SD and HSD when normal potassium concentration and oxygen supply are restored. Although not as dramatic as phosphate and pH gradients, mitochondrial Ca2+ uptake has a similar effect on neuronal behavior during these conditions."}], "ArticleTitle": "Mitochondrial dysfunction and role in spreading depolarization and seizure."}, "32901334": {"mesh": ["Adult", "Amblyopia", "Child", "Eye Movements", "Humans", "Models, Neurological", "Saccades"], "AbstractText": [{"section": null, "text": "This study analyzed the characteristics of pursuit and assessed the influence of prior and visual information on eye velocity and saccades in amblyopic and control children, in comparison to adults. Eye movements of 41 children (21 amblyopes and 20 controls) were compared to eye movements of 55 adults (18 amblyopes and 37 controls). Participants were asked to pursue a target moving at a constant velocity. The target was either a 'standard' target, with a uniform color intensity, or a 'noisy' target, with blurry edges, to mimic the blurriness of an amblyopic eye. Analysis of pursuit patterns showed that the onset was delayed, and the gain was decreased in control children with a noisy target in comparison to amblyopic or control children with a standard target. Furthermore, a significant effect of prior and visual information on pursuit velocity and saccades was found across all participants. Moreover, the modulation of the effect of visual information on the pursuit velocity by group, that is amblyopes or controls with a standard target, and controls with a noisy target, was more limited in children. In other words, the effect of visual information was higher in control adults with a standard target compared to control children with the same target. However, in the case of a blurry target, either in control participants with a noisy target or in amblyopic participants with a standard target, the effect of visual information was larger in children."}], "ArticleTitle": "Influence of prior and visual information on eye movements in amblyopic children."}, "32621105": {"mesh": ["Eye Movements", "Gabapentin", "Humans", "Memantine", "Models, Neurological", "Tremor"], "AbstractText": [{"section": null, "text": "Syndrome of oculopalatal tremor (OPT) causes pendular nystagmus of the eyes and its disabling consequence on the visual system. Classic pharmacotherapeutic studies revealed reduction in the eye velocity of the oscillatory waveforms. Subjective improvement in vision, however, remains out of proportionately low. Elegant models depicting quasi-sinusoidal coarse oscillations of the eyes highlighted two distinct oscillators; one at the inferior olive causing primary 2&#160;Hz oscillations, while the second, independent oscillator, at the cerebellum adding the randomness to the waveform. Here we examined whether pharmacotherapy affects the randomness of the oscillatory waveform. Horizontal, vertical, and torsional angular eye positions were measured independently from both eyes as six subjects with OPT directed gaze toward a straight-ahead target. The measurements were performed before administration of alpha-2-delta calcium channel blocker (gabapentin) or NMDA receptor antagonist (memantine) and after the subjects were treated with each of these drugs for at least 8&#160;days. Amplitude and velocity of eye oscillations were reduced by gabapentin and memantine, but there was an increase in the waveform randomness. We found that the increase in randomness was proportionate to the amount of reduction in the waveform velocity or amplitude. Hierarchical clustering revealed distinct patterns of oscillatory waveforms, with each subject belonging to a specific cluster group. The pharmacotherapy changed the waveform clustering pattern of the waveform in each subject. We conclude that in addition to incomplete resolution of the oscillation intensity, increased randomness could be one of the reasons why there is not enough clinical difference in the patients' visual quality."}], "ArticleTitle": "Gabapentin and memantine increases randomness of oscillatory waveform in ocular palatal tremor."}, "31025235": {"mesh": ["Acetophenones", "Benzopyrans", "Calcium", "Calcium Signaling", "Computer Simulation", "Cytosol", "Electrophysiological Phenomena", "Humans", "Large-Conductance Calcium-Activated Potassium Channels", "Membrane Potentials", "Myocytes, Smooth Muscle", "Ryanodine Receptor Calcium Release Channel", "Sarcoplasmic Reticulum", "Urinary Bladder", "Urinary Bladder, Overactive"], "AbstractText": [{"section": null, "text": "The large conductance voltage and calcium activated potassium (BK) channels play a crucial role in regulating the excitability of detrusor smooth muscle, which lines the wall of the urinary bladder. These channels have been widely characterized in terms of their molecular structure, pharmacology and electrophysiology. They control the repolarising and hyperpolarising phases of the action potential, thereby regulating the firing frequency and contraction profiles of the smooth muscle. Several groups have reported varied profiles of BK currents and I-V curves under similar experimental conditions. However, no single computational model has been able to reconcile these apparent discrepancies. In view of the channels' physiological importance, it is imperative to understand their mechanistic underpinnings so that a realistic model can be created. This paper presents a computational model of the BK channel, based on the Hodgkin-Huxley formalism, constructed by utilising three activation processes - membrane potential, calcium inflow from voltage-gated calcium channels on the membrane and calcium released from the ryanodine receptors present on the sarcoplasmic reticulum. In our model, we attribute the discrepant profiles to the underlying cytosolic calcium received by the channel during its activation. The model enables us to make heuristic predictions regarding the nature of the sub-membrane calcium dynamics underlying the BK channel's activation. We have employed the model to reproduce various physiological characteristics of the channel and found the simulated responses to be in accordance with the experimental findings. Additionally, we have used the model to investigate the role of this channel in electrophysiological signals, such as the action potential and spontaneous transient hyperpolarisations. Furthermore, the clinical effects of BK channel openers, mallotoxin and NS19504, were simulated for the detrusor smooth muscle cells. Our findings support the proposed application of these drugs for amelioration of the condition of overactive bladder. We thus propose a physiologically realistic BK channel model which can be integrated with other biophysical mechanisms such as ion channels, pumps and exchangers to further elucidate its micro-domain interaction with the intracellular calcium environment."}], "ArticleTitle": "A computational model of large conductance voltage and calcium activated potassium channels: implications for calcium dynamics and electrophysiology in detrusor smooth muscle cells."}, "30737596": {"mesh": ["Aging", "Algorithms", "Animals", "Axons", "Cerebral Cortex", "Child", "Child, Preschool", "Disease Models, Animal", "Humans", "Mice", "Models, Neurological", "Myelin Sheath", "Neural Conduction", "Neural Pathways", "Neurons", "Receptors, GABA-A", "Reticular Formation", "Seizures", "Thalamus"], "AbstractText": [{"section": null, "text": "We formulate a conductance-based model for a 3-neuron motif associated with Childhood Absence Epilepsy (CAE). The motif consists of neurons from the thalamic relay (TC) and reticular nuclei (RT) and the cortex (CT). We focus on a genetic defect common to the mouse homolog of CAE which is associated with loss of GABAA receptors on the TC neuron, and the fact that myelination of axons as children age can increase the conduction velocity between neurons. We show the combination of low GABAA mediated inhibition of TC neurons and the long corticothalamic loop delay gives rise to a variety of complex dynamics in the motif, including bistability. This bistability disappears as the corticothalamic conduction delay shortens even though GABAA activity remains impaired. Thus the combination of deficient GABAA activity and changing axonal myelination in the corticothalamic loop may be sufficient to account for the clinical course of CAE."}], "ArticleTitle": "Outgrowing seizures in Childhood Absence Epilepsy: time delays and bistability."}, "32338341": {"mesh": ["Algorithms", "Dominance, Ocular", "Feedback, Sensory", "Humans", "Models, Neurological", "Neural Networks, Computer", "Ocular Physiological Phenomena", "Stochastic Processes", "Vision Disparity", "Vision, Monocular", "Visual Perception"], "AbstractText": [{"section": null, "text": "Ambiguous visual images can generate dynamic and stochastic switches in perceptual interpretation known as perceptual rivalry. Such dynamics have primarily been studied in the context of rivalry between two percepts, but there is growing interest in the neural mechanisms that drive rivalry between more than two percepts. In recent experiments, we showed that split images presented to each eye lead to subjects perceiving four stochastically alternating percepts (Jacot-Guillarmod et al. Vision research, 133, 37-46, 2017): two single eye images and two interocularly grouped images. Here we propose a hierarchical neural network model that exhibits dynamics consistent with our experimental observations. The model consists of two levels, with the first representing monocular activity, and the second representing activity in higher visual areas. The model produces stochastically switching solutions, whose dependence on task parameters is consistent with four generalized Levelt Propositions, and with experiments. Moreover, dynamics restricted to invariant subspaces of the model demonstrate simpler forms of bistable rivalry. Thus, our hierarchical model generalizes past, validated models of binocular rivalry. This neuromechanistic model also allows us to probe the roles of interactions between populations at the network level. Generalized Levelt's Propositions hold as long as feedback from the higher to lower visual areas is weak, and the adaptation and mutual inhibition at the higher level is not too strong. Our results suggest constraints on the architecture of the visual system and show that complex visual stimuli can be used in perceptual rivalry experiments to develop more detailed mechanistic models of perceptual processing."}], "ArticleTitle": "A hierarchical model of perceptual multistability involving interocular grouping."}, "32399790": {"mesh": ["Algorithms", "Computer Simulation", "Electroencephalography", "Electroshock", "Extracellular Space", "Humans", "Models, Neurological", "Models, Theoretical", "Neuroglia", "Neurons", "Potassium", "Potassium Channels", "Seizures"], "AbstractText": [{"section": null, "text": "In this paper, we investigate the dynamics of a neuron-glia cell system and the underlying mechanism for the occurrence of seizures. For our mathematical and numerical investigation of the cell model we will use bifurcation analysis and some computational methods. It turns out that an increase of the potassium concentration in the reservoir is one trigger for seizures and is related to a torus bifurcation. In addition, we will study potassium dynamics of the model by considering a reduced version and we will show how both mechanisms are linked to each other. Moreover, the reduction of the potassium leak current will also induce seizures. Our study will show that an enhancement of the extracellular potassium concentration, which influences the Nernst potential of the potassium current, may lead to seizures. Furthermore, we will show that an external forcing term (e.g. electroshocks as unidirectional rectangular pulses also known as electroconvulsive therapy) will establish seizures similar to the unforced system with the increased extracellular potassium concentration. To this end, we describe the unidirectional rectangular pulses as an autonomous system of ordinary differential equations. These approaches will explain the appearance of seizures in the cellular model. Moreover, seizures, as they are measured by electroencephalography (EEG), spread on the macro-scale (cm). Therefore, we extend the cell model with a suitable homogenised monodomain model, propose a set of (numerical) experiment to complement the bifurcation analysis performed on the single-cell model. Based on these experiments, we introduce a bidomain model for a more realistic modelling of white and grey matter of the brain. Performing similar (numerical) experiment as for the monodomain model leads to a suitable comparison of both models. The individual cell model, with its seizures explained in terms of a torus bifurcation, extends directly to corresponding results in both the monodomain and bidomain models where the neural firing spreads almost synchronous through the domain as fast traveling waves, for physiologically relevant paramenters."}], "ArticleTitle": "Dynamics of a neuron-glia system: the occurrence of seizures and the influence of electroconvulsive stimuli : A mathematical and numerical study."}, "32895895": {"mesh": ["Action Potentials", "Animals", "Cats", "Computer Simulation", "Ion Channels", "Locomotion", "Models, Neurological", "Motor Neurons", "Recruitment, Neurophysiological", "Spinal Cord", "Synapses"], "AbstractText": [{"section": null, "text": "During fictive locomotion cat lumbar motoneurons exhibit changes in membrane proprieties including a decrease in voltage threshold (Vth), afterhyperpolarization (AHP) and input resistance (Rin) and an increase in non-linear membrane property. The impact of these changes on the motoneuron recruitment remains unknown. Using modeling approach we investigated the channel mechanism regulating the motoneuron recruitment. Three types of motoneuron pools including slow (S), fatigue-resistant (FR) and fast-fatigable (FF) motoneurons were constructed based on the membrane proprieties of cat lumbar motoneurons. The transient sodium (NaT), persistent sodium (NaP), delayed-rectifier potassium [K(DR)], Ca2+-dependent K+ [K(AHP)] and L-type calcium (CaL) channels were included in the models. Simulation results showed that (1) Strengthening synaptic inputs increased the number of recruitments in all three types of motoneurons following the size principle. (2) Increasing NaT or NaP or decreasing K(DR) or K(AHP) lowered rheobase of spike generation thus increased recruitment of motoneuron pools. (3) Decreasing Rin reduced recruitment in all three types of motoneurons. (4) The FF-type motoneuron pool, followed by FR- and S-type, were the most sensitive to increase of synaptic inputs, reduction of Rin, upregulation of NaT and NaP, and downregulation of K(DR) and K(AHP). (5) Increasing CaL enhanced overall discharge rate of motoneuron pools with little effect on the recruitment. Simulation results suggested that modulation of ionic channels altered the output of motoneuron pools with either modulating the number of recruited motoneurons or regulating the overall discharge rate of motoneuron pools. Multiple channels contributed to the recruitment of motoneurons with interaction of excitatory and inhibitory synaptic inputs during walking."}], "ArticleTitle": "A modeling study of spinal motoneuron recruitment regulated by ionic channels during fictive locomotion."}, "32944827": {"mesh": ["Aged", "Frontal Lobe", "Frontotemporal Dementia", "Humans", "Male", "Models, Neurological", "Saccades"], "AbstractText": [{"section": null, "text": "Prediction and time estimation are all but required for motor function in everyday life. In the context of eye movements, for instance, they allow predictive saccades and eye re-acceleration in anticipation of a target re-appearance. While the neural pathways involved are not fully understood, it is known that the frontal lobe plays an important role. As such, neurological disorders that affect it, such as frontotemporal (FTD) dementia, are likely to induce deficits in such movements. In this work, we study the performances of frontotemporal dementia patients in an oculomotor task designed to elicit predictive saccades at different rates, and compare them to young and older adults. Clear deficits in the production of predictive saccades were found in patients, in particular when the time between saccades was short (~500&#160;ms). Furthermore, one asymptomatic C9ORF72 mutation bearer showed patterns of oculomotor behavior similar to FTD patients. He exhibited FTD symptoms within 3&#160;years post-measure, suggesting that an impairment of oculomotor function could be an early clinical sign. Taken together, these results argue in favor of a role of the frontal lobe in predictive movements timing over short timescales, and suggest that predictive saccades in FTD patients warrant further investigation to fully assess their potential as a diagnostic aid."}], "ArticleTitle": "Frontotemporal dementia patients exhibit deficits in predictive saccades."}, "31953614": {"mesh": ["Action Potentials", "Algorithms", "Axons", "Computer Simulation", "Dendrites", "Electrodes", "Electrophysiological Phenomena", "Extracellular Space", "Humans", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Simulating extracellular recordings of neuronal populations is an important and challenging task both for understanding the nature and relationships between extracellular field potentials at different scales, and for the validation of methodological tools for signal analysis such as spike detection and sorting algorithms. Detailed neuronal multicompartmental models with active or passive compartments are commonly used in this objective. Although using such realistic NEURON models could lead to realistic extracellular potentials, it may require a high computational burden making the simulation of large populations difficult without a workstation. We propose in this paper a novel method to simulate extracellular potentials of firing neurons, taking into account the NEURON geometry and the relative positions of the electrodes. The simulator takes the form of a linear geometry based filter that models the shape of an action potential by taking into account its generation in the cell body / axon hillock and its propagation along the axon. The validity of the approach for different NEURON morphologies is assessed. We demonstrate that our method is able to reproduce realistic extracellular action potentials in a given range of axon/dendrites surface ratio, with a time-efficient computational burden."}], "ArticleTitle": "Fast simulation of extracellular action potential signatures based on a morphological filtering approximation."}, "31712945": {"mesh": ["Action Potentials", "Computer Simulation", "Humans", "Models, Neurological", "Muscle, Smooth", "Neuromuscular Junction", "Urinary Bladder"], "AbstractText": [{"section": null, "text": "The detrusor, a key component of the urinary bladder wall, is a densely innervated syncytial smooth muscle tissue. Random spontaneous release of neurotransmitter at neuromuscular junctions (NMJs) in the detrusor gives rise to spontaneous excitatory junction potentials (SEJPs). These sub-threshold passive signals not only offer insights into the syncytial nature of the tissue, their spatio-temporal integration is critical to the generation of spontaneous neurogenic action potentials which lead to focal contractions during the filling phase of the bladder. Given the structural complexity and the contractile nature of the tissue, electrophysiological investigations on spatio-temporal integration of SEJPs in the detrusor are technically challenging. Here we report a biophysically constrained computational model of a detrusor syncytium overlaid with spatially distributed innervation, using which we explored salient features of the integration of SEJPs in the tissue and the key factors that contribute to this integration. We validated our model against experimental data, ascertaining that observations were congruent with theoretical predictions. With the help of comparative studies, we propose that the amplitude of the spatio-temporally integrated SEJP is most sensitive to the inter-cellular coupling strength in the detrusor, while frequency of observed events depends more strongly on innervation density. An experimentally testable prediction arising from our study is that spontaneous release frequency of neurotransmitter may be implicated in the generation of detrusor overactivity. Set against histological observations, we also conjecture possible changes in the electrical activity of the detrusor during pathology involving patchy denervation. Our model thus provides a physiologically realistic, heuristic framework to investigate the spread and integration of passive potentials in an innervated syncytial tissue under normal conditions and in pathophysiology."}], "ArticleTitle": "Spontaneous synaptic drive in detrusor smooth muscle: computational investigation and implications for urinary bladder function."}, "31993923": {"mesh": ["Action Potentials", "Algorithms", "Animals", "Anterior Thalamic Nuclei", "Bayes Theorem", "Brain", "Electrophysiological Phenomena", "Entorhinal Cortex", "Head", "Information Theory", "Mice", "Neurons", "Rats", "Rodentia", "Space Perception"], "AbstractText": [{"section": null, "text": "Neuronal responses to complex stimuli and tasks can encompass a wide range of time scales. Understanding these responses requires measures that characterize how the information on these response patterns are represented across multiple temporal resolutions. In this paper we propose a metric - which we call multiscale relevance (MSR) - to capture the dynamical variability of the activity of single neurons across different time scales. The MSR is a non-parametric, fully featureless indicator in that it uses only the time stamps of the firing activity without resorting to any a priori covariate or invoking any specific structure in the tuning curve for neural activity. When applied to neural data from the mEC and from the ADn and PoS regions of freely-behaving rodents, we found that neurons having low MSR tend to have low mutual information and low firing sparsity across the correlates that are believed to be encoded by the region of the brain where the recordings were made. In addition, neurons with high MSR contain significant information on spatial navigation and allow to decode spatial position or head direction as efficiently as those neurons whose firing activity has high mutual information with the covariate to be decoded and significantly better than the set of neurons with high local variations in their interspike intervals. Given these results, we propose that the MSR can be used as a measure to rank and select neurons for their information content without the need to appeal to any a priori covariate."}], "ArticleTitle": "Multiscale relevance and informative encoding in neuronal spike trains."}, "29752691": {"mesh": ["Animals", "Computer Simulation", "Electric Stimulation", "Humans", "Models, Neurological", "Monte Carlo Method", "Neurons", "Receptors, AMPA", "Synapses", "Synaptic Transmission", "Temperature", "alpha-Amino-3-hydroxy-5-methyl-4-isoxazolepropionic Acid"], "AbstractText": [{"section": null, "text": "It was previously reported, that temperature may significantly influence neural dynamics on the different levels of brain function. Thus, in computational neuroscience, it would be useful to make models scalable for a wide range of various brain temperatures. However, lack of experimental data and an absence of temperature-dependent analytical models of synaptic conductance does not allow to include temperature effects at the multi-neuron modeling level. In this paper, we propose a first step to deal with this problem: A new analytical model of AMPA-type synaptic conductance, which is able to incorporate temperature effects in low-frequency stimulations. It was constructed based on Markov model description of AMPA receptor kinetics using the set of coupled ODEs. The closed-form solution for the set of differential equations was found using uncoupling assumption (introduced in the paper) with few simplifications motivated both from experimental data and from Monte Carlo simulation of synaptic transmission. The model may be used for computationally efficient and biologically accurate implementation of temperature effects on AMPA receptor conductance in large-scale neural network simulations. As a result, it may open a wide range of new possibilities for researching the influence of temperature on certain aspects of brain functioning."}], "ArticleTitle": "Analytical modelling of temperature effects on an AMPA-type synapse."}, "31912297": {"mesh": ["Adult", "Aging", "Algorithms", "Brain Chemistry", "Fetal Hypoxia", "Humans", "Hypoxia-Ischemia, Brain", "Infant, Newborn", "Models, Neurological", "Models, Theoretical", "Oxygen Consumption"], "AbstractText": [{"section": null, "text": "The brain is a metabolically demanding organ and its health directly depends on brain oxygen dynamics to prevent hypoxia and ischemia. Localized brain tissue oxygen is characterized by a baseline level combined with spontaneous oscillations. These oscillations are attributed to spontaneous changes of vascular tone at the level of arterioles and their frequencies depend on age. Specifically, lower frequencies are more typical for neonates than for adults. We have built a mathematical model which analyses the diffusion abilities of oxygen based on the frequency of source brain oxygen oscillations and neuronal demand. We have found that a lower frequency of spontaneous oscillations of localized brain tissue oxygen can support higher amplitudes of oxygen concentration at areas distant from a source relative to oscillations at higher frequencies. Since hypoxia and ischemia are very common events during early development and the neurovascular unit is underdeveloped in neonates, our results indicate that lower frequency oxygen oscillations can represent an effective passive method of neonatal brain protection against hypoxia. These results can have a potential impact on future studies aiming to find new treatment strategies for brain ischemia."}], "ArticleTitle": "Oscillations and concentration dynamics of brain tissue oxygen in neonates and adults."}, "30191352": {"mesh": ["Action Potentials", "Algorithms", "Animals", "Computer Simulation", "Interatrial Block", "Macaca mulatta", "Models, Neurological", "Nerve Net", "Neural Pathways", "Neurons", "Photic Stimulation", "ROC Curve", "Visual Cortex", "Visual Perception"], "AbstractText": [{"section": null, "text": "It is now common to record dozens to hundreds or more neurons simultaneously, and to ask how the network activity changes across experimental conditions. A natural framework for addressing questions of functional connectivity is to apply Gaussian graphical modeling to neural data, where each edge in the graph corresponds to a non-zero partial correlation between neurons. Because the number of possible edges is large, one strategy for estimating the graph has been to apply methods that aim to identify large sparse effects using an [Formula: see text] penalty. However, the partial correlations found in neural spike count data are neither large nor sparse, so techniques that perform well in sparse settings will typically perform poorly in the context of neural spike count data. Fortunately, the correlated firing for any pair of cortical neurons depends strongly on both their distance apart and the features for which they are tuned. We introduce a method that takes advantage of these known, strong effects by allowing the penalty to depend on them: thus, for example, the connection between pairs of neurons that are close together will be penalized less than pairs that are far apart. We show through simulations that this physiologically-motivated procedure performs substantially better than off-the-shelf generic tools, and we illustrate by applying the methodology to populations of neurons recorded with multielectrode arrays implanted in macaque visual cortex areas V1 and V4."}], "ArticleTitle": "Adjusted regularization of cortical covariance."}, "33009635": {"mesh": ["Animals", "Biomechanical Phenomena", "Brain", "Humans", "Learning", "Locomotion", "Models, Neurological", "Muscle, Skeletal", "Reflex", "Spinal Cord", "Torque"], "AbstractText": [{"section": null, "text": "The spinal cord is essential to the control of locomotion in legged animals and humans. However, the actual circuitry of the spinal controller remains only vaguely understood. Here we approach this problem from the viewpoint of learning. More precisely, we assume the circuitry evolves through the transfer of control from the brain to the spinal cord, propose a specific learning mechanism for this transfer based on the error between the cord and brain contributions to muscle control, and study the resulting structure of the spinal controller in a simplified neuromuscular model of human locomotion. The model focuses on the leg rebound behavior in stance and represents the spinal circuitry with 150 muscle reflexes. We find that after learning a spinal controller has evolved that produces leg rebound motions in the absence of a central brain input with only three structural reflex groups. These groups contain individual reflexes well known from physiological experiments but thought to serve separate purposes in the control of human locomotion. Our results suggest a more holistic interpretation of the role of individual sensory projections in spinal networks than is common. In addition, we discuss potential neural correlates for the proposed learning mechanism that may be probed in experiments. Together with such experiments, neuromuscular models of spinal learning likely will become effective tools for uncovering the structure and development of the spinal control circuitry."}], "ArticleTitle": "A model for the transfer of control from the brain to the spinal cord through synaptic learning."}, "32436129": {"mesh": ["Action Potentials", "Animals", "Axons", "Computer Simulation", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Focal axon swelling refers to localized swelling in axons that may occur because of trauma (e.g., traumatic brain injury) or neurodegenerative diseases (e.g., Alzheimer's disease). Since the swelling region can be many times larger than its original axon size, many researchers hypothesize that the swelling can alter the action potential (AP) signal. This article discusses the results of a series of newly developed computational studies to elucidate the possible intervention or blockage of AP signals due to swelling in the brain. We argue that the spherical geometry of the swelling site with its enlarged conducting interior causes the entering electric currents to spread evenly over the entire swelled membrane. As such, when the swelled surface becomes larger than the threshold size, the electric current will spread too thin to trigger the AP to spike. In this study, we have used a hybrid membrane model to simulate AP propagation across axons of different radii and swelling radii. We used an integrated model where a cylindrical symmetric 2D model is used to examine the electric current inside a spherical swelling site. In addition, two 1D models are used to capture the current flows along the upstream and downstream stretch before and after the swelling site. The parameters for this model are obtained from literature dedicated to modeling the experimental outcomes of mammal neurons. We observed two factors, which simultaneously affect AP transmission across a swelled axon: a) the axon radius and b) the ratio of the swelled and unswelled axon radii. In general, a thicker axon needs a smaller swelling size and axon ratio to block AP transmission. On the other hand, a thinner axon will reach the threshold at a larger swelling size and axon ratio. When only swelling size is considered, then thinner axons will block AP transmission at a smaller swelling radius. The AP transmission delay inside the swelled region determines whether the AP transmits forward or not. Notably, the blockage is worse if the AP fires at a high frequency. An increase in the charging and reset time due to swelling appears to be the main reason for the variation in axonal response."}], "ArticleTitle": "Effects of Focal Axonal Swelling Level on the Action Potential Signal Transmission."}, "29666978": {"mesh": ["Action Potentials", "Animals", "Feedback, Physiological", "Humans", "Models, Neurological", "Nerve Net", "Neural Inhibition", "Stochastic Processes", "Synapses", "Time Factors", "Time Perception"], "AbstractText": [{"section": null, "text": "Spike timing is believed to be a key factor in sensory information encoding and computations performed by the neurons and neuronal circuits. However, the considerable noise and variability, arising from the inherently stochastic mechanisms that exist in the neurons and the synapses, degrade spike timing precision. Computational modeling can help decipher the mechanisms utilized by the neuronal circuits in order to regulate timing precision. In this paper, we utilize semi-analytical techniques, which were adapted from previously developed methods for electronic circuits, for the stochastic characterization of neuronal circuits. These techniques, which are orders of magnitude faster than traditional Monte Carlo type simulations, can be used to directly compute the spike timing jitter variance, power spectral densities, correlation functions, and other stochastic characterizations of neuronal circuit operation. We consider three distinct neuronal circuit motifs: Feedback inhibition, synaptic integration, and synaptic coupling. First, we show that both the spike timing precision and the energy efficiency of a spiking neuron are improved with feedback inhibition. We unveil the underlying mechanism through which this is achieved. Then, we demonstrate that a neuron can improve on the timing precision of its synaptic inputs, coming from multiple sources, via synaptic integration: The phase of the output spikes of the integrator neuron has the same variance as that of the sample average of the phases of its inputs. Finally, we reveal that weak synaptic coupling among neurons, in a fully connected network, enables them to behave like a single neuron with a larger membrane area, resulting in an improvement in the timing precision through cooperation."}], "ArticleTitle": "Spike timing precision of neuronal circuits."}, "32458184": {"mesh": ["Adult", "Algorithms", "Arm", "Biomechanical Phenomena", "Humans", "Male", "Models, Neurological", "Movement", "Psychomotor Performance", "Young Adult"], "AbstractText": [{"section": null, "text": "There are observations indicating that the central nervous system (CNS) decomposes a movement into several successive sub-movements as an effective strategy to control the motor task. In this study, we propose an algorithm in which, Arm Reaching Movement (ARM) in 3D space is decomposed into several successive phases using zero joint angle jerk features of the arm kinematic data. The presented decomposition algorithm for 3D motions is, in fact, an improved and generalized version of the decomposition method proposed earlier by Emadi and Bahrami in 2012 for 2D movements. They assumed that the motion is coordinated by minimum jerk characteristics in joint angles space in each phase. However, at the first glance, it seems that in 3D ARM joint angles are not coordinated based on the minimum jerk features. Therefore, we defined a resultant variable in the joint space and showed that one can use its jerk properties together with those of the elbow joint in movement decomposition. We showed that phase borders determined with the proposed algorithm in 3D ARM, are defined with jerk characteristics of ARM's performance variable. We observed the same results in the Sit-to-Stand (STS) movement, too. Thus, based on our results, we suggested that any 3D motion can be decomposed into several phases, such that in each phase a set of principal patterns (PPs) extracted by Principal Component Analysis (PCA) method are linearly recruited to regenerate angle trajectories of each joint. Our results also suggest that the CNS, as the primary policy, may simplify the control of the ARMs by reducing the dimension of the control space. This dimension reduction might be accomplished by decomposing the movement into successive phases in which the movement satisfies the minimum joint angle jerk constraint. Then, in each phase, a set of PPs are recruited in the joint space to regenerate angle trajectory of each joint. Then, the dimension of the control space will be the number of the recruitment coefficients."}], "ArticleTitle": "3D human arm reaching movement planning with principal patterns in successive phases."}, "31797200": {"mesh": ["Algorithms", "Animals", "Astrocytes", "Calcium", "Calcium Signaling", "Cell Communication", "Computer Simulation", "Extracellular Space", "Glutamic Acid", "Humans", "Models, Neurological", "Myelin Sheath", "Presynaptic Terminals", "Ranvier's Nodes", "Synapses", "Synaptic Transmission"], "AbstractText": [{"section": null, "text": "Information transfer may not be limited only to synapses. Therefore, the processes and dynamics of biological neuron-astrocyte coupling and intercellular interaction within this domain are worth investigating. Existing models of tripartite synapse consider an astrocyte as a point process. Here, we extended the tripartite synapse model by considering the astrocytic processes (synaptic and perinodal) as compartments. The scattered extrinsic signals in the extracellular space and the presence of calcium stores in different astrocytic sites create local transient [Ca2+]. We investigated the Ca2+ dynamics and found that the increase in astrocytic intracellular [Ca2+] enhances the probability of neurotransmitter release. However, the period in which the extrasynaptic glutamate lingers in the extracellular space may cause excitotoxicity. We propose further biological investigation on intercellular communication, considering that unconventional sources (nonsynaptic) of glutamate may improve information processing in neuron-astrocyte networks."}], "ArticleTitle": "Spatiotemporal model of tripartite synapse with perinodal astrocytic process."}, "29589252": {"mesh": ["Animals", "Central Pattern Generators", "Humans", "Locomotion", "Models, Neurological", "Neural Networks, Computer", "Neural Pathways", "Psychomotor Performance"], "AbstractText": [{"section": null, "text": "Detailed neural network models of animal locomotion are important means to understand the underlying mechanisms that control the coordinated movement of individual limbs. Daun-Gruhn and T&#243;th, Journal of Computational Neuroscience 31(2), 43-60 (2011) constructed an inter-segmental network model of stick insect locomotion consisting of three interconnected central pattern generators (CPGs) that are associated with the protraction-retraction movements of the front, middle and hind leg. This model could reproduce the basic locomotion coordination patterns, such as tri- and tetrapod, and the transitions between them. However, the analysis of such a system is a formidable task because of its large number of variables and parameters. In this study, we employed phase reduction and averaging theory to this large network model in order to reduce it to a system of coupled phase oscillators. This enabled us to analyze the complex behavior of the system in a reduced parameter space. In this paper, we show that the reduced model reproduces the results of the original model. By analyzing the interaction of just two coupled phase oscillators, we found that the neighboring CPGs could operate within distinct regimes, depending on the phase shift between the sensory inputs from the extremities and the phases of the individual CPGs. We demonstrate that this dependence is essential to produce different coordination patterns and the transition between them. Additionally, applying averaging theory to the system of all three phase oscillators, we calculate the stable fixed points - they correspond to stable tripod or tetrapod coordination patterns and identify two ways of transition between them."}], "ArticleTitle": "The role of phase shifts of sensory inputs in walking revealed by means of phase reduction."}, "32632511": {"mesh": ["Animals", "Feedback", "Ferrets", "Geniculate Bodies", "Models, Neurological", "Neurons", "Optogenetics", "Photic Stimulation", "Visual Pathways"], "AbstractText": [{"section": null, "text": "In spite of their anatomical robustness, it has been difficult to establish the functional role of corticogeniculate circuits connecting primary visual cortex with the lateral geniculate nucleus of the thalamus (LGN) in the feedback direction. Growing evidence suggests that corticogeniculate feedback does not directly shape the spatial receptive field properties of LGN neurons, but rather regulates the timing and precision of LGN responses and the information coding capacity of LGN neurons. We propose that corticogeniculate feedback specifically stabilizes the response gain of LGN neurons, thereby increasing their information coding capacity. Inspired by early work by McClurkin et al. (1994), we manipulated the activity of corticogeniculate neurons to test this hypothesis. We used optogenetic methods to selectively and reversibly enhance the activity of corticogeniculate neurons in anesthetized ferrets while recording responses of LGN neurons to drifting gratings and white noise stimuli. We found that optogenetic activation of corticogeniculate feedback systematically reduced LGN gain variability and increased information coding capacity among LGN neurons. Optogenetic activation of corticogeniculate neurons generated similar increases in information encoded in LGN responses to drifting gratings and white noise stimuli. Together, these findings suggest that the influence of corticogeniculate feedback on LGN response precision and information coding capacity could be mediated through reductions in gain variability."}], "ArticleTitle": "Optogenetic activation of corticogeniculate feedback stabilizes response gain and increases information coding in LGN neurons."}, "28616843": {"mesh": ["Animals", "Axons", "Computer Simulation", "Electric Stimulation", "Humans", "Membrane Potentials", "Models, Neurological", "Neurons", "Nonlinear Dynamics"], "AbstractText": [{"section": null, "text": "We show that action potentials in the Hodgkin-Huxley neuron model result from a type I intermittency phenomenon that occurs in the proximity of a saddle-node bifurcation of limit cycles. For the Hodgkin-Huxley spatially extended model, describing propagation of action potential along axons, we show the existence of type I intermittency and a new type of chaotic intermittency, as well as space propagating regular and chaotic diffusion waves. Chaotic intermittency occurs in the transition from a turbulent regime to the resting regime of the transmembrane potential and is characterised by the existence of a sequence of action potential spikes occurring at irregular time intervals."}], "ArticleTitle": "Intermittency in the Hodgkin-Huxley model."}, "33063225": {"mesh": ["Animals", "Channelopathies", "Computer Simulation", "Delayed Rectifier Potassium Channels", "Membrane Potentials", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Channelopathies involving acquired or genetic modifications of the delayed rectifier K+ channel Kv1.1 include phenotypes characterized by enhanced neuronal excitability. Affected Kv1.1 channels exhibit combinations of altered expression, voltage sensitivity, and rates of activation and deactivation. Computational modeling and analysis can reveal the potential of particular channelopathies to alter neuronal excitability. A dynamical systems approach was taken to study the excitability and underlying dynamical structure of the Hodgkin-Huxley (HH) model of neural excitation as properties of the delayed rectifier K+ channel were altered. Bifurcation patterns of the HH model were determined as the amplitude of steady injection current was varied simultaneously with single parameters describing the delayed rectifier rates of activation and deactivation, maximal conductance, and voltage sensitivity. Relatively modest changes in the properties of the delayed rectifier K+ channel analogous to what is described for its channelopathies alter the bifurcation structure of the HH model and profoundly modify excitability of the HH model. Channelopathies associated with Kv1.1 can reduce the threshold for onset of neural activity. These studies also demonstrate how pathological delayed rectifier K+ channels could lead to the observation of the generalized Hopf bifurcation and, perhaps, other variants of the Hopf bifurcation. The observed bifurcation patterns collectively demonstrate that properties of the nominal delayed rectifier in the HH model appear optimized to permit activation of the HH model over the broadest possible range of input currents."}], "ArticleTitle": "Altered neuronal excitability in a Hodgkin-Huxley model incorporating channelopathies of the delayed rectifier potassium channel."}, "30788694": {"mesh": ["Algorithms", "Computer Simulation", "Electrophysiological Phenomena", "Entropy", "Humans", "Models, Neurological", "Nerve Net", "Neural Conduction", "Neural Networks, Computer", "Neurons"], "AbstractText": [{"section": null, "text": "Homogeneously structured, fluctuation-driven networks of spiking neurons can exhibit a wide variety of dynamical behaviors, ranging from homogeneity to synchrony. We extend our partitioned-ensemble average (PEA) formalism proposed in Zhang et al. (Journal of Computational Neuroscience, 37(1), 81-104, 2014a) to systematically coarse grain the heterogeneous dynamics of strongly coupled, conductance-based integrate-and-fire neuronal networks. The population dynamics models derived here successfully capture the so-called multiple-firing events (MFEs), which emerge naturally in fluctuation-driven networks of strongly coupled neurons. Although these MFEs likely play a crucial role in the generation of the neuronal avalanches observed in vitro and in vivo, the mechanisms underlying these MFEs cannot easily be understood using standard population dynamic models. Using our PEA formalism, we systematically generate a sequence of model reductions, going from Master equations, to Fokker-Planck equations, and finally, to an augmented system of ordinary differential equations. Furthermore, we show that these reductions can faithfully describe the heterogeneous dynamic regimes underlying the generation of MFEs in strongly coupled conductance-based integrate-and-fire neuronal networks."}], "ArticleTitle": "A coarse-graining framework for spiking neuronal networks: from strongly-coupled conductance-based integrate-and-fire neurons to augmented systems of ODEs."}, "30661144": {"mesh": ["Algorithms", "Computer Simulation", "Decision Making", "Electrophysiological Phenomena", "Humans", "Models, Neurological", "Neural Networks, Computer", "Neurons", "Nonlinear Dynamics", "Synapses"], "AbstractText": [{"section": null, "text": "Making a decision among numerous alternatives is a pervasive and central undertaking encountered by mammals in natural settings. While decision making for two-option tasks has been studied extensively both experimentally and theoretically, characterizing decision making in the face of a large set of alternatives remains challenging. We explore this issue by formulating a scalable mechanistic network model for decision making and analyzing the dynamics evoked given various potential network structures. In the case of a fully-connected network, we provide an analytical characterization of the model fixed points and their stability with respect to winner-take-all behavior for fair tasks. We compare several means of input integration, demonstrating a more gradual sigmoidal transfer function is likely evolutionarily advantageous relative to binary gain commonly utilized in engineered systems. We show via asymptotic analysis and numerical simulation that sigmoidal transfer functions with smaller steepness yield faster response times but depreciation in accuracy. However, in the presence of noise or degradation of connections, a sigmoidal transfer function garners significantly more robust and accurate decision-making dynamics. For fair tasks and sigmoidal gain, our model network also exhibits a stable parameter regime that produces high accuracy and persists across tasks with diverse numbers of alternatives and difficulties, satisfying physiological energetic constraints. In the case of more sparse and structured network topologies, including random, regular, and small-world connectivity, we show the high-accuracy parameter regime persists for biologically realistic connection densities. Our work shows how neural system architecture is potentially optimal in making economic, reliable, and advantageous decisions across tasks."}], "ArticleTitle": "Network structure and input integration in competing firing rate models for decision-making."}, "30206733": {"mesh": ["Action Potentials", "Acute Pain", "Animals", "Brain", "Brain-Computer Interfaces", "Evoked Potentials", "Male", "Models, Neurological", "Neurons", "Rats", "Support Vector Machine"], "AbstractText": [{"section": null, "text": "Brain-machine interfaces (BMIs) have been widely used to study basic and translational neuroscience questions. In real-time closed-loop neuroscience experiments, many practical issues arise, such as trial-by-trial variability, and spike sorting noise or multi-unit activity. In this paper, we propose a new framework for change-point detection based on ensembles of independent detectors in the context of BMI application for detecting acute pain signals. Motivated from ensemble learning, our proposed \"ensembles of change-point detectors\" (ECPDs) integrate multiple decisions from independent detectors, which may be derived based on data recorded from different trials, data recorded from different brain regions, data of different modalities, or models derived from different learning methods. By integrating multiple sources of information, the ECPDs aim to improve detection accuracy (in terms of true positive and true negative rates) and achieve an optimal trade-off of sensitivity and specificity. We validate our method using computer simulations and experimental recordings from freely behaving rats. Our results have shown superior and robust performance of ECPDS in detecting the onset of acute pain signals based on neuronal population spike activity (or combined with local field potentials) recorded from single or multiple brain regions."}], "ArticleTitle": "Ensembles of change-point detectors: implications for real-time BMI applications."}, "32388764": {"mesh": ["Algorithms", "Animals", "Arthropod Antennae", "Discrimination, Psychological", "GABA Antagonists", "Insecta", "Models, Neurological", "Neuronal Plasticity", "Neurons", "Odorants", "Presynaptic Terminals", "Smell", "Synaptic Transmission"], "AbstractText": [{"section": null, "text": "As the oldest, but least understood sensory system in evolution, the olfactory system represents one of the most challenging research targets in sensory neurobiology. Although a large number of computational models of the olfactory system have been proposed, they do not account for the diversity in physiology, connectivity of local neurons, and several recent discoveries in the insect antennal lobe, a major olfactory organ in insects. Recent studies revealed that the response of some projection neurons were reduced by application of a GABA antagonist, and that insects are sensitive to odor pulse frequency. To account for these observations, we propose a spiking neural circuit model of the insect antennal lobe. Based on recent anatomical and physiological studies, we included three sub-types of local neurons as well as synaptic short-term depression (STD) in the model and showed that the interaction between STD and local neurons resulted in frequency-sensitive responses. We further discovered that the unexpected response of the projection neurons to the GABA antagonist is the result of complex interactions between STD and presynaptic inhibition, which is required for enhancing sensitivity to odor stimuli. Finally, we found that odor discrimination is improved if the innervation of the local neurons in the glomeruli follows a specific pattern. Our findings suggest that STD, presynaptic inhibition and diverse physiology and connectivity of local neurons are not independent properties, but they interact to play key roles in the function of antennal lobes."}], "ArticleTitle": "Short term depression, presynaptic inhibition and local neuron diversity play key functional roles in the insect antennal lobe."}, "27629590": {"mesh": ["Brain", "Databases, Factual", "Humans", "Models, Neurological", "Neurons", "Neurosciences"], "AbstractText": [{"section": null, "text": "Neuron modeling may be said to have originated with the Hodgkin and Huxley action potential model in 1952 and Rall's models of integrative activity of dendrites in 1964. Over the ensuing decades, these approaches have led to a massive development of increasingly accurate and complex data-based models of neurons and neuronal circuits. ModelDB was founded in 1996 to support this new field and enhance the scientific credibility and utility of computational neuroscience models by providing a convenient venue for sharing them. It has grown to include over 1100 published models covering more than 130 research topics. It is actively curated and developed to help researchers discover and understand models of interest. ModelDB also provides mechanisms to assist running models both locally and remotely, and has a graphical tool that enables users to explore the anatomical and biophysical properties that are represented in a model. Each of its capabilities is undergoing continued refinement and improvement in response to user experience. Large research groups (Allen Brain Institute, EU Human Brain Project, etc.) are emerging that collect data across multiple scales and integrate that data into many complex models, presenting new challenges of scale. We end by predicting a future for neuroscience increasingly fueled by new technology and high performance computation, and increasingly in need of comprehensive user-friendly databases such as ModelDB to provide the means to integrate the data for deeper insights into brain function in health and disease."}], "ArticleTitle": "Twenty years of ModelDB and beyond: building essential modeling tools for the future of neuroscience."}, "28643213": {"mesh": ["Brain", "Humans", "Mathematics", "Models, Neurological"], "AbstractText": [{"section": null, "text": "Describing the human brain in mathematical terms is an important ambition of neuroscience research, yet the challenges remain considerable. It was Alan Turing, writing in 1950, who first sought to demonstrate how time-consuming such an undertaking would be. Through analogy to the computer program, Turing argued that arriving at a complete mathematical description of the mind would take well over a thousand years. In this opinion piece, we argue that - despite seventy years of progress in the field - his arguments remain both prescient and persuasive."}], "ArticleTitle": "The difficult legacy of Turing's wager."}, "30377880": {"mesh": ["Animals", "Computational Biology", "Computer Simulation", "Humans", "Models, Neurological", "Neurosciences", "Reproducibility of Results"], "AbstractText": [{"section": null, "text": "Replicability and reproducibility of computational models has been somewhat understudied by \"the replication movement.\" In this paper, we draw on methodological studies into the replicability of psychological experiments and on the mechanistic account of explanation to analyze the functions of model replications and model reproductions in computational neuroscience. We contend that model replicability, or independent researchers' ability to obtain the same output using original code and data, and model reproducibility, or independent researchers' ability to recreate a model without original code, serve different functions and fail for different reasons. This means that measures designed to improve model replicability may not enhance (and, in some cases, may actually damage) model reproducibility. We claim that although both are undesirable, low model reproducibility poses more of a threat to long-term scientific progress than low model replicability. In our opinion, low model reproducibility stems mostly from authors' omitting to provide crucial information in scientific papers and we stress that sharing all computer code and data is not a solution. Reports of computational studies should remain selective and include all and only relevant bits of code."}], "ArticleTitle": "Replicability or reproducibility? On the replication crisis in computational neuroscience and sharing only relevant detail."}, "29616382": {"mesh": ["Animals", "Brain", "Computer Simulation", "Humans", "Models, Neurological", "Neurons", "Time Factors"], "AbstractText": [{"section": null, "text": "Deep brain stimulation (DBS) is a common method of combating pathological conditions associated with Parkinson's disease, Tourette syndrome, essential tremor, and other disorders, but whose mechanisms are not fully understood. One hypothesis, supported experimentally, is that some symptoms of these disorders are associated with pathological synchronization of neurons in the basal ganglia and thalamus. For this reason, there has been interest in recent years in finding efficient ways to desynchronize neurons that are both fast-acting and low-power. Recent results on coordinated reset and periodically forced oscillators suggest that forming distinct clusters of neurons may prove to be more effective than achieving complete desynchronization, in particular by promoting plasticity effects that might persist after stimulation is turned off. Current proposed methods for achieving clustering frequently require either multiple input sources or precomputing the control signal. We propose here a control strategy for clustering, based on an analysis of the reduced phase model for a set of identical neurons, that allows for real-time, single-input control of a population of neurons with low-amplitude, low total energy signals. After demonstrating its effectiveness on phase models, we apply it to full state models to demonstrate its validity. We also discuss the effects of coupling on the efficacy of the strategy proposed and demonstrate that the clustering can still be accomplished in the presence of weak to moderate electrotonic coupling."}], "ArticleTitle": "Phase model-based neuron stabilization into arbitrary clusters."}, "31231777": {"mesh": ["Action Potentials", "Models, Neurological", "Neurons", "Patch-Clamp Techniques", "Signal-To-Noise Ratio"], "AbstractText": [{"section": null, "text": "At the level of individual neurons, various coding properties can be inferred from the input-output relationship of a cell. For small inputs, this relation is captured by the phase-response curve (PRC), which measures the effect of a small perturbation on the timing of the subsequent spike. Experimentally, however, an accurate experimental estimation of PRCs is challenging. Despite elaborate measurement efforts, experimental PRC estimates often cannot be related to those from modeling studies. In particular, experimental PRCs rarely resemble the characteristic theoretical PRC expected close to spike initiation, which is indicative of the underlying spike-onset bifurcation. Here, we show for conductance-based model neurons that the correspondence between theoretical and measured phase-response curve is lost when the stimuli used for the estimation are too large. In this case, the derived phase-response curve is distorted beyond recognition and takes on a generic shape that reflects the measurement protocol and masks the spike-onset bifurcation. We discuss how to identify appropriate stimulus strengths for perturbation and noise-stimulation methods, which permit to estimate PRCs that reliably reflect the spike-onset bifurcation - a task that is particularly difficult if a lower bound for the stimulus amplitude is dictated by prominent intrinsic neuronal noise."}], "ArticleTitle": "How to correctly quantify neuronal phase-response curves from noisy recordings."}, "31292816": {"mesh": ["Algorithms", "Animals", "Brain", "Computer Simulation", "Electroencephalography", "Humans", "Magnetoencephalography", "Models, Neurological", "Signal Processing, Computer-Assisted"], "AbstractText": [{"section": null, "text": "Electrophysiological signals (electroencephalography, EEG, and magnetoencephalography, MEG), as many natural processes, exhibit scale-invariance properties resulting in a power-law (1/f) spectrum. Interestingly, EEG and MEG differ in their slopes, which could be explained by several mechanisms, including non-resistive properties of tissues. Our goal in the present study is to estimate the impact of space/frequency structure of source signals as a putative mechanism to explain spectral scaling properties of neuroimaging signals. We performed simulations based on the summed contribution of cortical patches with different sizes (ranging from 0.4 to 104.2&#160;cm2). Small patches were attributed signals of high frequencies, whereas large patches were associated with signals of low frequencies, on a logarithmic scale. The tested parameters included i) the space/frequency structure (range of patch sizes and frequencies) and ii) the amplitude factor c parametrizing the spatial scale ratios. We found that the space/frequency structure may cause differences between EEG and MEG scale-free spectra that are compatible with real data findings reported in previous studies. We also found that below a certain spatial scale, there were no more differences between EEG and MEG, suggesting a limit for the resolution of both methods.Our work provides an explanation of experimental findings. This does not rule out other mechanisms for differences between EEG and MEG, but suggests an important role of spatio-temporal structure of neural dynamics. This can help the analysis and interpretation of power-law measures in EEG and MEG, and we believe our results can also impact computational modeling of brain dynamics, where different local connectivity structures could be used at different frequencies."}], "ArticleTitle": "Differences in MEG and EEG power-law scaling explained by a coupling between spatial coherence and frequency: a simulation study."}, "30218225": {"mesh": ["Action Potentials", "Animals", "Computer Simulation", "Linear Models", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Point process regression models, based on generalized linear model (GLM) technology, have been widely used for spike train analysis, but a recent paper by Gerhard et al. described a kind of instability, in which fitted models can generate simulated spike trains with explosive firing rates. We analyze the problem by extending the methods of Gerhard et al. First, we improve their instability diagnostic and extend it to a wider class of models. Next, we point out some common situations in which instability can be traced to model lack of fit. Finally, we investigate distinctions between models that use a single filter to represent the effects of all spikes prior to any particular time t, as in a 2008 paper by Pillow et al., and those that allow different filters for each spike prior to time t, as in a 2001 paper by Kass and Ventura. We re-analyze the data sets used by Gerhard et al., introduce an additional data set that exhibits bursting, and use a well-known model described by Izhikevich to simulate spike trains from various ground truth scenarios. We conclude that models with multiple filters tend to avoid instability, but there are unlikely to be universal rules. Instead, care in data fitting is required and models need to be assessed for each unique set of data."}], "ArticleTitle": "Stability of point process spiking neuron models."}, "31286380": {"mesh": ["Action Potentials", "Animals", "Computer Simulation", "Entorhinal Cortex", "Models, Neurological", "Neural Networks, Computer", "Orientation, Spatial", "Sensory Receptor Cells"], "AbstractText": [{"section": null, "text": "A neuron's firing correlates are defined as the features of the external world to which its activity is correlated. In many parts of the brain, neurons have quite simple such firing correlates. A striking example are grid cells in the rodent medial entorhinal cortex: their activity correlates with the animal's position in space, defining 'grid fields' arranged with a remarkable periodicity. Here, we show that the organization and evolution of grid fields relate very simply to physical space. To do so, we use an effective model and consider grid fields as point objects (particles) moving around in space under the influence of forces. We reproduce several observations on the geometry of grid patterns. This particle-like behavior is particularly salient in a recent experiment in which two separate grid patterns merge. We discuss pattern formation in the light of known results from physics of two-dimensional colloidal systems. Notably, we study the limitations of the widely used 'gridness score' and show how physics of 2d systems could be a source of inspiration, both for data analysis and computational modeling. Finally, we draw the relationship between our 'macroscopic' model for grid fields and existing 'microscopic' models of grid cell activity and discuss how a description at the level of grid fields allows to put constraints on the underlying grid cell network."}], "ArticleTitle": "Modeling grid fields instead of modeling grid cells : An effective model at the macroscopic level and its relationship with the underlying microscopic neural system."}, "29143250": {"mesh": ["Adult", "Brain", "Computer Simulation", "Connectome", "Female", "Healthy Volunteers", "Humans", "Male", "Models, Neurological", "Nerve Net", "Neural Pathways", "Young Adult"], "AbstractText": [{"section": null, "text": "Encoding brain regions and their connections as a network of nodes and edges captures many of the possible paths along which information can be transmitted as humans process and perform complex behaviors. Because cognitive processes involve large, distributed networks of brain areas, principled examinations of multi-node routes within larger connection patterns can offer fundamental insights into the complexities of brain function. Here, we investigate both densely connected groups of nodes that could perform local computations as well as larger patterns of interactions that would allow for parallel processing. Finding such structures necessitates that we move from considering exclusively pairwise interactions to capturing higher order relations, concepts naturally expressed in the language of algebraic topology. These tools can be used to study mesoscale network structures that arise from the arrangement of densely connected substructures called cliques in otherwise sparsely connected brain networks. We detect cliques (all-to-all connected sets of brain regions) in the average structural connectomes of 8 healthy adults scanned in triplicate and discover the presence of more large cliques than expected in null networks constructed via wiring minimization, providing architecture through which brain network can perform rapid, local processing. We then locate topological cavities of different dimensions, around which information may flow in either diverging or converging patterns. These cavities exist consistently across subjects, differ from those observed in null model networks, and - importantly - link regions of early and late evolutionary origin in long loops, underscoring their unique role in controlling brain function. These results offer a first demonstration that techniques from algebraic topology offer a novel perspective on structural connectomics, highlighting loop-like paths as crucial features in the human brain's structural architecture."}], "ArticleTitle": "Cliques and cavities in the human connectome."}, "30895410": {"mesh": ["Algorithms", "Biological Clocks", "Humans", "Linear Models", "Membrane Potentials", "Models, Neurological", "Neural Networks, Computer", "Neural Pathways", "Neurons", "Nonlinear Dynamics", "Synapses"], "AbstractText": [{"section": null, "text": "Several neuron types have been shown to exhibit (subthreshold) membrane potential resonance (MPR), defined as the occurrence of a peak in their voltage amplitude response to oscillatory input currents at a preferred (resonant) frequency. MPR has been investigated both experimentally and theoretically. However, whether MPR is simply an epiphenomenon or it plays a functional role for the generation of neuronal network oscillations and how the latent time scales present in individual, non-oscillatory cells affect the properties of the oscillatory networks in which they are embedded are open questions. We address these issues by investigating a minimal network model consisting of (i) a non-oscillatory linear resonator (band-pass filter) with 2D dynamics, (ii) a passive cell (low-pass filter) with 1D linear dynamics, and (iii) nonlinear graded synaptic connections (excitatory or inhibitory) with instantaneous dynamics. We demonstrate that (i) the network oscillations crucially depend on the presence of MPR in the resonator, (ii) they are amplified by the network connectivity, (iii) they develop relaxation oscillations for high enough levels of mutual inhibition/excitation, and (iv) the network frequency monotonically depends on the resonators resonant frequency. We explain these phenomena using a reduced adapted version of the classical phase-plane analysis that helps uncovering the type of effective network nonlinearities that contribute to the generation of network oscillations. We extend our results to networks having cells with 2D dynamics. Our results have direct implications for network models of firing rate type and other biological oscillatory networks (e.g, biochemical, genetic)."}], "ArticleTitle": "Membrane potential resonance in non-oscillatory neurons interacts with synaptic connectivity to produce network oscillations."}, "33161507": {"mesh": ["Animals", "Frontal Lobe", "Macaca mulatta", "Models, Neurological", "Saccades", "Superior Colliculi"], "AbstractText": [{"section": null, "text": "Saccades require a spatiotemporal transformation of activity between the intermediate layers of the superior colliculus (iSC) and downstream brainstem burst generator. The dynamic linear ensemble-coding model (Goossens and Van Opstal 2006) proposes that each iSC spike contributes a fixed mini-vector to saccade displacement. Although biologically-plausible, this model assumes cortical areas like the frontal eye fields (FEF) simply provide the saccadic goal to be executed by the iSC and brainstem burst generator. However, the FEF and iSC operate in unison during saccades, and a pathway from the FEF to the brainstem burst generator that bypasses the iSC exists. Here, we investigate the impact of large yet reversible inactivation of the FEF on iSC activity in the context of the model across four saccade tasks. We exploit the overlap of saccade vectors generated when the FEF is inactivated or not, comparing the number of iSC spikes for metrically-matched saccades. We found that the iSC emits fewer spikes for metrically-matched saccades during FEF inactivation. The decrease in spike count is task-dependent, with a greater decrease accompanying more cognitively-demanding saccades. Our results show that FEF integrity influences the readout of iSC activity in a task-dependent manner. We propose that the dynamic linear ensemble-coding model be modified so that FEF inactivation increases the gain of a readout parameter, effectively increasing the influence of a single iSC spike. We speculate that this modification could be instantiated by FEF and iSC pathways to the cerebellum that could modulate the excitability of the brainstem burst generator."}], "ArticleTitle": "Frontal eye field inactivation alters the readout of superior colliculus activity for saccade generation in a task-dependent manner."}, "31734803": {"mesh": ["Algorithms", "Decision Making", "Humans", "Models, Psychological"], "AbstractText": [{"section": null, "text": "Decision-making in dynamic environments typically requires adaptive evidence accumulation that weights new evidence more heavily than old observations. Recent experimental studies of dynamic decision tasks require subjects to make decisions for which the correct choice switches stochastically throughout a single trial. In such cases, an ideal observer's belief is described by an evolution equation that is doubly stochastic, reflecting stochasticity in the both observations and environmental changes. In these contexts, we show that the probability density of the belief can be represented using differential Chapman-Kolmogorov equations, allowing efficient computation of ensemble statistics. This allows us to reliably compare normative models to near-normative approximations using, as model performance metrics, decision response accuracy and Kullback-Leibler divergence of the belief distributions. Such belief distributions could be obtained empirically from subjects by asking them to report their decision confidence. We also study how response accuracy is affected by additional internal noise, showing optimality requires longer integration timescales as more noise is added. Lastly, we demonstrate that our method can be applied to tasks in which evidence arrives in a discrete, pulsatile fashion, rather than continuously."}], "ArticleTitle": "Analyzing dynamic decision-making models using Chapman-Kolmogorov equations."}, "29869761": {"mesh": ["Animals", "Macaca", "Models, Neurological", "Neural Networks, Computer", "Neurons", "Photic Stimulation", "Visual Cortex"], "AbstractText": [{"section": null, "text": "In this study, we evaluated the convolutional neural network (CNN) method for modeling V1 neurons of awake macaque monkeys in response to a large set of complex pattern stimuli. CNN models outperformed all the other baseline models, such as Gabor-based standard models for V1 cells and various variants of generalized linear models. We then systematically dissected different components of the CNN and found two key factors that made CNNs outperform other models: thresholding nonlinearity and convolution. In addition, we fitted our data using a pre-trained deep CNN via transfer learning. The deep CNN's higher layers, which encode more complex patterns, outperformed lower ones, and this result was consistent with our earlier work on the complexity of V1 neural code. Our study systematically evaluates the relative merits of different CNN components in the context of V1 neuron modeling."}], "ArticleTitle": "Convolutional neural network models of V1 responses to complex patterns."}, "29464489": {"mesh": ["Action Potentials", "Algorithms", "Animals", "Computer Simulation", "Humans", "Learning", "Models, Neurological", "Neural Networks, Computer", "Neural Pathways", "Neurons", "Synapses"], "AbstractText": [{"section": null, "text": "The connectivity of a neuronal network has a major effect on its functionality and role. It is generally believed that the complex network structure of the brain provides a physiological basis for information processing. Therefore, identifying the network's topology has received a lot of attentions in neuroscience and has been the center of many research initiatives such as Human Connectome Project. Nevertheless, direct and invasive approaches that slice and observe the neural tissue have proven to be time consuming, complex and costly. As a result, the inverse methods that utilize firing activity of neurons in order to identify the (functional) connections have gained momentum recently, especially in light of rapid advances in recording technologies; It will soon be possible to simultaneously monitor the activities of tens of thousands of neurons in real time. While there are a number of excellent approaches that aim to identify the functional connections from firing activities, the scalability of the proposed techniques plays a major challenge in applying them on large-scale datasets of recorded firing activities. In exceptional cases where scalability has not been an issue, the theoretical performance guarantees are usually limited to a specific family of neurons or the type of firing activities. In this paper, we formulate the neural network reconstruction as an instance of a graph learning problem, where we observe the behavior of nodes/neurons (i.e., firing activities) and aim to find the links/connections. We develop a scalable learning mechanism and derive the conditions under which the estimated graph for a network of Leaky Integrate and Fire (LIf) neurons matches the true underlying synaptic connections. We then validate the performance of the algorithm using artificially generated data (for benchmarking) and real data recorded from multiple hippocampal areas in rats."}], "ArticleTitle": "Learning neural connectivity from firing activity: efficient algorithms with provable guarantees on topology."}, "28353176": {"mesh": ["Animals", "Astrocytes", "Calcium", "Calcium Channels", "Calcium Signaling", "Models, Neurological"], "AbstractText": [{"section": null, "text": "We study evoked calcium dynamics in astrocytes, a major cell type in the mammalian brain. Experimental evidence has shown that such dynamics are highly variable between different trials, cells, and cell subcompartments. Here we present a qualitative analysis of a recent mathematical model of astrocyte calcium responses. We show how the major response types are generated in the model as a result of the underlying bifurcation structure. By varying key channel parameters, mimicking blockers used by experimentalists, we manipulate this underlying bifurcation structure and predict how the distributions of responses can change. We find that store-operated calcium channels, plasma membrane bound channels with little activity during calcium transients, have a surprisingly strong effect, underscoring the importance of considering these channels in both experiments and mathematical settings. Variation in the maximum flow in different calcium channels is also shown to determine the range of stable oscillations, as well as set the range of frequencies of the oscillations. Further, by conducting a randomized search through the parameter space and recording the resulting calcium responses, we create a database that can be used by experimentalists to help estimate the underlying channel distribution of their cells."}], "ArticleTitle": "Mathematical investigation of IP3-dependent calcium dynamics in astrocytes."}, "31165337": {"mesh": ["Action Potentials", "Animals", "Awareness", "Axons", "Brain Injuries, Traumatic", "Cognition Disorders", "Computer Simulation", "Consciousness", "Forecasting", "Gamma Rhythm", "Hippocampus", "Humans", "Memory, Short-Term", "Models, Neurological", "Neurodegenerative Diseases", "Rats", "Synaptic Transmission", "Vision, Ocular"], "AbstractText": [{"section": null, "text": "We introduce a computational model for the cellular level effects of firing rate filtering due to the major forms of neuronal injury, including demyelination and axonal swellings. Based upon experimental and computational observations, we posit simple phenomenological input/output rules describing spike train distortions and demonstrate that slow-gamma frequencies in the 38-41 Hz range emerge as the most robust to injury. Our signal-processing model allows us to derive firing rate filters at the cellular level for impaired neural activity with minimal assumptions. Specifically, we model eight experimentally observed spike train transformations by discrete-time filters, including those associated with increasing refractoriness and intermittent blockage. Continuous counterparts for the filters are also obtained by approximating neuronal firing rates from spike trains convolved with causal and Gaussian kernels. The proposed signal processing framework, which is robust to model parameter calibration, is an abstraction of the major cellular-level pathologies associated with neurodegenerative diseases and traumatic brain injuries that affect spike train propagation and impair neuronal network functionality. Our filters are well aligned with the spectrum of dynamic memory fields including working memory, visual consciousness, and other higher cognitive functions that operate in a frequency band that is - at a single cell level - optimally guarded against common types of pathological effects. In contrast, higher-frequency neural encoding, such as is observed with short-term memory, are susceptible to neurodegeneration and injury."}], "ArticleTitle": "Slow-gamma frequencies are optimally guarded against effects of neurodegenerative diseases and traumatic brain injuries."}, "28434057": {"mesh": ["Animals", "Grasshoppers", "Models, Neurological", "Movement", "Nervous System Diseases", "Neural Networks, Computer", "Reflex", "Robotics", "Walking"], "AbstractText": [{"section": null, "text": "In many animals intersegmental reflexes are important for postural and movement control but are still poorly undesrtood. Mathematical methods can be used to model the responses to stimulation, and thus go beyond a simple description of responses to specific inputs. Here we analyse an intersegmental reflex of the foot (tarsus) of the locust hind leg, which raises the tarsus when the tibia is flexed and depresses it when the tibia is extended. A novel method is described to measure and quantify the intersegmental responses of the tarsus to a stimulus to the femoro-tibial chordotonal organ. An Artificial Neural Network, the Time Delay Neural Network, was applied to understand the properties and dynamics of the reflex responses. The aim of this study was twofold: first to develop an accurate method to record and analyse the movement of an appendage and second, to apply methods to model the responses using Artificial Neural Networks. The results show that Artificial Neural Networks provide accurate predictions of tarsal movement when trained with an average reflex response to Gaussian White Noise stimulation compared to linear models. Furthermore, the Artificial Neural Network model can predict the individual responses of each animal and responses to others inputs such as a sinusoid. A detailed understanding of such a reflex response could be included in the design of orthoses or functional electrical stimulation treatments to improve walking in patients with neurological disorders as well as the bio/inspired design of robots."}], "ArticleTitle": "Predictive control of intersegmental tarsal movements in an insect."}, "31410632": {"mesh": ["Animals", "Anions", "Cations", "Dendritic Spines", "Excitatory Postsynaptic Potentials", "Ion Channel Gating", "Models, Neurological", "Receptors, AMPA", "Receptors, N-Methyl-D-Aspartate", "Static Electricity", "Synaptic Potentials"], "AbstractText": [{"section": null, "text": "The biophysical properties of dendritic spines play a critical role in neuronal integration but are still poorly understood, due to experimental difficulties in accessing them. Spine biophysics has been traditionally explored using theoretical models based on cable theory. However, cable theory generally assumes that concentration changes associated with ionic currents are negligible and, therefore, ignores electrodiffusion, i.e. the interaction between electric fields and ionic diffusion. This assumption, while true for large neuronal compartments, could be incorrect when applied to femto-liter size structures such as dendritic spines. To extend cable theory and explore electrodiffusion effects, we use here the Poisson (P) and Nernst-Planck (NP) equations, which relate electric field to charge and Fick's law of diffusion, to model ion concentration dynamics in spines receiving excitatory synaptic potentials (EPSPs). We use experimentally measured voltage transients from spines with nanoelectrodes to explore these dynamics with realistic parameters. We find that (i) passive diffusion and electrodiffusion jointly affect the dynamics of spine EPSPs; (ii) spine geometry plays a key role in shaping EPSPs; and, (iii) the spine-neck resistance dynamically decreases during EPSPs, leading to short-term synaptic facilitation. Our formulation, which complements and extends cable theory, can be easily adapted to model ionic biophysics in other nanoscale bio-compartments."}], "ArticleTitle": "Electrodiffusion models of synaptic potentials in dendritic spines."}, "30488148": {"mesh": ["Adult", "Brain", "Electroencephalography", "Female", "Humans", "Imagination", "Male", "Mental Fatigue", "Models, Neurological", "Movement", "Young Adult"], "AbstractText": [{"section": null, "text": "Even though it has long been felt that psychological state influences the performance of brain-computer interfaces (BCI), formal analysis to support this hypothesis has been scant. This study investigates the inter-relationship between motor imagery (MI) and mental fatigue using EEG: a. whether prolonged sequences of MI produce mental fatigue and b. whether mental fatigue affects MI EEG class separability. Eleven participants participated in the MI experiment, 5 of which quit in the middle because of experiencing high fatigue. The growth of fatigue was monitored using the Kernel Partial Least Square (KPLS) algorithm on the remaining 6 participants which shows that MI induces substantial mental fatigue. Statistical analysis of the effect of fatigue on motor imagery performance shows that high fatigue level significantly decreases MI EEG separability. Collectively, these results portray an MI-fatigue inter-connection, emphasizing the necessity of developing adaptive MI BCI by tracking mental fatigue."}], "ArticleTitle": "Motor imagery and mental fatigue: inter-relationship and EEG based estimation."}, "27778248": {"mesh": ["Animals", "Brachyura", "Fluorescent Dyes", "Models, Neurological", "Neurons", "Optics and Photonics", "Visual Perception"], "AbstractText": [{"section": null, "text": "The temporal relationship between the activities of neurons in biological neural systems is critically important for the correct delivery of the functionality of these systems. Fine measurement of temporal relationships of neural activities using micro-electrodes is possible but this approach is very limited due to spatial constraints in the context of physiologically valid settings of neural systems. Optical imaging with voltage-sensitive dyes or calcium dyes can provide data about the activity patterns of many neurons in physiologically valid settings, but the data is relatively noisy. Here we propose a numerical methodology for the analysis of optical neuro-imaging data that allows robust analysis of the dynamics of temporal relationships of neural activities. We provide a detailed description of the methodology and we also assess its robustness. The proposed methodology is applied to analyse the relationship between the activity patterns of PY neurons in the crab stomatogastric ganglion. We show for the first time in a physiologically valid setting that as expected on the basis of earlier results of single neuron recordings exposure to dopamine de-synchronises the activity of these neurons. We also discuss the wider implications and application of the proposed methodology."}], "ArticleTitle": "Analysis of the dynamics of temporal relationships of neural activities using optical imaging data."}, "27121476": {"mesh": ["Algorithms", "Animals", "Axons", "Computer Simulation", "Humans", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "This work concerns efficient and reliable numerical simulations of the dynamic behaviour of a moving-boundary model for tubulin-driven axonal growth. The model is nonlinear and consists of a coupled set of a partial differential equation (PDE) and two ordinary differential equations. The PDE is defined on a computational domain with a moving boundary, which is part of the solution. Numerical simulations based on standard explicit time-stepping methods are too time consuming due to the small time steps required for numerical stability. On the other hand standard implicit schemes are too complex due to the nonlinear equations that needs to be solved in each step. Instead, we propose to use the Peaceman-Rachford splitting scheme combined with temporal and spatial scalings of the model. Simulations based on this scheme have shown to be efficient, accurate, and reliable which makes it possible to evaluate the model, e.g. its dependency on biological and physical model parameters. These evaluations show among other things that the initial axon growth is very fast, that the active transport is the dominant reason over diffusion for the growth velocity, and that the polymerization rate in the growth cone does not affect the final axon length."}], "ArticleTitle": "Efficient simulations of tubulin-driven axonal growth."}, "32643083": {"mesh": ["Animals", "Association Learning", "Behavior, Animal", "Brain", "Conditioning, Operant", "Female", "Male", "Models, Neurological", "Neurons", "Rats", "Rats, Wistar"], "AbstractText": [{"section": null, "text": "We present a stochastic learning model that combines the essential elements of Hebbian and Rescorla-Wagner theories for operant conditioning. The model was used to predict the behavioral data of rats performing a vibrotactile yes/no detection task. Probabilistic nature of learning was implemented by trial-by-trial variability in the random distributions of associative strengths between the sensory and the response representations. By using measures derived from log-likelihoods (corrected Akaike and Bayesian information criteria), the proposed model and its subtypes were compared with each other, and with previous models in the literature, including reinforcement learning model with softmax rule and drift diffusion model. The main difference between these models was the level of stochasticity which was implemented as associative variation or response selection. The proposed model with subject-dependent variance coefficient (SVC) and with trial-dependent variance coefficient (TVC) resulted in better trial-by-trial fits to experimental data than the other tested models based on information criteria. Additionally, surrogate data were simulated with estimated parameters and the performance of the models were compared based on psychophysical measures (A': non-parametric sensitivity index, hits and false alarms on receiver operating characteristics). Especially the TVC model could produce psychophysical measures closer to those of the experimental data than the alternative models. The presented approach is novel for linking psychophysical response measures with learning in a yes/no detection task, and may be used in neural engineering applications."}], "ArticleTitle": "Psychophysical detection and learning in freely behaving rats: a probabilistic dynamical model for operant conditioning."}, "30298220": {"mesh": ["Action Potentials", "Brain", "Computer Simulation", "Humans", "Models, Neurological", "Models, Statistical", "Nerve Net", "Neurons", "Time Factors"], "AbstractText": [{"section": null, "text": "A critical component of any statistical modeling procedure is the ability to assess the goodness-of-fit between a model and observed data. For spike train models of individual neurons, many goodness-of-fit measures rely on the time-rescaling theorem and assess model quality using rescaled spike times. Recently, there has been increasing interest in statistical models that describe the simultaneous spiking activity of neuron populations, either in a single brain region or across brain regions. Classically, such models have used spike sorted data to describe relationships between the identified neurons, but more recently clusterless modeling methods have been used to describe population activity using a single model. Here we develop a generalization of the time-rescaling theorem that enables comprehensive goodness-of-fit analysis for either of these classes of population models. We use the theory of marked point processes to model population spiking activity, and show that under the correct model, each spike can be rescaled individually to generate a uniformly distributed set of events in time and the space of spike marks. After rescaling, multiple well-established goodness-of-fit procedures and statistical tests are available. We demonstrate the application of these methods both to simulated data and real population spiking in rat hippocampus. We have made the MATLAB and Python code used for the analyses in this paper publicly available through our Github repository at https://github.com/Eden-Kramer-Lab/popTRT ."}], "ArticleTitle": "A common goodness-of-fit framework for neural population models using marked point process time-rescaling."}, "30306384": {"mesh": ["Action Potentials", "Algorithms", "Brain", "Computer Simulation", "Humans", "Models, Neurological", "Models, Theoretical", "Nerve Net", "Neurons"], "AbstractText": [{"section": null, "text": "Despite the highly convoluted nature of the human brain, neural field models typically treat the cortex as a planar two-dimensional sheet of ne;urons. Here, we present an approach for solving neural field equations on surfaces more akin to the cortical geometries typically obtained from neuroimaging data. Our approach involves solving the integral form of the partial integro-differential equation directly using collocation techniques alongside efficient numerical procedures for determining geodesic distances between neural units. To illustrate our methods, we study localised activity patterns in a two-dimensional neural field equation posed on a periodic square domain, the curved surface of a torus, and the cortical surface of a rat brain, the latter of which is constructed using neuroimaging data. Our results are twofold: Firstly, we find that collocation techniques are able to replicate solutions obtained using more standard Fourier based methods on a flat, periodic domain, independent of the underlying mesh. This result is particularly significant given the highly irregular nature of the type of meshes derived from modern neuroimaging data. And secondly, by deploying efficient numerical schemes to compute geodesics, our approach is not only capable of modelling macroscopic pattern formation on realistic cortical geometries, but can also be extended to include cortical architectures of more physiological relevance. Importantly, such an approach provides a means by which to investigate the influence of cortical geometry upon the nucleation and propagation of spatially localised neural activity and beyond. It thus promises to provide model-based insights into disorders like epilepsy, or spreading depression, as well as healthy cognitive processes like working memory or attention."}], "ArticleTitle": "A numerical simulation of neural fields on curved geometries."}, "29222729": {"mesh": ["Animals", "Computer Simulation", "Humans", "Models, Neurological", "Neurons", "Signal Transduction", "Stochastic Processes", "Synaptic Transmission", "Time Factors"], "AbstractText": [{"section": null, "text": "We compare the information transmission of a time-dependent signal by two types of uncoupled neuron populations that differ in their sources of variability: i) a homogeneous population whose units receive independent noise and ii) a deterministic heterogeneous population, where each unit exhibits a different baseline firing rate ('disorder'). Our criterion for making both sources of variability quantitatively comparable is that the interspike-interval distributions are identical for both systems. Numerical simulations using leaky integrate-and-fire neurons unveil that a non-zero amount of both noise or disorder maximizes the encoding efficiency of the homogeneous and heterogeneous system, respectively, as a particular case of suprathreshold stochastic resonance. Our findings thus illustrate that heterogeneity can render similarly profitable effects for neuronal populations as dynamic noise. The optimal noise/disorder depends on the system size and the properties of the stimulus such as its intensity or cutoff frequency. We find that weak stimuli are better encoded by a noiseless heterogeneous population, whereas for strong stimuli a homogeneous population outperforms an equivalent heterogeneous system up to a moderate noise level. Furthermore, we derive analytical expressions of the coherence function for the cases of very strong noise and of vanishing intrinsic noise or heterogeneity, which predict the existence of an optimal noise intensity. Our results show that, depending on the type of signal, noise as well as heterogeneity can enhance the encoding performance of neuronal populations."}], "ArticleTitle": "Coding of time-dependent stimuli in homogeneous and heterogeneous neural populations."}, "30294750": {"mesh": ["Action Potentials", "Animals", "Humans", "Models, Neurological", "Neurons", "Nonlinear Dynamics", "Poisson Distribution", "Time Factors"], "AbstractText": [{"section": null, "text": "Prominent models of spike trains assume only one source of variability - stochastic (Poisson) spiking - when stimuli and behavior are fixed. However, spike trains may also reflect variability due to internal processes such as planning. For example, we can plan a movement at one point in time and execute it at some arbitrary later time. Neurons involved in planning may thus share an underlying time course that is not precisely locked to the actual movement. Here we combine the standard Linear-Nonlinear-Poisson (LNP) model with Dynamic Time Warping (DTW) to account for shared temporal variability. When applied to recordings from macaque premotor cortex, we find that time warping considerably improves predictions of neural activity. We suggest that such temporal variability is a widespread phenomenon in the brain which should be modeled."}], "ArticleTitle": "Linear-nonlinear-time-warp-poisson models of neural activity."}, "27221619": {"mesh": ["Action Potentials", "Animals", "Dendrites", "Hippocampus", "Models, Neurological", "Neural Conduction", "Pyramidal Cells"], "AbstractText": [{"section": null, "text": "The Pinsky-Rinzel model is a non-smooth 2-compartmental CA3 pyramidal cell model that has been used widely within the field of neuroscience. Here we propose a modified (smooth) system that captures the qualitative behaviour of the original model, while allowing the use of available, numerical continuation methods to perform full-system bifurcation and fast-slow analysis. We study the bifurcation structure of the full system as a function of the applied current and the maximal calcium conductance. We identify the bifurcations that shape the transitions between resting, bursting and spiking behaviours, and which lead to the disappearance of bursting when the calcium conductance is reduced. Insights gained from this analysis, are then used to firstly illustrate how the irregular spiking activity found between bursting and stable spiking states, can be influenced by phase differences in the calcium and dendritic voltage, which lead to corresponding changes in the calcium-sensitive potassium current. Furthermore, we use fast-slow analysis to investigate the mechanisms of bursting and show that bursting in the model is dependent on the intermediately slow variable, calcium, while the other slow variable, the activation gate of the afterhyperpolarisation current, does not contribute to setting the intraburst dynamics but participates in setting the interburst interval. Finally, we discuss how some of the described bifurcations affect spiking behaviour, during sharp-wave ripples, in a larger network of Pinsky-Rinzel cells."}], "ArticleTitle": "Bifurcation analysis of a two-compartment hippocampal pyramidal cell model."}, "27650865": {"mesh": ["Action Potentials", "Animals", "Humans", "Models, Neurological", "Models, Theoretical", "Nerve Net", "Neural Inhibition", "Neurons", "Nonlinear Dynamics"], "AbstractText": [{"section": null, "text": "Networks of inhibitory interneurons are found in many distinct classes of biological systems. Inhibitory interneurons govern the dynamics of principal cells and are likely to be critically involved in the coding of information. In this theoretical study, we describe the dynamics of a generic inhibitory network in terms of low-dimensional, simplified rate models. We study the relationship between the structure of external input applied to the network and the patterns of activity arising in response to that stimulation. We found that even a minimal inhibitory network can generate a great diversity of spatio-temporal patterning including complex bursting regimes with non-trivial ratios of burst firing. Despite the complexity of these dynamics, the network's response patterns can be predicted from the rankings of the magnitudes of external inputs to the inhibitory neurons. This type of invariant dynamics is robust to noise and stable in densely connected networks with strong inhibitory coupling. Our study predicts that the response dynamics generated by an inhibitory network may provide critical insights about the temporal structure of the sensory input it receives."}], "ArticleTitle": "Linking dynamics of the inhibitory network to the input structure."}, "29372434": {"mesh": ["Animals", "Biophysics", "Excitatory Postsynaptic Potentials", "Humans", "Ion Channels", "Models, Neurological", "Neurons", "Synapses"], "AbstractText": [{"section": null, "text": "In many theories of neural computation, linearly summed synaptic activation is a pervasive assumption for the computations performed by individual neurons. Indeed, for certain nominally optimal models, linear summation is required. However, the biophysical mechanisms needed to produce linear summation may add to the energy-cost of neural processing. Thus, the benefits provided by linear summation may be outweighed by the energy-costs. Using voltage-gated conductances in a relatively simple neuron model, this paper quantifies the cost of linearizing dendritically localized synaptic activation. Different combinations of voltage-gated conductances were examined, and many are found to produce linearization; here, four of these models are presented. Comparing the energy-costs to a purely passive model, reveals minimal or even no additional costs in some cases."}], "ArticleTitle": "Linearization of excitatory synaptic integration at no extra cost."}, "26922679": {"mesh": ["Action Potentials", "Humans", "Mental Recall", "Models, Neurological", "Nerve Net", "Neurons", "Nonlinear Dynamics"], "AbstractText": [{"section": null, "text": "Memory retrieval is of central importance to a wide variety of brain functions. To understand the dynamic nature of memory retrieval and its underlying neurophysiological mechanisms, we develop a biologically plausible spiking neural circuit model, and demonstrate that free memory retrieval of sequences of events naturally arises from the model under the condition of excitation-inhibition (E/I) balance. Using the mean-field model of the spiking circuit, we gain further theoretical insights into how such memory retrieval emerges. We show that the spiking neural circuit model quantitatively reproduces several salient features of free memory retrieval, including its semantic proximity effect and log-normal distributions of inter-retrieval intervals. In addition, we demonstrate that our model can serve as a platform to examine memory retrieval deficits observed in neuropsychiatric diseases such as Parkinson's and Alzheimer's diseases. Furthermore, our model allows us to make novel and experimentally testable predictions, such as the prediction that there are long-range correlations in the sequences of retrieved items."}], "ArticleTitle": "The dynamics of memory retrieval in hierarchical networks."}, "31720999": {"mesh": ["Decision Making", "Humans", "Models, Neurological", "Motor Cortex", "Neural Inhibition", "Neural Networks, Computer", "Neurons", "Pyramidal Cells", "Synaptic Transmission", "gamma-Aminobutyric Acid"], "AbstractText": [{"section": null, "text": "Interaction between sensory and motor cortices is crucial for perceptual decision-making, in which intracortical inhibition might have an important role. We simulated a neural network model consisting of a sensory network (NS) and a motor network (NM) to elucidate the significance of their interaction in perceptual decision-making in association with the level of GABA in extracellular space: extracellular GABA concentration. Extracellular GABA molecules acted on extrasynaptic receptors embedded in membranes of pyramidal cells and suppressed them. A reduction in extracellular GABA concentration either in NS or NM increased the rate of errors in perceptual decision-making, for which an increase in ongoing-spontaneous fluctuations in subthreshold neuronal activity in NM prior to sensory stimulation was responsible. Feedback (NM-to-NS) signaling enhanced selective neuronal responses in NS, which in turn increased stimulus-evoked neuronal activity in NM. We suggest that GABA in extracellular space contributes to reducing variability in motor cortex activity at a resting state and thereby the motor cortex can respond correctly to a subsequent sensory stimulus. Feedback signaling from the motor cortex improves the selective responsiveness of the sensory cortex, which ensures the fidelity of information transmission to the motor cortex, leading to reliable perceptual decision-making."}], "ArticleTitle": "Reducing variability in motor cortex activity at a resting state by extracellular GABA for reliable perceptual decision-making."}, "29882174": {"mesh": ["Action Potentials", "Biophysical Phenomena", "Biophysics", "Electric Stimulation", "Fourier Analysis", "Humans", "Models, Neurological", "Models, Theoretical", "Nerve Net", "Neural Networks, Computer", "Neurons", "Nonlinear Dynamics", "Synapses", "Time Factors"], "AbstractText": [{"section": null, "text": "Excessive synchronization in neural activity is a hallmark of Parkinson's disease (PD). A promising technique for treating PD is coordinated reset (CR) neuromodulation in which a neural population is desynchronized by the delivery of spatially-distributed current stimuli using multiple electrodes. In this study, we perform numerical optimization to find the energy-optimal current waveform for desynchronizing neuronal network with CR stimulation, by proposing and applying a new optimization method based on the direct search algorithm. In the proposed optimization method, the stimulating current is described as a Fourier series, and each Fourier coefficient as well as the stimulation period are directly optimized by evaluating the order parameter, which quantifies the synchrony level, from network simulation. This direct optimization scheme has an advantage that arbitrary changes in the dynamical properties of the network can be taken into account in the search process. By harnessing this advantage, we demonstrate the significant influence of externally applied oscillatory inputs and non-random network topology on the efficacy of CR modulation. Our results suggest that the effectiveness of brain stimulation for desynchronization may depend on various factors modulating the dynamics of the target network. We also discuss the possible relevance of the results to the efficacy of the stimulation in PD treatment."}], "ArticleTitle": "Numerical optimization of coordinated reset stimulation for desynchronizing neuronal network dynamics."}, "31974719": {"mesh": ["Algorithms", "Cerebellum", "Cerebral Cortex", "Computer Simulation", "Electrophysiological Phenomena", "Humans", "Models, Neurological", "Nerve Fibers", "Neurons", "Normal Distribution", "Patch-Clamp Techniques", "Pyramidal Cells", "Reproducibility of Results", "Synapses"], "AbstractText": [{"section": null, "text": "We developed a general method to generate populations of artificial spike trains (ASTs) that match the statistics of recorded neurons. The method is based on computing a Gaussian local rate function of the recorded spike trains, which results in rate templates from which ASTs are drawn as gamma distributed processes with a refractory period. Multiple instances of spike trains can be sampled from the same rate templates. Importantly, we can manipulate rate-covariances between spike trains by performing simple algorithmic transformations on the rate templates, such as filtering or amplifying specific frequency bands, and adding behavior related rate modulations. The method was examined for accuracy and limitations using surrogate data such as sine wave rate templates, and was then verified for recorded spike trains from cerebellum and cerebral cortex. We found that ASTs generated with this method can closely follow the firing rate and local as well as global spike time variance and power spectrum. The method is primarily intended to generate well-controlled spike train populations as inputs for dynamic clamp studies or biophysically realistic multicompartmental models. Such inputs are essential to study detailed properties of synaptic integration with well-controlled input patterns that mimic the in vivo situation while allowing manipulation of input rate covariances at different time scales."}], "ArticleTitle": "A general method to generate artificial spike train populations matching recorded neurons."}, "31502234": {"mesh": ["Association Learning", "Bayes Theorem", "Cerebral Cortex", "Computer Simulation", "Humans", "Memory", "Models, Neurological", "Nerve Net", "Neuronal Plasticity", "Neurons"], "AbstractText": [{"section": null, "text": "We present an electrophysiological model of double bouquet cells and integrate them into an established cortical columnar microcircuit model that has previously been used as a spiking attractor model for memory. Learning in that model relies on a Hebbian-Bayesian learning rule to condition recurrent connectivity between pyramidal cells. We here demonstrate that the inclusion of a biophysically plausible double bouquet cell model can solve earlier concerns about learning rules that simultaneously learn excitation and inhibition and might thus violate Dale's principle. We show that learning ability and resulting effective connectivity between functional columns of previous network models is preserved when pyramidal synapses onto double bouquet cells are plastic under the same Hebbian-Bayesian learning rule. The proposed architecture draws on experimental evidence on double bouquet cells and effectively solves the problem of duplexed learning of inhibition and excitation by replacing recurrent inhibition between pyramidal cells in functional columns of different stimulus selectivity with a plastic disynaptic pathway. We thus show that the resulting change to the microcircuit architecture improves the model's biological plausibility without otherwise impacting the model's spiking activity, basic operation, and learning abilities."}], "ArticleTitle": "Introducing double bouquet cells into a modular cortical associative memory model."}, "28389716": {"mesh": ["Algorithms", "Electricity", "Electrochemistry", "Humans", "Models, Neurological", "Neurosciences"], "AbstractText": [{"section": null, "text": "Multiscale modeling by means of co-simulation is a powerful tool to address many vital questions in neuroscience. It can for example be applied in the study of the process of learning and memory formation in the brain. At the same time the co-simulation technique makes it possible to take advantage of interoperability between existing tools and multi-physics models as well as distributed computing. However, the theoretical basis for multiscale modeling is not sufficiently understood. There is, for example, a need of efficient and accurate numerical methods for time integration. When time constants of model components are different by several orders of magnitude, individual dynamics and mathematical definitions of each component all together impose stability, accuracy and efficiency challenges for the time integrator. Following our numerical investigations in Brocke et al. (Frontiers in Computational Neuroscience, 10, 97, 2016), we present a new multirate algorithm that allows us to handle each component of a large system with a step size appropriate to its time scale. We take care of error estimates in a recursive manner allowing individual components to follow their discretization time course while keeping numerical error within acceptable bounds. The method is developed with an ultimate goal of minimizing the communication between the components. Thus it is especially suitable for co-simulations. Our preliminary results support our confidence that the multirate approach can be used in the class of problems we are interested in. We show that the dynamics ofa communication signal as well as an appropriate choice of the discretization order between system components may have a significant impact on the accuracy of the coupled simulation. Although, the ideas presented in the paper have only been tested on a single model, it is likely that they can be applied to other problems without loss of generality. We believe that this work may significantly contribute to the establishment of a firm theoretical basis and to the development of an efficient computational framework for multiscale modeling and simulations."}], "ArticleTitle": "Multirate method for co-simulation of electrical-chemical systems in multiscale modeling."}, "30315514": {"mesh": ["Adolescent", "Adult", "Brain", "Brain Mapping", "Child", "Child, Preschool", "Electroencephalography", "Epilepsy", "Female", "Humans", "Male", "Middle Aged", "Models, Neurological", "Seizures", "Signal Processing, Computer-Assisted", "Young Adult"], "AbstractText": [{"section": null, "text": "Networks are naturally occurring phenomena that are studied across many disciplines. The topological features of a network can provide insight into the dynamics of a system as it evolves, and can be used to predict changes in state. The brain is a complex network whose temporal and spatial behavior can be measured using electroencephalography (EEG). This data can be reconstructed to form a family of graphs that represent the state of the brain over time, and the evolution of these graphs can be used to predict changes in brain states, such as the transition from preictal to ictal in patients with epilepsy. This research proposes objective indications of seizure onset observed from minimally invasive scalp EEG. The approach considers the brain as a complex nonlinear dynamical system whose state can be derived through time-delay embedding of the EEG data and characterized to determine change in brain dynamics related to the preictal state. This method targets phase-space graph spectra as biomarkers for seizure prediction, correlates historical degrees of change in spectra, and makes accurate prediction of seizure onset. A significant trend of normalized dissimilarity over time indicates a departure from the norm, and thus a change in state. Our methods show high sensitivity (90-100%) and specificity (90%) on 241 h of scalp EEG training data, and sensitivity and specificity of 70%-90% on test data. Moreover, the algorithm was capable of processing 12.7 min of data per second on an Intel Core i3 CPU in Matlab, showing that real-time analysis is viable."}], "ArticleTitle": "Predicting state transitions in brain dynamics through spectral difference of phase-space graphs."}, "27942935": {"mesh": ["Action Potentials", "Animals", "Computer Simulation", "Models, Neurological", "Nerve Net", "Neural Networks, Computer", "Neurons", "Rats"], "AbstractText": [{"section": null, "text": "Developing networks of neural systems can exhibit spontaneous, synchronous activities called neural bursts, which can be important in the organization of functional neural circuits. Before the network matures, the activity level of a burst can reverberate in repeated rise-and-falls in periods of hundreds of milliseconds following an initial wave-like propagation of spiking activity, while the burst itself lasts for seconds. To investigate the spatiotemporal structure of the reverberatory bursts, we culture dissociated, rat cortical neurons on a high-density multi-electrode array to record the dynamics of neural activity over the growth and maturation of the network. We find the synchrony of the spiking significantly reduced following the initial wave and the activities become broadly distributed spatially. The synchrony recovers as the system reverberates until the end of the burst. Using a propagation model we infer the spreading speed of the spiking activity, which increases as the culture ages. We perform computer simulations of the system using a physiological model of spiking networks in two spatial dimensions and find the parameters that reproduce the observed resynchronization of spiking in the bursts. An analysis of the simulated dynamics suggests that the depletion of synaptic resources causes the resynchronization. The spatial propagation dynamics of the simulations match well with observations over the course of a burst and point to an interplay of the synaptic efficacy and the noisy neural self-activation in producing the morphology of the bursts."}], "ArticleTitle": "Propagation and synchronization of reverberatory bursts in developing cultured networks."}, "29327161": {"mesh": ["Action Potentials", "Animals", "Axons", "Computer Simulation", "Decapodiformes", "Entropy", "Ion Channels", "Models, Neurological", "Neurons", "Noise", "Stochastic Processes"], "AbstractText": [{"section": null, "text": "Action potentials are the information carriers of neural systems. The generation of action potentials involves the cooperative opening and closing of sodium and potassium channels. This process is metabolically expensive because the ions flowing through open channels need to be restored to maintain concentration gradients of these ions. Toxins like tetraethylammonium can block working ion channels, thus affecting the function and energy cost of neurons. In this paper, by computer simulation of the Hodgkin-Huxley neuron model, we studied the effects of channel blocking with toxins on the information transmission and energy efficiency in squid giant axons. We found that gradually blocking sodium channels will sequentially maximize the information transmission and energy efficiency of the axons, whereas moderate blocking of potassium channels will have little impact on the information transmission and will decrease the energy efficiency. Heavy blocking of potassium channels will cause self-sustained oscillation of membrane potentials. Simultaneously blocking sodium and potassium channels with the same ratio increases both information transmission and energy efficiency. Our results are in line with previous studies suggesting that information processing capacity and energy efficiency can be maximized by regulating the number of active ion channels, and this indicates a viable avenue for future experimentation."}], "ArticleTitle": "Effects of channel blocking on information transmission and energy efficiency in squid giant axons."}, "30547292": {"mesh": ["Action Potentials", "Animals", "Biophysics", "Dendrites", "Models, Neurological", "Neurons", "Receptors, AMPA", "Receptors, GABA", "Sodium Channels", "Synapses"], "AbstractText": [{"section": null, "text": "Many neurons possess dendrites enriched with sodium channels and are capable of generating action potentials. However, the role of dendritic sodium spikes remain unclear. Here, we study computational models of neurons to investigate the functional effects of dendritic spikes. In agreement with previous studies, we found that point neurons or neurons with passive dendrites increase their somatic firing rate in response to the correlation of synaptic bombardment for a wide range of input conditions, i.e. input firing rates, synaptic conductances, or refractory periods. However, neurons with active dendrites show the opposite behavior: for a wide range of conditions the firing rate decreases as a function of correlation. We found this property in three types of models of dendritic excitability: a Hodgkin-Huxley model of dendritic spikes, a model with integrate and fire dendrites, and a discrete-state dendritic model. We conclude that fast dendritic spikes confer much broader computational properties to neurons, sometimes opposite to that of point neurons."}], "ArticleTitle": "Dendritic sodium spikes endow neurons with inverse firing rate response to correlated synaptic activity."}, "30146661": {"mesh": ["Action Potentials", "Animals", "Computer Simulation", "Electric Stimulation", "Linear Models", "Models, Neurological", "Nerve Net", "Neurons", "Nonlinear Dynamics"], "AbstractText": [{"section": null, "text": "Capturing the response behavior of spiking neuron models with rate-based models facilitates the investigation of neuronal networks using powerful methods for rate-based network dynamics. To this end, we investigate the responses of two widely used neuron model types, the Izhikevich and augmented multi-adapative threshold (AMAT) models, to a range of spiking inputs ranging from step responses to natural spike data. We find (i) that linear-nonlinear firing rate models fitted to test data can be used to describe the firing-rate responses of AMAT and Izhikevich spiking neuron models in many cases; (ii) that firing-rate responses are generally too complex to be captured by first-order low-pass filters but require bandpass filters instead; (iii) that linear-nonlinear models capture the response of AMAT models better than of Izhikevich models; (iv) that the wide range of response types evoked by current-injection experiments collapses to few response types when neurons are driven by stationary or sinusoidally modulated Poisson input; and (v) that AMAT and Izhikevich models show different responses to spike input despite identical responses to current injections. Together, these findings suggest that rate-based models of network dynamics may capture a wider range of neuronal response properties by incorporating second-order bandpass filters fitted to responses of spiking model neurons. These models may contribute to bringing rate-based network modeling closer to the reality of biological neuronal networks."}], "ArticleTitle": "Firing-rate models for neurons with a broad repertoire of spiking behaviors."}, "28924628": {"mesh": ["Attention", "Feedback, Physiological", "Humans", "Models, Neurological", "Neurons", "Photic Stimulation", "Visual Cortex", "Visual Perception"], "AbstractText": [{"section": null, "text": "Visual processing of objects makes use of both feedforward and feedback streams of information. However, the nature of feedback signals is largely unknown, as is the identity of the neuronal populations in lower visual areas that receive them. Here, we develop a recurrent neural model to address these questions in the context of contour integration and figure-ground segregation. A key feature of our model is the use of grouping neurons whose activity represents tentative objects (\"proto-objects\") based on the integration of local feature information. Grouping neurons receive input from an organized set of local feature neurons, and project modulatory feedback to those same neurons. Additionally, inhibition at both the local feature level and the object representation level biases the interpretation of the visual scene in agreement with principles from Gestalt psychology. Our model explains several sets of neurophysiological results (Zhou et al. Journal of Neuroscience, 20(17), 6594-6611 2000; Qiu et al. Nature Neuroscience, 10(11), 1492-1499 2007; Chen et al. Neuron, 82(3), 682-694 2014), and makes testable predictions about the influence of neuronal feedback and attentional selection on neural responses across different visual areas. Our model also provides a framework for understanding how object-based attention is able to select both objects and the features associated with them."}], "ArticleTitle": "A recurrent neural model for proto-object based contour integration and figure-ground segregation."}, "27342462": {"mesh": ["Action Potentials", "Axons", "Kinetics", "Membrane Potentials", "Models, Neurological", "Temperature"], "AbstractText": [{"section": null, "text": "Recent studies indicate that a rapid increase in local temperature plays an important role in nerve stimulation by laser. To analyze the temperature effect, our study modified the classical HH axonal model by incorporating a membrane capacitance-temperature relationship. The modified model successfully simulated the generation and propagation of action potentials induced by a rapid increase in local temperature when the Curie temperature of membrane capacitance is below 40&#160;&#176;C, while the classical model failed to simulate the axonal excitation by temperature stimulation. The new model predicts that a rapid increase in local temperature produces a rapid increase in membrane capacitance, which causes an inward membrane current across the membrane capacitor strong enough to depolarize the membrane and generate an action potential. If the Curie temperature of membrane capacitance is 31&#160;&#176;C, a temperature increase of 6.6-11.2&#160;&#176;C within 0.1-2.6&#160;ms is required for axonal excitation and the required increase is smaller for a faster increase. The model also predicts that: (1) the temperature increase could be smaller if the global axon temperature is higher; (2) axons of small diameter require a smaller temperature increase than axons of large diameter. Our study indicates that the axonal membrane capacitance-temperature relationship plays a critical role in inducing the transient membrane depolarization by a rapidly increasing temperature, while the effects of temperature on ion channel kinetics cannot induce depolarization. The axonal model developed in this study will be very useful for analyzing the axonal response to local heating induced by pulsed infrared laser."}], "ArticleTitle": "Axonal model for temperature stimulation."}, "29797294": {"mesh": ["Adaptation, Physiological", "Brain Waves", "Computer Simulation", "Humans", "Models, Neurological", "Neural Inhibition", "Neurons", "Seizures"], "AbstractText": [{"section": null, "text": "In this paper we study the influence of inhibition on an activity-based neural field model consisting of an excitatory population with a linear adaptation term that directly regulates the activity of the excitatory population. Such a model has been used to replicate traveling wave data as observed in high density local field potential recordings (Gonz&#225;lez-Ram&#237;rez et al. PLoS Computational Biology, 11(2), e1004065, 2015). In this work, we show that by adding an inhibitory population to this model we can still replicate wave properties as observed in human clinical data preceding seizure termination, but the parameter range over which such waves exist becomes more restricted. This restriction depends on the strength of the inhibition and the timescale at which the inhibition acts. In particular, if inhibition acts on a slower timescale relative to excitation then it is possible to still replicate traveling wave patterns as observed in the clinical data even with a relatively strong effect of inhibition. However, if inhibition acts on the same timescale as the excitation, or faster, then traveling wave patterns with the desired characteristics cease to exist when the inhibition becomes sufficiently strong."}], "ArticleTitle": "The effect of inhibition on the existence of traveling wave solutions for a neural field model of human seizure termination."}, "27287487": {"mesh": ["Brain", "Brain Mapping", "Filtration", "Humans", "Models, Neurological", "Models, Theoretical", "Nerve Net", "Neurons"], "AbstractText": [{"section": null, "text": "The language of graph theory, or network science, has proven to be an exceptional tool for addressing myriad problems in neuroscience. Yet, the use of networks is predicated on a critical simplifying assumption: that the quintessential unit of interest in a brain is a dyad - two nodes (neurons or brain regions) connected by an edge. While rarely mentioned, this fundamental assumption inherently limits the types of neural structure and function that graphs can be used to model. Here, we describe a generalization of graphs that overcomes these limitations, thereby offering a broad range of new possibilities in terms of modeling and measuring neural phenomena. Specifically, we explore the use of simplicial complexes: a structure developed in the field of mathematics known as algebraic topology, of increasing applicability to real data due to a rapidly growing computational toolset. We review the underlying mathematical formalism as well as the budding literature applying simplicial complexes to neural data, from electrophysiological recordings in animal models to hemodynamic fluctuations in humans. Based on the exceptional flexibility of the tools and recent ground-breaking insights into neural function, we posit that this framework has the potential to eclipse graph theory in unraveling the fundamental mysteries of cognition."}], "ArticleTitle": "Two's company, three (or more) is a simplex : Algebraic-topological tools for understanding higher-order structure in neural data."}, "32761409": {"mesh": ["GABA Plasma Membrane Transport Proteins", "Humans", "Models, Neurological", "Neuroglia", "Neurons", "Schizophrenia", "Synaptic Transmission", "gamma-Aminobutyric Acid"], "AbstractText": [{"section": null, "text": "In schizophrenic patients, sensory tuning performance tends to be deteriorated (i.e., flattened sensory tuning), for which impaired intracortical tonic inhibition arising from a reduction in GABA concentration in extracellular space might be responsible. The &#948; subunit-containing GABAA receptor, located on extrasynaptic sites, is known to be involved in mediating tonic inhibitory currents in cortical pyramidal cells and is considered to be one of the beneficial therapeutic targets for the treatment of schizophrenia. The transporter GAT-1 in glial (astrocytic) membrane controls concentration of GABA molecules by removing them from extracellular space. We speculated that the upregulation of extrasynaptic receptors might compensate for the impaired tonic inhibition and thus improve their sensory tuning performance, in which the astrocytic GABA transporter might play an important role. To test our hypothesis, we simulated a schizophrenic neural network model with a GABAergic gliotransmission (i.e., GABA transport by transporters embedded in astrocytic membranes) mechanism that modulates local ambient (extracellular) GABA levels in a neuronal activity-dependent manner. Upregulating extrasynaptic GABA receptors compensated the impaired tonic inhibition and sharpened the sensory tuning, provided that ambient GABA molecules around stimulus-sensitive pyramidal cells were actively removed during sensory stimulation. We suggest that the upregulation of extrasynaptic GABA receptors can improve the performance of sensory tuning in schizophrenic patients, for which spatiotemporal regulation of ambient GABA concentration by gliotransmission may be crucial."}], "ArticleTitle": "Spatiotemporal regulation of GABA concentration in extracellular space by gliotransmission crucial for extrasynaptic receptor-mediated improvement of sensory tuning performance in schizophrenia."}, "27299885": {"mesh": ["Excitatory Postsynaptic Potentials", "Glutamic Acid", "Humans", "Models, Neurological", "N-Methylaspartate", "Receptors, AMPA", "Receptors, N-Methyl-D-Aspartate", "Synapses", "alpha-Amino-3-hydroxy-5-methyl-4-isoxazolepropionic Acid"], "AbstractText": [{"section": null, "text": "Coexistence of AMPA and NMDA receptors in glutamatergic synapses leads to a cooperative effect that can be very complex. This effect is dependent on many parameters including the relative and absolute number of the two types of receptors and biophysical parameters that can vary among synapses of the same cell. Herein we simulate the AMPA/NMDA cooperativity by using different number of the two types of receptors and considering the effect of the spine resistance on the EPSC production. Our results show that the relative number of NMDA with respect to AMPA produces a different degree of cooperation which depends also on the spine resistance."}], "ArticleTitle": "AMPA/NMDA cooperativity and integration during a single synaptic event."}, "30317462": {"mesh": ["Adult", "Brain", "Brain Mapping", "Electroencephalography", "Epilepsy", "Female", "Humans", "Language", "Male", "Middle Aged", "Models, Neurological", "Nerve Net", "Speech"], "AbstractText": [{"section": null, "text": "Language is mediated by pathways connecting distant brain regions that have diverse functional roles. For word production, the network includes a ventral pathway, connecting temporal and inferior frontal regions, and a dorsal pathway, connecting parietal and frontal regions. Despite the importance of word production for scientific and clinical purposes, the functional connectivity underlying this task has received relatively limited attention, and mostly from techniques limited in either spatial or temporal resolution. Here, we exploited data obtained from depth intra-cerebral electrodes stereotactically implanted in eight epileptic patients. The signal was recorded directly from various structures of the neocortex with high spatial and temporal resolution. The neurophysiological activity elicited by a picture naming task was analyzed in the time-frequency domain (10-150 Hz), and functional connectivity between brain areas among ten regions of interest was examined. Task related-activities detected within a network of the regions of interest were consistent with findings in the literature, showing task-evoked desynchronization in the beta band and synchronization in the gamma band. Surprisingly, long-range functional connectivity was not particularly stronger in the beta than in the high-gamma band. The latter revealed meaningful sub-networks involving, notably, the temporal pole and the inferior frontal gyrus (ventral pathway), and parietal regions and inferior frontal gyrus (dorsal pathway). These findings are consistent with the hypothesized network, but were not detected in every patient. Further research will have to explore their robustness with larger samples."}], "ArticleTitle": "An intracerebral exploration of functional connectivity during word production."}, "33175283": {"mesh": ["Animals", "Hippocampus", "Long-Term Potentiation", "Models, Neurological", "Neuronal Plasticity", "Pharmaceutical Preparations", "Signal Transduction"], "AbstractText": [{"section": null, "text": "Genetic disorders such as Rubinstein-Taybi syndrome (RTS) and Coffin-Lowry syndrome (CLS) cause lifelong cognitive disability, including deficits in learning and memory. Can pharmacological therapies be suggested that improve learning and memory in these disorders? To address this question, we simulated drug effects within a computational model describing induction of late long-term potentiation (L-LTP). Biochemical pathways impaired in these and other disorders converge on a common target, histone acetylation by acetyltransferases such as CREB binding protein (CBP), which facilitates gene induction necessary for L-LTP. We focused on four drug classes: tropomyosin receptor kinase B (TrkB) agonists, cAMP phosphodiesterase inhibitors, histone deacetylase inhibitors, and ampakines. Simulations suggested each drug type alone may rescue deficits in L-LTP. A potential disadvantage, however, was the necessity of simulating strong drug effects (high doses), which could produce adverse side effects. Thus, we investigated the effects of six drug pairs among the four classes described above. These combination treatments normalized impaired L-LTP with substantially smaller individual drug 'doses'. In addition three of these combinations, a TrkB agonist paired with an ampakine and a cAMP phosphodiesterase inhibitor paired with a TrkB agonist or an ampakine, exhibited strong synergism in L-LTP rescue. Therefore, we suggest these drug combinations are promising candidates for further empirical studies in animal models of genetic disorders that impair histone acetylation, L-LTP, and learning."}], "ArticleTitle": "Modeling suggests combined-drug treatments for disorders impairing synaptic plasticity via shared signaling pathways."}, "29027605": {"mesh": ["Feedback, Physiological", "Humans", "Memory, Short-Term", "Models, Neurological", "Nerve Net", "Neural Inhibition", "Neural Networks, Computer", "Neurons", "Somatosensory Cortex"], "AbstractText": [{"section": null, "text": "Persistent neuronal activity is usually studied in the context of short-term memory localized in central cortical areas. Recent studies show that early sensory areas also can have persistent representations of stimuli which emerge quickly (over tens of milliseconds) and decay slowly (over seconds). Traditional positive feedback models cannot explain sensory persistence for at least two reasons: (i) They show attractor dynamics, with transient perturbations resulting in a quasi-permanent change of system state, whereas sensory systems return to the original state after a transient. (ii) As we show, those positive feedback models which decay to baseline lose their persistence when their recurrent connections are subject to short-term depression, a common property of excitatory connections in early sensory areas. Dual time constant network behavior has also been implemented by nonlinear afferents producing a large transient input followed by much smaller steady state input. We show that such networks require unphysiologically large onset transients to produce the rise and decay observed in sensory areas. Our study explores how memory and persistence can be implemented in another model class, derivative feedback networks. We show that these networks can operate with two vastly different time courses, changing their state quickly when new information is coming in but retaining it for a long time, and that these capabilities are robust to short-term depression. Specifically, derivative feedback networks with short-term depression that acts differentially on positive and negative feedback projections are capable of dynamically changing their time constant, thus allowing fast onset and slow decay of responses without requiring unrealistically large input transients."}], "ArticleTitle": "Short-term depression and transient memory in sensory cortex."}, "29546529": {"mesh": ["Action Potentials", "Attention", "Computer Simulation", "Humans", "Memory, Short-Term", "Models, Neurological", "Neurons", "Nonlinear Dynamics", "Photic Stimulation", "Synapses", "Visual Perception"], "AbstractText": [{"section": null, "text": "Working memory (WM) is limited in its temporal length and capacity. Classic conceptions of WM capacity assume the system possesses a finite number of slots, but recent evidence suggests WM may be a continuous resource. Resource models typically assume there is no hard upper bound on the number of items that can be stored, but WM fidelity decreases with the number of items. We analyze a neural field model of multi-item WM that associates each item with the location of a bump in a finite spatial domain, considering items that span a one-dimensional continuous feature space. Our analysis relates the neural architecture of the network to accumulated errors and capacity limitations arising during the delay period of a multi-item WM task. Networks with stronger synapses support wider bumps that interact more, whereas networks with weaker synapses support narrower bumps that are more susceptible to noise perturbations. There is an optimal synaptic strength that both limits bump interaction events and the effects of noise perturbations. This optimum shifts to weaker synapses as the number of items stored in the network is increased. Our model not only provides a circuit-based explanation for WM capacity, but also speaks to how capacity relates to the arrangement of stored items in a feature space."}], "ArticleTitle": "Synaptic efficacy shapes resource limitations in working memory."}, "27416961": {"mesh": ["4-Aminopyridine", "Entorhinal Cortex", "Epilepsy", "Epilepsy, Temporal Lobe", "Hippocampus", "Humans", "In Vitro Techniques", "Models, Neurological"], "AbstractText": [{"section": null, "text": "In this paper, we propose a comprehensive computational model that is able to reproduce three epileptiform activities. The model targets a hippocampal formation that is known to be an important lesion in medial temporal lobe epilepsy. It consists of four sub-networks consisting of excitatory and inhibitory neurons and well-known signal pathways, with consideration of propagation delay. The three epileptiform activities involve fast and slow interictal discharge and ictal discharge, and those activities can be induced in vitro by application of 4-Aminopyridine in entorhinal cortex combined hippocampal slices. We model the three epileptiform activities upon previously reported biological mechanisms and verify the simulation results by comparing them with in vitro experimental data obtained using a microelectrode array. We use the results of Granger causality analysis of recorded data to set input gains of signal pathways in the model, so that the compatibility between the computational and experimental models can be improved. The proposed model can be expanded to evaluate the suppression effect of epileptiform activities due to new treatment methods."}], "ArticleTitle": "Computational modeling of epileptiform activities in medial temporal lobe epilepsy combined with in vitro experiments."}, "29210004": {"mesh": ["Animals", "Astrocytes", "Cell Communication", "Humans", "Models, Neurological", "Models, Theoretical", "Neurons"], "AbstractText": [{"section": null, "text": "A detailed biophysical model for a neuron/astrocyte network is developed in order to explore mechanisms responsible for the initiation and propagation of recurrent cortical spreading depolarizations. The model incorporates biophysical processes not considered in the earlier models. This includes a model for the Na+-glutamate transporter, which allows for a detailed description of reverse glutamate uptake. In particular, we consider the specific roles of elevated extracellular glutamate and K+ in the initiation, propagation and recurrence of spreading depolarizations."}], "ArticleTitle": "A mathematical model of recurrent spreading depolarizations."}, "30511274": {"mesh": ["Algorithms", "Brain", "Brain Mapping", "Data Analysis", "Electroencephalography", "Humans", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "High-resolution whole brain recordings have the potential to uncover unknown functionality but also present the challenge of how to find such associations between brain and behavior when presented with a large number of regions and spectral frequencies. In this paper, we propose an exploratory data analysis method that sorts through a massive quantity of multivariate neural recordings to quickly extract a subset of brain regions and frequencies that encode behavior. This approach combines existing tools and exploits low-rank approximation of matrices without a priori selection of regions and frequency bands for analysis. In detail, the spectral content of neural activity across all frequencies of each recording contact is computed and represented as a matrix. Then, the rank-1 approximation of the matrix is computed using singular value decomposition and the associated singular vectors are extracted. The temporal singular vector, which captures the salient features of the spectrogram, is then correlated to the trial-varying behavioral signal. The distribution of correlations for each brain region is efficiently computed and used to find a subset of regions and frequency bands of interest for further examination. As an illustration, we apply this approach to a data set of local field potentials collected using stereoelectroencephalography from a human subject performing a reaching task. Using the proposed procedure, we produced a comprehensive set of brain regions and frequencies related to our specific behavior. We demonstrate how this tool can produce preliminary results that capture neural patterns related to behavior and aid in formulating data-driven hypotheses, hence reducing the time it takes for any scientist to transition from the exploratory to the confirmatory phase."}], "ArticleTitle": "An exploratory data analysis method for identifying brain regions and frequencies of interest from large-scale neural recordings."}, "33123952": {"mesh": ["Humans", "Models, Neurological", "Signal Detection, Psychological"], "AbstractText": [{"section": null, "text": "The optimal template for signal detection in white additive noise is the signal itself: the ideal observer matches each stimulus against this template and selects the stimulus associated with largest match. In the noisy ideal observer, internal noise is added to the decision variable returned by the template. While the ideal observer represents an unrealistic approximation to the human visual process, the noisy ideal observer may be applicable under certain experimental conditions. For template values constrained to lie within a specified range, theory predicts that the template associated with a noisy ideal observer should be a clipped image of the signal, a result which we demonstrate analytically using variational calculus. It is currently unknown whether the human process conforms to theory. We report a targeted analysis of the theoretical prediction for an experimental protocol that maximizes template-matching on the part of human participants. We find indicative evidence to support the theoretical expectation when internal noise is compared across participants, but not within each participant. Our results indicate that implicit knowledge about internal variability in different individuals is reflected by their detection templates; no implicit knowledge is retained for internal-noise fluctuations experienced by a given participant during data collection. The results also indicate that template encoding is constrained by the dynamic range of weight specification, rather than the range of output values transduced by the template-matching process."}], "ArticleTitle": "Optimal templates for signal extraction by noisy ideal detectors and human observers."}, "29574632": {"mesh": ["Action Potentials", "Humans", "Models, Neurological", "Neurons", "Noise", "Stochastic Processes"], "AbstractText": [{"section": null, "text": "Long-range dependence (LRD) has been observed in a variety of phenomena in nature, and for several years also in the spiking activity of neurons. Often, this is interpreted as originating from a non-Markovian system. Here we show that a purely Markovian integrate-and-fire (IF) model, with a noisy slow adaptation term, can generate interspike intervals (ISIs) that appear as having LRD. However a proper analysis shows that this is not the case asymptotically. For comparison, we also consider a new model of individual IF neuron with fractional (non-Markovian) noise. The correlations of its spike trains are studied and proven to have LRD, unlike classical IF models. On the other hand, to correctly measure long-range dependence, it is usually necessary to know if the data are stationary. Thus, a methodology to evaluate stationarity of the ISIs is presented and applied to the various IF models. We explain that Markovian IF models may seem to have LRD because of non-stationarities."}], "ArticleTitle": "An integrate-and-fire model to generate spike trains with long-range dependence."}, "28895002": {"mesh": ["Action Potentials", "Animals", "Computer Simulation", "Humans", "Models, Neurological", "Nerve Net", "Neural Networks, Computer", "Neurons", "Normal Distribution", "Synapses", "Synaptic Transmission"], "AbstractText": [{"section": null, "text": "Correlated neural activities such as synchronizations can significantly alter the characteristics of spike transfer between neural layers. However, it is not clear how this synchronization-dependent spike transfer can be affected by the structure of convergent feedforward wiring. To address this question, we implemented computer simulations of model neural networks: a source and a target layer connected with different types of convergent wiring rules. In the Gaussian-Gaussian (GG) model, both the connection probability and the strength are given as Gaussian distribution as a function of spatial distance. In the Uniform-Constant (UC) and Uniform-Exponential (UE) models, the connection probability density is a uniform constant within a certain range, but the connection strength is set as a constant value or an exponentially decaying function, respectively. Then we examined how the spike transfer function is modulated under these conditions, while static or synchronized input patterns were introduced to simulate different levels of feedforward spike synchronization. We observed that the synchronization-dependent modulation of the transfer function appeared noticeably different for each convergence condition. The modulation of the spike transfer function was largest in the UC model, and smallest in the UE model. Our analysis showed that this difference was induced by the different spike weight distributions that was generated from convergent synapses in each model. Our results suggest that, the structure of the feedforward convergence is a crucial factor for correlation-dependent spike control, thus must be considered important to understand the mechanism of information transfer in the brain."}], "ArticleTitle": "Synaptic convergence regulates synchronization-dependent spike transfer in feedforward neural networks."}, "28791522": {"mesh": ["Action Potentials", "Algorithms", "Animals", "Electric Stimulation", "Entropy", "Extremities", "Female", "Grasshoppers", "Male", "Models, Neurological", "Motor Neurons", "Movement", "Normal Distribution", "Spectrum Analysis"], "AbstractText": [{"section": null, "text": "Directed information transfer measures are increasingly being employed in modeling neural system behavior due to their model-free approach, applicability to nonlinear and stochastic signals, and the potential to integrate repetitions of an experiment. Intracellular physiological recordings of graded synaptic potentials provide a number of additional challenges compared to spike signals due to non-stationary behaviour generated through extrinsic processes. We therefore propose a method to overcome this difficulty by using a preprocessing step based on Singular Spectrum Analysis (SSA) to remove nonlinear trends and discontinuities. We apply the method to intracellular recordings of synaptic responses of identified motor neurons evoked by stimulation of a proprioceptor that monitors limb position in leg of the desert locust. We then apply normalized delayed transfer entropy measures to neural responses evoked by displacements of the proprioceptor, the femoral chordotonal organ, that contains sensory neurones that monitor movements about the femoral-tibial joint. We then determine the consistency of responses within an individual recording of an identified motor neuron in a single animal, between repetitions of the same experiment in an identified motor neurons in the same animal and in repetitions of the same experiment from the same identified motor neuron in different animals. We found that delayed transfer entropy measures were consistent for a given identified neuron within and between animals and that they predict neural connectivity for the fast extensor tibiae motor neuron."}], "ArticleTitle": "Pre-processing and transfer entropy measures in motor neurons controlling limb movements."}, "29124505": {"mesh": ["Action Potentials", "Correlation of Data", "Humans", "Models, Neurological", "Nerve Net", "Neural Networks, Computer", "Neurons", "Stochastic Processes", "Synapses", "Time Factors"], "AbstractText": [{"section": null, "text": "The study of correlations in neural circuits of different size, from the small size of cortical microcolumns to the large-scale organization of distributed networks studied with functional imaging, is a topic of central importance to systems neuroscience. However, a theory that explains how the parameters of mesoscopic networks composed of a few tens of neurons affect the underlying correlation structure is still missing. Here we consider a theory that can be applied to networks of arbitrary size with multiple populations of homogeneous fully-connected neurons, and we focus its analysis to a case of two populations of small size. We combine the analysis of local bifurcations of the dynamics of these networks with the analytical calculation of their cross-correlations. We study the correlation structure in different regimes, showing that a variation of the external stimuli causes the network to switch from asynchronous states, characterized by weak correlation and low variability, to synchronous states characterized by strong correlations and wide temporal fluctuations. We show that asynchronous states are generated by strong stimuli, while synchronous states occur through critical slowing down when the stimulus moves the network close to a local bifurcation. In particular, strongly positive correlations occur at the saddle-node and Andronov-Hopf bifurcations of the network, while strongly negative correlations occur when the network undergoes a spontaneous symmetry-breaking at the branching-point bifurcations. These results show how the correlation structure of firing-rate network models is strongly modulated by the external stimuli, even keeping the anatomical connections fixed. These results also suggest an effective mechanism through which biological networks may dynamically modulate the encoding and integration of sensory information."}], "ArticleTitle": "Transitions between asynchronous and synchronous states: a theory of correlations in small neural circuits."}, "29063491": {"mesh": ["Basal Ganglia", "Biomechanical Phenomena", "Computer Simulation", "Humans", "Models, Neurological", "Motor Neurons", "Movement", "Neural Networks, Computer", "Neural Pathways", "Parkinson Disease", "Range of Motion, Articular"], "AbstractText": [{"section": null, "text": "The main hypothesis of this study, based on experimental data showing the relations between the BG activities and kinematic variables, is that BG are involved in computing inverse kinematics (IK) as a part of planning and decision-making. Indeed, it is assumed that based on the desired kinematic variables (such as velocity) of a limb in the workspace, angular kinematic variables in the joint configuration space are calculated. Therefore, in this paper, a system-level computational model of BG is proposed based on geometrical rules, which is able to compute IK. Next, the functionality of each part in the presented model is interpreted as a function of a nucleus or a pathway of BG. Moreover, to overcome existing redundancy in possible trajectories, an optimization problem minimizing energy consumption is defined and solved to select an optimal movement trajectory among an infinite number of possible ones. The validity of the model is checked by simulating it to control a three-segment manipulator with rotational joints in a plane. The performance of the model is studied for different types of movement including different reaching movements, a continuous circular movement and a sequence of tracking movements. Furthermore, to demonstrate the physiological similarity of the presented model to the BG structure, the neuronal activity of each part of the model considered as a BG nucleus is verified. Some changes in model parameters, inspired by the dopamine deficiency, also allow simulating some symptoms of Parkinson's disease such as bradykinesia and akinesia."}], "ArticleTitle": "A possible correlation between the basal ganglia motor function and the inverse kinematics calculation."}, "30382451": {"mesh": ["Action Potentials", "Brain Waves", "Computer Simulation", "Electrical Synapses", "Epilepsy", "Hippocampus", "Humans", "Magnetic Resonance Imaging", "Models, Theoretical", "Nerve Net", "Neurons", "Synapses"], "AbstractText": [{"section": null, "text": "The mechanisms underlying the broad variety of oscillatory rhythms measured in the hippocampus during the sleep-wake cycle are not yet fully understood. In this article, we propose a computational model of the hippocampal formation based on a realistic topology and synaptic connectivity, and we analyze the effect of different changes on the network, namely the variation of synaptic conductances, the variations of the CAN channel conductance and the variation of inputs. By using a detailed simulation of intracerebral recordings, we show that this is able to reproduce both the theta-nested gamma oscillations that are seen in awake brains and the sharp-wave ripple complexes measured during slow-wave sleep. The results of our simulations support the idea that the functional connectivity of the hippocampus, modulated by the sleep-wake variations in Acetylcholine concentration, is a key factor in controlling its rhythms."}], "ArticleTitle": "A detailed anatomical and mathematical model of the hippocampal formation for the generation of sharp-wave ripples and theta-nested gamma oscillations."}, "27696002": {"mesh": ["Animals", "Calcium", "Glutamic Acid", "Models, Neurological", "Mossy Fibers, Hippocampal", "Receptors, AMPA", "Receptors, N-Methyl-D-Aspartate", "Synapses", "Time Factors", "Zinc"], "AbstractText": [{"section": null, "text": "Zinc, a transition metal existing in very high concentrations in the hippocampal mossy fibers from CA3 area, is assumed to be co-released with glutamate and to have a neuromodulatory role at the corresponding synapses. The synaptic action of zinc is determined both by the spatiotemporal characteristics of the zinc release process and by the kinetics of zinc binding to sites located in the cleft area, as well as by their concentrations. This work addresses total, free and complexed zinc concentration changes, in an individual synaptic cleft, following single, short and long periods of evoked zinc release. The results estimate the magnitude and time course of the concentrations of zinc complexes, assuming that the dynamics of the release processes are similar to those of glutamate. It is also considered that, for the cleft zinc concentrations used in the model (&#8804; 1&#160;&#956;M), there is no postsynaptic zinc entry. For this reason, all released zinc ends up being reuptaken in a process that is several orders of magnitude slower than that of release and has thus a much smaller amplitude. The time derivative of the total zinc concentration in the cleft is represented by the difference between two alpha functions, corresponding to the released and uptaken components. These include specific parameters that were chosen assuming zinc and glutamate co-release, with similar time courses. The peak amplitudes of free zinc in the cleft were selected based on previously reported experimental cleft zinc concentration changes evoked by single and multiple stimulation protocols. The results suggest that following a low amount of zinc release, similar to that associated with one or a few stimuli, zinc clearance is mainly mediated by zinc binding to the high-affinity sites on the NMDA receptors and to the low-affinity sites on the highly abundant GLAST glutamate transporters. In the case of higher zinc release brought about by a larger group of stimuli, most zinc binding occurs essentially to the GLAST transporters, having the corresponding zinc complex a maximum concentration that is more than one order of magnitude larger than that for the high and low affinity NMDA sites. The other zinc complexes considered in the model, namely those formed with sites on the AMPA receptors, calcium and KATP channels and with ATP molecules, have much smaller contributions to the synaptic zinc clearance."}], "ArticleTitle": "Modelling zinc changes at the hippocampal mossy fiber synaptic cleft."}, "28025784": {"mesh": ["Action Potentials", "Algorithms", "Humans", "Models, Neurological", "Neurons", "Probability"], "AbstractText": [{"section": null, "text": "The statistical analysis of neuronal spike trains by models of point processes often relies on the assumption of constant process parameters. However, it is a well-known problem that the parameters of empirical spike trains can be highly variable, such as for example the firing rate. In order to test the null hypothesis of a constant rate and to estimate the change points, a Multiple Filter Test (MFT) and a corresponding algorithm (MFA) have been proposed that can be applied under the assumption of independent inter spike intervals (ISIs). As empirical spike trains often show weak dependencies in the correlation structure of ISIs, we extend the MFT here to point processes associated with short range dependencies. By specifically estimating serial dependencies in the test statistic, we show that the new MFT can be applied to a variety of empirical firing patterns, including positive and negative serial correlations as well as tonic and bursty firing. The new MFT is applied to a data set of empirical spike trains with serial correlations, and simulations show improved performance against methods that assume independence. In case of positive correlations, our new MFT is necessary to reduce the number of false positives, which can be highly enhanced when falsely assuming independence. For the frequent case of negative correlations, the new MFT shows an improved detection probability of change points and thus, also a higher potential of signal extraction from noisy spike trains."}], "ArticleTitle": "Multi-scale detection of rate changes in spike trains with weak dependencies."}, "28271301": {"mesh": ["Animals", "Brain", "Brain Mapping", "Cerebral Cortex", "Humans", "Models, Neurological", "Primates"], "AbstractText": [{"section": null, "text": "Mammalian cerebral cortices are characterized by elaborate convolutions. Radial convolutions exhibit homology across primate species and generally are easily identified in individuals of the same species. In contrast, circumferential convolutions vary across species as well as individuals of the same species. However, systematic study of circumferential convolution patterns is lacking. To address this issue, we utilized structural MRI (sMRI) and diffusion MRI (dMRI) data from primate brains. We quantified cortical thickness and circumferential convolutions on gyral banks in relation to axonal pathways and density along the gray matter/white matter boundaries. Based on these observations, we performed a series of computational simulations. Results demonstrated that the interplay of heterogeneous cortex growth and mechanical forces along axons plays a vital role in the regulation of circumferential convolutions. In contrast, gyral geometry controls the complexity of circumferential convolutions. These findings offer insight into the mystery of circumferential convolutions in primate brains."}], "ArticleTitle": "Mechanisms of circumferential gyral convolution in primate brains."}, "28509116": {"mesh": ["Action Potentials", "Animals", "Computer Simulation", "Humans", "Models, Neurological", "Nerve Net", "Neural Inhibition", "Neural Networks, Computer", "Neurons", "Nonlinear Dynamics", "gamma-Aminobutyric Acid"], "AbstractText": [{"section": null, "text": "The impact of tonic conductance upon population activity was investigated. An extra tonic transmembrane current through GABA-activated extrasynaptic GABA A -receptors was found to control stationary asynchronous firing both quantitatively and qualitatively. Quantitative regulation consisted in alterating a current level of stationary population activity while qualitative regulation manifested itself in appearance of resilient asynchronous spiking in case GABA reversal potential exceeded a certain threshold. The study was based on a modified rate model after Wilson and Cowan and backed up with a computer simulation of an explicit network model."}], "ArticleTitle": "Tonic regulation of stationary asynchronous firing of a neural network."}, "28484899": {"mesh": ["CA1 Region, Hippocampal", "Hippocampus", "Humans", "Memory", "Models, Neurological", "Neurons", "Reproducibility of Results"], "AbstractText": [{"section": null, "text": "Hippocampus stores spatial representations, or maps, which are recalled each time a subject is placed in the corresponding environment. Across different environments of similar geometry, these representations show strong orthogonality in CA3 of hippocampus, whereas in the CA1 subfield a considerable overlap between the maps can be seen. The lower orthogonality decreases reliability of various decoders developed in an attempt to identify which of the stored maps is active at the moment. Especially, the problem with decoding emerges with a need to analyze data at high temporal resolution. Here, we introduce a functional-connectivity-based decoder, which accounts for the pairwise correlations between the spiking activities of neurons in each map and does not require any positional information, i.e. any knowledge about place fields. We first show, on recordings of hippocampal activity in constant environmental conditions, that our decoder outperforms existing decoding methods in CA1. Our decoder is then applied to data from teleportation experiments, in which an instantaneous switch between the environment identity triggers a recall of the corresponding spatial representation . We test the sensitivity of our approach on the transition dynamics between the respective memory states (maps). We find that the rate of spontaneous state shifts (flickering) after a teleportation event is increased not only within the first few seconds as already reported, but this instability is sustained across much longer (> 1 min.) periods."}], "ArticleTitle": "Functional connectivity models for decoding of spatial representations from hippocampal CA1 recordings."}, "29923159": {"mesh": ["Action Potentials", "Algorithms", "Animals", "Cerebral Cortex", "Models, Neurological", "Nerve Net", "Neural Inhibition", "Neural Networks, Computer", "Neurons", "Noise", "Nonlinear Dynamics", "Periodicity", "Synapses", "Synaptic Transmission"], "AbstractText": [{"section": null, "text": "Spontaneous cortical population activity exhibits a multitude of oscillatory patterns, which often display synchrony during slow-wave sleep or under certain anesthetics and stay asynchronous during quiet wakefulness. The mechanisms behind these cortical states and transitions among them are not completely understood. Here we study spontaneous population activity patterns in random networks of spiking neurons of mixed types modeled by Izhikevich equations. Neurons are coupled by conductance-based synapses subject to synaptic noise. We localize the population activity patterns on the parameter diagram spanned by the relative inhibitory synaptic strength and the magnitude of synaptic noise. In absence of noise, networks display transient activity patterns, either oscillatory or at constant level. The effect of noise is to turn transient patterns into persistent ones: for weak noise, all activity patterns are asynchronous non-oscillatory independently of synaptic strengths; for stronger noise, patterns have oscillatory and synchrony characteristics that depend on the relative inhibitory synaptic strength. In the region of parameter space where inhibitory synaptic strength exceeds the excitatory synaptic strength and for moderate noise magnitudes networks feature intermittent switches between oscillatory and quiescent states with characteristics similar to those of synchronous and asynchronous cortical states, respectively. We explain these oscillatory and quiescent patterns by combining a phenomenological global description of the network state with local descriptions of individual neurons in their partial phase spaces. Our results point to a bridge from events at the molecular scale of synapses to the cellular scale of individual neurons to the collective scale of neuronal populations."}], "ArticleTitle": "Dynamics of spontaneous activity in random networks with multiple neuron subtypes and synaptic noise : Spontaneous activity in networks with synaptic noise."}, "30062615": {"mesh": ["Algorithms", "Brain", "Brain Waves", "Electroencephalography", "Humans", "Models, Neurological", "Principal Component Analysis", "Reproducibility of Results", "Signal Processing, Computer-Assisted", "Time Factors"], "AbstractText": [{"section": null, "text": "We propose a method - Frequency extracted hierarchical decomposition (FEHD) - for studying multivariate time series that identifies linear combinations of its components that possess a causally hierarchical structure - the method orders the components so that those at the \"top\" of the hierarchy drive those below. The method shares many of the features of the \"hierarchical decomposition\" method of Repucci et al. (Annals of Biomedical Engineering, 29, 1135-1149, 2001) but makes a crucial advance - the proposed method is capable of determining this causal hierarchy over arbitrarily specified frequency bands. Additionally, a novel minimization strategy is used to generate the decomposition resulting in an increase in stability, reliability, and an improvement in the sensitivity to model parameters. We demonstrate the utility of the method by applying it to both artificial time series constructed to have specific causal graphs, and to the EEG of healthy volunteers and patient subjects who are recovering from a severe brain injury."}], "ArticleTitle": "A method for decomposing multivariate time series into a causal hierarchy within specific frequency bands."}, "29139049": {"mesh": ["Action Potentials", "Animals", "Brain", "Computer Simulation", "Humans", "Models, Neurological", "Nerve Net", "Neural Inhibition", "Neural Networks, Computer", "Neurons", "Time Factors"], "AbstractText": [{"section": null, "text": "Directed information transmission is paramount for many social, physical, and biological systems. For neural systems, scientists have studied this problem under the paradigm of feedforward networks for decades. In most models of feedforward networks, activity is exclusively driven by excitatory neurons and the wiring patterns between them, while inhibitory neurons play only a stabilizing role for the network dynamics. Motivated by recent experimental discoveries of hippocampal circuitry, cortical circuitry, and the diversity of inhibitory neurons throughout the brain, here we illustrate that one can construct such networks even if the connectivity between the excitatory units in the system remains random. This is achieved by endowing inhibitory nodes with a more active role in the network. Our findings demonstrate that apparent feedforward activity can be caused by a much broader network-architectural basis than often assumed."}], "ArticleTitle": "Feedforward architectures driven by inhibitory interactions."}, "26975615": {"mesh": ["Action Potentials", "Animals", "Calcium Channels", "Gonadotropin-Releasing Hormone", "Humans", "Models, Neurological", "Neurons", "Potassium Channels", "Sodium Channel Blockers", "Tetrodotoxin"], "AbstractText": [{"section": null, "text": "Gonadotropin-releasing hormone (GnRH) neurons exhibit at least two intrinsic modes of action potential burst firing, referred to as parabolic and irregular bursting. Parabolic bursting is characterized by a slow wave in membrane potential that can underlie periodic clusters of action potentials with increased interspike interval at the beginning and at the end of each cluster. Irregular bursting is characterized by clusters of action potentials that are separated by varying durations of interburst intervals and a relatively stable baseline potential. Based on recent studies of isolated ionic currents, a stochastic Hodgkin-Huxley (HH)-like model for the GnRH neuron is developed to reproduce each mode of burst firing with an appropriate set of conductances. Model outcomes for bursting are in agreement with the experimental recordings in terms of interburst interval, interspike interval, active phase duration, and other quantitative properties specific to each mode of bursting. The model also shows similar outcomes in membrane potential to those seen experimentally when tetrodotoxin (TTX) is used to block action potentials during bursting, and when estradiol transitions cells exhibiting slow oscillations to irregular bursting mode in vitro. Based on the parameter values used to reproduce each mode of bursting, the model suggests that GnRH neurons can switch between the two through changes in the maximum conductance of certain ionic currents, notably the slow inward Ca(2+) current I s, and the Ca(2+) -activated K(+) current I KCa. Bifurcation analysis of the model shows that both modes of bursting are similar from a dynamical systems perspective despite differences in burst characteristics."}], "ArticleTitle": "A unified model for two modes of bursting in GnRH neurons."}, "27469424": {"mesh": ["Action Potentials", "Animals", "Models, Neurological", "Nerve Net", "Neurons", "Prefrontal Cortex", "Rats", "Sleep", "Time Factors", "Wakefulness"], "AbstractText": [{"section": null, "text": "We present two graphical model-based approaches to analyse the distribution of neural activities in the prefrontal cortex of behaving rats. The first method aims at identifying cell assemblies, groups of synchronously activating neurons possibly representing the units of neural coding and memory. A graphical (Ising) model distribution of snapshots of the neural activities, with an effective connectivity matrix reproducing the correlation statistics, is inferred from multi-electrode recordings, and then simulated in the presence of a virtual external drive, favoring high activity (multi-neuron) configurations. As the drive increases groups of neurons may activate together, and reveal the existence of cell assemblies. The identified groups are then showed to strongly coactivate in the neural spiking data and to be highly specific of the inferred connectivity network, which offers a sparse representation of the correlation pattern across neural cells. The second method relies on the inference of a Generalized Linear Model, in which spiking events are integrated over time by neurons through an effective connectivity matrix. The functional connectivity matrices inferred with the two approaches are compared. Sampling of the inferred GLM distribution allows us to study the spatio-temporal patterns of activation of neurons within the identified cell assemblies, particularly their activation order: the prevalence of one order with respect to the others is weak and reflects the neuron average firing rates and the strength of the largest effective connections. Other properties of the identified cell assemblies (spatial distribution of coactivation events and firing rates of coactivating neurons) are discussed."}], "ArticleTitle": "Neural assemblies revealed by inferred connectivity-based models of prefrontal cortex recordings."}, "32715350": {"mesh": ["Action Potentials", "Association", "Computer Simulation", "Humans", "Memory", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Networks of spiking neurons can have persistently firing stable bump attractors to represent continuous spaces (like temperature). This can be done with a topology with local excitatory synapses and local surround inhibitory synapses. Activating large ranges in the attractor can lead to multiple bumps, that show repeller and attractor dynamics; however, these bumps can be merged by overcoming the repeller dynamics. A simple associative memory can include these bump attractors, allowing the use of continuous variables in these memories, and these associations can be learned by Hebbian rules. These simulations are related to biological networks, showing that this is a step toward a more complete neural cognitive associative memory."}], "ArticleTitle": "Hot coffee: associative memory with bump attractor cell assemblies of spiking neurons."}, "29946922": {"mesh": ["Action Potentials", "Animals", "Arm", "Axons", "Biophysics", "Computer Simulation", "Electric Stimulation", "Humans", "Linear Models", "Median Nerve", "Models, Neurological", "Motor Neurons", "Neural Conduction", "Ranvier's Nodes", "Sensory Receptor Cells"], "AbstractText": [{"section": null, "text": "Surface electrical stimulation has the potential to be a powerful and non-invasive treatment for a variety of medical conditions but currently it is difficult to obtain consistent evoked responses. A viable clinical system must be able to adapt to variations in individuals to produce repeatable results. To more fully study the effect of these variations without performing exhaustive testing on human subjects, a system of computer models was created to predict motor and sensory axon activation in the median nerve due to surface electrical stimulation at the elbow. An anatomically-based finite element model of the arm was built to accurately predict voltages resulting from surface electrical stimulation. In addition, two axon models were developed based on previously published models to incorporate physiological differences between sensory and motor axons. This resulted in axon models that could reproduce experimental results for conduction velocity, strength-duration curves and activation threshold. Differences in experimentally obtained action potential shape between the motor and sensory axons were reflected in the models. The models predicted a lower threshold for sensory axons than motor axons of the same diameter, allowing a range of sensory axons to be activated before any motor axons. This system of models will be a useful tool for development of surface electrical stimulation as a method to target specific neural functions."}], "ArticleTitle": "A model of motor and sensory axon activation in the median nerve using surface electrical stimulation."}, "28102460": {"mesh": ["Brain", "Electroencephalography", "Epilepsy", "Hippocampus", "Humans", "Models, Neurological", "Seizures"], "AbstractText": [{"section": null, "text": "Epilepsy is one of the most common neurological disorders and is characterized by recurrent seizures. We use theoretical neuroscience tools to study brain dynamics during seizures. We derive and simulate a computational model of a network of hippocampal neuronal populations. Each population within the network is based on a model that has been shown to replicate the electrophysiological dynamics observed during seizures. The results provide insights into possible mechanisms for seizure spread. We observe that epileptiform activity remains localized to a pathological region when a global connectivity parameter is less than a critical value. After establishing the critical value for seizure spread, we explored how to correct the effect by altering particular synaptic gains. The spreading of seizures is quantified using numerical methods for seizure detection. The results from this study provide a new avenue of exploration for seizure control."}], "ArticleTitle": "Neural mass models as a tool to investigate neural dynamics during seizures."}, "26968615": {"mesh": ["Action Potentials", "Afferent Pathways", "Animals", "Computer Simulation", "Electric Stimulation", "Humans", "Models, Neurological", "Neurons", "Reflex", "Spinal Cord", "Urinary Bladder"], "AbstractText": [{"section": null, "text": "Electrical stimulation of the pudendal nerve (PN) is a promising approach to restore continence and micturition following bladder dysfunction resulting from neurological disease or injury. Although the pudendo-vesical reflex and its physiological properties are well established, there is limited understanding of the specific neural mechanisms that mediate this reflex. We sought to develop a computational model of the spinal neural network that governs the reflex bladder response to PN stimulation. We implemented and validated a neural network architecture based on previous neuroanatomical and electrophysiological studies. Using synaptically-connected integrate and fire model neurons, we created a network model with realistic spiking behavior. The model produced expected sacral parasympathetic nucleus (SPN) neuron firing rates from prescribed neural inputs and predicted bladder activation and inhibition with different frequencies of pudendal afferent stimulation. In addition, the model matched experimental results from previous studies of temporal patterns of pudendal afferent stimulation and selective pharmacological blockade of inhibitory neurons. The frequency- and pattern-dependent effects of pudendal afferent stimulation were determined by changes in firing rate of spinal interneurons, suggesting that neural network interactions at the lumbosacral level can mediate the bladder response to different frequencies or temporal patterns of pudendal afferent stimulation. Further, the anatomical structure of excitatory and inhibitory interneurons in the network model was necessary and sufficient to reproduce the critical features of the pudendo-vesical reflex, and this model may prove useful to guide development of novel, more effective electrical stimulation techniques for bladder control."}], "ArticleTitle": "Modeling the spinal pudendo-vesical reflex for bladder control by pudendal afferent stimulation."}, "29230640": {"mesh": ["Action Potentials", "Brain", "Brain Waves", "Computer Simulation", "Electroencephalography", "Humans", "Models, Neurological", "Neural Pathways", "Neurons", "Nonlinear Dynamics", "Sodium Channels", "Time Factors"], "AbstractText": [{"section": null, "text": "During slow-wave sleep, brain electrical activity is dominated by the slow (< 1&#160;Hz) electroencephalogram (EEG) oscillations characterized by the periodic transitions between active (or Up) and silent (or Down) states in the membrane voltage of the cortical and thalamic neurons. Sleep slow oscillation is believed to play critical role in consolidation of recent memories. Past computational studies, based on the Hodgkin-Huxley type neuronal models, revealed possible intracellular and network mechanisms of the neuronal activity during sleep, however, they failed to explore the large-scale cortical network dynamics depending on collective behavior in the large populations of neurons. In this new study, we developed a novel class of reduced discrete time spiking neuron models for large-scale network simulations of wake and sleep dynamics. In addition to the spiking mechanism, the new model implemented nonlinearities capturing effects of the leak current, the Ca2+ dependent K+ current and the persistent Na+ current that were found to be critical for transitions between Up and Down states of the slow oscillation. We applied the new model to study large-scale two-dimensional cortical network activity during slow-wave sleep. Our study explained traveling wave dynamics and characteristic synchronization properties of transitions between Up and Down states of the slow oscillation as observed in vivo in recordings from cats. We further predict a critical role of synaptic noise and slow adaptive currents for spike sequence replay as found during sleep related memory consolidation."}], "ArticleTitle": "New class of reduced computationally efficient neuronal models for large-scale simulations of brain dynamics."}, "29766393": {"mesh": ["Algorithms", "Animals", "Ankle Joint", "Biomechanical Phenomena", "Feedback, Sensory", "Ganglia, Spinal", "Knee Joint", "Male", "Models, Neurological", "Movement", "Proprioception", "Rabbits", "Range of Motion, Articular"], "AbstractText": [{"section": null, "text": "Proprioceptive afferent activities recorded by a multichannel microelectrode have been used to decode limb movements to provide sensory feedback signals for closed-loop control in a functional electrical stimulation (FES) system. However, analyzing the high dimensionality of neural activity is one of the major challenges in real-time applications. This paper proposes a linear feature projection method for the real-time decoding of ankle and knee joint angles. Single-unit activity was extracted as a feature vector from proprioceptive afferent signals that were recorded from the L7 dorsal root ganglion during passive movements of ankle and knee joints. The dimensionality of this feature vector was then reduced using a linear feature projection composed of projection pursuit and negentropy maximization (PP/NEM). Finally, a time-delayed Kalman filter was used to estimate the ankle and knee joint angles. The PP/NEM approach had a better decoding performance than did other feature projection methods, and all processes were completed within the real-time constraints. These results suggested that the proposed method could be a useful decoding method to provide real-time feedback signals in closed-loop FES systems."}], "ArticleTitle": "Linear feature projection-based real-time decoding of limb state from dorsal root ganglion recordings."}, "28573354": {"mesh": ["Feedback", "Humans", "Models, Neurological", "Motor Cortex", "Motor Neurons", "Movement", "Neurons"], "AbstractText": [{"section": null, "text": "Primary motor cortex (M1) neurons are tuned in response to several parameters related to motor control, and it was recently reported that M1 is important in feedback control. However, it remains unclear how M1 neurons encode information to control the musculoskeletal system. In this study, we examined the underlying computational mechanisms of M1 based on optimal feedback control (OFC) theory, which is a plausible hypothesis for neuromotor control. We modelled an isometric torque production task that required joint torque to be regulated and maintained at desired levels in a musculoskeletal system physically constrained by muscles, which act by pulling rather than pushing. Then, a feedback controller was computed using an optimisation approach under the constraint. In the presence of neuromotor noise, known as signal-dependent noise, the sensory feedback gain is tuned to an extrinsic motor output, such as the hand force, like a population response of M1 neurons. Moreover, a distribution of the preferred directions (PDs) of M1 neurons can be predicted via feedback gain. Therefore, we suggest that neural activity in M1 is optimised for the musculoskeletal system. Furthermore, if the feedback controller is represented in M1, OFC can describe multiple representations of M1, including not only the distribution of PDs but also the response of the neuronal population."}], "ArticleTitle": "Optimal feedback control to describe multiple representations of primary motor cortex neurons."}, "27066796": {"mesh": ["Animals", "Brain", "Brain Mapping", "Electroencephalography", "Humans", "Models, Neurological", "Nerve Net", "Sleep", "Wakefulness"], "AbstractText": [{"section": null, "text": "In mammals, sleep is categorized by two main sleep stages, rapid eye movement (REM) and non-REM (NREM) sleep that are known to fulfill different functional roles, the most notable being the consolidation of memory. While REM sleep is characterized by brain activity similar to wakefulness, the EEG activity changes drastically with the emergence of K-complexes, sleep spindles and slow oscillations during NREM sleep. These changes are regulated by circadian and ultradian rhythms, which emerge from an intricate interplay between multiple neuronal populations in the brainstem, forebrain and hypothalamus and the resulting varying levels of neuromodulators. Recently, there has been progress in the understanding of those rhythms both from a physiological as well as theoretical perspective. However, how these neuromodulators affect the generation of the different EEG patterns and their temporal dynamics is poorly understood. Here, we build upon previous work on a neural mass model of the sleeping cortex and investigate the effect of those neuromodulators on the dynamics of the cortex and the corresponding transition between wakefulness and the different sleep stages. We show that our simplified model is sufficient to generate the essential features of human EEG over a full day. This approach builds a bridge between sleep regulatory networks and EEG generating neural mass models and provides a valuable tool for model validation."}], "ArticleTitle": "Modeling the effect of sleep regulation on a neural mass model."}, "27909841": {"mesh": ["Biophysics", "Membrane Potentials", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "The generation of intrinsic subthreshold (membrane potential) oscillations (STOs) in neuronal models requires the interaction between two processes: a relatively fast positive feedback that favors changes in voltage and a slower negative feedback that opposes these changes. These are provided by the so-called resonant and amplifying gating variables associated to the participating ionic currents. We investigate both the biophysical and dynamic mechanisms of generation of STOs and how their attributes (frequency and amplitude) depend on the model parameters for biophysical (conductance-based) models having qualitatively different types of resonant currents (activating and inactivating) and an amplifying current. Combinations of the same types of ionic currents (same models) in different parameter regimes give rise to different types of nonlinearities in the voltage equation: quasi-linear, parabolic-like and cubic-like. On the other hand, combinations of different types of ionic currents (different models) may give rise to the same type of nonlinearities. We examine how the attributes of the resulting STOs depend on the combined effect of these resonant and amplifying ionic processes, operating at different effective time scales, and the various types of nonlinearities. We find that, while some STO properties and attribute dependencies on the model parameters are determined by the specific combinations of ionic currents (biophysical properties), and are different for models with different such combinations, others are determined by the type of nonlinearities and are common for models with different types of ionic currents. Our results highlight the richness of STO behavior in single cells as the result of the various ways in which resonant and amplifying currents interact and affect the generation and termination of STOs as control parameters change. We make predictions that can be tested experimentally and are expected to contribute to the understanding of how rhythmic activity in neuronal networks emerge from the interplay of the intrinsic properties of the participating neurons and the network connectivity."}], "ArticleTitle": "The shaping of intrinsic membrane potential oscillations: positive/negative feedback, ionic resonance/amplification, nonlinearities and time scales."}, "30443813": {"mesh": ["Action Potentials", "Animals", "Computer Simulation", "Electric Stimulation", "Humans", "Models, Neurological", "Nerve Fibers, Myelinated", "Neural Conduction", "Reproducibility of Results"], "AbstractText": [{"section": null, "text": "Electrical stimulation of nerve fibers is used as a therapeutic tool to treat neurophysiological disorders. Despite efforts to model the effects of stimulation, its underlying mechanisms remain unclear. Current mechanistic models quantify the effects that the electrical field produces near the fiber but do not capture interactions between action potentials (APs) initiated by stimulus and APs initiated by underlying physiological activity. In this study, we aim to quantify the effects of stimulation frequency and fiber diameter on AP interactions involving collisions and loss of excitability. We constructed a mechanistic model of a myelinated nerve fiber receiving two inputs: the underlying physiological activity at the terminal end of the fiber, and an external stimulus applied to the middle of the fiber. We define conduction reliability as the percentage of physiological APs that make it to the somatic end of the nerve fiber. At low input frequencies, conduction reliability is greater than 95% and decreases with increasing frequency due to an increase in AP interactions. Conduction reliability is less sensitive to fiber diameter and only decreases slightly with increasing fiber diameter. Finally, both the number and type of AP interactions significantly vary with both input frequencies and fiber diameter. Modeling the interactions between APs initiated by stimulus and APs initiated by underlying physiological activity in a nerve fiber opens opportunities towards understanding mechanisms of electrical stimulation therapies."}], "ArticleTitle": "Modeling the interactions between stimulation and physiologically induced APs in a mammalian nerve fiber: dependence on frequency and fiber diameter."}, "27844245": {"mesh": ["Calcium", "Hair", "Hair Cells, Auditory", "Membrane Potentials", "Models, Neurological", "Patch-Clamp Techniques", "Semicircular Canals", "Signal Transduction"], "AbstractText": [{"section": null, "text": "A computational model has been developed to simulate the electrical behavior of the type II hair cell dissected from the crista ampullaris of frog semicircular canals. In its basolateral membrane, it hosts a system of four voltage-dependent conductances (g A , g KV , g KCa , g Ca ). The conductance behavior was mathematically described using original patch-clamp experimental data. The transient K current, IA, was isolated as the difference between the currents obtained before and after removing IA inactivation. The remaining current, IKD, results from the summation of a voltage-dependent K current, IKV, a voltage-calcium-dependent K current, IKCa, and the calcium current, ICa. IKD was modeled as a single lumped current, since the physiological role of each component is actually not discernible. To gain a clear understanding of its prominent role in sustaining transmitter release at the cytoneural junction, ICa was modeled under different experimental conditions. The model includes the description of voltage- and time-dependent kinetics for each single current. After imposing any starting holding potential, the system sets the pertinent values of the variables and continually updates them in response to variations in membrane potential. The model reconstructs the individual I-V curves obtained in voltage-clamp experiments and simulations compare favorably with the experimental data. The model proves useful in describing the early steps of signal processing that results from the interaction of the apical receptor current with the basolateral voltage-dependent conductances. The program is thus helpful in understanding aspects of sensory transduction that are hard to analyze in the native hair cell of the crista ampullaris."}], "ArticleTitle": "A model of signal processing at the isolated hair cell of the frog semicircular canal."}, "27704337": {"mesh": ["Autonomic Nervous System Diseases", "Humans", "Models, Neurological", "Neurons", "Pressoreceptors", "Synaptic Transmission"], "AbstractText": [{"section": null, "text": "The baroreceptor neurons serve as the primary transducers of blood pressure for the autonomic nervous system and are thus critical in enabling the body to respond effectively to changes in blood pressure. These neurons can be separated into two types (A and C) based on the myelination of their axons and their distinct firing patterns elicited in response to specific pressure stimuli. This study has developed a comprehensive model of the afferent baroreceptor discharge built on physiological knowledge of arterial wall mechanics, firing rate responses to controlled pressure stimuli, and ion channel dynamics within the baroreceptor neurons. With this model, we were able to predict firing rates observed in previously published experiments in both A- and C-type neurons. These results were obtained by adjusting model parameters determining the maximal ion-channel conductances. The observed variation in the model parameters are hypothesized to correspond to physiological differences between A- and C-type neurons. In agreement with published experimental observations, our simulations suggest that a twofold lower potassium conductance in C-type neurons is responsible for the observed sustained basal firing, where as a tenfold higher mechanosensitive conductance is responsible for the greater firing rate observed in A-type neurons. A better understanding of the difference between the two neuron types can potentially be used to gain more insight about pathophysiology and treatment of diseases related to baroreflex function, e.g. in patients with autonomic failure, a syndrome that is difficult to diagnose in terms of its pathophysiology."}], "ArticleTitle": "Modeling the differentiation of A- and C-type baroreceptor firing patterns."}, "33420615": {"mesh": ["Brain", "Electroencephalography", "Epilepsy", "Humans", "Models, Neurological", "Seizures"], "AbstractText": [{"section": null, "text": "Clinical scalp electroencephalographic recordings from patients with epilepsy are distinguished by the presence of epileptic discharges i.e. spikes or sharp waves. These often occur randomly on a background of fluctuating potentials. The spike rate varies between different brain states (sleep and awake) and patients. Epileptogenic tissue and regions near these often show increased spike rates in comparison to other cortical regions. Several studies have shown a relation between spike rate and background activity although the underlying reason for this is still poorly understood. Both these processes, spike occurrence and background activity show evidence of being at least partly stochastic processes. In this study we show that epileptic discharges seen on scalp electroencephalographic recordings and background activity are driven at least partly by a common biological noise. Furthermore, our results indicate noise induced quiescence of spike generation which, in analogy with computational models of spiking, indicate spikes to be generated by transitions between semi-stable states of the brain, similar to the generation of epileptic seizure activity. The deepened physiological understanding of spike generation in epilepsy that this study provides could be useful in the electrophysiological assessment of different therapies for epilepsy including the effect of different drugs or electrical stimulation."}], "ArticleTitle": "Noise induced quiescence of epileptic spike generation in patients with epilepsy."}, "29152668": {"mesh": ["Animals", "Astrocytes", "Calcium Channels", "Computer Simulation", "Extracellular Space", "Humans", "Models, Biological", "Neurovascular Coupling", "Nitric Oxide", "Signal Transduction", "TRPV Cation Channels"], "AbstractText": [{"section": null, "text": "Neuronal activity evokes a localised change in cerebral blood flow in a response known as neurovascular coupling (NVC). Although NVC has been widely studied the exact mechanisms that mediate this response remain unclear; in particular the role of astrocytic calcium is controversial. Mathematical modelling can be a useful tool for investigating the contribution of various signalling pathways towards NVC and for analysing the underlying cellular mechanisms. The lumped parameter model of a neurovascular unit with both potassium and nitric oxide (NO) signalling pathways and comprised of neurons, astrocytes, and vascular cells has been extended to include the glutamate induced astrocytic calcium pathway with epoxyeicosatrienoic acid (EET) signalling and the stretch dependent TRPV4 calcium channel on the astrocytic endfoot. Results show that the potassium pathway governs the fast onset of vasodilation while the NO pathway has a delayed response, maintaining dilation longer following neuronal stimulation. Increases in astrocytic calcium concentration via the calcium signalling pathway and/or TRPV4 channel to levels consistent with experimental data are insufficient for inducing either vasodilation or constriction, in contrast to a number of experimental results. It is shown that the astrocyte must depolarise in order to produce a significant potassium flux through the astrocytic BK channel. However astrocytic calcium is shown to strengthen potassium induced NVC by opening the BK channel further, consequently allowing more potassium into the perivascular space. The overall effect is vasodilation with a higher maximal vessel radius."}], "ArticleTitle": "The role of astrocytic calcium and TRPV4 channels in neurovascular coupling."}, "29139050": {"mesh": ["Animals", "Cerebral Cortex", "Macaca mulatta", "Male", "Membrane Potentials", "Models, Neurological", "Nerve Net", "Neural Networks, Computer", "Neurons", "Nonlinear Dynamics", "Photic Stimulation", "Synapses", "Voltage-Sensitive Dye Imaging"], "AbstractText": [{"section": null, "text": "Voltage-sensitive dye imaging (VSDi) has revealed fundamental properties of neocortical processing at macroscopic scales. Since for each pixel VSDi signals report the average membrane potential over hundreds of neurons, it seems natural to use a mean-field formalism to model such signals. Here, we present a mean-field model of networks of Adaptive Exponential (AdEx) integrate-and-fire neurons, with conductance-based synaptic interactions. We study a network of regular-spiking (RS) excitatory neurons and fast-spiking (FS) inhibitory neurons. We use a Master Equation formalism, together with a semi-analytic approach to the transfer function of AdEx neurons to describe the average dynamics of the coupled populations. We compare the predictions of this mean-field model to simulated networks of RS-FS cells, first at the level of the spontaneous activity of the network, which is well predicted by the analytical description. Second, we investigate the response of the network to time-varying external input, and show that the mean-field model predicts the response time course of the population. Finally, to model VSDi signals, we consider a one-dimensional ring model made of interconnected RS-FS mean-field units. We found that this model can reproduce the spatio-temporal patterns seen in VSDi of awake monkey visual cortex as a response to local and transient visual stimuli. Conversely, we show that the model allows one to infer physiological parameters from the experimentally-recorded spatio-temporal patterns."}], "ArticleTitle": "Modeling mesoscopic cortical dynamics using a mean-field model of conductance-based networks of adaptive exponential integrate-and-fire neurons."}, "32681230": {"mesh": ["Animals", "Contrast Sensitivity", "Haplorhini", "Macaca", "Models, Neurological", "Motion Perception", "Photic Stimulation"], "AbstractText": [{"section": null, "text": "When two-frame apparent motion stimuli are presented with an appropriate inter-stimulus interval (ISI), motion is perceived in the direction opposite to the actual image shift. Herein, we measured a simple eye movement, ocular following responses (OFRs), in macaque monkeys to examine the ISI reversal effect on oculomotor. Two-frame movies with an ISI induced reversed OFRs. Without ISI, the OFRs to the two-frame movie were induced in the direction of the stimulus shift. However, with ISIs &#8805;10&#160;ms, OFRs in the direction opposite to the phase shift were observed. This directional reversal persisted for ISIs up to 160&#160;ms; for longer ISIs virtually no ocular response was observed. Furthermore, longer exposure to the initial image (Motion onset delay: MOD) reduced OFRs. We show that these dependences on ISIs/MODs can be explained by the motion energy model. Furthermore, we examined the dependence on ISI reversal using various spatial frequencies. To account for our findings, the optimal frequency of the temporal filters of the energy model must decrease between 0.5 and 1&#160;cycles/&#176;, suggesting that there are at least two channels with different temporal characteristics. These results are consistent with those from humans, suggesting that the temporal filters embedded in human and macaque visual systems are similar. Thus, the macaque monkey is a good animal model for the early visual processing of humans to understand the neural substrates underlying the visual motion detectors that elicit OFRs."}], "ArticleTitle": "Macaque monkeys show reversed ocular following responses to two-frame-motion stimulus presented with inter-stimulus intervals."}, "28748303": {"mesh": ["Beta Rhythm", "Electroencephalography", "Humans", "Magnetoencephalography", "Models, Neurological", "Movement", "Neural Networks, Computer", "Neurons", "Synapses"], "AbstractText": [{"section": null, "text": "In electrophysiological recordings of the brain, the transition from high amplitude to low amplitude signals are most likely caused by a change in the synchrony of underlying neuronal population firing patterns. Classic examples of such modulations are the strong stimulus-related oscillatory phenomena known as the movement related beta decrease (MRBD) and post-movement beta rebound (PMBR). A sharp decrease in neural oscillatory power is observed during movement (MRBD) followed by an increase above baseline on movement cessation (PMBR). MRBD and PMBR represent important neuroscientific phenomena which have been shown to have clinical relevance. Here, we present a parsimonious model for the dynamics of synchrony within a synaptically coupled spiking network that is able to replicate a human MEG power spectrogram showing the evolution from MRBD to PMBR. Importantly, the high-dimensional spiking model has an exact mean field description in terms of four ordinary differential equations that allows considerable insight to be obtained into the cause of the experimentally observed time-lag from movement termination to the onset of PMBR (&#8764; 0.5 s), as well as the subsequent long duration of PMBR (&#8764; 1 - 10 s). Our model represents the first to predict these commonly observed and robust phenomena and represents a key step in their understanding, in health and disease."}], "ArticleTitle": "A mean field model for movement induced changes in the beta rhythm."}, "28528529": {"mesh": ["Action Potentials", "Epilepsy", "Humans", "Models, Neurological", "Neurons", "Seizures"], "AbstractText": [{"section": null, "text": "The inhibitory restraint necessary to suppress aberrant activity can fail when inhibitory neurons cease to generate action potentials as they enter depolarization block. We investigate possible bifurcation structures that arise at the onset of seizure-like activity resulting from depolarization block in inhibitory neurons. Networks of conductance-based excitatory and inhibitory neurons are simulated to characterize different types of transitions to the seizure state, and a mean field model is developed to verify the generality of the observed phenomena of excitatory-inhibitory dynamics. Specifically, the inhibitory population's activation function in the Wilson-Cowan model is modified to be non-monotonic to reflect that inhibitory neurons enter depolarization block given strong input. We find that a physiological state and a seizure state can coexist, where the seizure state is characterized by high excitatory and low inhibitory firing rate. Bifurcation analysis of the mean field model reveals that a transition to the seizure state may occur via a saddle-node bifurcation or a homoclinic bifurcation. We explain the hysteresis observed in network simulations using these two bifurcation types. We also demonstrate that extracellular potassium concentration affects the depolarization block threshold; the consequent changes in bifurcation structure enable the network to produce the tonic to clonic phase transition observed in biological epileptic networks."}], "ArticleTitle": "The influence of depolarization block on seizure-like activity in networks of excitatory and inhibitory neurons."}, "27677889": {"mesh": ["Humans", "Models, Neurological", "Motor Activity", "Motor Neurons", "Movement", "Muscle Spindles", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Muscle spindle discharge during active movement is a function of mechanical and neural parameters. Muscle length changes (and their derivatives) represent its primary mechanical, fusimotor drive its neural component. However, neither the action nor the function of fusimotor and in particular of &#947;-drive, have been clearly established, since &#947;-motor activity during voluntary, non-locomotor movements remains largely unknown. Here, using a computational approach, we explored whether &#947;-drive emerges in an artificial neural network model of the corticospinal system linked to a biomechanical antagonist wrist simulator. The wrist simulator included length-sensitive and &#947;-drive-dependent type Ia and type II muscle spindle activity. Network activity and connectivity were derived by a gradient descent algorithm to generate reciprocal, known target &#945;-motor unit activity during wrist flexion-extension (F/E) movements. Two tasks were simulated: an alternating F/E task and a slow F/E tracking task. Emergence of &#947;-motor activity in the alternating F/E network was a function of &#945;-motor unit drive: if muscle afferent (together with supraspinal) input was required for driving &#945;-motor units, then &#947;-drive emerged in the form of &#945;-&#947; coactivation, as predicted by empirical studies. In the slow F/E tracking network, &#947;-drive emerged in the form of &#945;-&#947; dissociation and provided critical, bidirectional muscle afferent activity to the cortical network, containing known bidirectional target units. The model thus demonstrates the complementary aspects of spindle output and hence &#947;-drive: i) muscle spindle activity as a driving force of &#945;-motor unit activity, and ii) afferent activity providing continuous sensory information, both of which crucially depend on &#947;-drive."}], "ArticleTitle": "Emergence of gamma motor activity in an artificial neural network model of the corticospinal system."}, "28939929": {"mesh": ["Action Potentials", "Animals", "Computer Simulation", "Electric Stimulation", "Humans", "Intralaminar Thalamic Nuclei", "Models, Neurological", "Nerve Net", "Neural Inhibition", "Neural Networks, Computer", "Neurons", "Nonlinear Dynamics"], "AbstractText": [{"section": null, "text": "It is believed that thalamic reticular nucleus (TRN) controls spindles and spike-wave discharges (SWD) in seizure or sleeping processes. The dynamical mechanisms of spatiotemporal evolutions between these two types of activity, however, are not well understood. In light of this, we first use a single-compartment thalamocortical neural field model to investigate the effects of TRN on occurrence of SWD and its transition. Results show that the increasing inhibition from TRN to specific relay nuclei (SRN) can lead to the transition of system from SWD to slow-wave oscillation. Specially, it is shown that stimulations applied in the cortical neuronal populations can also initiate the SWD and slow-wave oscillation from the resting states under the typical inhibitory intensity from TRN to SRN. Then, we expand into a 3-compartment coupled thalamocortical model network in linear and circular structures, respectively, to explore the spatiotemporal evolutions of wave states in different compartments. The main results are: (i) for the open-ended model network, SWD induced by stimulus in the first compartment can be transformed into sleep-like slow UP-DOWN and spindle states as it propagates into the downstream compartments; (ii) for the close-ended model network, weak stimulations performed in the first compartment can result in the consistent experimentally observed spindle oscillations in all three compartments; in contrast, stronger periodic single-pulse stimulations applied in the first compartment can induce periodic transitions between SWD and spindle oscillations. Detailed investigations reveal that multi-attractor coexistence mechanism composed of SWD, spindles and background state underlies these state evolutions. What's more, in order to demonstrate the state evolution stability with respect to the topological structures of neural network, we further expand the 3-compartment coupled network into 10-compartment coupled one, with linear and circular structures, and nearest-neighbor (NN) coupled network as well as its realization of small-world (SW) topology via random rewiring, respectively. Interestingly, for the cases of linear and circular connetivities, qualitatively similar results were obtained in addition to the more irregularity of firings. However, SWD can be eventually transformed into the consistent low-amplitude oscillations for both NN and SW networks. In particular, SWD evolves into the slow spindling oscillations and background tonic oscillations within the NN and SW network, respectively. Our modeling and simulation studies highlight the effect of network topology in the evolutions of SWD and spindling oscillations, which provides new insights into the mechanisms of cortical seizures development."}], "ArticleTitle": "Stimulus-induced transitions between spike-wave discharges and spindles with the modulation of thalamic reticular nucleus."}, "27033230": {"mesh": ["Action Potentials", "Animals", "Computer Simulation", "Electricity", "Humans", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Models of electrical activity in excitable cells involve nonlinear interactions between many ionic currents. Changing parameters in these models can produce a variety of activity patterns with sometimes unexpected effects. Further more, introducing new currents will have different effects depending on the initial parameter set. In this study we combined global sampling of parameter space and local analysis of representative parameter sets in a pituitary cell model to understand the effects of adding K (+) conductances, which mediate some effects of hormone action on these cells. Global sampling ensured that the effects of introducing K (+) conductances were captured across a wide variety of contexts of model parameters. For each type of K (+) conductance we determined the types of behavioral transition that it evoked. Some transitions were counterintuitive, and may have been missed without the use of global sampling. In general, the wide range of transitions that occurred when the same current was applied to the model cell at different locations in parameter space highlight the challenge of making accurate model predictions in light of cell-to-cell heterogeneity. Finally, we used bifurcation analysis and fast/slow analysis to investigate why specific transitions occur in representative individual models. This approach relies on the use of a graphics processing unit (GPU) to quickly map parameter space to model behavior and identify parameter sets for further analysis. Acceleration with modern low-cost GPUs is particularly well suited to exploring the moderate-sized (5-20) parameter spaces of excitable cell and signaling models."}], "ArticleTitle": "From global to local: exploring the relationship between parameters and behaviors in models of electrical excitability."}, "28585050": {"mesh": ["Action Potentials", "Humans", "Markov Chains", "Models, Neurological", "Neurons", "Noise", "Stochastic Processes"], "AbstractText": [{"section": null, "text": "A neuron receives input from other neurons via electrical pulses, so-called spikes. The pulse-like nature of the input is frequently neglected in analytical studies; instead, the input is usually approximated to be Gaussian. Recent experimental studies have shown, however, that an assumption underlying this approximation is often not met: Individual presynaptic spikes can have a significant effect on a neuron's dynamics. It is thus desirable to explicitly account for the pulse-like nature of neural input, i.e. consider neurons driven by a shot noise - a long-standing problem that is mathematically challenging. In this work, we exploit the fact that excitatory shot noise with exponentially distributed weights can be obtained as a limit case of dichotomous noise, a Markovian two-state process. This allows us to obtain novel exact expressions for the stationary voltage density and the moments of the interspike-interval density of general integrate-and-fire neurons driven by such an input. For the special case of leaky integrate-and-fire neurons, we also give expressions for the power spectrum and the linear response to a signal. We verify and illustrate our expressions by comparison to simulations of leaky-, quadratic- and exponential integrate-and-fire neurons."}], "ArticleTitle": "Exact analytical results for integrate-and-fire neurons driven by excitatory shot noise."}, "27624733": {"mesh": ["Action Potentials", "Algorithms", "Animals", "Bayes Theorem", "Computer Simulation", "Conditioning, Operant", "Hippocampus", "Locomotion", "Markov Chains", "Models, Neurological", "Nerve Net", "Neurons", "Rats", "Time Factors"], "AbstractText": [{"section": null, "text": "We present a hidden Markov model that describes variation in an animal's position associated with varying levels of activity in action potential spike trains of individual place cell neurons. The model incorporates a coarse-graining of position, which we find to be a more parsimonious description of the system than other models. We use a sequential Monte Carlo algorithm for Bayesian inference of model parameters, including the state space dimension, and we explain how to estimate position from spike train observations (decoding). We obtain greater accuracy over other methods in the conditions of high temporal resolution and small neuronal sample size. We also present a novel, model-based approach to the study of replay: the expression of spike train activity related to behaviour during times of motionlessness or sleep, thought to be integral to the consolidation of long-term memories. We demonstrate how we can detect the time, information content and compression rate of replay events in simulated and real hippocampal data recorded from rats in two different environments, and verify the correlation between the times of detected replay events and of sharp wave/ripples in the local field potential."}], "ArticleTitle": "A hidden Markov model for decoding and the analysis of replay in spike trains."}, "29064059": {"mesh": ["Action Potentials", "Animals", "Biological Clocks", "Biophysics", "Humans", "Models, Neurological", "Models, Theoretical", "Neural Conduction", "Neurons", "Nonlinear Dynamics"], "AbstractText": [{"section": null, "text": "The generation of spiking resonances in neurons (preferred spiking responses to oscillatory inputs) requires the interplay of the intrinsic ionic currents that operate at the subthreshold voltage level and the spiking mechanisms. Combinations of the same types of ionic currents in different parameter regimes may give rise to different types of nonlinearities in the voltage equation (e.g., parabolic- and cubic-like), generating subthreshold (membrane potential) oscillations patterns with different properties. These nonlinearities are not apparent in the model equations, but can be uncovered by plotting the voltage nullclines in the phase-plane diagram. We investigate the spiking resonant properties of conductance-based models that are biophysically equivalent at the subthreshold level (same ionic currents), but dynamically different (parabolic- and cubic-like voltage nullclines). As a case study we consider a model having a persistent sodium and a hyperpolarization-activated (h-) currents, which exhibits subthreshold resonance in the theta frequency band. We unfold the concept of spiking resonance into evoked and output spiking resonance. The former focuses on the input frequencies that are able to generate spikes, while the latter focuses on the output spiking frequencies regardless of the input frequency that generated these spikes. A cell can exhibit one or both types of resonances. We also measure spiking phasonance, which is an extension of subthreshold phasonance (zero-phase-shift response to oscillatory inputs) to the spiking regime. The subthreshold resonant properties of both types of models are communicated to the spiking regime for low enough input amplitudes as the voltage response for the subthreshold resonant frequency band raises above threshold. For higher input amplitudes evoked spiking resonance is no longer present in these models, but output spiking resonance is present primarily in the parabolic-like model due to a cycle skipping mechanism (involving mixed-mode oscillations), while the cubic-like model shows a better 1:1 entrainment. We use dynamical systems tools to explain the underlying mechanisms and the mechanistic differences between the resonance types. Our results demonstrate that the effective time scales that operate at the subthreshold regime to generate intrinsic subthreshold oscillations, mixed-mode oscillations and subthreshold resonance do not necessarily determine the existence of a preferred spiking response to oscillatory inputs in the same frequency band. The results discussed in this paper highlight both the complexity of the suprathreshold responses to oscillatory inputs in neurons having resonant and amplifying currents with different time scales and the fact that the identity of the participating ionic currents is not enough to predict the resulting patterns, but additional dynamic information, captured by the geometric properties of the phase-space diagram, is needed."}], "ArticleTitle": "Spiking resonances in models with the same slow resonant and fast amplifying currents but different subthreshold dynamic properties."}, "27008191": {"mesh": ["Action Potentials", "Animals", "Bayes Theorem", "Humans", "Linear Models", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "A key observation in systems neuroscience is that neural responses vary, even in controlled settings where stimuli are held constant. Many statistical models assume that trial-to-trial spike count variability is Poisson, but there is considerable evidence that neurons can be substantially more or less variable than Poisson depending on the stimuli, attentional state, and brain area. Here we examine a set of spike count models based on the Conway-Maxwell-Poisson (COM-Poisson) distribution that can flexibly account for both over- and under-dispersion in spike count data. We illustrate applications of this noise model for Bayesian estimation of tuning curves and peri-stimulus time histograms. We find that COM-Poisson models with group/observation-level dispersion, where spike count variability is a function of time or stimulus, produce more accurate descriptions of spike counts compared to Poisson models as well as negative-binomial models often used as alternatives. Since dispersion is one determinant of parameter standard errors, COM-Poisson models are also likely to yield more accurate model comparison. More generally, these methods provide a useful, model-based framework for inferring both the mean and variability of neural responses."}], "ArticleTitle": "Flexible models for spike count data with both over- and under- dispersion."}, "29387993": {"mesh": ["Animals", "Cerebral Cortex", "Humans", "Judgment", "Models, Neurological", "Nerve Net", "Neural Inhibition", "Neural Networks, Computer", "Neuroglia", "Neurons", "Perception", "Synapses", "Synaptic Transmission", "gamma-Aminobutyric Acid"], "AbstractText": [{"section": null, "text": "Recurrent input to sensory cortex, via long-range reciprocal projections between motor and sensory cortices, is essential for accurate perceptual judgments. GABA levels in sensory cortices correlate with perceptual performance. We simulated a neuron-astrocyte network model to investigate how top-down, feedback signaling from a motor network (Nmot) to a sensory network (Nsen) affects perceptual judgments in association with ambient (extracellular) GABA levels. In the Nsen, astrocytic transporters modulated ambient GABA levels around pyramidal cells. A simple perceptual task was implemented: detection of a feature stimulus presented to the Nsen. The Nmot showed distinct perceptual behaviors: hit, fault, and miss. A hit is a correct response to the stimulus, a fault is a wrong response to the stimulus, and a miss is no response to the stimulus. In hits, the feedback signaling increased the gain of Nsen pyramidal cells and accelerated the reaction speed of Nmot pyramidal cells to the stimulus. Without feedback signaling, the Nsen but not Nmot responded to the stimulus, resulting in a miss. With too strong feedback signaling, the Nmot resulted in a fault, namely, stimulus-insensitive but not stimulus-sensitive pyramidal cells wrongly responded. Balancing the feedforward and feedback signaling formed a coherent, ongoing-spontaneous neuronal state, by which the highest hit rate was achieved. A transient reduction in local ambient GABA levels, triggered by the stimulus, contributed to accelerating the reaction speed under noisy environmental conditions. Adjusting the basal ambient GABA level ensured high hit rates. We suggest that motor cortex feedback may accelerate reaction speed to sensory stimulation by promoting coherency in ongoing-spontaneous neuronal activity between sensory and motor cortices, thereby achieving prompt perceptual judgments. Spatiotemporal modulation of ambient GABA levels, possibly by astrocytic transporters, assists in making reliable perceptual judgments."}], "ArticleTitle": "Perceptual judgments via sensory-motor interaction assisted by cortical GABA."}, "27629491": {"mesh": ["Artifacts", "Cerebral Cortex", "Electrophysiological Phenomena", "Humans", "Models, Neurological"], "AbstractText": [{"section": null, "text": "Electrophysiological data acquisition systems introduce various distortions into the signals they record. While such distortions were discussed previously, their effects are often not appreciated. Here I show that the biphasic shape of cortical spike-triggered LFP average (stLFP), reported in multiple studies, is likely an artefact introduced by high-pass filter of the neural data acquisition system when the actual stLFP has a single trough around the zero lag."}], "ArticleTitle": "Artefactual origin of biphasic cortical spike-LFP correlation."}, "28393281": {"mesh": ["Brain Injuries", "Brain Injuries, Traumatic", "Decision Making", "Humans", "Models, Neurological", "Neurons", "Reaction Time"], "AbstractText": [{"section": null, "text": "The presence of diffuse Focal Axonal Swellings (FAS) is a hallmark cellular feature in many neurological diseases and traumatic brain injury. Among other things, the FAS have a significant impact on spike-train encodings that propagate through the affected neurons, leading to compromised signal processing on a neuronal network level. This work merges, for the first time, three fields of study: (i) signal processing in excitatory-inhibitory (EI) networks of neurons via population codes, (ii) decision-making theory driven by the production of evidence from stimulus, and (iii) compromised spike-train propagation through FAS. As such, we demonstrate a mathematical architecture capable of characterizing compromised decision-making driven by cellular mechanisms. The computational model also leads to several novel predictions and diagnostics for understanding injury level and cognitive deficits, including a key finding that decision-making reaction times, rather than accuracy, are indicative of network level damage. The results have a number of translational implications, including that the level of network damage can be characterized by the reaction times in simple cognitive and motor tests."}], "ArticleTitle": "Reaction time impairments in decision-making networks as a diagnostic marker for traumatic brain injuries and neurological diseases."}, "29047010": {"mesh": ["Acoustic Stimulation", "Auditory Cortex", "Cholinergic Agents", "Computer Simulation", "Electroencephalography", "Female", "Gamma Rhythm", "Hallucinations", "Humans", "Male", "Models, Neurological", "Neural Inhibition", "Schizophrenia", "Synapses"], "AbstractText": [{"section": null, "text": "The pathophysiology of auditory hallucination, a common symptom of schizophrenia, has yet been understood, but during auditory hallucination, primary auditory cortex (A1) shows paradoxical responses. When auditory stimuli are absent, A1 becomes hyperactive, while A1 responses to auditory stimuli are reduced. Such activation pattern of A1 responses during auditory hallucination is consistent with aberrant gamma rhythms in schizophrenia observed during auditory tasks, raising the possibility that the pathology underlying abnormal gamma rhythms can account for auditory hallucination. Moreover, A1 receives top-down signals in the gamma frequency band from an adjacent association area (Par2), and cholinergic modulation regulates interactions between A1 and Par2. In this study, we utilized a computational model of A1 to ask if disrupted cholinergic modulation could underlie abnormal gamma rhythms in schizophrenia. Furthermore, based on our simulation results, we propose potential pathology by which A1 can directly contribute to auditory hallucination."}], "ArticleTitle": "Disrupted cholinergic modulation can underlie abnormal gamma rhythms in schizophrenia and auditory hallucination."}, "27259518": {"mesh": ["Action Potentials", "Animals", "Calcium", "Humans", "Models, Neurological", "Nerve Net", "Neuronal Plasticity", "Neurons", "Transcranial Magnetic Stimulation"], "AbstractText": [{"section": null, "text": "The calcium dependent plasticity (CaDP) approach to the modeling of synaptic weight change is applied using a neural field approach to realistic repetitive transcranial magnetic stimulation (rTMS) protocols. A spatially-symmetric nonlinear neural field model consisting of populations of excitatory and inhibitory neurons is used. The plasticity between excitatory cell populations is then evaluated using a CaDP approach that incorporates metaplasticity. The direction and size of the plasticity (potentiation or depression) depends on both the amplitude of stimulation and duration of the protocol. The breaks in the inhibitory theta-burst stimulation protocol are crucial to ensuring that the stimulation bursts are potentiating in nature. Tuning the parameters of a spike-timing dependent plasticity (STDP) window with a Monte Carlo approach to maximize agreement between STDP predictions and the CaDP results reproduces a realistically-shaped window with two regions of depression in agreement with the existing literature. Developing understanding of how TMS interacts with cells at a network level may be important for future investigation."}], "ArticleTitle": "Calcium dependent plasticity applied to repetitive transcranial magnetic stimulation with a neural field model."}, "28389715": {"mesh": ["Animals", "Cats", "Cerebral Cortex", "Electroencephalography", "Models, Neurological", "Seizures"], "AbstractText": [{"section": null, "text": "Epileptiform discharges on an isolated cortex are explored using neural field theory. A neural field model of the isolated cortex is used that consists of three neural populations, excitatory, inhibitory, and excitatory bursting. Mechanisms by which an isolated cortex gives rise to seizure-like waveforms thought to underly pathological EEG waveforms on the deafferented cortex are explored. It is shown that the model reproduces similar time series and oscillatory frequencies for paroxysmal discharges when compared with physiological recordings both during acute and chronic deafferentation states. Furthermore, within our model ictal activity arises from perturbations to steady-states very close to the dynamical system's instability boundary; hence, these are distinct from corticothalamic seizures observed in the model for the intact brain which involved limit-cycle dynamics. The results are applied to experiments in deafferented cats."}], "ArticleTitle": "Neural field model of seizure-like activity in isolated cortex."}, "33999326": {"mesh": ["Animals", "Computer Simulation", "Escape Reaction", "Models, Neurological", "Neurons", "Swimming", "Zebrafish"], "AbstractText": [{"section": null, "text": "Fish escape from approaching threats via a stereotyped escape behavior. This behavior, and the underlying neural circuit organized around the Mauthner cell command neurons, have both been extensively investigated experimentally, mainly in two laboratory model organisms, the goldfish and the zebrafish. However, fish biodiversity is enormous, a number of variants of the basal escape behavior exist. In marine gobies (a family of small benthic fishes) which share burrows with alpheid shrimp, the escape behavior has likely been partially modified into a tactile communication system which allow the fish to communicate the approach of a predatory fish to the shrimp. In this communication system, the goby responds to intermediate-strength threats with a brief tail-flick which the shrimp senses with its antennae.We investigated the shrimp goby escape and communication system with computational models. We asked how the circuitry of the basal escape behavior could be modified to produce behavior akin to the shrimp-goby communication system. In a simple model, we found that mutual inhibitions between Mauthner cells can be tuned to produce an oscillatory response to intermediate strength inputs, albeit only in a narrow parameter range.Using a more detailed model, we found that two modifications of the fish locomotor system transform it into a model reproducing the shrimp goby behavior. These modifications are: 1. modifying the central pattern generator which drives swimming such that it is quiescent when receiving no inputs; 2. introducing a direct sensory input to this central pattern generator, bypassing the Mauthner cells."}], "ArticleTitle": "A computational model of the shrimp-goby escape and communication system."}, "27726048": {"mesh": ["Acoustic Stimulation", "Algorithms", "Computer Simulation", "Models, Neurological", "Neurons", "Social Support"], "AbstractText": [{"section": null, "text": "Particle swarm optimization (PSO) has gained widespread use as a general mathematical programming paradigm and seen use in a wide variety of optimization and machine learning problems. In this work, we introduce a new variant on the PSO social network and apply this method to the inverse problem of input parameter selection from recorded auditory neuron tuning curves. The topology of a PSO social network is a major contributor to optimization success. Here we propose a new social network which draws influence from winner-take-all coding found in visual cortical neurons. We show that the winner-take-all network performs exceptionally well on optimization problems with greater than 5 dimensions and runs at a lower iteration count as compared to other PSO topologies. Finally we show that this variant of PSO is able to recreate auditory frequency tuning curves and modulation transfer functions, making it a potentially useful tool for computational neuroscience models."}], "ArticleTitle": "Hierarchical winner-take-all particle swarm optimization social network for neural model fitting."}, "29192377": {"mesh": ["Action Potentials", "Animals", "Computer Simulation", "Models, Neurological", "Neurons", "Reaction Time", "Signal Transduction", "Synapses"], "AbstractText": [{"section": null, "text": "The noisy threshold regime, where even a small set of presynaptic neurons can significantly affect postsynaptic spike-timing, is suggested as a key requisite for computation in neurons with high variability. It also has been proposed that signals under the noisy conditions are successfully transferred by a few strong synapses and/or by an assembly of nearly synchronous synaptic activities. We analytically investigate the impact of a transient signaling input on a leaky integrate-and-fire postsynaptic neuron that receives background noise near the threshold regime. The signaling input models a single strong synapse or a set of synchronous synapses, while the background noise represents a lot of weak synapses. We find an analytic solution that explains how the first-passage time (ISI) density is changed by transient signaling input. The analysis allows us to connect properties of the signaling input like spike timing and amplitude with postsynaptic first-passage time density in a noisy environment. Based on the analytic solution, we calculate the Fisher information with respect to the signaling input's amplitude. For a wide range of amplitudes, we observe a non-monotonic behavior for the Fisher information as a function of background noise. Moreover, Fisher information non-trivially depends on the signaling input's amplitude; changing the amplitude, we observe one maximum in the high level of the background noise. The single maximum splits into two maximums in the low noise regime. This finding demonstrates the benefit of the analytic solution in investigating signal transfer by neurons."}], "ArticleTitle": "How does transient signaling input affect the spike timing of postsynaptic neuron near the threshold regime: an analytical study."}, "26945993": {"mesh": ["Biophysics", "Humans", "Models, Neurological", "Nerve Net", "Neurons", "Periodicity"], "AbstractText": [{"section": null, "text": "We extend the theory of weakly coupled oscillators to incorporate slowly varying inputs and parameters. We employ a combination of regular perturbation and an adiabatic approximation to derive equations for the phase-difference between a pair of oscillators. We apply this to the simple Hopf oscillator and then to a biophysical model. The latter represents the behavior of a neuron that is subject to slow modulation of a muscarinic current such as would occur during transient attention through cholinergic activation. Our method extends and simplifies the recent work of Kurebayashi (Physical Review Letters, 111, 214101, 2013) to include coupling. We apply the method to an all-to-all network and show that there is a waxing and waning of synchrony of modulated neurons."}], "ArticleTitle": "Weakly coupled oscillators in a slowly varying world."}, "34003421": {"mesh": ["Models, Neurological", "Nerve Net", "Stochastic Processes", "Synapses"], "AbstractText": [{"section": null, "text": "It has previously been shown that the encoding of time-dependent signals by feedforward networks (FFNs) of processing units exhibits suprathreshold stochastic resonance (SSR), which is an optimal signal transmission for a finite level of independent, individual stochasticity in the single units. In this study, a recurrent spiking network is simulated to demonstrate that SSR can be also caused by network noise in place of intrinsic noise. The level of autonomously generated fluctuations in the network can be controlled by the strength of synapses, and hence the coding fraction (our measure of information transmission) exhibits a maximum as a function of the synaptic coupling strength. The presence of a coding peak at an optimal coupling strength is robust over a wide range of individual, network, and signal parameters, although the optimal strength and peak magnitude depend on the parameter being varied. We also perform control experiments with an FFN illustrating that the optimized coding fraction is due to the change in noise level and not from other effects entailed when changing the coupling strength. These results also indicate that the non-white (temporally correlated) network noise in general provides an extra boost to encoding performance compared to the FFN driven by intrinsic white noise fluctuations."}], "ArticleTitle": "Recurrence-mediated suprathreshold stochastic resonance."}, "34032982": {"mesh": ["Animals", "Brain Stem", "Electric Fish", "Female", "Male", "Models, Neurological", "Potassium", "Sex Characteristics"], "AbstractText": [{"section": null, "text": "Intrinsic oscillators in the central nervous system play a preeminent role in the neural control of rhythmic behaviors, yet little is known about how the ionic milieu regulates their output patterns. A powerful system to address this question is the pacemaker nucleus of the weakly electric fish Apteronotus leptorhynchus. A neural network comprised of an average of 87 pacemaker cells and 20 relay cells produces tonic oscillations, with higher frequencies in males compared to females. Previous empirical studies have suggested that this sexual dimorphism develops and is maintained through modulation of buffering of extracellular K+ by a massive meshwork of astrocytes enveloping the pacemaker and relay cells. Here, we constructed a model of this neural network that can generate sustained spontaneous oscillations. Sensitivity analysis revealed the potassium equilibrium potential, EK (as a proxy of extracellular K+ concentration), and corresponding somatic channel conductances as critical determinants of oscillation frequency and amplitude. In models of both the pacemaker nucleus network and isolated pacemaker and relay cells, the frequency increased almost linearly with EK, whereas the amplitude decreased nonlinearly with increasing EK. Our simulations predict that this frequency increase is largely caused by a shift in the minimum K+ conductance over one oscillation period. This minimum is close to zero at more negative EK, converging to the corresponding maximum at less negative EK. This brings the resting membrane potential closer to the threshold potential at which voltage-gated Na+ channels become active, increasing the excitability, and thus the frequency, of pacemaker and relay cells."}], "ArticleTitle": "Modeling of sustained spontaneous network oscillations of a sexually dimorphic brainstem nucleus: the role of potassium equilibrium potential."}, "28367595": {"mesh": ["Action Potentials", "Calcium", "Dopaminergic Neurons", "Dorsal Raphe Nucleus", "Humans", "In Vitro Techniques", "Membrane Potentials", "Models, Neurological", "Neurons", "Patch-Clamp Techniques", "Periaqueductal Gray"], "AbstractText": [{"section": null, "text": "Dopamine (DA) neurons of the ventrolateral periaqueductal gray (vlPAG) and dorsal raphe nucleus (DRN) fire spontaneous action potentials (APs) at slow, regular patterns in vitro but a detailed account of their intrinsic membrane properties responsible for spontaneous firing is currently lacking. To resolve this, we performed a voltage-clamp electrophysiological study in brain slices to describe their major ionic currents and then constructed a computer model and used simulations to understand the mechanisms behind autorhythmicity in silico. We found that vlPAG/DRN DA neurons exhibit a number of voltage-dependent currents activating in the subthreshold range including, a hyperpolarization-activated cation current (IH), a transient, A-type, potassium current (IA), a background, 'persistent' (INaP) sodium current and a transient, low voltage activated (LVA) calcium current (ICaLVA). Brain slice pharmacology, in good agreement with computer simulations, showed that spontaneous firing occurred independently of IH, IA or calcium currents. In contrast, when blocking sodium currents, spontaneous firing ceased and a stable, non-oscillating membrane potential below AP threshold was attained. Using the DA neuron model we further show that calcium currents exhibit little activation (compared to sodium) during the interspike interval (ISI) repolarization while, any individual potassium current alone, whose blockade positively modulated AP firing frequency, is not required for spontaneous firing. Instead, blockade of a number of potassium currents simultaneously is necessary to eliminate autorhythmicity. Repolarization during ISI is mediated initially via the deactivation of the delayed rectifier potassium current, while a sodium background 'persistent' current is essentially indispensable for autorhythmicity by driving repolarization towards AP threshold."}], "ArticleTitle": "Ionic currents influencing spontaneous firing and pacemaker frequency in dopamine neurons of the ventrolateral periaqueductal gray and dorsal raphe nucleus (vlPAG/DRN): A voltage-clamp and computational modelling study."}, "33507429": {"mesh": ["Action Potentials", "Models, Neurological", "Models, Statistical", "Neurons"], "AbstractText": [{"section": null, "text": "Observations of finely-timed spike relationships in population recordings have been used to support partial reconstruction of neural microcircuit diagrams. In this approach, fine-timescale components of paired spike train interactions are isolated and subsequently attributed to synaptic parameters. Recent perturbation studies strengthen the case for such an inference, yet the complete set of measurements needed to calibrate statistical models is unavailable. To address this gap, we study features of pairwise spiking in a large-scale in vivo dataset where presynaptic neurons were explicitly decoupled from network activity by juxtacellular stimulation. We then construct biophysical models of paired spike trains to reproduce the observed phenomenology of in vivo monosynaptic interactions, including both fine-timescale spike-spike correlations and firing irregularity. A key characteristic of these models is that the paired neurons are coupled by rapidly-fluctuating background inputs. We quantify a monosynapse's causal effect by comparing the postsynaptic train with its counterfactual, when the monosynapse is removed. Subsequently, we develop statistical techniques for estimating this causal effect from the pre- and post-synaptic spike trains. A particular focus is the justification and application of a nonparametric separation of timescale principle to implement synaptic inference. Using simulated data generated from the biophysical models, we characterize the regimes in which the estimators accurately identify the monosynaptic effect. A secondary goal is to initiate a critical exploration of neurostatistical assumptions in terms of biophysical mechanisms, particularly with regards to the challenging but arguably fundamental issue of fast, unobservable nonstationarities in background dynamics."}], "ArticleTitle": "Monosynaptic inference via finely-timed spikes."}, "27714569": {"mesh": ["Acoustic Stimulation", "Animals", "Bayes Theorem", "Cues", "Models, Neurological", "Neurons", "Sound Localization", "Strigiformes"], "AbstractText": [{"section": null, "text": "Integration of multiple sensory cues can improve performance in detection and estimation tasks. There is an open theoretical question of the conditions under which linear or nonlinear cue combination is Bayes-optimal. We demonstrate that a neural population decoded by a population vector requires nonlinear cue combination to approximate Bayesian inference. Specifically, if cues are conditionally independent, multiplicative cue combination is optimal for the population vector. The model was tested on neural and behavioral responses in the barn owl's sound localization system where space-specific neurons owe their selectivity to multiplicative tuning to sound localization cues interaural phase (IPD) and level (ILD) differences. We found that IPD and ILD cues are approximately conditionally independent. As a result, the multiplicative combination selectivity to IPD and ILD of midbrain space-specific neurons permits a population vector to perform Bayesian cue combination. We further show that this model describes the owl's localization behavior in azimuth and elevation. This work provides theoretical justification and experimental evidence supporting the optimality of nonlinear cue combination."}], "ArticleTitle": "Optimal nonlinear cue integration for sound localization."}, "27488433": {"mesh": ["Humans", "Models, Neurological", "Neocortex", "Neuroglia", "Potassium", "Seizures"], "AbstractText": [{"section": null, "text": "How focal seizures initiate and evolve in human neocortex remains a fundamental problem in neuroscience. Here, we use biophysical neuronal network models of neocortical patches to study how the interaction between inhibition and extracellular potassium ([K (+)] o ) dynamics may contribute to different types of focal seizures. Three main types of propagated focal seizures observed in recent intracortical microelectrode recordings in humans were modelled: seizures characterized by sustained (&#8764;30-60 Hz) gamma local field potential (LFP) oscillations; seizures where the onset in the propagated site consisted of LFP spikes that later evolved into rhythmic (&#8764;2-3 Hz) spike-wave complexes (SWCs); and seizures where a brief stage of low-amplitude fast-oscillation (&#8764;10-20 Hz) LFPs preceded the SWC activity. Our findings are fourfold: (1) The interaction between elevated [K (+)] o (due to abnormal potassium buffering by glial cells) and the strength of synaptic inhibition plays a predominant role in shaping these three types of seizures. (2) Strengthening of inhibition leads to the onset of sustained narrowband gamma seizures. (3) Transition into SWC seizures is obtained either by the weakening of inhibitory synapses, or by a transient strengthening followed by an inhibitory breakdown (e.g. GABA depletion). This reduction or breakdown of inhibition among fast-spiking (FS) inhibitory interneurons increases their spiking activity and leads them eventually into depolarization block. Ictal spike-wave discharges in the model are then sustained solely by pyramidal neurons. (4) FS cell dynamics are also critical for seizures where the evolution into SWC activity is preceded by low-amplitude fast oscillations. Different levels of elevated [K (+)] o were important for transitions into and maintenance of sustained gamma oscillations and SWC discharges. Overall, our modelling study predicts that the interaction between inhibitory interneurons and [K (+)] o glial buffering under abnormal conditions may explain different types of ictal transitions and dynamics during propagated seizures in human focal epilepsy. "}], "ArticleTitle": "Interaction between synaptic inhibition and glial-potassium dynamics leads to diverse seizure transition modes in biophysical models of human focal seizures."}, "27530214": {"mesh": ["Feedback, Physiological", "Humans", "Models, Neurological", "Psychomotor Performance", "Signal Detection, Psychological", "Time Factors"], "AbstractText": [{"section": null, "text": "We propose that feedback-delayed manual tracking performance is limited by fundamental constraints imposed by the physics of negative group delay. To test this hypothesis, the results of an experiment in which subjects demonstrate both reactive and predictive dynamics are modeled by a linear system with delay-induced negative group delay. Although one of the simplest real-time predictors conceivable, this model explains key components of experimental observations. Most notably, it explains the observation that prediction time linearly increases with feedback delay, up to a certain point when tracking performance deteriorates. It also explains the transition from reactive to predictive behavior with increasing feedback delay. The model contains only one free parameter, the feedback gain, which has been fixed by comparison with one set of experimental observations for the reactive case. Our model provides quantitative predictions that can be tested in further experiments."}], "ArticleTitle": "A negative group delay model for feedback-delayed manual tracking performance."}, "29124504": {"mesh": ["Action Potentials", "Animals", "Computer Simulation", "Electric Fish", "Electric Stimulation", "Female", "Male", "Models, Neurological", "Neural Networks, Computer", "Neural Pathways", "Pyramidal Cells", "Rhombencephalon", "Time Factors"], "AbstractText": [{"section": null, "text": "Heterogeneity of firing rate statistics is known to have severe consequences on neural coding. Recent experimental recordings in weakly electric fish indicate that the distribution-width of superficial pyramidal cell firing rates (trial- and time-averaged) in the electrosensory lateral line lobe (ELL) depends on the stimulus, and also that network inputs can mediate changes in the firing rate distribution across the population. We previously developed theoretical methods to understand how two attributes (synaptic and intrinsic heterogeneity) interact and alter the firing rate distribution in a population of integrate-and-fire neurons with random recurrent coupling. Inspired by our experimental data, we extend these theoretical results to a delayed feedforward spiking network that qualitatively capture the changes of firing rate heterogeneity observed in in-vivo recordings. We demonstrate how heterogeneous neural attributes alter firing rate heterogeneity, accounting for the effect with various sensory stimuli. The model predicts how the strength of the effective network connectivity is related to intrinsic heterogeneity in such delayed feedforward networks: the strength of the feedforward input is positively correlated with excitability (threshold value for spiking) when firing rate heterogeneity is low and is negatively correlated with excitability with high firing rate heterogeneity. We also show how our theory can be used to predict effective neural architecture. We demonstrate that neural attributes do not interact in a simple manner but rather in a complex stimulus-dependent fashion to control neural heterogeneity and discuss how it can ultimately shape population codes."}], "ArticleTitle": "Variable synaptic strengths controls the firing rate distribution in feedforward neural networks."}, "27106692": {"mesh": ["Action Potentials", "Aging", "Algorithms", "Animals", "Biological Evolution", "Computer Simulation", "Dendrites", "In Vitro Techniques", "Ion Channels", "Kinetics", "Macaca mulatta", "Models, Neurological", "Pyramidal Cells"], "AbstractText": [{"section": null, "text": "Conductance-based compartment modeling requires tuning of many parameters to fit the neuron model to target electrophysiological data. Automated parameter optimization via evolutionary algorithms (EAs) is a common approach to accomplish this task, using error functions to quantify differences between model and target. We present a three-stage EA optimization protocol for tuning ion channel conductances and kinetics in a generic neuron model with minimal manual intervention. We use the technique of Latin hypercube sampling in a new way, to choose weights for error functions automatically so that each function influences the parameter search to a similar degree. This protocol requires no specialized physiological data collection and is applicable to commonly-collected current clamp data and either single- or multi-objective optimization. We applied the protocol to two representative pyramidal neurons from layer 3 of the prefrontal cortex of rhesus monkeys, in which action potential firing rates are significantly higher in aged compared to young animals. Using an idealized dendritic topology and models with either 4 or 8 ion channels (10 or 23 free parameters respectively), we produced populations of parameter combinations fitting the target datasets in less than 80 hours of optimization each. Passive parameter differences between young and aged models were consistent with our prior results using simpler models and hand tuning. We analyzed parameter values among fits to a single neuron to facilitate refinement of the underlying model, and across fits to multiple neurons to show how our protocol will lead to predictions of parameter differences with aging in these neurons."}], "ArticleTitle": "Automated evolutionary optimization of ion channel conductances and kinetics in models of young and aged rhesus monkey pyramidal neurons."}, "28569367": {"mesh": ["Biophysics", "Humans", "Linear Models", "Membrane Potentials", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Subthreshold (membrane potential) resonance and phasonance (preferred amplitude and zero-phase responses to oscillatory inputs) in single neurons arise from the interaction between positive and negative feedback effects provided by relatively fast amplifying currents and slower resonant currents. In 2D neuronal systems, amplifying currents are required to be slave to voltage (instantaneously fast) for these phenomena to occur. In higher dimensional systems, additional currents operating at various effective time scales may modulate and annihilate existing resonances and generate antiresonance (minimum amplitude response) and antiphasonance (zero-phase response with phase monotonic properties opposite to phasonance). We use mathematical modeling, numerical simulations and dynamical systems tools to investigate the mechanisms underlying these phenomena in 3D linear models, which are obtained as the linearization of biophysical (conductance-based) models. We characterize the parameter regimes for which the system exhibits the various types of behavior mentioned above in the rather general case in which the underlying 2D system exhibits resonance. We consider two cases: (i) the interplay of two resonant gating variables, and (ii) the interplay of one resonant and one amplifying gating variables. Increasing levels of an amplifying current cause (i) a response amplification if the amplifying current is faster than the resonant current, (ii) resonance and phasonance attenuation and annihilation if the amplifying and resonant currents have identical dynamics, and (iii) antiresonance and antiphasonance if the amplifying current is slower than the resonant current. We investigate the underlying mechanisms by extending the envelope-plane diagram approach developed in previous work (for 2D systems) to three dimensions to include the additional gating variable, and constructing the corresponding envelope curves in these envelope-space diagrams. We find that antiresonance and antiphasonance emerge as the result of an asymptotic boundary layer problem in the frequency domain created by the different balances between the intrinsic time constants of the cell and the input frequency f as it changes. For large enough values of f the envelope curves are quasi-2D and the impedance profile decreases with the input frequency. In contrast, for f &#8810; 1 the dynamics are quasi-1D and the impedance profile increases above the limiting value in the other regime. Antiresonance is created because the continuity of the solution requires the impedance profile to connect the portions belonging to the two regimes. If in doing so the phase profile crosses the zero value, then antiphasonance is also generated."}], "ArticleTitle": "Resonance modulation, annihilation and generation of anti-resonance and anti-phasonance in 3D neuronal systems: interplay of resonant and amplifying currents with slow dynamics."}, "33528721": {"mesh": ["Animals", "Models, Neurological", "Neuronal Plasticity", "Neurons", "Synapses", "Synaptic Transmission"], "AbstractText": [{"section": null, "text": "Excitatory synaptic signaling in cortical circuits is thought to be metabolically expensive. Two fundamental brain functions, learning and memory, are associated with long-term synaptic plasticity, but we know very little about energetics of these slow biophysical processes. This study investigates the energy requirement of information storing in plastic synapses for an extended version of BCM plasticity with a decay term, stochastic noise, and nonlinear dependence of neuron's firing rate on synaptic current (adaptation). It is shown that synaptic weights in this model exhibit bistability. In order to analyze the system analytically, it is reduced to a simple dynamic mean-field for a population averaged plastic synaptic current. Next, using the concepts of nonequilibrium thermodynamics, we derive the energy rate (entropy production rate) for plastic synapses and a corresponding Fisher information for coding presynaptic input. That energy, which is of chemical origin, is primarily used for battling fluctuations in the synaptic weights and presynaptic firing rates, and it increases steeply with synaptic weights, and more uniformly though nonlinearly with presynaptic firing. At the onset of synaptic bistability, Fisher information and memory lifetime both increase sharply, by a few orders of magnitude, but the plasticity energy rate changes only mildly. This implies that a huge gain in the precision of stored information does not have to cost large amounts of metabolic energy, which suggests that synaptic information is not directly limited by energy consumption. Interestingly, for very weak synaptic noise, such a limit on synaptic coding accuracy is imposed instead by a derivative of the plasticity energy rate with respect to the mean presynaptic firing, and this relationship has a general character that is independent of the plasticity type. An estimate for primate neocortex reveals that a relative metabolic cost of BCM type synaptic plasticity, as a fraction of neuronal cost related to fast synaptic transmission and spiking, can vary from negligible to substantial, depending on the synaptic noise level and presynaptic firing."}], "ArticleTitle": "Energetics of stochastic BCM type synaptic plasticity and storing of accurate information."}, "28236135": {"mesh": ["Brain", "Brain Mapping", "Electroencephalography", "Humans", "Models, Neurological", "Thalamus"], "AbstractText": [{"section": null, "text": "Cognitive functions such as sensory processing and memory processes lead to phase synchronization in the electroencephalogram or local field potential between different brain regions. There are a lot of computational researches deriving phase locking values (PLVs), which are an index of phase synchronization intensity, from neural models. However, these researches derive PLVs numerically. To the best of our knowledge, there have been no reports on the derivation of a theoretical PLV. In this study, we propose an analytical method for deriving theoretical PLVs from a cortico-thalamic neural mass model described by a delay differential equation. First, the model for generating neural signals is transformed into a normal form of the Hopf bifurcation using center manifold reduction. Second, the normal form is transformed into a phase model that is suitable for analyzing synchronization phenomena. Third, the Fokker-Planck equation of the phase model is derived and the phase difference distribution is obtained. Finally, the PLVs are calculated from the stationary distribution of the phase difference. The validity of the proposed method is confirmed via numerical simulations. Furthermore, we apply the proposed method to a working memory process, and discuss the neurophysiological basis behind the phase synchronization phenomenon. The results demonstrate the importance of decreasing the intensity of independent noise during the working memory process. The proposed method will be of great use in various experimental studies and simulations relevant to phase synchronization, because it enables the effect of neurophysiological changes on PLVs to be analyzed from a mathematical perspective."}], "ArticleTitle": "Deriving theoretical phase locking values of a coupled cortico-thalamic neural mass model using center manifold reduction."}, "33464428": {"mesh": ["Deep Brain Stimulation", "Eye Movements", "Humans", "Models, Neurological", "Parkinson Disease", "Saccades", "Subthalamic Nucleus"], "AbstractText": [{"section": null, "text": "Miniature yoked eye movements, fixational saccades, are critical to counteract visual fading. Fixational saccades are followed by a return saccades forming squarewaves. Present in healthy states, squarewaves, if too many or too big, affect visual stability. Parkinson's disease (PD), where visual deficits are not uncommon, is associated with the squarewaves that are excessive in number or size. Our working hypothesis is that the basal ganglia are at the epicenter of the abnormal fixational saccades and squarewaves in PD; the effects are manifested through their connections to the superior colliculus (affecting saccade frequency and amplitude) and the cerebellum (affecting velocity and amplitude). We predict that the subthalamic deep brain stimulation (DBS) variably affects the amplitude, frequency, and velocity of fixational saccade and that the effect depends on the electrode's proximity or the volume of activated tissue in the subthalamic nucleus' connections with the superior colliculus or the cerebellum. We found that DBS modulated saccade amplitude, frequency, and velocity in 11 PD patients. Although all three parameters were affected, the extent of the effects varied amongst subjects. The modulation was dependent upon the location and size of the electrically activated volume of the subthalamic region."}], "ArticleTitle": "Effects of subthalamic deep brain stimulation on fixational eye movements in Parkinson's disease."}, "32705430": {"mesh": [], "AbstractText": [{"section": null, "text": "The original version of this article unfortunately has some typographical errors in equations (5), (6), (7), (8), and (12)."}], "ArticleTitle": "Correction to: Psychophysical detection and learning in freely behaving rats: a probabilistic dynamical model for operant conditioning."}, "27812835": {"mesh": ["Action Potentials", "Learning", "Models, Neurological", "Nerve Net", "Neural Networks, Computer", "Neuronal Plasticity", "Neurons"], "AbstractText": [{"section": null, "text": "Neuronal circuits in the rodent barrel cortex are characterized by stable low firing rates. However, recent experiments show that short spike trains elicited by electrical stimulation in single neurons can induce behavioral responses. Hence, the underlying neural networks provide stability against internal fluctuations in the firing rate, while simultaneously making the circuits sensitive to small external perturbations. Here we studied whether stability and sensitivity are affected by the connectivity structure in recurrently connected spiking networks. We found that anti-correlation between the number of afferent (in-degree) and efferent (out-degree) synaptic connections of neurons increases stability against pathological bursting, relative to networks where the degrees were either positively correlated or uncorrelated. In the stable network state, stimulation of a few cells could lead to a detectable change in the firing rate. To quantify the ability of networks to detect the stimulation, we used a receiver operating characteristic (ROC) analysis. For a given level of background noise, networks with anti-correlated degrees displayed the lowest false positive rates, and consequently had the highest stimulus detection performance. We propose that anti-correlation in the degree distribution may be a computational strategy employed by sensory cortices to increase the detectability of external stimuli. We show that networks with anti-correlated degrees can in principle be formed by applying learning rules comprised of a combination of spike-timing dependent plasticity, homeostatic plasticity and pruning to networks with uncorrelated degrees. To test our prediction we suggest a novel experimental method to estimate correlations in the degree distribution."}], "ArticleTitle": "Anti-correlations in the degree distribution increase stimulus detection performance in noisy spiking neural networks."}, "27585661": {"mesh": ["Action Potentials", "Animals", "Brain", "Computer Simulation", "Humans", "Models, Neurological", "Neural Networks, Computer", "Neurons", "Nonlinear Dynamics", "Synapses"], "AbstractText": [{"section": null, "text": "A large body of experimental and theoretical work on neural coding suggests that the information stored in brain circuits is represented by time-varying patterns of neural activity. Reservoir computing, where the activity of a recurrently connected pool of neurons is read by one or more units that provide an output response, successfully exploits this type of neural activity. However, the question of system robustness to small structural perturbations, such as failing neurons and synapses, has been largely overlooked. This contrasts with well-studied dynamical perturbations that lead to divergent network activity in the presence of chaos, as is the case for many reservoir networks. Here, we distinguish between two types of structural network perturbations, namely local (e.g., individual synaptic or neuronal failure) and global (e.g., network-wide fluctuations). Surprisingly, we show that while global perturbations have a limited impact on the ability of reservoir models to perform various tasks, local perturbations can produce drastic effects. To address this limitation, we introduce a new architecture where the reservoir is driven by a layer of oscillators that generate stable and repeatable trajectories. This model outperforms previous implementations while being resistant to relatively large local and global perturbations. This finding has implications for the design of reservoir models that capture the capacity of brain circuits to perform cognitively and behaviorally relevant tasks while remaining robust to various forms of perturbations. Further, our work proposes a novel role for neuronal oscillations found in cortical circuits, where they may serve as a collection of inputs from which a network can robustly generate complex dynamics and implement rich computations."}], "ArticleTitle": "Driving reservoir models with oscillations: a solution to the extreme structural sensitivity of chaotic networks."}, "27491968": {"mesh": ["Action Potentials", "Animals", "Computer Simulation", "In Vitro Techniques", "Models, Neurological", "Neurons", "Nonlinear Dynamics", "Respiratory Center", "Rodentia", "Time Factors"], "AbstractText": [{"section": null, "text": "Experimental results in rodent medullary slices containing the pre-B&#246;tzinger complex (pre-B&#246;tC) have identified multiple bursting mechanisms based on persistent sodium current (I NaP) and intracellular Ca2+. The classic two-timescale approach to the analysis of pre-B&#246;tC bursting treats the inactivation of I NaP, the calcium concentration, as well as the Ca2+-dependent inactivation of IP 3 as slow variables and considers other evolving quantities as fast variables. Based on its time course, however, it appears that a novel mixed bursting (MB) solution, observed both in recordings and in model pre-B&#246;tC neurons, involves at least three timescales. In this work, we consider a single-compartment model of a pre-B&#246;tC inspiratory neuron that can exhibit both I NaP and Ca2+ oscillations and has the ability to produce MB solutions. We use methods of dynamical systems theory, such as phase plane analysis, fast-slow decomposition, and bifurcation analysis, to better understand the mechanisms underlying the MB solution pattern. Rather surprisingly, we discover that a third timescale is not actually required to generate mixed bursting solutions. Through our analysis of timescales, we also elucidate how the pre-B&#246;tC neuron model can be tuned to improve the robustness of the MB solution."}], "ArticleTitle": "Multiple timescale mixed bursting dynamics in a respiratory neuron model."}, "33818659": {"mesh": ["Brain", "Cerebral Cortex", "Gamma Rhythm", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "An important problem in systems neuroscience is to understand how information is communicated among brain regions, and it has been proposed that communication is mediated by neuronal oscillations, such as rhythms in the gamma band. We sought to investigate this idea by using a network model with two components, a source (sending) and a target (receiving) component, both built to resemble local populations in the cerebral cortex. To measure the effectiveness of communication, we used population-level correlations in spike times between the source and target. We found that after correcting for a response time that is independent of initial conditions, spike-time correlations between the source and target are significant, due in large measure to the alignment of firing events in their gamma rhythms. But, we also found that regular oscillations cannot produce the results observed in our model simulations of cortical neurons. Surprisingly, it is the irregularity of gamma rhythms, the absence of internal clocks, together with the malleability of these rhythms and their tendency to align with external pulses - features that are known to be present in gamma rhythms in the real cortex - that produced the results observed. These findings and the mechanistic explanations we offered are our primary results. Our secondary result is a mathematical relationship between correlations and the sizes of the samples used for their calculation. As improving technology enables recording simultaneously from increasing numbers of neurons, this relationship could be useful for interpreting results from experimental recordings."}], "ArticleTitle": "Malleability of gamma rhythms enhances population-level correlations."}, "27515518": {"mesh": ["Models, Neurological", "Nerve Net", "Neurons"], "AbstractText": [{"section": null, "text": "We investigate the properties of recently proposed \"shotgun\" sampling approach for the common inputs problem in the functional estimation of neuronal connectivity. We study the asymptotic correctness, the speed of convergence, and the data size requirements of such an approach. We show that the shotgun approach can be expected to allow the inference of complete connectivity matrix in large neuronal populations under some rather general conditions. However, we find that the posterior error of the shotgun connectivity estimator grows quickly with the size of unobserved neuronal populations, the square of average connectivity strength, and the square of observation sparseness. This implies that the shotgun connectivity estimation will require significantly larger amounts of neuronal activity data whenever the number of neurons in observed neuronal populations remains small. We present a numerical approach for solving the shotgun estimation problem in general settings and use it to demonstrate the shotgun connectivity inference in the examples of simulated synfire and weakly coupled cortical neuronal networks. "}], "ArticleTitle": "Consistent estimation of complete neuronal connectivity in large neuronal populations using sparse \"shotgun\" neuronal activity sampling."}, "33825082": {"mesh": ["Models, Neurological", "Neuronal Plasticity", "Neurons", "Synapses", "Visual Cortex"], "AbstractText": [{"section": null, "text": "The principle of constraint-induced therapy is widely practiced in rehabilitation. In hemiplegic cerebral palsy (CP) with impaired contralateral corticospinal projection due to unilateral injury, function improves after imposing a temporary constraint on limbs from the less affected hemisphere. This type of partially-reversible impairment in motor control by early brain injury bears a resemblance to the experience-dependent plastic acquisition and modification of neuronal response selectivity in the visual cortex. Previously, such mechanism was modeled within the framework of BCM (Bienenstock-Cooper-Munro) theory, a rate-based synaptic modification theory. Here, we demonstrate a minimally complex yet sufficient neural network model which provides a fundamental explanation for inter-hemispheric competition using a simplified spike-based model of information transmission and plasticity. We emulate the restoration of function in hemiplegic CP by simulating the competition between cells of the ipsilateral and contralateral corticospinal tracts. We use a high-speed hardware neural simulation to provide realistic numbers of spikes and realistic magnitudes of synaptic modification. We demonstrate that the phenomenon of constraint-induced partial reversal of hemiplegia can be modeled by simplified neural descending tracts with 2 layers of spiking neurons and synapses with spike-timing-dependent plasticity (STDP). We further demonstrate that persistent hemiplegia following unilateral cortical inactivation or deprivation is predicted by the STDP-based model but is inconsistent with BCM model. Although our model is a highly simplified and limited representation of the corticospinal system, it offers an explanation of how constraint as an intervention can help the system to escape from a suboptimal solution. This is a display of an emergent phenomenon from the synaptic competition."}], "ArticleTitle": "Constraint-induced intervention as an emergent phenomenon from synaptic competition in biological systems."}, "28660531": {"mesh": ["Animals", "Computer Simulation", "Decision Making", "Humans", "Male", "Models, Neurological", "Models, Theoretical", "Nerve Net", "Neural Networks, Computer", "Neurons", "Nonlinear Dynamics", "Social Behavior", "Synapses"], "AbstractText": [{"section": null, "text": "We propose a mathematical model of a continuous attractor network that controls social behaviors. The model is examined with bifurcation analysis and computer simulations. The results show that the model exhibits stable steady states and thresholds for steady state transitions corresponding to some experimentally observed behaviors, such as aggression control. The performance of the model and the relation with experimental evidence are discussed."}], "ArticleTitle": "Decision-making neural circuits mediating social behaviors : An attractor network model."}, "33595765": {"mesh": ["Animals", "Gyrus Cinguli", "Models, Neurological", "Pain", "Pain Perception", "Rats", "Rats, Sprague-Dawley", "Somatosensory Cortex"], "AbstractText": [{"section": null, "text": "Pain is a complex, multidimensional experience that involves dynamic interactions between sensory-discriminative and affective-emotional processes. Pain experiences have a high degree of variability depending on their context and prior anticipation. Viewing pain perception as a perceptual inference problem, we propose a predictive coding paradigm to characterize evoked and non-evoked pain. We record the local field potentials (LFPs) from the primary somatosensory cortex (S1) and the anterior cingulate cortex (ACC) of freely behaving rats-two regions known to encode the sensory-discriminative and affective-emotional aspects of pain, respectively. We further use predictive coding to investigate the temporal coordination of oscillatory activity between the S1 and ACC. Specifically, we develop a phenomenological predictive coding model to describe the macroscopic dynamics of bottom-up and top-down activity. Supported by recent experimental data, we also develop a biophysical neural mass model to describe the mesoscopic neural dynamics in the S1 and ACC populations, in both naive and chronic pain-treated animals. Our proposed predictive coding models not only replicate important experimental findings, but also provide new prediction about the impact of the model parameters on the physiological or behavioral read-out-thereby yielding mechanistic insight into the uncertainty of expectation, placebo or nocebo effect, and chronic pain."}], "ArticleTitle": "Predictive coding models for pain perception."}, "27480847": {"mesh": ["Action Potentials", "Cochlear Nerve", "Ion Channels", "Models, Neurological", "Neurons", "Potassium Channels"], "AbstractText": [{"section": null, "text": "Neural spike trains are commonly characterized as a Poisson point process. However, the Poisson assumption is a poor model for spiking in auditory nerve fibres because it is known that interspike intervals display positive correlation over long time scales and negative correlation over shorter time scales. We have therefore developed a biophysical model based on the well-known Meddis model of the peripheral auditory system, to produce simulated auditory nerve fibre spiking statistics that more closely match the firing correlations observed in empirical data. We achieve this by introducing biophysically realistic ion channel noise to an inner hair cell membrane potential model that includes fractal fast potassium channels and deterministic slow potassium channels. We succeed in producing simulated spike train statistics that match empirically observed firing correlations. Our model thus replicates macro-scale stochastic spiking statistics in the auditory nerve fibres due to modeling stochasticity at the micro-scale of potassium channels. "}], "ArticleTitle": "Ion channel noise can explain firing correlation in auditory nerves."}, "27909842": {"mesh": ["Action Potentials", "Electronic Data Processing", "Hippocampus", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Spatiotemporal patterns of action potentials are considered to be closely related to information processing in the brain. Auto-generating neurons contributing to these processing tasks are known to cause multifractal behavior in the inter-spike intervals of the output action potentials. In this paper we define a novel relationship between this multifractality and the adaptive Nernst equilibrium in hippocampal neurons. Using this relationship we are able to differentiate between various drugs at varying dosages. Conventional methods limit their ability to account for cellular charge depletion by not including these adaptive Nernst equilibria. Our results provide a new theoretical approach for measuring the effects which drugs have on single-cell dynamics."}], "ArticleTitle": "The relationship between nernst equilibrium variability and the multifractality of interspike intervals in the hippocampus."}, "34003422": {"mesh": ["Adult", "Cerebellar Diseases", "Cerebellum", "Eye Movements", "Female", "Humans", "Models, Neurological", "Nystagmus, Pathologic"], "AbstractText": [{"section": null, "text": "A woman, age 44, with a positive anti-YO paraneoplastic cerebellar syndrome and normal imaging developed an ocular motor disorder including periodic alternating nystagmus (PAN), gaze-evoked nystagmus (GEN) and rebound nystagmus (RN). During fixation there was typical PAN but changes in gaze position evoked complex, time-varying oscillations of GEN and RN. To unravel the pathophysiology of this unusual pattern of nystagmus, we developed a mathematical model of normal function of the circuits mediating the vestibular-ocular reflex and gaze-holding including their adaptive mechanisms. Simulations showed that all the findings of our patient could be explained by two, small, isolated changes in cerebellar circuits: reducing the time constant of the gaze-holding integrator, producing GEN and RN, and increasing the gain of the vestibular velocity-storage positive feedback loop, producing PAN. We conclude that the gaze- and time-varying pattern of nystagmus in our patient can be accounted for by superposition of one model that produces typical PAN and another model that produces typical GEN and RN, without requiring a new oscillator in the gaze-holding system or a more complex, nonlinear interaction between the two models. This analysis suggest a strategy for uncovering gaze-evoked and rebound nystagmus in the setting of a time-varying nystagmus such as PAN. Our results are also consistent with current ideas of compartmentalization of cerebellar functions for the control of the vestibular velocity-storage mechanism (nodulus and ventral uvula) and for holding horizontal gaze steady (the flocculus and tonsil)."}], "ArticleTitle": "Modeling the interaction among three cerebellar disorders of eye movements: periodic alternating, gaze-evoked and rebound nystagmus."}, "31520248": {"mesh": [], "AbstractText": [{"section": null, "text": "The authors would like to note an omission, in the published paper, of the Matlab code initially included as Electronic Supplementary Material. Therefore, we hereby re-submit the code in question."}], "ArticleTitle": "Correction to: From receptive profiles to a metric model of V1."}, "27085337": {"mesh": ["Action Potentials", "Adaptation, Physiological", "Animals", "Electric Conductivity", "Humans", "Models, Neurological", "Neurons", "Potassium Chloride"], "AbstractText": [{"section": null, "text": "A neuron that is stimulated by rectangular current injections initially responds with a high firing rate, followed by a decrease in the firing rate. This phenomenon is called spike-frequency adaptation and is usually mediated by slow K(+) currents, such as the M-type K(+) current (I M ) or the Ca(2+)-activated K(+) current (I AHP ). It is not clear how the detailed biophysical mechanisms regulate spike generation in a cortical neuron. In this study, we investigated the impact of slow K(+) currents on spike generation mechanism by reducing a detailed conductance-based neuron model. We showed that the detailed model can be reduced to a multi-timescale adaptive threshold model, and derived the formulae that describe the relationship between slow K(+) current parameters and reduced model parameters. Our analysis of the reduced model suggests that slow K(+) currents have a differential effect on the noise tolerance in neural coding."}], "ArticleTitle": "Impact of slow K(+) currents on spike generation can be described by an adaptive threshold model."}, "33904004": {"mesh": ["Action Potentials", "Computer Simulation", "Models, Neurological", "Nerve Net", "Neurons"], "AbstractText": [{"section": null, "text": "We propose a novel phase based analysis with the purpose of quantifying the periodic bursts of activity observed in various neuronal systems. The way bursts are intiated and propagate in a spatial network is still insufficiently characterized. In particular, we investigate here how these spatiotemporal dynamics depend on the mean connection length. We use a simplified description of a neuron's state as a time varying phase between firings. This leads to a definition of network bursts, that does not depend on the practitioner's individual judgment as the usage of subjective thresholds and time scales. This allows both an easy and objective characterization of the bursting dynamics, only depending on system's proper scales. Our approach thus ensures more reliable and reproducible measurements. We here use it to describe the spatiotemporal processes in networks of intrinsically oscillating neurons. The analysis rigorously reveals the role of the mean connectivity length in spatially embedded networks in determining the existence of \"leader\" neurons during burst initiation, a feature incompletely understood observed in several neuronal cultures experiments. The precise definition of a burst with our method allowed us to rigorously characterize the initiation dynamics of bursts and show how it depends on the mean connectivity length. Although presented with simulations, the methodology can be applied to other forms of neuronal spatiotemporal data. As shown in a preliminary study with MEA recordings, it is not limited to in silico modeling."}], "ArticleTitle": "A novel methodology to describe neuronal networks activity reveals spatiotemporal recruitment dynamics of synchronous bursting states."}, "27272510": {"mesh": ["Acoustic Stimulation", "Autism Spectrum Disorder", "Humans", "Models, Neurological", "Speech Perception", "Visual Perception"], "AbstractText": [{"section": null, "text": "One of the most common examples of audiovisual speech integration is the McGurk effect. As an example, an auditory syllable /ba/ recorded over incongruent lip movements that produce \"ga\" typically causes listeners to hear \"da\". This report hypothesizes reasons why certain clinical and listeners who are hard of hearing might be more susceptible to visual influence. Conversely, we also examine why other listeners appear less susceptible to the McGurk effect (i.e., they report hearing just the auditory stimulus without being influenced by the visual). Such explanations are accompanied by a mechanistic explanation of integration phenomena including visual inhibition of auditory information, or slower rate of accumulation of inputs. First, simulations of a linear dynamic parallel interactive model were instantiated using inhibition and facilitation to examine potential mechanisms underlying integration. In a second set of simulations, we systematically manipulated the inhibition parameter values to model data obtained from listeners with autism spectrum disorder. In summary, we argue that cross-modal inhibition parameter values explain individual variability in McGurk perceptibility. Nonetheless, different mechanisms should continue to be explored in an effort to better understand current data patterns in the audiovisual integration literature."}], "ArticleTitle": "Parallel linear dynamic models can mimic the McGurk effect in clinical populations."}, "33712942": {"mesh": ["Animals", "Macaca mulatta", "Models, Neurological", "Neurons", "Saccades", "Superior Colliculi"], "AbstractText": [{"section": null, "text": "The goal of this short review is to call attention to a yawning gap of knowledge that separates two processes essential for saccade production. On the one hand, knowledge about the saccade generation circuitry within the brainstem is detailed and precise - push-pull interactions between gaze-shifting and gaze-holding processes control the time of saccade initiation, which begins when omnipause neurons are inhibited and brainstem burst neurons are excited. On the other hand, knowledge about the cortical and subcortical premotor circuitry accomplishing saccade initiation has crystalized around the concept of stochastic accumulation - the accumulating activity of saccade neurons reaching a fixed value triggers a saccade. Here is the gap: we do not know how the reaching of a threshold by premotor neurons causes the critical pause and burst of brainstem neurons that initiates saccades. Why this problem matters and how it can be addressed will be discussed. Closing the gap would unify two rich but curiously disconnected empirical and theoretical domains."}], "ArticleTitle": "The unknown but knowable relationship between Presaccadic Accumulation of activity and Saccade initiation."}, "27075919": {"mesh": ["Acoustic Stimulation", "Action Potentials", "Algorithms", "Animals", "Auditory Perception", "Humans", "Models, Neurological", "Neuronal Plasticity", "Neurons"], "AbstractText": [{"section": null, "text": "Extracting invariant features in an unsupervised manner is crucial to perform complex computation such as object recognition, analyzing music or understanding speech. While various algorithms have been proposed to perform such a task, Slow Feature Analysis (SFA) uses time as a means of detecting those invariants, extracting the slowly time-varying components in the input signals. In this work, we address the question of how such an algorithm can be implemented by neurons, and apply it in the context of audio stimuli. We propose a projected gradient implementation of SFA that can be adapted to a Hebbian like learning rule dealing with biologically plausible neuron models. Furthermore, we show that a Spike-Timing Dependent Plasticity learning rule, shaped as a smoothed second derivative, implements SFA for spiking neurons. The theory is supported by numerical simulations, and to illustrate a simple use of SFA, we have applied it to auditory signals. We show that a single SFA neuron can learn to extract the tempo in sound recordings."}], "ArticleTitle": "Slow feature analysis with spiking neurons and its application to audio stimuli."}, "33595764": {"mesh": ["Animals", "Face", "Models, Neurological", "Neural Networks, Computer", "Neurons", "Photic Stimulation", "Primates", "Temporal Lobe"], "AbstractText": [{"section": null, "text": "Feed-forward deep neural networks have better performance in object categorization tasks than other models of computer vision. To understand the relationship between feed-forward deep networks and the primate brain, we investigated representations of upright and inverted faces in a convolutional deep neural network model and compared them with representations by neurons in the monkey anterior inferior-temporal cortex, area TE. We applied principal component analysis to feature vectors in each model layer to visualize the relationship between the vectors of the upright and inverted faces. The vectors of the upright and inverted monkey faces were more separated through the convolution layers. In the fully-connected layers, the separation among human individuals for upright faces was larger than for inverted faces. The Spearman correlation between each model layer and TE neurons reached a maximum at the fully-connected layers. These results indicate that the processing of faces in the fully-connected layers might resemble the asymmetric representation of upright and inverted faces by the TE neurons. The separation of upright and inverted faces might take place by feed-forward processing in the visual cortex, and separations among human individuals for upright faces, which were larger than those for inverted faces, might occur in area TE."}], "ArticleTitle": "Comparison of neuronal responses in primate inferior-temporal cortex and feed-forward deep neural network model with regard to information processing of faces."}, "33826050": {"mesh": ["Algorithms", "Animals", "Models, Neurological", "Neurons", "Zebrafish"], "AbstractText": [{"section": null, "text": "An inverse procedure is developed and tested to recover functional and structural information from global signals of brains activity. The method assumes a leaky-integrate and fire model with excitatory and inhibitory neurons, coupled via a directed network. Neurons are endowed with a heterogenous current value, which sets their associated dynamical regime. By making use of a heterogenous mean-field approximation, the method seeks to reconstructing from global activity patterns the distribution of in-coming degrees, for both excitatory and inhibitory neurons, as well as the distribution of the assigned currents. The proposed inverse scheme is first validated against synthetic data. Then, time-lapse acquisitions of a zebrafish larva recorded with a two-photon light sheet microscope are used as an input to the reconstruction algorithm. A power law distribution of the in-coming connectivity of the excitatory neurons is found. Local degree distributions are also computed by segmenting the whole brain in sub-regions traced from annotated atlas."}], "ArticleTitle": "Reconstruction scheme for excitatory and inhibitory dynamics with quenched disorder: application to zebrafish imaging."}, "33839988": {"mesh": ["Models, Neurological", "Saccades"], "AbstractText": [{"section": null, "text": "Voluntary rapid eye movements (saccades) redirect the fovea toward objects of visual interest. The saccadic system can be considered as a dual-mode system: in one mode the eye is fixating, in the other it is making a saccade. In this review, we consider two examples of dysfunctional saccades, interrupted saccades in late-onset Tay-Sachs disease and gaze-position dependent opsoclonus after concussion, which fail to properly shift between fixation and saccade modes. Insights and benefits gained from bi-directional collaborative exchange between clinical and basic scientists are emphasized. In the case of interrupted saccades, existing mathematical models were sufficiently detailed to provide support for the cause of interrupted saccades. In the case of gaze-position dependent opsoclonus, existing models could not explain the behavior, but further development provided a reasonable hypothesis for the mechanism underlying the behavior. Collaboration between clinical and basic science is a rich source of progress for developing biologically plausible models and understanding neurological disease. Approaching a clinical problem with a specific hypothesis (model) in mind often prompts new experimental tests and provides insights into basic mechanisms."}], "ArticleTitle": "Dysfunctional mode switching between fixation and saccades: collaborative insights into two unusual clinical disorders."}, "32372937": {"mesh": [], "AbstractText": [{"section": null, "text": "Attention is the important ability to flexibly control limited computational resources. It has been studied in conjunction with many other topics in neuroscience and psychology including awareness, vigilance, saliency, executive control, and learning. It has also recently been applied in several domains in machine learning. The relationship between the study of biological attention and its use as a tool to enhance artificial neural networks is not always clear. This review starts by providing an overview of how attention is conceptualized in the neuroscience and psychology literature. It then covers several use cases of attention in machine learning, indicating their biological counterparts where they exist. Finally, the ways in which artificial attention can be further inspired by biology for the production of complex and integrative systems is explored."}], "ArticleTitle": "Attention in Psychology, Neuroscience, and Machine Learning."}, "32848684": {"mesh": [], "AbstractText": [{"section": null, "text": "Recent advances in artificial intelligence (AI) and neuroscience are impressive. In AI, this includes the development of computer programs that can beat a grandmaster at GO or outperform human radiologists at cancer detection. A great deal of these technological developments are directly related to progress in artificial neural networks-initially inspired by our knowledge about how the brain carries out computation. In parallel, neuroscience has also experienced significant advances in understanding the brain. For example, in the field of spatial navigation, knowledge about the mechanisms and brain regions involved in neural computations of cognitive maps-an internal representation of space-recently received the Nobel Prize in medicine. Much of the recent progress in neuroscience has partly been due to the development of technology used to record from very large populations of neurons in multiple regions of the brain with exquisite temporal and spatial resolution in behaving animals. With the advent of the vast quantities of data that these techniques allow us to collect there has been an increased interest in the intersection between AI and neuroscience, many of these intersections involve using AI as a novel tool to explore and analyze these large data sets. However, given the common initial motivation point-to understand the brain-these disciplines could be more strongly linked. Currently much of this potential synergy is not being realized. We propose that spatial navigation is an excellent area in which these two disciplines can converge to help advance what we know about the brain. In this review, we first summarize progress in the neuroscience of spatial navigation and reinforcement learning. We then turn our attention to discuss how spatial navigation has been modeled using descriptive, mechanistic, and normative approaches and the use of AI in such models. Next, we discuss how AI can advance neuroscience, how neuroscience can advance AI, and the limitations of these approaches. We finally conclude by highlighting promising lines of research in which spatial navigation can be the point of intersection between neuroscience and AI and how this can contribute to the advancement of the understanding of intelligent behavior."}], "ArticleTitle": "The Neuroscience of Spatial Navigation and the Relationship to Artificial Intelligence."}, "31680923": {"mesh": [], "AbstractText": [{"section": null, "text": "Alzheimer's disease (AD), including its mild cognitive impairment (MCI) phase that may or may not progress into the AD, is the most ordinary form of dementia. It is extremely important to correctly identify patients during the MCI stage because this is the phase where AD may or may not develop. Thus, it is crucial to predict outcomes during this phase. Thus far, many researchers have worked on only using a single modality of a biomarker for the diagnosis of AD or MCI. Although recent studies show that a combination of one or more different biomarkers may provide complementary information for the diagnosis, it also increases the classification accuracy distinguishing between different groups. In this paper, we propose a novel machine learning-based framework to discriminate subjects with AD or MCI utilizing a combination of four different biomarkers: fluorodeoxyglucose positron emission tomography (FDG-PET), structural magnetic resonance imaging (sMRI), cerebrospinal fluid (CSF) protein levels, and Apolipoprotein-E (APOE) genotype. The Alzheimer's Disease Neuroimaging Initiative (ADNI) baseline dataset was used in this study. In total, there were 158 subjects for whom all four modalities of biomarker were available. Of the 158 subjects, 38 subjects were in the AD group, 82 subjects were in MCI groups (including 46 in MCIc [MCI converted; conversion to AD within 24 months of time period], and 36 in MCIs [MCI stable; no conversion to AD within 24 months of time period]), and the remaining 38 subjects were in the healthy control (HC) group. For each image, we extracted 246 regions of interest (as features) using the Brainnetome template image and NiftyReg toolbox, and later we combined these features with three CSF and two APOE genotype features obtained from the ADNI website for each subject using early fusion technique. Here, a different kernel-based multiclass support vector machine (SVM) classifier with a grid-search method was applied. Before passing the obtained features to the classifier, we have used truncated singular value decomposition (Truncated SVD) dimensionality reduction technique to reduce high dimensional features into a lower-dimensional feature. As a result, our combined method achieved an area under the receiver operating characteristic (AU-ROC) curve of 98.33, 93.59, 96.83, 94.64, 96.43, and 95.24% for AD vs. HC, MCIs vs. MCIc, AD vs. MCIs, AD vs. MCIc, HC vs. MCIc, and HC vs. MCIs subjects which are high relative to single modality results and other state-of-the-art approaches. Moreover, combined multimodal methods have improved the classification performance over the unimodal classification."}], "ArticleTitle": "Prediction and Classification of Alzheimer's Disease Based on Combined Features From Apolipoprotein-E Genotype, Cerebrospinal Fluid, MR, and FDG-PET Imaging Biomarkers."}, "29184491": {"mesh": [], "AbstractText": [{"section": null, "text": "Aesthetics has been the subject of long-standing debates by philosophers and psychologists alike. In psychology, it is generally agreed that aesthetic experience results from an interaction between perception, cognition, and emotion. By experimental means, this triad has been studied in the field of experimental aesthetics, which aims to gain a better understanding of how aesthetic experience relates to fundamental principles of human visual perception and brain processes. Recently, researchers in computer vision have also gained interest in the topic, giving rise to the field of computational aesthetics. With computing hardware and methodology developing at a high pace, the modeling of perceptually relevant aspect of aesthetic stimuli has a huge potential. In this review, we present an overview of recent developments in computational aesthetics and how they relate to experimental studies. In the first part, we cover topics such as the prediction of ratings, style and artist identification as well as computational methods in art history, such as the detection of influences among artists or forgeries. We also describe currently used computational algorithms, such as classifiers and deep neural networks. In the second part, we summarize results from the field of experimental aesthetics and cover several isolated image properties that are believed to have a effect on the aesthetic appeal of visual stimuli. Their relation to each other and to findings from computational aesthetics are discussed. Moreover, we compare the strategies in the two fields of research and suggest that both fields would greatly profit from a joined research effort. We hope to encourage researchers from both disciplines to work more closely together in order to understand visual aesthetics from an integrated point of view."}], "ArticleTitle": "Computational and Experimental Approaches to Visual Aesthetics."}, "29670517": {"mesh": [], "AbstractText": [{"section": null, "text": "The computational neuroscience field has heavily concentrated on the modeling of neuronal functions, largely ignoring other brain cells, including one type of glial cell, the astrocytes. Despite the short history of modeling astrocytic functions, we were delighted about the hundreds of models developed so far to study the role of astrocytes, most often in calcium dynamics, synchronization, information transfer, and plasticity in vitro, but also in vascular events, hyperexcitability, and homeostasis. Our goal here is to present the state-of-the-art in computational modeling of astrocytes in order to facilitate better understanding of the functions and dynamics of astrocytes in the brain. Due to the large number of models, we concentrated on a hundred models that include biophysical descriptions for calcium signaling and dynamics in astrocytes. We categorized the models into four groups: single astrocyte models, astrocyte network models, neuron-astrocyte synapse models, and neuron-astrocyte network models to ease their use in future modeling projects. We characterized the models based on which earlier models were used for building the models and which type of biological entities were described in the astrocyte models. Features of the models were compared and contrasted so that similarities and differences were more readily apparent. We discovered that most of the models were basically generated from a small set of previously published models with small variations. However, neither citations to all the previous models with similar core structure nor explanations of what was built on top of the previous models were provided, which made it possible, in some cases, to have the same models published several times without an explicit intention to make new predictions about the roles of astrocytes in brain functions. Furthermore, only a few of the models are available online which makes it difficult to reproduce the simulation results and further develop the models. Thus, we would like to emphasize that only via reproducible research are we able to build better computational models for astrocytes, which truly advance science. Our study is the first to characterize in detail the biophysical and biochemical mechanisms that have been modeled for astrocytes."}], "ArticleTitle": "Computational Models for Calcium-Mediated Astrocyte Functions."}, "29375355": {"mesh": [], "AbstractText": [{"section": null, "text": "New developments in AI and neuroscience are revitalizing the quest to understanding natural intelligence, offering insight about how to equip machines with human-like capabilities. This paper reviews some of the computational principles relevant for understanding natural intelligence and, ultimately, achieving strong AI. After reviewing basic principles, a variety of computational modeling approaches is discussed. Subsequently, I concentrate on the use of artificial neural networks as a framework for modeling cognitive processes. This paper ends by outlining some of the challenges that remain to fulfill the promise of machines that show human-like intelligence."}], "ArticleTitle": "Computational Foundations of Natural Intelligence."}, "31572152": {"mesh": [], "AbstractText": [{"section": null, "text": "Stochastic Resonance (SR) and Coherence Resonance (CR) are non-linear phenomena, in which an optimal amount of noise maximizes an objective function, such as the sensitivity for weak signals in SR, or the coherence of stochastic oscillations in CR. Here, we demonstrate a related phenomenon, which we call \"Recurrence Resonance\" (RR): noise can also improve the information flux in recurrent neural networks. In particular, we show for the case of three-neuron motifs with ternary connection strengths that the mutual information between successive network states can be maximized by adding a suitable amount of noise to the neuron inputs. This striking result suggests that noise in the brain may not be a problem that needs to be suppressed, but indeed a resource that is dynamically regulated in order to optimize information processing."}], "ArticleTitle": "Recurrence Resonance\" in Three-Neuron Motifs."}, "31920608": {"mesh": [], "AbstractText": [{"section": null, "text": "Data augmentation is a popular technique which helps improve generalization capabilities of deep neural networks, and can be perceived as implicit regularization. It plays a pivotal role in scenarios in which the amount of high-quality ground-truth data is limited, and acquiring new examples is costly and time-consuming. This is a very common problem in medical image analysis, especially tumor delineation. In this paper, we review the current advances in data-augmentation techniques applied to magnetic resonance images of brain tumors. To better understand the practical aspects of such algorithms, we investigate the papers submitted to the Multimodal Brain Tumor Segmentation Challenge (BraTS 2018 edition), as the BraTS dataset became a standard benchmark for validating existent and emerging brain-tumor detection and segmentation techniques. We verify which data augmentation approaches were exploited and what was their impact on the abilities of underlying supervised learners. Finally, we highlight the most promising research directions to follow in order to synthesize high-quality artificial brain-tumor examples which can boost the generalization abilities of deep models."}], "ArticleTitle": "Data Augmentation for Brain-Tumor Segmentation: A Review."}, "31456677": {"mesh": [], "AbstractText": [{"section": null, "text": "Artificial neural networks (ANNs) are important building blocks in technical applications. They rely on noiseless continuous signals in stark contrast to the discrete action potentials stochastically exchanged among the neurons in real brains. We propose to bridge this gap with Spike-by-Spike (SbS) networks which represent a compromise between non-spiking and spiking versions of generative models. What is missing, however, are algorithms for finding weight sets that would optimize the output performances of deep SbS networks with many layers. Here, a learning rule for feed-forward SbS networks is derived. The properties of this approach are investigated and its functionality is demonstrated by simulations. In particular, a Deep Convolutional SbS network for classifying handwritten digits achieves a classification performance of roughly 99.3% on the MNIST test data when the learning rule is applied together with an optimizer. Thereby it approaches the benchmark results of ANNs without extensive parameter optimization. We envision this learning rule for SBS networks to provide a new basis for research in neuroscience and for technical applications, especially when they become implemented on specialized computational hardware."}], "ArticleTitle": "Back-Propagation Learning in Deep Spike-By-Spike Networks."}, "32038212": {"mesh": [], "AbstractText": [{"section": null, "text": "Neural processing of sounds in the dorsal and ventral streams of the (human) auditory cortex is optimized for analyzing fine-grained temporal and spectral information, respectively. Here we use a Wilson and Cowan firing-rate modeling framework to simulate spectro-temporal processing of sounds in these auditory streams and to investigate the link between neural population activity and behavioral results of psychoacoustic experiments. The proposed model consisted of two core (A1 and R, representing primary areas) and two belt (Slow and Fast, representing rostral and caudal processing respectively) areas, differing in terms of their spectral and temporal response properties. First, we simulated the responses to amplitude modulated (AM) noise and tones. In agreement with electrophysiological results, we observed an area-dependent transition from a temporal (synchronization) to a rate code when moving from low to high modulation rates. Simulated neural responses in a task of amplitude modulation detection suggested that thresholds derived from population responses in core areas closely resembled those of psychoacoustic experiments in human listeners. For tones, simulated modulation threshold functions were found to be dependent on the carrier frequency. Second, we simulated the responses to complex tones with missing fundamental stimuli and found that synchronization of responses in the Fast area accurately encoded pitch, with the strength of synchronization depending on number and order of harmonic components. Finally, using speech stimuli, we showed that the spectral and temporal structure of the speech was reflected in parallel by the modeled areas. The analyses highlighted that the Slow stream coded with high spectral precision the aspects of the speech signal characterized by slow temporal changes (e.g., prosody), while the Fast stream encoded primarily the faster changes (e.g., phonemes, consonants, temporal pitch). Interestingly, the pitch of a speaker was encoded both spatially (i.e., tonotopically) in Slow area and temporally in Fast area. Overall, performed simulations showed that the model is valuable for generating hypotheses on how the different cortical areas/streams may contribute toward behaviorally relevant aspects of auditory processing. The model can be used in combination with physiological models of neurovascular coupling to generate predictions for human functional MRI experiments."}], "ArticleTitle": "Spectro-Temporal Processing in a Two-Stream Computational Model of Auditory Cortex."}, "31057385": {"mesh": [], "AbstractText": [{"section": null, "text": "Nature exhibits countless examples of adaptive networks, whose topology evolves constantly coupled with the activity due to its function. The brain is an illustrative example of a system in which a dynamic complex network develops by the generation and pruning of synaptic contacts between neurons while memories are acquired and consolidated. Here, we consider a recently proposed brain developing model to study how mechanisms responsible for the evolution of brain structure affect and are affected by memory storage processes. Following recent experimental observations, we assume that the basic rules for adding and removing synapses depend on local synaptic currents at the respective neurons in addition to global mechanisms depending on the mean connectivity. In this way a feedback loop between \"form\" and \"function\" spontaneously emerges that influences the ability of the system to optimally store and retrieve sensory information in patterns of brain activity or memories. In particular, we report here that, as a consequence of such a feedback-loop, oscillations in the activity of the system among the memorized patterns can occur, depending on parameters, reminding mind dynamical processes. Such oscillations have their origin in the destabilization of memory attractors due to the pruning dynamics, which induces a kind of structural disorder or noise in the system at a long-term scale. This constantly modifies the synaptic disorder induced by the interference among the many patterns of activity memorized in the system. Such new intriguing oscillatory behavior is to be associated only to long-term synaptic mechanisms during the network evolution dynamics, and it does not depend on short-term synaptic processes, as assumed in other studies, that are not present in our model."}], "ArticleTitle": "How Memory Conforms to Brain Development."}, "33071765": {"mesh": [], "AbstractText": [{"section": null, "text": "Even the simplest cognitive processes involve interactions between cortical regions. To study these processes, we usually rely on averaging across several repetitions of a task or across long segments of data to reach a statistically valid conclusion. Neuronal oscillations reflect synchronized excitability fluctuations in ensembles of neurons and can be observed in electrophysiological recordings in the presence or absence of an external stimulus. Oscillatory brain activity has been viewed as sustained increase in power at specific frequency bands. However, this perspective has been challenged in recent years by the notion that oscillations may occur as transient burst-like events that occur in individual trials and may only appear as sustained activity when multiple trials are averaged together. In this review, we examine the idea that oscillatory activity can manifest as a transient burst as well as a sustained increase in power. We discuss the technical challenges involved in the detection and characterization of transient events at the single trial level, the mechanisms that might generate them and the features that can be extracted from these events to study single-trial dynamics of neuronal ensemble activity."}], "ArticleTitle": "Oscillatory Bursting as a Mechanism for Temporal Coupling and Information Coding."}, "31632260": {"mesh": [], "AbstractText": [{"section": null, "text": "Statistical learning is a learning mechanism based on transition probability in sequences such as music and language. Recent computational and neurophysiological studies suggest that the statistical learning contributes to production, action, and musical creativity as well as prediction and perception. The present study investigated how statistical structure interacts with tonalities in music based on various-order statistical models. To verify this in all 24 major and minor keys, the transition probabilities of the sequences containing the highest pitches in Bach's Well-Tempered Clavier, which is a collection of two series (No. 1 and No. 2) of preludes and fugues in all of the 24 major and minor keys, were calculated based on nth-order Markov models. The transition probabilities of each sequence were compared among tonalities (major and minor), two series (No. 1 and No. 2), and music types (prelude and fugue). The differences in statistical characteristics between major and minor keys were detected in lower- but not higher-order models. The results also showed that statistical knowledge in music might be modulated by tonalities and composition periods. Furthermore, the principal component analysis detected the shared components of related keys, suggesting that the tonalities modulate statistical characteristics in music. The present study may suggest that there are at least two types of statistical knowledge in music that are interdependent on and independent of tonality, respectively."}], "ArticleTitle": "Tonality Tunes the Statistical Characteristics in Music: Computational Approaches on Statistical Learning."}, "30745868": {"mesh": [], "AbstractText": [{"section": null, "text": "It is hypothesized that cortical neuronal circuits operate in a global balanced state, i.e., the majority of neurons fire irregularly by receiving balanced inputs of excitation and inhibition. Meanwhile, it has been observed in experiments that sensory information is often sparsely encoded by only a small set of firing neurons, while neurons in the rest of the network are silent. The phenomenon of sparse coding challenges the hypothesis of a global balanced state in the brain. To reconcile this, here we address the issue of whether a balanced state can exist in a small number of firing neurons by taking account of the heterogeneity of network structure such as scale-free and small-world networks. We propose necessary conditions and show that, under these conditions, for sparsely but strongly connected heterogeneous networks with various types of single-neuron dynamics, despite the fact that the whole network receives external inputs, there is a small active subnetwork (active core) inherently embedded within it. The neurons in this active core have relatively high firing rates while the neurons in the rest of the network are quiescent. Surprisingly, although the whole network is heterogeneous and unbalanced, the active core possesses a balanced state and its connectivity structure is close to a homogeneous Erd&#246;s-R&#233;nyi network. The dynamics of the active core can be well-predicted using the Fokker-Planck equation. Our results suggest that the balanced state may be maintained by a small group of spiking neurons embedded in a large heterogeneous network in the brain. The existence of the small active core reconciles the balanced state and the sparse coding, and also provides a potential dynamical scenario underlying sparse coding in neuronal networks."}], "ArticleTitle": "Balanced Active Core in Heterogeneous Neuronal Networks."}, "31191281": {"mesh": [], "AbstractText": [{"section": null, "text": "Today, face biometric systems are becoming widely accepted as a standard method for identity authentication in many security settings. For example, their deployment in automated border control gates plays a crucial role in accurate document authentication and reduced traveler flow rates in congested border zones. The proliferation of such systems is further spurred by the advent of portable devices. On the one hand, modern smartphone and tablet cameras have in-built user authentication applications while on the other hand, their displays are being consistently exploited for face spoofing. Similar to biometric systems of other physiological biometric identifiers, face biometric systems have their own unique set of potential vulnerabilities. In this work, these vulnerabilities (presentation attacks) are being explored via a biologically-inspired presentation attack detection model which is termed \"BIOPAD.\" Our model employs Gabor features in a feedforward hierarchical structure of layers that progressively process and train from visual information of people's faces, along with their presentation attacks, in the visible and near-infrared spectral regions. BIOPAD's performance is directly compared with other popular biologically-inspired layered models such as the \"Hierarchical Model And X\" (HMAX) that applies similar handcrafted features, and Convolutional Neural Networks (CNN) that discover low-level features through stochastic descent training. BIOPAD shows superior performance to both HMAX and CNN in all of the three presentation attack databases examined and these results were consistent in two different classifiers (Support Vector Machine and k-nearest neighbor). In certain cases, our findings have shown that BIOPAD can produce authentication rates with 99% accuracy. Finally, we further introduce a new presentation attack database with visible and near-infrared information for direct comparisons. Overall, BIOPAD's operation, which is to fuse information from different spectral bands at both feature and score levels for the purpose of face presentation attack detection, has never been attempted before with a biologically-inspired algorithm. Obtained detection rates are promising and confirm that near-infrared visual information significantly assists in overcoming presentation attacks."}], "ArticleTitle": "Bio-Inspired Presentation Attack Detection for Face Biometrics."}, "32256331": {"mesh": [], "AbstractText": [{"section": null, "text": "Hodgkin-Huxley (HH) model has been one of the most successful electrical interpretation of nerve membrane which led to revolutions in the field of computational neuroscience. On the contrary, experimental observations indicate that, an Action Potential (AP) is accompanied with certain physiological changes in the nerve membrane such as, production and absorption of heat; variation of axon diameter, pressure and length. Although, in the early 1900's a Pressure Wave Theory was proposed by E. Wilke, but, due to lack of sophisticated experimental techniques it was left uncharted. Until recently, when Heimburg-Jackson, Hady-Machta and Rvachev, independently proposed Soliton Theory (thermodynamic interpretation of nerve membrane), Mechanical Surface Waves theory (electro-mechanical interpretation) and Rvachev Model (mechano-electrical activation of voltage gated sodium ion channels) respectively; encouraging a deviation from the traditional HH interpretation with justification for the physical changes in the nerve membrane observed experimentally. But, these theories lead to a \"hit and miss\" scenario because, they do explain certain features (increase/decrease in axon diameter) but miss to explain, correlation between the strength of stimuli and spike rate of AP. Bio-physical models of nerve membrane are thus important for enhancing our understanding regarding the governing dynamics of neural activities encompassing the experimental observations. A novel theory is proposed here which, unravels vortex ring formation due to ion currents in the intracellular and extracellular region leading to variation of pressure causing the increment/decrement in axon diameter. These formations manifest as membrane oscillations which are used to establish a correlation between the strength of stimuli and spike rate of AP. The theory proposed in this paper, brings a paradigm shift in our understanding of neural dynamics from a thorough bio-physical and physiological perspective with promising applications."}], "ArticleTitle": "Action Potential: A Vortex Phenomena; Driving Membrane Oscillations."}, "31178710": {"mesh": [], "AbstractText": [{"section": null, "text": "Previous studies have shown that the auditory cortex can enhance the perception of behaviorally important sounds in the presence of background noise, but the mechanisms by which it does this are not yet elucidated. Rapid plasticity of spectrotemporal receptive fields (STRFs) in the primary (A1) cortical neurons is observed during behavioral tasks that require discrimination of particular sounds. This rapid task-related change is believed to be one of the processing strategies utilized by the auditory cortex to selectively attend to one stream of sound in the presence of mixed sounds. However, the mechanism by which the brain evokes this rapid plasticity in the auditory cortex remains unclear. This paper uses a neural network model to investigate how synaptic transmission within the cortical neuron network can change the receptive fields of individual neurons. A sound signal was used as input to a model of the cochlea and auditory periphery, which activated or inhibited integrate-and-fire neuron models to represent networks in the primary auditory cortex. Each neuron in the network was tuned to a different frequency. All neurons were interconnected with excitatory or inhibitory synapses of varying strengths. Action potentials in one of the model neurons were used to calculate the receptive field using reverse correlation. The results were directly compared to previously recorded electrophysiological data from ferrets performing behavioral tasks that require discrimination of particular sounds. The neural network model could reproduce complex STRFs observed experimentally through optimizing the synaptic weights in the model. The model predicts that altering synaptic drive between cortical neurons and/or bottom-up synaptic drive from the cochlear model to the cortical neurons can account for rapid task-related changes observed experimentally in A1 neurons. By identifying changes in the synaptic drive during behavioral tasks, the model provides insights into the neural mechanisms utilized by the auditory cortex to enhance the perception of behaviorally salient sounds."}], "ArticleTitle": "Computational Neural Modeling of Auditory Cortical Receptive Fields."}, "32194390": {"mesh": [], "AbstractText": [{"section": null, "text": "Synchronization of neural activity across brain regions is critical to processes that include perception, learning, and memory. After traumatic brain injury (TBI), neuronal degeneration is one possible effect and can alter communication between neural circuits. Consequently, synchronization between neurons may change and can contribute to both lasting changes in functional brain networks and cognitive impairment in patients. However, fundamental principles relating exactly how TBI at the cellular scale affects synchronization of mesoscale circuits are not well understood. In this work, we use computational networks of Izhikevich integrate-and-fire neurons to study synchronized, oscillatory activity between clusters of neurons, which also adapt according to spike-timing-dependent plasticity (STDP). We study how the connections within and between these neuronal clusters change as unidirectional connections form between the two neuronal populations. In turn, we examine how neuronal deletion, intended to mimic the temporary or permanent loss of neurons in the mesoscale circuit, affects these dynamics. We determine synchronization of two neuronal circuits requires very modest connectivity between these populations; approximately 10% of neurons projecting from one circuit to another circuit will result in high synchronization. In addition, we find that synchronization level inversely affects the strength of connection between neuronal microcircuits - moderately synchronized microcircuits develop stronger intercluster connections than do highly synchronized circuits. Finally, we find that highly synchronized circuits are largely protected against the effects of neuronal deletion but may display changes in frequency properties across circuits with targeted neuronal loss. Together, our results suggest that strongly and weakly connected regions differ in their inherent resilience to damage and may serve different roles in a larger network."}], "ArticleTitle": "Neuronal Degeneration Impairs Rhythms Between Connected Microcircuits."}, "31354463": {"mesh": [], "AbstractText": [{"section": null, "text": "Studying and understanding human brain structures and functions have become one of the most challenging issues in neuroscience today. However, the mammalian nervous system is made up of hundreds of millions of neurons and billions of synapses. This complexity made it impossible to reconstruct such a huge nervous system in the laboratory. So, most researchers focus on C. elegans neural network. The C. elegans neural network is the only biological neural network that is fully mapped. This nervous system is the simplest neural network that exists. However, many fundamental behaviors like movement emerge from this basic network. These features made C. elegans a convenient case to study the nervous systems. Many studies try to propose a network formation model for C. elegans neural network. However, these studies could not meet all characteristics of C. elegans neural network, such as significant factors that play a role in the formation of C. elegans neural network. Thus, new models are needed to be proposed in order to explain all aspects of C. elegans neural network. In this paper, a new model based on game theory is proposed in order to understand the factors affecting the formation of nervous systems, which meet the C. elegans frontal neural network characteristics. In this model, neurons are considered to be agents. The strategy for each neuron includes either making or removing links to other neurons. After choosing the basic network, the utility function is built using structural and functional factors. In order to find the coefficients for each of these factors, linear programming is used. Finally, the output network is compared with C. elegans frontal neural network and previous models. The results implicate that the game-theoretical model proposed in this paper can better predict the influencing factors in the formation of C. elegans neural network compared to previous models."}], "ArticleTitle": "A Game-Theoretical Network Formation Model for C. elegans Neural Network."}, "30034331": {"mesh": [], "AbstractText": [{"section": null, "text": "Everyday human behavior relies upon extraordinary feats of coordination within the brain. In this perspective paper, we argue that the rich temporal structure of music provides an informative context in which to investigate how the brain coordinates its complex activities in time, and how that coordination can be disrupted. We bring insights from the neuroscience of musical rhythm to considerations of timing deficits in Attention Deficit/Hyperactivity Disorder (ADHD), highlighting the significant overlap between neural systems involved in processing musical rhythm and those implicated in ADHD. We suggest that timing deficits warrant closer investigation since they could lead to the identification of potentially informative phenotypes, tied to neurobiological and genetic factors. Our novel interdisciplinary approach builds upon recent trends in both fields of research: in the neuroscience of rhythm, an increasingly nuanced understanding of the specific contributions of neural systems to rhythm processing, and in ADHD, an increasing focus on differentiating phenotypes and identifying distinct etiological pathways associated with the disorder. Finally, we consider the impact of musical experience on rhythm processing and the potential value of musical rhythm in therapeutic interventions."}], "ArticleTitle": "Timing Deficits in ADHD: Insights From the Neuroscience of Musical Rhythm."}, "32477089": {"mesh": [], "AbstractText": [{"section": null, "text": "Historically, neuroscience principles have heavily influenced artificial intelligence (AI), for example the influence of the perceptron model, essentially a simple model of a biological neuron, on artificial neural networks. More recently, notable recent AI advances, for example the growing popularity of reinforcement learning, often appear more aligned with cognitive neuroscience or psychology, focusing on function at a relatively abstract level. At the same time, neuroscience stands poised to enter a new era of large-scale high-resolution data and appears more focused on underlying neural mechanisms or architectures that can, at times, seem rather removed from functional descriptions. While this might seem to foretell a new generation of AI approaches arising from a deeper exploration of neuroscience specifically for AI, the most direct path for achieving this is unclear. Here we discuss cultural differences between the two fields, including divergent priorities that should be considered when leveraging modern-day neuroscience for AI. For example, the two fields feed two very different applications that at times require potentially conflicting perspectives. We highlight small but significant cultural shifts that we feel would greatly facilitate increased synergy between the two fields."}], "ArticleTitle": "Crossing the Cleft: Communication Challenges Between Neuroscience and Artificial Intelligence."}, "31551743": {"mesh": [], "AbstractText": [{"section": null, "text": "Disrupting the pathological synchronous firing patterns of neurons with high frequency stimulation is a common treatment for Parkinsonian symptoms and epileptic seizures when pharmaceutical drugs fail. In this paper, our goal is to design a desynchronization strategy for large networks of spiking neurons such that the neuronal activity of the network remains in the desynchronized regime for a long period of time after the removal of the stimulation. We develop a novel \"Forced Temporal-Spike Time Stimulation (FTSTS)\" strategy that harnesses the spike-timing dependent plasticity to control the synchronization of neural activity in the network by forcing the neurons in the network to artificially fire in a specific temporal pattern. Our strategy modulates the synaptic strengths of selective synapses to achieve a desired synchrony of neural activity in the network. Our simulation results show that the FTSTS strategy can effectively synchronize or desynchronize neural activity in large spiking neuron networks and keep them in the desired state for a long period of time after the removal of the external stimulation. Using simulations, we demonstrate the robustness of our strategy in desynchronizing neural activity of networks against uncertainties in the designed stimulation pulses and network parameters. Additionally, we show in simulation, how our strategy could be incorporated within the existing desynchronization strategies to improve their overall efficacy in desynchronizing large networks. Our proposed strategy provides complete control over the synchronization of neurons in large networks and can be used to either synchronize or desynchronize neural activity based on specific applications. Moreover, it can be incorporated within other desynchronization strategies to improve the efficacy of existing therapies for numerous neurological and psychiatric disorders associated with pathological synchronization."}], "ArticleTitle": "Controlling Synchronization of Spiking Neuronal Networks by Harnessing Synaptic Plasticity."}, "33013340": {"mesh": [], "AbstractText": [{"section": null, "text": "To understand the function of the neocortex, which is a hierarchical distributed network, it is useful giving meaning to the signals transmitted between these areas from the computational viewpoint. The overall anatomical structure or organs related to this network, including the neocortex, thalamus, and basal ganglia, has been roughly revealed, and much physiological knowledge, though often fragmentary, is being accumulated. The computational theories involving the neocortex have also been developed considerably. By introducing the assumption \"The signals transmitted by interarea axonal projections of pyramidal cells in the neocortex carry different meanings for each cell type, common to all areas,\" derived from its nature as a distributed network in the neocortex, allows us to specify the computational meanings of interarea signals. In this paper, first, the types of signals exchanged between neocortical areas are investigated, taking into account biological constraints, and employing theories such as predictive coding, reinforcement learning, representation emulation theory, and BDI logic as theoretical starting points, two types of feedforward signals (observation and deviation) and three types of feedback signals (prediction, plan, and intention) are identified. Next, based on the anatomical knowledge of the neocortex and thalamus, the pathways connecting the areas are organized and summarized as three corticocortical pathways and two thalamocortical pathways. Using this summation as preparation, this paper proposes a hypothesis that gives meaning to each type of signals transmitted in the different pathways in the neocortex, from the viewpoint of their functions. This hypothesis reckons that the feedforward corticocortical pathway transmits observation signals, the feedback corticocortical pathway transmits prediction signals, and the corticothalamic pathway mediated by core relay cells transmits deviation signals. The thalamocortical pathway, which is mediated by matrix relay cells, would be responsible for transmitting the signals that activate a part of prediction signals as intentions, due to the reason that the nature of the other available feedback pathways are not sufficient for conveying plans and intentions as signals. The corticocortical pathway, which is projected from various IT cells to the first layer, would be responsible for transmitting signals that activate a part of prediction signals as plans."}], "ArticleTitle": "Revealing the Computational Meaning of Neocortical Interarea Signals."}, "32009924": {"mesh": [], "AbstractText": [{"section": null, "text": "Two strikingly distinct types of activity have been observed in various brain structures during delay periods of delayed response tasks: Persistent activity (PA), in which a sub-population of neurons maintains an elevated firing rate throughout an entire delay period; and Sequential activity (SA), in which sub-populations of neurons are activated sequentially in time. It has been hypothesized that both types of dynamics can be \"learned\" by the relevant networks from the statistics of their inputs, thanks to mechanisms of synaptic plasticity. However, the necessary conditions for a synaptic plasticity rule and input statistics to learn these two types of dynamics in a stable fashion are still unclear. In particular, it is unclear whether a single learning rule is able to learn both types of activity patterns, depending on the statistics of the inputs driving the network. Here, we first characterize the complete bifurcation diagram of a firing rate model of multiple excitatory populations with an inhibitory mechanism, as a function of the parameters characterizing its connectivity. We then investigate how an unsupervised temporally asymmetric Hebbian plasticity rule shapes the dynamics of the network. Consistent with previous studies, we find that for stable learning of PA and SA, an additional stabilization mechanism is necessary. We show that a generalized version of the standard multiplicative homeostatic plasticity (Renart et al., 2003; Toyoizumi et al., 2014) stabilizes learning by effectively masking excitatory connections during stimulation and unmasking those connections during retrieval. Using the bifurcation diagram derived for fixed connectivity, we study analytically the temporal evolution and the steady state of the learned recurrent architecture as a function of parameters characterizing the external inputs. Slow changing stimuli lead to PA, while fast changing stimuli lead to SA. Our network model shows how a network with plastic synapses can stably and flexibly learn PA and SA in an unsupervised manner."}], "ArticleTitle": "Unsupervised Learning of Persistent and Sequential Activity."}, "32547379": {"mesh": [], "AbstractText": [{"section": null, "text": "Theta-nested gamma oscillations have been reported in many areas of the brain and are believed to represent a fundamental mechanism to transfer information across spatial and temporal scales. In a series of recent experiments in vitro it has been possible to replicate with an optogenetic theta frequency stimulation several features of cross-frequency coupling (CFC) among theta and gamma rhythms observed in behaving animals. In order to reproduce the main findings of these experiments we have considered a new class of neural mass models able to reproduce exactly the macroscopic dynamics of spiking neural networks. In this framework, we have examined two set-ups able to support collective gamma oscillations: namely, the pyramidal interneuronal network gamma (PING) and the interneuronal network gamma (ING). In both set-ups we observe the emergence of theta-nested gamma oscillations by driving the system with a sinusoidal theta-forcing in proximity of a Hopf bifurcation. These mixed rhythms always display phase amplitude coupling. However, two different types of nested oscillations can be identified: one characterized by a perfect phase locking between theta and gamma rhythms, corresponding to an overall periodic behavior; another one where the locking is imperfect and the dynamics is quasi-periodic or even chaotic. From our analysis it emerges that the locked states are more frequent in the ING set-up. In agreement with the experiments, we find theta-nested gamma oscillations for forcing frequencies in the range [1:10] Hz, whose amplitudes grow proportionally to the forcing intensity and which are clearly modulated by the theta phase. Furthermore, analogously to the experiments, the gamma power and the frequency of the gamma-power peak increase with the forcing amplitude. At variance with experimental findings, the gamma-power peak does not shift to higher frequencies by increasing the theta frequency. This effect can be obtained, in our model, only by incrementing, at the same time, also the stimulation power. An effect achieved by increasing the amplitude either of the noise or of the forcing term proportionally to the theta frequency. On the basis of our analysis both the PING and the ING mechanism give rise to theta-nested gamma oscillations with almost identical features."}], "ArticleTitle": "Theta-Nested Gamma Oscillations in Next Generation Neural Mass Models."}, "31024282": {"mesh": [], "AbstractText": [{"section": null, "text": "Excessively high, neural synchronization has been associated with epileptic seizures, one of the most common brain diseases worldwide. A better understanding of neural synchronization mechanisms can thus help control or even treat epilepsy. In this paper, we study neural synchronization in a random network where nodes are neurons with excitatory and inhibitory synapses, and neural activity for each node is provided by the adaptive exponential integrate-and-fire model. In this framework, we verify that the decrease in the influence of inhibition can generate synchronization originating from a pattern of desynchronized spikes. The transition from desynchronous spikes to synchronous bursts of activity, induced by varying the synaptic coupling, emerges in a hysteresis loop due to bistability where abnormal (excessively high synchronous) regimes exist. We verify that, for parameters in the bistability regime, a square current pulse can trigger excessively high (abnormal) synchronization, a process that can reproduce features of epileptic seizures. Then, we show that it is possible to suppress such abnormal synchronization by applying a small-amplitude external current on > 10% of the neurons in the network. Our results demonstrate that external electrical stimulation not only can trigger synchronous behavior, but more importantly, it can be used as a means to reduce abnormal synchronization and thus, control or treat effectively epileptic seizures."}], "ArticleTitle": "Bistable Firing Pattern in a Neural Network Model."}, "33424572": {"mesh": [], "AbstractText": [{"section": null, "text": "Humans learn motor skills (MSs) through practice and experience and may then retain them for recruitment, which is effective as a rapid response for novel contexts. For an MS to be recruited for novel contexts, its recruitment range must be extended. In addressing this issue, we hypothesized that an MS is dynamically modulated according to the feedback context to expand its recruitment range into novel contexts, which do not involve the learning of an MS. The following two sub-issues are considered. We previously demonstrated that the learned MS could be recruited in novel contexts through its modulation, which is driven by dynamically regulating the synergistic redundancy between muscles according to the feedback context. However, this modulation is trained in the dynamics under the MS learning context. Learning an MS in a specific condition naturally causes movement deviation from the desired state when the MS is executed in a novel context. We hypothesized that this deviation can be reduced with the additional modulation of an MS, which tunes the MS-produced muscle activities by using the feedback gain signals driven by the deviation from the desired state. Based on this hypothesis, we propose a feedback gain signal-driven tuning model of a learned MS for its robust recruitment. This model is based on the neurophysiological architecture in the cortico-basal ganglia circuit, in which an MS is plausibly retained as it was learned and is then recruited by tuning its muscle control signals according to the feedback context. In this study, through computational simulation, we show that the proposed model may be used to neurophysiologically describe the recruitment of a learned MS in novel contexts."}], "ArticleTitle": "Dynamic Modulation of a Learned Motor Skill for Its Recruitment."}, "32038213": {"mesh": [], "AbstractText": [{"section": null, "text": "Head-direction cells have been found in several areas in the mammalian brains. The firing rate of an ideal head-direction cell reaches its peak value only when the animal's head points in a specific direction, and this preferred direction stays the same regardless of spatial location. In this paper we combine mathematical analytical techniques and numerical simulations to fully analyze the equilibrium states of a generic ring attractor network, which is a widely used modeling framework for the head-direction system. Under specific conditions, all solutions of the ring network are bounded, and there exists a Lyapunov function that guarantees the stability of the network for any given inputs, which may come from multiple sources in the biological system, including self-motion information for inertially based updating and landmark information for calibration. We focus on the first few terms of the Fourier series of the ring network to explicitly solve for all possible equilibrium states, followed by a stability analysis based on small perturbations. In particular, these equilibrium states include the standard single-peaked activity pattern as well as double-peaked activity pattern, whose existence is unknown but has testable experimental implications. To our surprise, we have also found an asymmetric equilibrium activity profile even when the network connectivity is strictly symmetric. Finally we examine how these different equilibrium solutions depend on the network parameters and obtain the phase diagrams in the parameter space of the ring network."}], "ArticleTitle": "Equilibrium States and Their Stability in the Head-Direction Ring Network."}, "32595464": {"mesh": [], "AbstractText": [{"section": null, "text": "Neural synchrony in the brain at rest is usually variable and intermittent, thus intervals of predominantly synchronized activity are interrupted by intervals of desynchronized activity. Prior studies suggested that this temporal structure of the weakly synchronous activity might be functionally significant: many short desynchronizations may be functionally different from few long desynchronizations even if the average synchrony level is the same. In this study, we used computational neuroscience methods to investigate the effects of spike-timing dependent plasticity (STDP) on the temporal patterns of synchronization in a simple model. We employed a small network of conductance-based model neurons that were connected via excitatory plastic synapses. The dynamics of this network was subjected to the time-series analysis methods used in prior experimental studies. We found that STDP could alter the synchronized dynamics in the network in several ways, depending on the time scale that plasticity acts on. However, in general, the action of STDP in the simple network considered here is to promote dynamics with short desynchronizations (i.e., dynamics reminiscent of that observed in experimental studies). Complex interplay of the cellular and synaptic dynamics may lead to the activity-dependent adjustment of synaptic strength in such a way as to facilitate experimentally observed short desynchronizations in the intermittently synchronized neural activity."}], "ArticleTitle": "Spike-Timing Dependent Plasticity Effect on the Temporal Patterning of Neural Synchronization."}, "32038210": {"mesh": [], "AbstractText": [{"section": null, "text": "Recent research in neuroscience indicates the importance of tripartite synapses and gliotransmission mediated by astrocytes in neuronal system modulation. Although the astrocyte and neuronal network functions are interrelated, they are fundamentally different in their signaling patterns and, possibly, the time scales at which they operate. However, the exact nature of gliotransmission and the effect of the tripartite synapse function at the network level are currently elusive. In this paper, we propose a computational model of interactions between an astrocyte network and a neuron network, starting from tripartite synapses and spanning to a joint network level. Our model focuses on a two-dimensional setup emulating a mixed in vitro neuron-astrocyte cell culture. The model depicts astrocyte-released gliotransmitters exerting opposing effects on the neurons: increasing the release probability of the presynaptic neuron while hyperpolarizing the post-synaptic one at a longer time scale. We simulated the joint networks with various levels of astrocyte contributions and neuronal activity levels. Our results indicate that astrocytes prolong the burst duration of neurons, while restricting hyperactivity. Thus, in our model, the effect of astrocytes is homeostatic; the firing rate of the network stabilizes to an intermediate level independently of neuronal base activity. Our computational model highlights the plausible roles of astrocytes in interconnected astrocytic and neuronal networks. Our simulations support recent findings in neurons and astrocytes in vivo and in vitro suggesting that astrocytic networks provide a modulatory role in the bursting of the neuronal network."}], "ArticleTitle": "A Computational Model of Interactions Between Neuronal and Astrocytic Networks: The Role of Astrocytes in the Stability of the Neuronal Firing Rate."}, "31354464": {"mesh": [], "AbstractText": [{"section": null, "text": "Orientation selectivity is a fundamental property of visual cortical neurons and plays a crucial role in pattern perception. Although many studies have dedicated to explain how the orientation selectivity emerged, the mechanism underlying orientation selectivity is still not clear. In this work, we investigated the synchronization between spikes and local field potentials (LFP) in gamma band, with the aim of providing a new avenue to analyze the orientation selectivity. The experimental data were recorded by utilizing two chronically implanted multi-electrode arrays, where each array consisted of 48 electrodes and was placed over V1 and V4, respectively, in two macaques performing a selective visual attention task. An unbiased and robust measure for quantifying the synchronization between spikes and LFP was employed in the analysis process, which is termed as spike-triggered correlation matrix synchronization (SCMS) and performs reliably for limited samples of data. We observed the spike-LFP synchronization in three cases, i.e., spikes and LFP in V1, spikes and LFP in V4, spikes in V4 and LFP in V1. From the orientation tuning curves based on the spike-LFP synchronization, it is found that there is a strong correlation between the synchronization and grating orientation. The neurons in both V1 and V4 exhibit orientation selectivity, but V1 is stronger. In addition, the spike-LFP synchronization strength between V1 and V4 also shows orientation selectivity to drifting gratings. It means that the synchronization not only reflects the basic features of visual stimulation, but also describes the orientation tuning characteristics of neurons in different regions. Our results suggest that the spike-LFP synchronization can be used as an alternative and effective method to study the mechanism for generating orientation selectivity of visual neurons."}], "ArticleTitle": "The Orientation Selectivity of Spike-LFP Synchronization in Macaque V1 and V4."}, "33304260": {"mesh": [], "AbstractText": [{"section": null, "text": "In this paper we investigate the active inference framework as a means to enable autonomous behavior in artificial agents. Active inference is a theoretical framework underpinning the way organisms act and observe in the real world. In active inference, agents act in order to minimize their so called free energy, or prediction error. Besides being biologically plausible, active inference has been shown to solve hard exploration problems in various simulated environments. However, these simulations typically require handcrafting a generative model for the agent. Therefore we propose to use recent advances in deep artificial neural networks to learn generative state space models from scratch, using only observation-action sequences. This way we are able to scale active inference to new and challenging problem domains, whilst still building on the theoretical backing of the free energy principle. We validate our approach on the mountain car problem to illustrate that our learnt models can indeed trade-off instrumental value and ambiguity. Furthermore, we show that generative models can also be learnt using high-dimensional pixel observations, both in the OpenAI Gym car racing environment and a real-world robotic navigation task. Finally we show that active inference based policies are an order of magnitude more sample efficient than Deep Q Networks on RL tasks."}], "ArticleTitle": "Learning Generative State Space Models for Active Inference."}, "30713494": {"mesh": [], "AbstractText": [{"section": null, "text": "Boundary completion is one of the desired properties of a robust object boundary detection model, since in real-word images the object boundaries are commonly not fully and clearly seen. An extreme example of boundary completion occurs in images with illusory contours, where the visual system completes boundaries in locations without intensity gradient. Most illusory contour models extract special image features, such as L and T junctions, while the task is known to be a difficult issue in real-world images. The proposed model uses a functional optimization approach, in which a cost value is assigned to any boundary arrangement to find the arrangement with minimal cost. The functional accounts for basic object properties, such as alignment with the image, object boundary continuity, and boundary simplicity. The encoding of these properties in the functional does not require special features extraction, since the alignment with the image only requires extraction of the image edges. The boundary arrangement is represented by a border ownership map, holding object boundary segments in discrete locations and directions. The model finds multiple possible image interpretations, which are ranked according to the probability that they are supposed to be perceived. This is achieved by using a novel approach to represent the different image interpretations by multiple functional local minima. The model is successfully applied to objects with real and illusory contours. In the case of Kanizsa illusion the model predicts both illusory and real (pacman) image interpretations. The model is a proof of concept and is currently restricted to synthetic gray-scale images with solid regions."}], "ArticleTitle": "Predicting Illusory Contours Without Extracting Special Image Features."}, "31354461": {"mesh": [], "AbstractText": [{"section": null, "text": "Real-life decisions often require a comparison of multi-attribute options with various benefits and costs, and the evaluation of each option depends partly on the others in the choice set (i.e., the choice context). Although reinforcement learning models have successfully described choice behavior, how to account for multi-attribute information when making a context-dependent decision remains unclear. Here we develop a computational model of attention control that includes context effects on multi-attribute decisions, linking a context-dependent choice model with a reinforcement learning model. The overall model suggests that the distinctiveness of attributes guides an individual's preferences among multi-attribute options via an attention-control mechanism that determines whether choices are selectively biased toward the most distinctive attribute (selective attention) or proportionally distributed based on the relative distinctiveness of attributes (divided attention). To test the model, we conducted a behavioral experiment in rhesus monkeys, in which they made simple multi-attribute decisions over three conditions that manipulated the degree of distinctiveness between alternatives: (1) four foods of different size and calorie; (2) four pieces of the same food in different colors; and (3) four identical pieces of food. The model simulation of the choice behavior captured the preference bias (i.e., overall preference structure) and the choice persistence (repeated choices) in the empirical data, providing evidence for the respective influences of attention and memory on preference bias and choice persistence. Our study provides insights into computations underlying multi-attribute decisions, linking attentional control to decision-making processes."}], "ArticleTitle": "A Computational Model of Attention Control in Multi-Attribute, Context-Dependent Decision Making."}, "33013341": {"mesh": [], "AbstractText": [{"section": null, "text": "Dysfunction in cholinergic modulation has been linked to a variety of cognitive disorders including Alzheimer's disease. The important role of this neurotransmitter has been explored in a variety of experiments, yet many questions remain unanswered about the contribution of cholinergic modulation to healthy hippocampal function. To address this question, we have developed a model of CA1 pyramidal neuron that takes into consideration muscarinic receptor activation in response to changes in extracellular concentration of acetylcholine and its effects on cellular excitability and downstream intracellular calcium dynamics. This model incorporates a variety of molecular agents to accurately simulate several processes heretofore ignored in computational modeling of CA1 pyramidal neurons. These processes include the inhibition of ionic channels by phospholipid depletion along with the release of calcium from intracellular stores (i.e., the endoplasmic reticulum). This paper describes the model and the methods used to calibrate its behavior to match experimental results. The result of this work is a compartmental model with calibrated mechanisms for simulating the intracellular calcium dynamics of CA1 pyramidal cells with a focus on those related to release from calcium stores in the endoplasmic reticulum. From this model we also make various predictions for how the inhibitory and excitatory responses to cholinergic modulation vary with agonist concentration. This model expands the capabilities of CA1 pyramidal cell models through the explicit modeling of molecular interactions involved in healthy cognitive function and disease. Through this expanded model we come closer to simulating these diseases and gaining the knowledge required to develop novel treatments."}], "ArticleTitle": "A Computational Model of the Cholinergic Modulation of CA1 Pyramidal Cell Activity."}, "32116621": {"mesh": [], "AbstractText": [{"section": null, "text": "Spiking Neural Networks (SNNs) have shown favorable performance recently. Nonetheless, the time-consuming computation on neuron level and complex optimization limit their real-time application. Curiosity has shown great performance in brain learning, which helps biological brains grasp new knowledge efficiently and actively. Inspired by this leaning mechanism, we propose a curiosity-based SNN (CBSNN) model, which contains four main learning processes. Firstly, the network is trained with biologically plausible plasticity principles to get the novelty estimations of all samples in only one epoch; secondly, the CBSNN begins to repeatedly learn the samples whose novelty estimations exceed the novelty threshold and dynamically update the novelty estimations of samples according to the learning results in five epochs; thirdly, in order to avoid the overfitting of the novel samples and forgetting of the learned samples, CBSNN retrains all samples in one epoch; finally, step two and step three are periodically taken until network convergence. Compared with the state-of-the-art Voltage-driven Plasticity-centric SNN (VPSNN) under standard architecture, our model achieves a higher accuracy of 98.55% with only 54.95% of its computation cost on the MNIST hand-written digit recognition dataset. Similar conclusion can also be found out in other datasets, i.e., Iris, NETtalk, Fashion-MNIST, and CIFAR-10, respectively. More experiments and analysis further prove that such curiosity-based learning theory is helpful in improving the efficiency of SNNs. As far as we know, this is the first practical combination of the curiosity mechanism and SNN, and these improvements will make the realistic application of SNNs possible on more specific tasks within the von Neumann framework."}], "ArticleTitle": "A Curiosity-Based Learning Method for Spiking Neural Networks."}, "31427939": {"mesh": [], "AbstractText": [{"section": null, "text": "Neural network simulation is an important tool for generating and evaluating hypotheses on the structure, dynamics, and function of neural circuits. For scientific questions addressing organisms operating autonomously in their environments, in particular where learning is involved, it is crucial to be able to operate such simulations in a closed-loop fashion. In such a set-up, the neural agent continuously receives sensory stimuli from the environment and provides motor signals that manipulate the environment or move the agent within it. So far, most studies requiring such functionality have been conducted with custom simulation scripts and manually implemented tasks. This makes it difficult for other researchers to reproduce and build upon previous work and nearly impossible to compare the performance of different learning architectures. In this work, we present a novel approach to solve this problem, connecting benchmark tools from the field of machine learning and state-of-the-art neural network simulators from computational neuroscience. The resulting toolchain enables researchers in both fields to make use of well-tested high-performance simulation software supporting biologically plausible neuron, synapse and network models and allows them to evaluate and compare their approach on the basis of standardized environments with various levels of complexity. We demonstrate the functionality of the toolchain by implementing a neuronal actor-critic architecture for reinforcement learning in the NEST simulator and successfully training it on two different environments from the OpenAI Gym. We compare its performance to a previously suggested neural network model of reinforcement learning in the basal ganglia and a generic Q-learning algorithm."}], "ArticleTitle": "A Closed-Loop Toolchain for Neural Network Simulations of Learning Autonomous Agents."}, "31920605": {"mesh": [], "AbstractText": [{"section": null, "text": "Neurobiological systems rely on hierarchical and modular architectures to carry out intricate computations using minimal resources. A prerequisite for such systems to operate adequately is the capability to reliably and efficiently transfer information across multiple modules. Here, we study the features enabling a robust transfer of stimulus representations in modular networks of spiking neurons, tuned to operate in a balanced regime. To capitalize on the complex, transient dynamics that such networks exhibit during active processing, we apply reservoir computing principles and probe the systems' computational efficacy with specific tasks. Focusing on the comparison of random feed-forward connectivity and biologically inspired topographic maps, we find that, in a sequential set-up, structured projections between the modules are strictly necessary for information to propagate accurately to deeper modules. Such mappings not only improve computational performance and efficiency, they also reduce response variability, increase robustness against interference effects, and boost memory capacity. We further investigate how information from two separate input streams is integrated and demonstrate that it is more advantageous to perform non-linear computations on the input locally, within a given module, and subsequently transfer the result downstream, rather than transferring intermediate information and performing the computation downstream. Depending on how information is integrated early on in the system, the networks achieve similar task-performance using different strategies, indicating that the dimensionality of the neural responses does not necessarily correlate with nonlinear integration, as predicted by previous studies. These findings highlight a key role of topographic maps in supporting fast, robust, and accurate neural communication over longer distances. Given the prevalence of such structural feature, particularly in the sensory systems, elucidating their functional purpose remains an important challenge toward which this work provides relevant, new insights. At the same time, these results shed new light on important requirements for designing functional hierarchical spiking networks."}], "ArticleTitle": "Passing the Message: Representation Transfer in Modular Balanced Networks."}, "33178003": {"mesh": [], "AbstractText": [{"section": null, "text": "Visual information processing in the brain goes from global to local. A large volume of experimental studies has suggested that among global features, the brain perceives the topological information of an image first. Here, we propose a neural network model to elucidate the underlying computational mechanism. The model consists of two parts. The first part is a neural network in which neurons are coupled through gap junctions, mimicking the neural circuit formed by alpha ganglion cells in the retina. Gap junction plays a key role in the model, which, on one hand, facilitates the synchronized firing of a neuron group covering a connected region of an image, and on the other hand, staggers the firing moments of different neuron groups covering disconnected regions of the image. These two properties endow the network with the capacity of detecting the connectivity and closure of images. The second part of the model is a read-out neuron, which reads out the topological information that has been converted into the number of synchronized firings in the retina network. Our model provides a simple yet effective mechanism for the neural system to detect the topological information of images in ultra-speed."}], "ArticleTitle": "A Neural Network Model With Gap Junction for Topological Detection."}, "31616272": {"mesh": [], "AbstractText": [{"section": null, "text": "Pyramidal cells in layer V of the neocortex are one of the most widely studied neuron types in the mammalian brain. Due to their role as integrators of feedforward and cortical feedback inputs, they are well-positioned to contribute to the symptoms and pathology in mental disorders-such as schizophrenia-that are characterized by a mismatch between the internal perception and external inputs. In this modeling study, we analyze the input/output properties of layer V pyramidal cells and their sensitivity to modeled genetic variants in schizophrenia-associated genes. We show that the excitability of layer V pyramidal cells and the way they integrate inputs in space and time are altered by many types of variants in ion-channel and Ca2+ transporter-encoding genes that have been identified as risk genes by recent genome-wide association studies. We also show that the variability in the output patterns of spiking and Ca2+ transients in layer V pyramidal cells is altered by these model variants. Importantly, we show that many of the predicted effects are robust to noise and qualitatively similar across different computational models of layer V pyramidal cells. Our modeling framework reveals several aspects of single-neuron excitability that can be linked to known schizophrenia-related phenotypes and existing hypotheses on disease mechanisms. In particular, our models predict that single-cell steady-state firing rate is positively correlated with the coding capacity of the neuron and negatively correlated with the amplitude of a prepulse-mediated adaptation and sensitivity to coincidence of stimuli in the apical dendrite and the perisomatic region of a layer V pyramidal cell. These results help to uncover the voltage-gated ion-channel and Ca2+ transporter-associated genetic underpinnings of schizophrenia phenotypes and biomarkers."}], "ArticleTitle": "Computational Modeling of Genetic Contributions to Excitability and Neural Coding in Layer V Pyramidal Cells: Applications to Schizophrenia Pathology."}, "32848681": {"mesh": [], "AbstractText": [{"section": null, "text": "The connectivity structure of neuronal networks in cortex is highly dynamic. This ongoing cortical rewiring is assumed to serve important functions for learning and memory. We analyze in this article a model for the self-organization of synaptic inputs onto dendritic branches of pyramidal cells. The model combines a generic stochastic rewiring principle with a simple synaptic plasticity rule that depends on local dendritic activity. In computer simulations, we find that this synaptic rewiring model leads to synaptic clustering, that is, temporally correlated inputs become locally clustered on dendritic branches. This empirical finding is backed up by a theoretical analysis which shows that rewiring in our model favors network configurations with synaptic clustering. We propose that synaptic clustering plays an important role in the organization of computation and memory in cortical circuits: we find that synaptic clustering through the proposed rewiring mechanism can serve as a mechanism to protect memories from subsequent modifications on a medium time scale. Rewiring of synaptic connections onto specific dendritic branches may thus counteract the general problem of catastrophic forgetting in neural networks."}], "ArticleTitle": "Emergence of Stable Synaptic Clusters on Dendrites Through Synaptic Rewiring."}, "33192426": {"mesh": [], "AbstractText": [{"section": null, "text": "Despite the recent progress in AI powered by deep learning in solving narrow tasks, we are not close to human intelligence in its flexibility, versatility, and efficiency. Efficient learning and effective generalization come from inductive biases, and building Artificial General Intelligence (AGI) is an exercise in finding the right set of inductive biases that make fast learning possible while being general enough to be widely applicable in tasks that humans excel at. To make progress in AGI, we argue that we can look at the human brain for such inductive biases and principles of generalization. To that effect, we propose a strategy to gain insights from the brain by simultaneously looking at the world it acts upon and the computational framework to support efficient learning and generalization. We present a neuroscience-inspired generative model of vision as a case study for such approach and discuss some open problems about the path to AGI."}], "ArticleTitle": "From CAPTCHA to Commonsense: How Brain Can Teach Us About Artificial Intelligence."}, "32390818": {"mesh": [], "AbstractText": [{"section": null, "text": "It has been suggested that neurons can represent sensory input using probability distributions and neural circuits can perform probabilistic inference. Lateral connections between neurons have been shown to have non-random connectivity and modulate responses to stimuli within the classical receptive field. Large-scale efforts mapping local cortical connectivity describe cell type specific connections from inhibitory neurons and like-to-like connectivity between excitatory neurons. To relate the observed connectivity to computations, we propose a neuronal network model that approximates Bayesian inference of the probability of different features being present at different image locations. We show that the lateral connections between excitatory neurons in a circuit implementing contextual integration in this should depend on correlations between unit activities, minus a global inhibitory drive. The model naturally suggests the need for two types of inhibitory gates (normalization, surround inhibition). First, using natural scene statistics and classical receptive fields corresponding to simple cells parameterized with data from mouse primary visual cortex, we show that the predicted connectivity qualitatively matches with that measured in mouse cortex: neurons with similar orientation tuning have stronger connectivity, and both excitatory and inhibitory connectivity have a modest spatial extent, comparable to that observed in mouse visual cortex. We incorporate lateral connections learned using this model into convolutional neural networks. Features are defined by supervised learning on the task, and the lateral connections provide an unsupervised learning of feature context in multiple layers. Since the lateral connections provide contextual information when the feedforward input is locally corrupted, we show that incorporating such lateral connections into convolutional neural networks makes them more robust to noise and leads to better performance on noisy versions of the MNIST dataset. Decomposing the predicted lateral connectivity matrices into low-rank and sparse components introduces additional cell types into these networks. We explore effects of cell-type specific perturbations on network computation. Our framework can potentially be applied to networks trained on other tasks, with the learned lateral connections aiding computations implemented by feedforward connections when the input is unreliable and demonstrate the potential usefulness of combining supervised and unsupervised learning techniques in real-world vision tasks."}], "ArticleTitle": "Contextual Integration in Cortical and Convolutional Neural Networks."}, "30687053": {"mesh": [], "AbstractText": [{"section": null, "text": "Deep artificial neural networks are feed-forward architectures capable of very impressive performances in diverse domains. Indeed stacking multiple layers allows a hierarchical composition of local functions, providing efficient compact mappings. Compared to the brain, however, such architectures are closer to a single pipeline and require huge amounts of data, while concrete cases for either human or machine learning systems are often restricted to not-so-big data sets. Furthermore, interpretability of the obtained results is a key issue: since deep learning applications are increasingly present in society, it is important that the underlying processes be accessible and understandable to every one. In order to target these challenges, in this contribution we analyze how considering prototypes in a rather generalized sense (with respect to the state of the art) allows to reasonably work with small data sets while providing an interpretable view of the obtained results. Some mathematical interpretation of this proposal is discussed. Sensitivity to hyperparameters is a key issue for reproducible deep learning results, and is carefully considered in our methodology. Performances and limitations of the proposed setup are explored in details, under different hyperparameter sets, in an analogous way as biological experiments are conducted. We obtain a rather simple architecture, easy to explain, and which allows, combined with a standard method, to target both performances and interpretability."}], "ArticleTitle": "Bio-inspired Analysis of Deep Learning on Not-So-Big Data Using Data-Prototypes."}, "33328945": {"mesh": [], "AbstractText": [{"section": null, "text": "The Fano factor, defined as the variance-to-mean ratio of spike counts in a time window, is often used to measure the variability of neuronal spike trains. However, despite its transparent definition, careless use of the Fano factor can easily lead to distorted or even wrong results. One of the problems is the unclear dependence of the Fano factor on the spiking rate, which is often neglected or handled insufficiently. In this paper we aim to explore this problem in more detail and to study the possible solution, which is to evaluate the Fano factor in the operational time. We use equilibrium renewal and Markov renewal processes as spike train models to describe the method in detail, and we provide an illustration on experimental data."}], "ArticleTitle": "Fano Factor: A Potentially Useful Information."}, "33362499": {"mesh": [], "AbstractText": [{"section": null, "text": "Recently, deep convolutional neural networks (DCNNs) have attained human-level performances on challenging object recognition tasks owing to their complex internal representation. However, it remains unclear how objects are represented in DCNNs with an overwhelming number of features and non-linear operations. In parallel, the same question has been extensively studied in primates' brain, and three types of coding schemes have been found: one object is coded by the entire neuronal population (distributed coding), or by one single neuron (local coding), or by a subset of neuronal population (sparse coding). Here we asked whether DCNNs adopted any of these coding schemes to represent objects. Specifically, we used the population sparseness index, which is widely-used in neurophysiological studies on primates' brain, to characterize the degree of sparseness at each layer in representative DCNNs pretrained for object categorization. We found that the sparse coding scheme was adopted at all layers of the DCNNs, and the degree of sparseness increased along the hierarchy. That is, the coding scheme shifted from distributed-like coding at lower layers to local-like coding at higher layers. Further, the degree of sparseness was positively correlated with DCNNs' performance in object categorization, suggesting that the coding scheme was related to behavioral performance. Finally, with the lesion approach, we demonstrated that both external learning experiences and built-in gating operations were necessary to construct such a hierarchical coding scheme. In sum, our study provides direct evidence that DCNNs adopted a hierarchically-evolved sparse coding scheme as the biological brain does, suggesting the possibility of an implementation-independent principle underling object recognition."}], "ArticleTitle": "Hierarchical Sparse Coding of Objects in Deep Convolutional Neural Networks."}, "32390820": {"mesh": [], "AbstractText": [{"section": null, "text": "Spatio-temporal brain activities with variable delay detectable in resting-state functional magnetic resonance imaging (rs-fMRI) give rise to highly reproducible structures, termed cortical lag threads, that propagate from one brain region to another. Using a computational topology of data approach, we found that persistent, recurring blood oxygen level dependent (BOLD) signals in triangulated rs-fMRI videoframes display previously undetected topological findings, i.e., vortex structures that cover brain activated regions. Measure of persistence of vortex shapes in BOLD signal propagation is carried out in terms of Betti numbers that rise and fall over time during spontaneous activity of the brain. Importantly, a topology of data given in terms of geometric shapes of BOLD signal propagation offers a practical approach in coping with and sidestepping massive noise in neurodata, such as unwanted dark (low intensity) regions in the neighborhood of non-zero BOLD signals. Our findings have been codified and visualized in plots able to track the non-trivial BOLD signals that appear intermittently in a sequence of rs-fMRI videoframes. The end result of this tracking of changing lag structures is a so-called persistent barcode, which is a pictograph that offers a convenient visual means of exhibiting, comparing, and classifying brain activation patterns."}], "ArticleTitle": "Topological View of Flows Inside the BOLD Spontaneous Activity of the Human Brain."}, "33408623": {"mesh": [], "AbstractText": [{"section": null, "text": "In the main control room (MCR) of a nuclear power plant (NPP), the quality of an operator's performance can depend on their level of attention to the task. Insufficient operator attention accounted for more than 26% of the total causes of human errors and is the highest category for errors. It is therefore necessary to check whether operators are sufficiently attentive either as supervisors or peers during reactor operation. Recently, digital control technologies have been introduced to the operating environment of an NPP MCR. These upgrades are expected to enhance plant and operator performance. At the same time, because personal computers are used in the advanced MCR, the operators perform more cognitive works than physical work. However, operators may not consciously check fellow operators' attention in this environment indicating potentially higher importance of the role of operator attention. Therefore, remote measurement of an operator's attention in real time would be a useful tool, providing feedback to supervisors. The objective of this study is to investigate the development of quantitative indicators that can identify an operator's attention, to diagnose or detect a lack of operator attention thus preventing potential human errors in advanced MCRs. To establish a robust baseline of operator attention, this study used two of the widely used biosignals: electroencephalography (EEG) and eye movement. We designed an experiment to collect EEG and eye movements of the subjects who were monitoring and diagnosing nuclear operator safety-relevant tasks. There was a statistically significant difference between biosignals with and without appropriate attention. Furthermore, an average classification accuracy of about 90% was obtained by the k-nearest neighbors and support vector machine classifiers with a few EEG and eye movements features. Potential applications of EEG and eye movement measures in monitoring and diagnosis tasks in an NPP MCR are also discussed."}], "ArticleTitle": "Biosignal-Based Attention Monitoring to Support Nuclear Operator Safety-Relevant Tasks."}, "29740306": {"mesh": [], "AbstractText": [{"section": null, "text": "Hippocampal cognitive map-a neuronal representation of the spatial environment-is widely discussed in the computational neuroscience literature for decades. However, more recent studies point out that hippocampus plays a major role in producing yet another cognitive framework-the memory space-that incorporates not only spatial, but also non-spatial memories. Unlike the cognitive maps, the memory spaces, broadly understood as \"networks of interconnections among the representations of events,\" have not yet been studied from a theoretical perspective. Here we propose a mathematical approach that allows modeling memory spaces constructively, as epiphenomena of neuronal spiking activity and thus to interlink several important notions of cognitive neurophysiology. First, we suggest that memory spaces have a topological nature-a hypothesis that allows treating both spatial and non-spatial aspects of hippocampal function on equal footing. We then model the hippocampal memory spaces in different environments and demonstrate that the resulting constructions naturally incorporate the corresponding cognitive maps and provide a wider context for interpreting spatial information. Lastly, we propose a formal description of the memory consolidation process that connects memory spaces to the Morris' cognitive schemas-heuristic representations of the acquired memories, used to explain the dynamics of learning and memory consolidation in a given environment. The proposed approach allows evaluating these constructs as the most compact representations of the memory space's structure."}], "ArticleTitle": "Topological Schemas of Memory Spaces."}, "29875646": {"mesh": [], "AbstractText": [{"section": null, "text": "Cognitive deficits in schizophrenia are correlated with the dysfunctions of distinct brain regions including anterior cingulate cortex (ACC) and prefrontal cortex (PFC). Apart from the dysfunctions of the intrinsic connectivity of related areas, how the coupled neural populations work is also crucial in related processes. Twenty-four patients with schizophrenia (SZs) and 24 matched healthy controls (HCs) were recruited in our study. Based on the electroencephalogram (EEG) datasets recorded, the Dynamic Causal Modeling (DCM) was then adopted to estimate how the brain architecture adapts among related areas in SZs and to investigate the mechanism that accounts for their cognitive deficits. The distinct winning models in SZs and HCs consistently emphasized the importance of ACC in regulating the elicitations of P300s. Specifically, comparing to that in HCs, the winning model in SZs uncovered a compensatory pathway from dorsolateral PFC to intraparietal sulcus that promised the SZs' accomplishing P300 tasks. The findings demonstrated that the \"disconnectivity hypothesis\" is helpful and useful in explaining the cognitive deficits in SZs, while the brain architecture adapted with related compensatory pathway promises the limited brain cognitions in SZs. This study provides a new viewpoint that deepens our understanding of the cognitive deficits in schizophrenia."}], "ArticleTitle": "Top-Down Disconnectivity in Schizophrenia During P300 Tasks."}, "30018546": {"mesh": [], "AbstractText": [{"section": null, "text": "In spike-timing dependent plasticity (STDP) change in synaptic strength depends on the timing of pre- vs. postsynaptic spiking activity. Since STDP is in compliance with Hebb's postulate, it is considered one of the major mechanisms of memory storage and recall. STDP comprises a system of two coincidence detectors with N-methyl-D-aspartate receptor (NMDAR) activation often posited as one of the main components. Numerous studies have unveiled a third component of this coincidence detection system, namely neuromodulation and glia activity shaping STDP. Even though dopaminergic control of STDP has most often been reported, acetylcholine, noradrenaline, nitric oxide (NO), brain-derived neurotrophic factor (BDNF) or gamma-aminobutyric acid (GABA) also has been shown to effectively modulate STDP. Furthermore, it has been demonstrated that astrocytes, via the release or uptake of glutamate, gate STDP expression. At the most fundamental level, the timing properties of STDP are expected to depend on the spatiotemporal dynamics of the underlying signaling pathways. However in most cases, due to technical limitations experiments grant only indirect access to these pathways. Computational models carefully constrained by experiments, allow for a better qualitative understanding of the molecular basis of STDP and its regulation by neuromodulators. Recently, computational models of calcium dynamics and signaling pathway molecules have started to explore STDP emergence in ex and in vivo-like conditions. These models are expected to reproduce better at least part of the complex modulation of STDP as an emergent property of the underlying molecular pathways. Elucidation of the mechanisms underlying STDP modulation and its consequences on network dynamics is of critical importance and will allow better understanding of the major mechanisms of memory storage and recall both in health and disease."}], "ArticleTitle": "Modulation of Spike-Timing Dependent Plasticity: Towards the Inclusion of a Third Factor in Computational Models."}, "30483088": {"mesh": [], "AbstractText": [{"section": null, "text": "To infer the causes of its sensations, the brain must call on a generative (predictive) model. This necessitates passing local messages between populations of neurons to update beliefs about hidden variables in the world beyond its sensory samples. It also entails inferences about how we will act. Active inference is a principled framework that frames perception and action as approximate Bayesian inference. This has been successful in accounting for a wide range of physiological and behavioral phenomena. Recently, a process theory has emerged that attempts to relate inferences to their neurobiological substrates. In this paper, we review and develop the anatomical aspects of this process theory. We argue that the form of the generative models required for inference constrains the way in which brain regions connect to one another. Specifically, neuronal populations representing beliefs about a variable must receive input from populations representing the Markov blanket of that variable. We illustrate this idea in four different domains: perception, planning, attention, and movement. In doing so, we attempt to show how appealing to generative models enables us to account for anatomical brain architectures. Ultimately, committing to an anatomical theory of inference ensures we can form empirical hypotheses that can be tested using neuroimaging, neuropsychological, and electrophysiological experiments."}], "ArticleTitle": "The Anatomy of Inference: Generative Models and Brain Structure."}, "32116622": {"mesh": [], "AbstractText": [{"section": null, "text": "Sleep and wakefulness are promoted not by a single neural pathway but via wake or sleep-promoting nodes distributed across layers of the brain. We equate each layer with a brain region in proposing a layered subsumption model for arousal based on a computational architecture. Beyond the brainstem the layers include the diencephalon (hypothalamus, thalamus), basal ganglia, and cortex. In light of existing empirical evidence, we propose that each layer have sleep and wake computations driven by similar high-level architecture and processing units. Specifically, an interconnected wake-promoting system is suggested as driving arousal in each brain layer with the processing converging to produce the features of wakefulness. In contrast, sleep-promoting GABAergic neurons largely project to and inhibit wake-promoting neurons. We propose a general pattern of caudal wake-promoting and sleep-promoting neurons having a strong effect on overall behavior. However, while rostral brain layers have less influence on sleep and wake, through descending projections, they can subsume the activity of caudal brain layers to promote arousal. The two models presented in this work will suggest computations for the layering and hierarchy. Through dynamic system theory several hypotheses are introduced for the interaction of controllers and systems that correspond to the different populations of neurons at each layer. The models will be drawn-upon to discuss future experiments to elucidate the structure of the hierarchy that exists among the sleep-arousal architecture."}], "ArticleTitle": "A Layered Control Architecture of Sleep and Arousal."}, "33414712": {"mesh": [], "AbstractText": [{"section": null, "text": "The question of whether artificial beings or machines could become self-aware or conscious has been a philosophical question for centuries. The main problem is that self-awareness cannot be observed from an outside perspective and the distinction of being really self-aware or merely a clever imitation cannot be answered without access to knowledge about the mechanism's inner workings. We investigate common machine learning approaches with respect to their potential ability to become self-aware. We realize that many important algorithmic steps toward machines with a core consciousness have already been taken."}], "ArticleTitle": "Will We Ever Have Conscious Machines?"}, "32848683": {"mesh": [], "AbstractText": [{"section": null, "text": "Electrical and chemical synapses shape the dynamics of neural networks, and their functional roles in information processing have been a longstanding question in neurobiology. In this paper, we investigate the role of synapses on the optimization of the phenomenon of self-induced stochastic resonance in a delayed multiplex neural network by using analytical and numerical methods. We consider a two-layer multiplex network in which, at the intra-layer level, neurons are coupled either by electrical synapses or by inhibitory chemical synapses. For each isolated layer, computations indicate that weaker electrical and chemical synaptic couplings are better optimizers of self-induced stochastic resonance. In addition, regardless of the synaptic strengths, shorter electrical synaptic delays are found to be better optimizers of the phenomenon than shorter chemical synaptic delays, while longer chemical synaptic delays are better optimizers than longer electrical synaptic delays; in both cases, the poorer optimizers are, in fact, worst. It is found that electrical, inhibitory, or excitatory chemical multiplexing of the two layers having only electrical synapses at the intra-layer levels can each optimize the phenomenon. Additionally, only excitatory chemical multiplexing of the two layers having only inhibitory chemical synapses at the intra-layer levels can optimize the phenomenon. These results may guide experiments aimed at establishing or confirming to the mechanism of self-induced stochastic resonance in networks of artificial neural circuits as well as in real biological neural networks."}], "ArticleTitle": "Optimal Self-Induced Stochastic Resonance in Multiplex Neural Networks: Electrical vs. Chemical Synapses."}, "31680922": {"mesh": [], "AbstractText": [{"section": null, "text": "The hippocampus plays important roles in memory formation and retrieval through sharp-wave-ripples. Recent studies have shown that certain neuron populations in the prefrontal cortex (PFC) exhibit coordinated reactivations during awake ripple events. These experimental findings suggest that the awake ripple is an important biomarker, through which the hippocampus interacts with the neocortex to assist memory formation and retrieval. However, the computational mechanisms of this ripple based hippocampal-cortical coordination are still not clear due to the lack of unified models that include both the hippocampal and cortical networks. In this work, using a coupled biophysical model of both CA1 and PFC, we investigate possible mechanisms of hippocampal-cortical memory trace transfer and the conditions that assist reactivation of the transferred memory traces in the PFC. To validate our model, we first show that the local field potentials generated in the hippocampus and PFC exhibit ripple range activities that are consistent with the recent experimental studies. Then we demonstrate that during ripples, sequence replays can successfully transfer the information stored in the hippocampus to the PFC recurrent networks. We investigate possible mechanisms of memory retrieval in PFC networks. Our results suggest that the stored memory traces in the PFC network can be retrieved through two different mechanisms, namely the cell-specific input representing external stimuli and non-specific spontaneous background noise representing spontaneous memory recall events. Importantly, in both cases, the memory reactivation quality is robust to network connection loss. Finally, we find out that the quality of sequence reactivations is enhanced by both increased number of SWRs and an optimal background noise intensity, which tunes the excitability of neurons to a proper level. Our study presents a mechanistic explanation for the memory trace transfer from the hippocampus to neocortex through ripple coupling in awake states and reports two different mechanisms by which the stored memory traces can be successfully retrieved."}], "ArticleTitle": "Hippocampal-Cortical Memory Trace Transfer and Reactivation Through Cell-Specific Stimulus and Spontaneous Background Noise."}, "31551742": {"mesh": [], "AbstractText": [{"section": null, "text": "The Multiple-Network Poroelastic Theory (MPET) is a numerical model to characterize the transport of multiple fluid networks in the brain, which overcomes the problem of conducting separate analyses on individual fluid compartments and losing the interactions between tissue and fluids, in addition to the interaction between the different fluids themselves. In this paper, the blood perfusion results from MPET modeling are partially validated using cerebral blood flow (CBF) data obtained from arterial spin labeling (ASL) magnetic resonance imaging (MRI), which uses arterial blood water as an endogenous tracer to measure CBF. Two subjects-one healthy control and one patient with unilateral middle cerebral artery (MCA) stenosis are included in the validation test. The comparison shows several similarities between CBF data from ASL and blood perfusion results from MPET modeling, such as higher blood perfusion in the gray matter than in the white matter, higher perfusion in the periventricular region for both the healthy control and the patient, and asymmetric distribution of blood perfusion for the patient. Although the partial validation is mainly conducted in a qualitative way, it is one important step toward the full validation of the MPET model, which has the potential to be used as a testing bed for hypotheses and new theories in neuroscience research."}], "ArticleTitle": "On the Validation of a Multiple-Network Poroelastic Model Using Arterial Spin Labeling MRI Data."}, "31178711": {"mesh": [], "AbstractText": [{"section": null, "text": "Clustering is a powerful machine learning tool for detecting structures in datasets. In the medical field, clustering has been proven to be a powerful tool for discovering patterns and structure in labeled and unlabeled datasets. Unlike supervised methods, clustering is an unsupervised method that works on datasets in which there is no outcome (target) variable nor is anything known about the relationship between the observations, that is, unlabeled data. In this paper, we focus on studying and reviewing clustering methods that have been applied to datasets of neurological diseases, especially Alzheimer's disease (AD). The aim is to provide insights into which clustering technique is more suitable for partitioning patients of AD based on their similarity. This is important as clustering algorithms can find patterns across patients that are difficult for medical practitioners to find. We further discuss the implications of the use of clustering algorithms in the treatment of AD. We found that clustering analysis can point to several features that underlie the conversion from early-stage AD to advanced AD. Furthermore, future work can apply semi-clustering algorithms on AD datasets, which will enhance clusters by including additional information."}], "ArticleTitle": "The Application of Unsupervised Clustering Methods to Alzheimer's Disease."}, "31040776": {"mesh": [], "AbstractText": [{"section": null, "text": "It has been suggested that the human nervous system controls motions in the task (or operational) space. However, little attention has been given to the separation of the control of the task-related and task-irrelevant degrees of freedom. Aim: We investigate how muscle synergies may be used to separately control the task-related and redundant degrees of freedom in a computational model. Approach: We generalize an existing motor control model, and assume that the task and redundant spaces have orthogonal basis vectors. This assumption originates from observations that the human nervous system tightly controls the task-related variables, and leaves the rest uncontrolled. In other words, controlling the variables in one space does not affect the other space; thus, the actuations must be orthogonal in the two spaces. We implemented this assumption in the model by selecting muscle synergies that produce force vectors with orthogonal directions in the task and redundant spaces. Findings: Our experimental results show that the orthogonality assumption performs well in reconstructing the muscle activities from the measured kinematics/dynamics in the task and redundant spaces. Specifically, we found that approximately 70% of the variation in the measured muscle activity can be captured with the orthogonality assumption, while allowing efficient separation of the control in the two spaces. Implications: The developed motor control model is a viable tool in real-time simulations of musculoskeletal systems, as well as model-based control of bio-mechatronic systems, where a computationally efficient representation of the human motion controller is needed."}], "ArticleTitle": "On the Relationship Between Muscle Synergies and Redundant Degrees of Freedom in Musculoskeletal Systems."}, "31156415": {"mesh": [], "AbstractText": [{"section": null, "text": "The brain as a neuronal system has very complex structures with a large diversity of neuronal types. The most basic complexity is seen from the structure of neuronal morphology, which usually has a complex tree-like structure with dendritic spines distributed in branches. To simulate a large-scale network with spiking neurons, the simple point neuron, such as the integrate-and-fire neuron, is often used. However, recent experimental evidence suggests that the computational ability of a single neuron is largely enhanced by its morphological structure, in particular, by various types of dendritic dynamics. As the morphology reduction of detailed biophysical models is a classic question in systems neuroscience, much effort has been taken to simulate a neuron with a few compartments to include the interaction between the soma and dendritic spines. Yet, novel reduction methods are still needed to deal with the complex dendritic tree. Here, using 10 individual Purkinje cells of the cerebellum from three species of guinea-pig, mouse and rat, we consider four types of reduction methods and study their effects on the coding capacity of Purkinje cells in terms of firing rate, timing coding, spiking pattern, and modulated firing under different stimulation protocols. We found that there is a variation of reduction performance depending on individual cells and species, however, all reduction methods can preserve, to some degree, firing activity of the full model of Purkinje cell. Therefore, when stimulating large-scale network of neurons, one has to choose a proper type of reduced neuronal model depending on the questions addressed. Among these reduction schemes, Branch method, that preserves the geometrical volume of neurons, can achieve the best balance among different performance measures of accuracy, simplification, and computational efficiency, and reproduce various phenomena shown in the full morphology model of Purkinje cells. Altogether, these results suggest that the Branch reduction scheme seems to provide a general guideline for reducing complex morphology into a few compartments without the loss of basic characteristics of the firing properties of neurons."}], "ArticleTitle": "Coding Capacity of Purkinje Cells With Different Schemes of Morphological Reduction."}, "32774245": {"mesh": [], "AbstractText": [{"section": null, "text": "Generalization is the ability to apply past experience to similar but non-identical situations. It not only affects stimulus-outcome relationships, as observed in conditioning experiments, but may also be essential for adaptive behaviors, which involve the interaction between individuals and their environment. Computational modeling could potentially clarify the effect of generalization on adaptive behaviors and how this effect emerges from the underlying computation. Recent neurobiological observation indicated that the striatal dopamine system achieves generalization and subsequent discrimination by updating the corticostriatal synaptic connections in differential response to reward and punishment. In this study, we analyzed how computational characteristics in this neurobiological system affects adaptive behaviors. We proposed a novel reinforcement learning model with multilayer neural networks in which the synaptic weights of only the last layer are updated according to the prediction error. We set fixed connections between the input and hidden layers to maintain the similarity of inputs in the hidden-layer representation. This network enabled fast generalization of reward and punishment learning, and thereby facilitated safe and efficient exploration of spatial navigation tasks. Notably, it demonstrated a quick reward approach and efficient punishment aversion in the early learning phase, compared to algorithms that do not show generalization. However, disturbance of the network that causes noisy generalization and impaired discrimination induced maladaptive valuation. These results suggested the advantage and potential drawback of computation by the striatal dopamine system with regard to adaptive behaviors."}], "ArticleTitle": "Computational Characteristics of the Striatal Dopamine System Described by Reinforcement Learning With Fast Generalization."}, "31649521": {"mesh": [], "AbstractText": [{"section": null, "text": "The fact that seeing with two eyes is universal among vertebrates raises a problem that has long challenged vision scientists: how do animals with overlapping visual fields combine non-identical right and left eye images to achieve fusion and the perception of depth that follows? Most theories address this problem in terms of matching corresponding images on the right and left retinas. Here we suggest an alternative theory of binocular vision based on anatomical correspondence that circumvents the correspondence problem and provides a rationale for ocular dominance."}], "ArticleTitle": "An Alternative Theory of Binocularity."}, "31456676": {"mesh": [], "AbstractText": [{"section": null, "text": "Introduction: While the prevalence of neurodegenerative diseases associated with dementia such as Alzheimer's disease (AD) increases, our knowledge on the underlying mechanisms, outcome predictors, or therapeutic targets is limited. In this work, we demonstrate how computational multi-scale brain modeling links phenomena of different scales and therefore identifies potential disease mechanisms leading the way to improved diagnostics and treatment. Methods: The Virtual Brain (TVB; thevirtualbrain.org) neuroinformatics platform allows standardized large-scale structural connectivity-based simulations of whole brain dynamics. We provide proof of concept for a novel approach that quantitatively links the effects of altered molecular pathways onto neuronal population dynamics. As a novelty, we connect chemical compounds measured with positron emission tomography (PET) with neural function in TVB addressing the phenomenon of hyperexcitability in AD related to the protein amyloid beta (Abeta). We construct personalized virtual brains based on an averaged healthy connectome and individual PET derived distributions of Abeta in patients with mild cognitive impairment (MCI, N = 8) and Alzheimer's Disease (AD, N = 10) and in age-matched healthy controls (HC, N = 15) using data from ADNI-3 data base (http://adni.loni.usc.edu). In the personalized virtual brains, individual Abeta burden modulates regional Excitation-Inhibition balance, leading to local hyperexcitation with high Abeta loads. We analyze simulated regional neural activity and electroencephalograms (EEG). Results: Known empirical alterations of EEG in patients with AD compared to HCs were reproduced by simulations. The virtual AD group showed slower frequencies in simulated local field potentials and EEG compared to MCI and HC groups. The heterogeneity of the Abeta load is crucial for the virtual EEG slowing which is absent for control models with homogeneous Abeta distributions. Slowing phenomena primarily affect the network hubs, independent of the spatial distribution of Abeta. Modeling the N-methyl-D-aspartate (NMDA) receptor antagonism of memantine in local population models, reveals potential functional reversibility of the observed large-scale alterations (reflected by EEG slowing) in virtual AD brains. Discussion: We demonstrate how TVB enables the simulation of systems effects caused by pathogenetic molecular candidate mechanisms in human virtual brains."}], "ArticleTitle": "Linking Molecular Pathways and Large-Scale Computational Modeling to Assess Candidate Disease Mechanisms and Pharmacodynamics in Alzheimer's Disease."}, "31798436": {"mesh": [], "AbstractText": [{"section": null, "text": "Synaptic changes induced by neural activity need to be consolidated to maintain memory over a timescale of hours. In experiments, synaptic consolidation can be induced by repeating a stimulation protocol several times and the effectiveness of consolidation depends crucially on the repetition frequency of the stimulations. We address the question: is there an understandable reason why induction protocols with repetitions at some frequency work better than sustained protocols-even though the accumulated stimulation strength might be exactly the same in both cases? In real synapses, plasticity occurs on multiple time scales from seconds (induction), to several minutes (early phase of long-term potentiation) to hours and days (late phase of synaptic consolidation). We use a simplified mathematical model of just two times scales to elucidate the above question in a purified setting. Our mathematical results show that, even in such a simple model, the repetition frequency of stimulation plays an important role for the successful induction, and stabilization, of potentiation."}], "ArticleTitle": "Optimal Stimulation Protocol in a Bistable Synaptic Consolidation Model."}, "32390817": {"mesh": [], "AbstractText": [{"section": null, "text": "This paper offers a prospectus of what might be achievable in the development of emotional recognition devices. It provides a conceptual overview of the free energy principle; including Markov blankets, active inference, and-in particular-a discussion of selfhood and theory of mind, followed by a brief explanation of how these concepts can explain both neural and cultural models of emotional inference. The underlying hypothesis is that emotion recognition and inference devices will evolve from state-of-the-art deep learning models into active inference schemes that go beyond marketing applications and become adjunct to psychiatric practice. Specifically, this paper proposes that a second wave of emotion recognition devices will be equipped with an emotional lexicon (or the ability to epistemically search for one), allowing the device to resolve uncertainty about emotional states by actively eliciting responses from the user and learning from these responses. Following this, a third wave of emotional devices will converge upon the user's generative model, resulting in the machine and human engaging in a reciprocal, prosocial emotional interaction, i.e., sharing a generative model of emotional states."}], "ArticleTitle": "An Investigation of the Free Energy Principle for Emotion Recognition."}, "32116619": {"mesh": [], "AbstractText": [{"section": null, "text": "Optogenetics is revolutionizing Neuroscience, but an often neglected effect of light stimulation of the brain is the generation of heat. In extreme cases, light-generated heat kills neurons, but mild temperature changes alter neuronal function. To date, most in vivo experiments rely on light stimulation of neural tissue using fiber-coupled lasers of various wavelengths. Brain tissue is irradiated with high light power that can be deleterious to neuronal function. Furthermore, absorbed light generates heat that can lead to permanent tissue damage and affect neuronal excitability. Thus, light alone can generate effects in neuronal function that are unrelated to the genuine \"optogenetic effect.\" In this work, we perform a theoretical analysis to investigate the effects of heat transfer in rodent brain tissue for standard optogenetic protocols. More precisely, we first use the Kubelka-Munk model for light propagation in brain tissue to observe the absorption phenomenon. Then, we model the optothermal effect considering the common laser wavelengths (473 and 593 nm) used in optogenetic experiments approaching the time/space numerical solution of Pennes' bio-heat equation with the Finite Element Method. Finally, we then modeled channelrhodopsin-2 in a single and spontaneous-firing neuron to explore the effect of heat in light stimulated neurons. We found that, at commonly used light intensities, laser radiation considerably increases the temperature in the surrounding tissue. This effect alters action potential size and shape and causes an increase in spontaneous firing frequency in a neuron model. However, the shortening of activation time constants generated by heat in the single firing neuron model produces action potential failures in response to light stimulation. We also found changes in the power spectrum density and a reduction in the time required for synchronization in an interneuron network model of gamma oscillations. Our findings indicate that light stimulation with intensities used in optogenetic experiments may affect neuronal function not only by direct excitation of light sensitive ion channels and/or pumps but also by generating heat. This approach serves as a guide to design optogenetic experiments that minimize the role of tissue heating in the experimental outcome."}], "ArticleTitle": "Modeling the Effect of Temperature on Membrane Response of Light Stimulation in Optogenetically-Targeted Neurons."}, "32733224": {"mesh": [], "AbstractText": [{"section": null, "text": "The storage of temporally precise spike patterns can be realized by a single neuron. A spiking neural network (SNN) model is utilized to demonstrate the ability to precisely recall a spike pattern after presenting a single input. We show by using a simulation study that the temporal properties of input patterns can be transformed into spatial patterns of local dendritic spikes. The localization of time-points of spikes is facilitated by phase-shift of the subthreshold membrane potential oscillations (SMO) in the dendritic branches, which modifies their excitability. In reference to the points in time of the arriving input, the dendritic spikes are triggered in different branches. To store spatially distributed patterns, two unsupervised learning mechanisms are utilized. Either synaptic weights to the branches, spatial representation of the temporal input pattern, are enhanced by spike-timing-dependent plasticity (STDP) or the oscillation power of SMOs in spiking branches is increased by dendritic spikes. For retrieval, spike bursts activate stored spatiotemporal patterns in dendritic branches, which reactivate the original somatic spike patterns. The simulation of the prototypical model demonstrates the principle, how linking time to space enables the storage of temporal features of an input. Plausibility, advantages, and some variations of the proposed model are also discussed."}], "ArticleTitle": "A Model of Memory Linking Time to Space."}, "32296323": {"mesh": [], "AbstractText": [{"section": null, "text": "Objective: In brain machine interfaces (BMIs), the functional mapping between neural activities and kinematic parameters varied over time owing to changes in neural recording conditions. The variability in neural recording conditions might result in unstable long-term decoding performance. Relevant studies trained decoders with several days of training data to make them inherently robust to changes in neural recording conditions. However, these decoders might not be robust to changes in neural recording conditions when only a few days of training data are available. In time-series prediction and feedback control system, an error feedback was commonly adopted to reduce the effects of model uncertainty. This motivated us to introduce an error feedback to a neural decoder for dealing with the variability in neural recording conditions. Approach: We proposed an evolutionary constructive and pruning neural network with error feedback (ECPNN-EF) as a neural decoder. The ECPNN-EF with partially connected topology decoded the instantaneous firing rates of each sorted unit into forelimb movement of a rat. Furthermore, an error feedback was adopted as an additional input to provide kinematic information and thus compensate for changes in functional mapping. The proposed neural decoder was trained on data collected from a water reward-related lever-pressing task for a rat. The first 2 days of data were used to train the decoder, and the subsequent 10 days of data were used to test the decoder. Main Results: The ECPNN-EF under different settings was evaluated to better understand the impact of the error feedback and partially connected topology. The experimental results demonstrated that the ECPNN-EF achieved significantly higher daily decoding performance with smaller daily variability when using the error feedback and partially connected topology. Significance: These results suggested that the ECPNN-EF with partially connected topology could cope with both within- and across-day changes in neural recording conditions. The error feedback in the ECPNN-EF compensated for decreases in decoding performance when neural recording conditions changed. This mechanism made the ECPNN-EF robust against changes in functional mappings and thus improved the long-term decoding stability when only a few days of training data were available."}], "ArticleTitle": "Inhibition of Long-Term Variability in Decoding Forelimb Trajectory Using Evolutionary Neural Networks With Error-Correction Learning."}, "31354462": {"mesh": [], "AbstractText": [{"section": null, "text": "Magnetic resonance images of brain tumors are routinely used in neuro-oncology clinics for diagnosis, treatment planning, and post-treatment tumor surveillance. Currently, physicians spend considerable time manually delineating different structures of the brain. Spatial and structural variations, as well as intensity inhomogeneity across images, make the problem of computer-assisted segmentation very challenging. We propose a new image segmentation framework for tumor delineation that benefits from two state-of-the-art machine learning architectures in computer vision, i.e., Inception modules and U-Net image segmentation architecture. Furthermore, our framework includes two learning regimes, i.e., learning to segment intra-tumoral structures (necrotic and non-enhancing tumor core, peritumoral edema, and enhancing tumor) or learning to segment glioma sub-regions (whole tumor, tumor core, and enhancing tumor). These learning regimes are incorporated into a newly proposed loss function which is based on the Dice similarity coefficient (DSC). In our experiments, we quantified the impact of introducing the Inception modules in the U-Net architecture, as well as, changing the objective function for the learning algorithm from segmenting the intra-tumoral structures to glioma sub-regions. We found that incorporating Inception modules significantly improved the segmentation performance (p < 0.001) for all glioma sub-regions. Moreover, in architectures with Inception modules, the models trained with the learning objective of segmenting the intra-tumoral structures outperformed the models trained with the objective of segmenting the glioma sub-regions for the whole tumor (p < 0.001). The improved performance is linked to multiscale features extracted by newly introduced Inception module and the modified loss function based on the DSC."}], "ArticleTitle": "Inception Modules Enhance Brain Tumor Segmentation."}, "32038208": {"mesh": [], "AbstractText": [{"section": null, "text": "Brain computer interfaces (BCI) for the rehabilitation of motor impairments exploit sensorimotor rhythms (SMR) in the electroencephalogram (EEG). However, the neurophysiological processes underpinning the SMR often vary over time and across subjects. Inherent intra- and inter-subject variability causes covariate shift in data distributions that impede the transferability of model parameters amongst sessions/subjects. Transfer learning includes machine learning-based methods to compensate for inter-subject and inter-session (intra-subject) variability manifested in EEG-derived feature distributions as a covariate shift for BCI. Besides transfer learning approaches, recent studies have explored psychological and neurophysiological predictors as well as inter-subject associativity assessment, which may augment transfer learning in EEG-based BCI. Here, we highlight the importance of measuring inter-session/subject performance predictors for generalized BCI frameworks for both normal and motor-impaired people, reducing the necessity for tedious and annoying calibration sessions and BCI training."}], "ArticleTitle": "Intra- and Inter-subject Variability in EEG-Based Sensorimotor Brain Computer Interface: A Review."}, "32477088": {"mesh": [], "AbstractText": [{"section": null, "text": "In computational neuroscience, spiking neurons are often analyzed as computing devices that register bits of information, with each action potential carrying at most one bit of Shannon entropy. Here, I question this interpretation by using Landauer's principle to estimate an upper limit for the quantity of thermodynamic information that can be processed within a single action potential in a typical mammalian neuron. A straightforward calculation shows that an action potential in a typical mammalian cortical pyramidal cell can process up to approximately 3.4 &#183; 1011 bits of thermodynamic information, or about 4.9 &#183; 1011 bits of Shannon entropy. This result suggests that an action potential can, in principle, carry much more than a single bit of Shannon entropy."}], "ArticleTitle": "Upper Limit on the Thermodynamic Information Content of an Action Potential."}, "31244636": {"mesh": [], "AbstractText": [{"section": null, "text": "Event-related fMRI have been widely used in locating brain regions which respond to specific tasks. However, activities of brain regions which modulate or indirectly participate in the response to a specific task are not event-related. Event-related fMRI can't locate these regulatory regions, detrimental to the integrity of the result that event-related fMRI revealed. Direct-current EEG shifts (DC shifts) have been found linked to the inner brain activity, a fusion DC shifts-fMRI method may have the ability to reveal a more complete response of the brain. In this study, we used DC shifts-fMRI to verify that even when responding to a very simple task, (1) The response of the brain is more complicated than event-related fMRI generally revealed and (2) DC shifts-fMRI have the ability of revealing brain regions whose responses are not in event-related way. We used a classical and simple paradigm which is often used in auditory cortex tonotopic mapping. Data were recorded from 50 subjects (25 male, 25 female) who were presented with randomly presented pure tone sequences with six different frequencies (200, 400, 800, 1,600, 3,200, 6,400 Hz). Our traditional fMRI results are consistent with previous findings that the activations are concentrated on the auditory cortex. Our DC shifts-fMRI results showed that the cingulate-caudate-thalamus network which underpins sustained attention is positively activated while the dorsal attention network and the right middle frontal gyrus which underpin attention orientation are negatively activated. The regional-specific correlations between DC shifts and brain networks indicate the complexity of the response of the brain even to a simple task and that the DC shifts can effectively reflect these non-event-related inner brain activities."}], "ArticleTitle": "DC Shifts-fMRI: A Supplement to Event-Related fMRI."}, "32528269": {"mesh": [], "AbstractText": [{"section": null, "text": "Comprehending how the brain functions requires an understanding of the dynamics of neuronal assemblies. Previous work used a mean-field reduction method to determine the collective dynamics of a large heterogeneous network of uniformly and globally coupled theta neurons, which are a canonical formulation of Type I neurons. However, in modeling neuronal networks, it is unreasonable to assume that the coupling strength between every pair of neurons is identical. The goal in the present work is to analytically examine the collective macroscopic behavior of a network of theta neurons that is more realistic in that it includes heterogeneity in the coupling strength as well as in neuronal excitability. We consider the occurrence of dynamical structures that give rise to complicated dynamics via bifurcations of macroscopic collective quantities, concentrating on two biophysically relevant cases: (1) predominantly excitable neurons with mostly excitatory connections, and (2) predominantly spiking neurons with inhibitory connections. We find that increasing the synaptic diversity moves these dynamical structures to distant extremes of parameter space, leaving simple collective equilibrium states in the physiologically relevant region. We also study the node vs. focus nature of stable macroscopic equilibrium solutions and discuss our results in the context of recent literature."}], "ArticleTitle": "Synaptic Diversity Suppresses Complex Collective Behavior in Networks of Theta Neurons."}, "32848685": {"mesh": [], "AbstractText": [{"section": null, "text": "Reliable propagation of slow-modulations of the firing rate across multiple layers of a feedforward network (FFN) has proven difficult to capture in spiking neural models. In this paper, we explore necessary conditions for reliable and stable propagation of time-varying asynchronous spikes whose instantaneous rate of changes-in fairly short time windows [20-100] msec-represents information of slow fluctuations of the stimulus. Specifically, we study the effect of network size, level of background synaptic noise, and the variability of synaptic delays in an FFN with all-to-all connectivity. We show that network size and the level of background synaptic noise, together with the strength of synapses, are substantial factors enabling the propagation of asynchronous spikes in deep layers of an FFN. In contrast, the variability of synaptic delays has a minor effect on signal propagation."}], "ArticleTitle": "Necessary Conditions for Reliable Propagation of Slowly Time-Varying Firing Rate."}, "33071766": {"mesh": [], "AbstractText": [{"section": null, "text": "The amount of visual information projected from the retina to the brain exceeds the information processing capacity of the latter. Attention, therefore, functions as a filter to highlight important information at multiple stages of the visual pathway that requires further and more detailed analysis. Among other functions, this determines where to fixate since only the fovea allows for high resolution imaging. Visual saliency modeling, i.e. understanding how the brain selects important information to analyze further and to determine where to fixate next, is an important research topic in computational neuroscience and computer vision. Most existing bottom-up saliency models use low-level features such as intensity and color, while some models employ high-level features, like faces. However, little consideration has been given to mid-level features, such as texture, for visual saliency models. In this paper, we extend a biologically plausible proto-object based saliency model by adding simple texture channels which employ nonlinear operations that mimic the processing performed by primate visual cortex. The extended model shows statistically significant improved performance in predicting human fixations compared to the previous model. Comparing the performance of our model with others on publicly available benchmarking datasets, we find that our biologically plausible model matches the performance of other models, even though those were designed entirely for maximal performance with little regard to biological realism."}], "ArticleTitle": "Proto-Object Based Saliency Model With Texture Detection Channel."}, "33178000": {"mesh": [], "AbstractText": [{"section": null, "text": "Learning from limited exemplars (few-shot learning) is a fundamental, unsolved problem that has been laboriously explored in the machine learning community. However, current few-shot learners are mostly supervised and rely heavily on a large amount of labeled examples. Unsupervised learning is a more natural procedure for cognitive mammals and has produced promising results in many machine learning tasks. In this paper, we propose an unsupervised feature learning method for few-shot learning. The proposed model consists of two alternate processes, progressive clustering and episodic training. The former generates pseudo-labeled training examples for constructing episodic tasks; and the later trains the few-shot learner using the generated episodic tasks which further optimizes the feature representations of data. The two processes facilitate each other, and eventually produce a high quality few-shot learner. In our experiments, our model achieves good generalization performance in a variety of downstream few-shot learning tasks on Omniglot and MiniImageNet. We also construct a new few-shot person re-identification dataset FS-Market1501 to demonstrate the feasibility of our model to a real-world application."}], "ArticleTitle": "Unsupervised Few-Shot Feature Learning via Self-Supervised Training."}, "33192427": {"mesh": [], "AbstractText": [{"section": null, "text": "Oscillations in the beta/low gamma range (10-45 Hz) are recorded in diverse neural structures. They have successfully been modeled as sparsely synchronized oscillations arising from reciprocal interactions between randomly connected excitatory (E) pyramidal cells and local interneurons (I). The synchronization of spatially distant oscillatory spiking E-I modules has been well-studied in the rate model framework but less so for modules of spiking neurons. Here, we first show that previously proposed modifications of rate models provide a quantitative description of spiking E-I modules of Exponential Integrate-and-Fire (EIF) neurons. This allows us to analyze the dynamical regimes of sparsely synchronized oscillatory E-I modules connected by long-range excitatory interactions, for two modules, as well as for a chain of such modules. For modules with a large number of neurons (> 105), we obtain results similar to previously obtained ones based on the classic deterministic Wilson-Cowan rate model, with the added bonus that the results quantitatively describe simulations of spiking EIF neurons. However, for modules with a moderate (~ 104) number of neurons, stochastic variations in the spike emission of neurons are important and need to be taken into account. On the one hand, they modify the oscillations in a way that tends to promote synchronization between different modules. On the other hand, independent fluctuations on different modules tend to disrupt synchronization. The correlations between distant oscillatory modules can be described by stochastic equations for the oscillator phases that have been intensely studied in other contexts. On shorter distances, we develop a description that also takes into account amplitude modes and that quantitatively accounts for our simulation data. Stochastic dephasing of neighboring modules produces transient phase gradients and the transient appearance of phase waves. We propose that these stochastically-induced phase waves provide an explanative framework for the observations of traveling waves in the cortex during beta oscillations."}], "ArticleTitle": "Synchronization, Stochasticity, and Phase Waves in Neuronal Networks With Spatially-Structured Connectivity."}, "33224031": {"mesh": [], "AbstractText": [{"section": null, "text": "Many current computational models that aim to simulate cortical and hippocampal modules of the brain depend on artificial neural networks. However, such classical or even deep neural networks are very slow, sometimes taking thousands of trials to obtain the final response with a considerable amount of error. The need for a large number of trials at learning and the inaccurate output responses are due to the complexity of the input cue and the biological processes being simulated. This article proposes a computational model for an intact and a lesioned cortico-hippocampal system using quantum-inspired neural networks. This cortico-hippocampal computational quantum-inspired (CHCQI) model simulates cortical and hippocampal modules by using adaptively updated neural networks entangled with quantum circuits. The proposed model is used to simulate various classical conditioning tasks related to biological processes. The output of the simulated tasks yielded the desired responses quickly and efficiently compared with other computational models, including the recently published Green model."}], "ArticleTitle": "Cortico-Hippocampal Computational Modeling Using Quantum-Inspired Neural Networks."}, "31507397": {"mesh": [], "AbstractText": [{"section": null, "text": "The mechanisms underlying an effective propagation of high intensity information over a background of irregular firing and response latency in cognitive processes remain unclear. Here we propose a SSCCPI circuit to address this issue. We hypothesize that when a high-intensity thalamic input triggers synchronous spike events (SSEs), dense spikes are scattered to many receiving neurons within a cortical column in layer IV, many sparse spike trains are propagated in parallel along minicolumns at a substantially high speed and finally integrated into an output spike train toward or in layer Va. We derive the sufficient conditions for an effective (fast, reliable, and precise) SSCCPI circuit: (i) SSEs are asynchronous (near synchronous); (ii) cortical columns prevent both repeatedly triggering SSEs and incorrectly synaptic connections between adjacent columns; and (iii) the propagator in interneurons is temporally complete fidelity and reliable. We encode the membrane potential responses to stimuli using the non-linear autoregressive integrated process derived by applying Newton's second law to stochastic resilience systems. We introduce a multithreshold decoder to correct encoding errors. Evidence supporting an effective SSCCPI circuit includes that for the condition, (i) time delay enhances SSEs, suggesting that response latency induces SSEs in high-intensity stimuli; irregular firing causes asynchronous SSEs; asynchronous SSEs relate to healthy neurons; and rigorous SSEs relate to brain disorders. For the condition (ii) neurons within a given minicolumn are stereotypically interconnected in the vertical dimension, which prevents repeated triggering SSEs and ensures signal parallel propagation; columnar segregation avoids incorrect synaptic connections between adjacent columns; and signal propagation across layers overwhelmingly prefers columnar direction. For the condition (iii), accumulating experimental evidence supports temporal transfer precision with millisecond fidelity and reliability in interneurons; homeostasis supports a stable fixed-point encoder by regulating changes to synaptic size, synaptic strength, and ion channel function in the membrane; together all-or-none modulation, active backpropagation, additive effects of graded potentials, and response variability functionally support the multithreshold decoder; our simulations demonstrate that the encoder-decoder is temporally complete fidelity and reliable in special intervals contained within the stable fixed-point range. Hence, the SSCCPI circuit provides a possible mechanism of effective signal propagation in cortical networks."}], "ArticleTitle": "Cellular and Network Mechanisms for Temporal Signal Propagation in a Cortical Network Model."}, "31316363": {"mesh": [], "AbstractText": [{"section": null, "text": "Time is a continuous, homogeneous, one-way, and independent signal that cannot be modified by human will. The mechanism of how the brain processes temporal information remains elusive. According to previous work, time-keeping in medial premotor cortex (MPC) is governed by four kinds of ramp cell populations (Merchant et al., 2011). We believe that these cell populations participate in temporal information processing in MPC. Hence, in this the present study, we present a model that uses spiking neuron, including these cell populations, to construct a complete circuit for temporal processing. By combining the time-adaptive drift-diffusion model (TDDM) with the transmission of impulse information between neurons, this new model is able to successfully reproduce the result of synchronization-continuation tapping task (SCT). We also discovered that the neurons that we used exhibited some of the firing properties of time-related neurons detected by electrophysiological experiments in other studies. Therefore, we believe that our model reflects many of the physiological of neural circuits in the biological brain and can explain some of the phenomena in the temporal-perception process."}], "ArticleTitle": "A Temporal Signal-Processing Circuit Based on Spiking Neuron and Synaptic Learning."}, "33117139": {"mesh": [], "AbstractText": [{"section": null, "text": "Synapses are highly stochastic transmission units. A classical model describing this stochastic transmission is called the binomial model, and its underlying parameters can be estimated from postsynaptic responses to evoked stimuli. The accuracy of parameter estimates obtained via such a model-based approach depends on the identifiability of the model. A model is said to be structurally identifiable if its parameters can be uniquely inferred from the distribution of its outputs. However, this theoretical property does not necessarily imply practical identifiability. For instance, if the number of observations is low or if the recording noise is high, the model's parameters can only be loosely estimated. Structural identifiability, which is an intrinsic property of a model, has been widely characterized; but practical identifiability, which is a property of both the model and the experimental protocol, is usually only qualitatively assessed. Here, we propose a formal definition for the practical identifiability domain of a statistical model. For a given experimental protocol, this domain corresponds to the set of parameters for which the model is correctly identified as the ground truth compared to a simpler alternative model. Considering a model selection problem instead of a parameter inference problem allows to derive a non-arbitrary criterion for practical identifiability. We apply our definition to the study of neurotransmitter release at a chemical synapse. Our contribution to the analysis of synaptic stochasticity is three-fold: firstly, we propose a quantitative criterion for the practical identifiability of a statistical model, and compute the identifiability domains of different variants of the binomial release model (uni or multi-quantal, with or without short-term plasticity); secondly, we extend the Bayesian Information Criterion (BIC), a classically used tool for model selection, to models with correlated data (which is the case for most models of chemical synapses); finally, we show that our approach allows to perform data free model selection, i.e., to verify if a model used to fit data was indeed identifiable even without access to the data, but having only access to the fitted parameters."}], "ArticleTitle": "Identifiability of a Binomial Synapse."}, "33013344": {"mesh": [], "AbstractText": [{"section": null, "text": "Large cortical and hippocampal pyramidal neurons are elements of neuronal circuitry that have been implicated in cross-frequency coupling (CFC) during cognitive tasks. We investigate potential mechanisms for CFC within these neurons by examining the role that the hyperpolarization-activated mixed cation current (Ih) plays in modulating CFC characteristics in multicompartment neuronal models. We quantify CFC along the soma-apical dendrite axis and tuft of three models configured to have different spatial distributions of Ih conductance density: (1) exponential gradient along the soma-apical dendrite axis, (2) uniform distribution, and (3) no Ih conductance. We simulated two current injection scenarios: distal apical 4 Hz modulation and perisomatic 4 Hz modulation, each with perisomatic, mid-apical, and distal apical 40 Hz injections. We used two metrics to quantify CFC strength-modulation index and height ratio-and we analyzed CFC phase properties. For all models, CFC was strongest in distal apical regions when the 40 Hz injection occurred near the soma and the 4 Hz modulation occurred in distal apical dendrite. The strongest CFC values were observed in the model with uniformly distributed Ih conductance density, but when the exponential gradient in Ih conductance density was added, CFC strength decreased by almost 50%. When Ih was in the model, regions with much larger membrane potential fluctuations at 4 Hz than at 40 Hz had stronger CFC. Excluding the Ih conductance from the model resulted in CFC either reduced or comparable in strength relative to the model with the exponential gradient in Ih conductance. The Ih conductance also imposed order on the phase characteristics of CFC such that minimum (maximum) amplitude 40 Hz membrane potential oscillations occurred during Ih conductance deactivation (activation). On the other hand, when there was no Ih conductance, phase relationships between minimum and maximum 40 Hz oscillation often inverted and occurred much closer together. This analysis can help experimentalists discriminate between CFC that originates from different underlying physiological mechanisms and can help illuminate the reasons why there are differences between CFC strength observed in different regions of the brain and between different populations of neurons based on the configuration of the Ih conductance."}], "ArticleTitle": "Assessing the Impact of Ih Conductance on Cross-Frequency Coupling in Model Pyramidal Neurons."}, "30853906": {"mesh": [], "AbstractText": [{"section": null, "text": "Electrical stimulation is a promising tool for interacting with neuronal dynamics to identify neural mechanisms that underlie cognitive function. Since effects of a single short stimulation pulse typically vary greatly and depend on the current network state, many experimental paradigms have rather resorted to continuous or periodic stimulation in order to establish and maintain a desired effect. However, such an approach explicitly leads to forced and \"unnatural\" brain activity. Further, continuous stimulation can make it hard to parse the recorded activity and separate neural signal from stimulation artifacts. In this study we propose an alternate strategy: by monitoring a system in realtime, we use the existing preferred states or attractors of the network and apply short and precise pulses in order to switch between those states. When pushed into one of its attractors, one can use the natural tendency of the system to remain in such a state to prolong the effect of a stimulation pulse, opening a larger window of opportunity to observe the consequences on cognitive processing. To elaborate on this idea, we consider flexible information routing in the visual cortex as a prototypical example. When processing a stimulus, neural populations in the visual cortex have been found to engage in synchronized gamma activity. In this context, selective signal routing is achieved by changing the relative phase between oscillatory activity in sending and receiving populations (communication through coherence, CTC). In order to explore how perturbations interact with CTC, we investigate a network of interneuronal gamma (ING) oscillators composed of integrate-and-fire neurons exhibiting similar synchronization and signal routing phenomena. We develop a closed-loop stimulation paradigm based on the phase-response characteristics of the network and demonstrate its ability to establish desired synchronization states. By measuring information content throughout the model, we evaluate the effect of signal contamination caused by the stimulation in relation to the magnitude of the injected pulses and intrinsic noise in the system. Finally, we demonstrate that, up to a critical noise level, precisely timed perturbations can be used to artificially induce the effect of attention by selectively routing visual signals to higher cortical areas."}], "ArticleTitle": "Causally Investigating Cortical Dynamics and Signal Processing by Targeting Natural System Attractors With Precisely Timed (Electrical) Stimulation."}, "32655388": {"mesh": [], "AbstractText": [{"section": null, "text": "The ventral visual stream (VVS) is a fundamental pathway involved in visual object identification and recognition. In this work, we present a hypothesis of a sequence of computations performed by the VVS during object recognition. The operations performed by the inferior temporal (IT) cortex are represented as not being akin to a neural-network, but rather in-line with a dynamic inference instantiation of the untangling notion. The presentation draws upon a technique for dynamic maximum a posteriori probability (MAP) sequence estimation based on the Viterbi algorithm. Simulation results are presented to show that the decoding portion of the architecture that is associated with the IT can effectively untangle object identity when presented with synthetic data. More importantly, we take a step forward in visual neuroscience by presenting a framework for an inference-based approach that is biologically inspired via attributes implicated in primate object recognition. The analysis will provide insight in explaining the exceptional proficiency of the VVS."}], "ArticleTitle": "Object Recognition at Higher Regions of the Ventral Visual Stream via Dynamic Inference."}, "32694989": {"mesh": [], "AbstractText": [{"section": null, "text": "Normative models of neural computation offer simplified yet lucid mathematical descriptions of murky biological phenomena. Previously, online Principal Component Analysis (PCA) was used to model a network of single-compartment neurons accounting for weighted summation of upstream neural activity in the soma and Hebbian/anti-Hebbian synaptic learning rules. However, synaptic plasticity in biological neurons often depends on the integration of synaptic currents over a dendritic compartment rather than total current in the soma. Motivated by this observation, we model a pyramidal neuronal network using online Canonical Correlation Analysis (CCA). Given two related datasets represented by distal and proximal dendritic inputs, CCA projects them onto the subspace which maximizes the correlation between their projections. First, adopting a normative approach and starting from a single-channel CCA objective function, we derive an online gradient-based optimization algorithm whose steps can be interpreted as the operation of a pyramidal neuron. To model networks of pyramidal neurons, we introduce a novel multi-channel CCA objective function, and derive from it an online gradient-based optimization algorithm whose steps can be interpreted as the operation of a pyramidal neuron network including its architecture, dynamics, and synaptic learning rules. Next, we model a neuron with more than two dendritic compartments by deriving its operation from a known objective function for multi-view CCA. Finally, we confirm the functionality of our networks via numerical simulations. Overall, our work presents a simplified but informative abstraction of learning in a pyramidal neuron network, and demonstrates how such networks can integrate multiple sources of inputs."}], "ArticleTitle": "Neurons as Canonical Correlation Analyzers."}, "32038214": {"mesh": [], "AbstractText": [{"section": null, "text": "Space generally overshadows time in the construction of theories in cognitive neuroscience. In this paper, we pivot from the spatial axes to the temporal, analyzing fMRI image series to reveal structures in time rather than space. To determine affinities among global brain patterns at different times, core concepts in network analysis (derived from graph theory) were applied temporally, as relations among brain images at every time point during an fMRI scanning epoch. To explore the temporal structures observed through this adaptation of network analysis, data from 180 subjects in the Human Connectome Project were examined, during two experimental conditions: passive movie viewing and rest. The temporal brain, like the spatial brain, exhibits a modular structure, where \"modules\" are intermittent (distributed in time). These temporal entities are here referred to as themes. Short sequences of themes - motifs - were studied in sequences from 4 to 11 s in length. Many motifs repeated at constant intervals, and are therefore rhythmic; rhythms, converted to frequencies, were often harmonic. We speculate that the structure and interaction of these global oscillations underwrites the capacity to experience and navigate a world which is both recognizably stable and noticeably changing at every moment - a temporal world. In its temporal structure, this brain-constituted world resembles music."}], "ArticleTitle": "The Musical Structure of Time in the Brain: Repetition, Rhythm, and Harmony in fMRI During Rest and Passive Movie Viewing."}, "31019458": {"mesh": [], "AbstractText": [{"section": null, "text": "We show that deep networks can be trained using Hebbian updates yielding similar performance to ordinary back-propagation on challenging image datasets. To overcome the unrealistic symmetry in connections between layers, implicit in back-propagation, the feedback weights are separate from the feedforward weights. The feedback weights are also updated with a local rule, the same as the feedforward weights-a weight is updated solely based on the product of activity of the units it connects. With fixed feedback weights as proposed in Lillicrap et al. (2016) performance degrades quickly as the depth of the network increases. If the feedforward and feedback weights are initialized with the same values, as proposed in Zipser and Rumelhart (1990), they remain the same throughout training thus precisely implementing back-propagation. We show that even when the weights are initialized differently and at random, and the algorithm is no longer performing back-propagation, performance is comparable on challenging datasets. We also propose a cost function whose derivative can be represented as a local Hebbian update on the last layer. Convolutional layers are updated with tied weights across space, which is not biologically plausible. We show that similar performance is achieved with untied layers, also known as locally connected layers, corresponding to the connectivity implied by the convolutional layers, but where weights are untied and updated separately. In the linear case we show theoretically that the convergence of the error to zero is accelerated by the update of the feedback weights."}], "ArticleTitle": "Deep Learning With Asymmetric Connections and Hebbian Updates."}, "31191280": {"mesh": [], "AbstractText": [{"section": null, "text": "Many brain regions communicate information through synchronized network activity. Electrical coupling among the dendrites of interneurons in the cortex has been implicated in forming and sustaining such activity in the cortex. Evidence for the existence of electrical coupling among cortical pyramidal cells, however, has been largely absent. A recent experimental study measured properties of electrical connections between pyramidal cells in the cortex deemed \"electrotonic couplings.\" These junctions were seen to occur pair-wise, sparsely, and often coexist with electrically-coupled interneurons. Here, we construct a network model to investigate possible roles for these rare, electrotonically-coupled pyramidal-cell pairs. Through simulations, we show that electrical coupling among pyramidal-cell pairs significantly enhances coincidence-detection capabilities and increases network spike-timing precision. Further, a network containing multiple pairs exhibits large variability in its firing pattern, possessing a rich coding structure."}], "ArticleTitle": "A Role for Electrotonic Coupling Between Cortical Pyramidal Cells."}, "32581756": {"mesh": [], "AbstractText": [{"section": null, "text": "Objective: Assessing brain connectivity from electrophysiological signals is of great relevance in neuroscience, but results are still debated and depend crucially on how connectivity is defined and on mathematical instruments utilized. Aim of this work is to assess the capacity of bivariate Transfer Entropy (TE) to evaluate connectivity, using data generated from simple neural mass models of connected Regions of Interest (ROIs). Approach: Signals simulating mean field potentials were generated assuming two, three or four ROIs, connected via excitatory or by-synaptic inhibitory links. We investigated whether the presence of a statistically significant connection can be detected and if connection strength can be quantified. Main Results: Results suggest that TE can reliably estimate the strength of connectivity if neural populations work in their linear regions, and if the epoch lengths are longer than 10 s. In case of multivariate networks, some spurious connections can emerge (i.e., a statistically significant TE even in the absence of a true connection); however, quite a good correlation between TE and synaptic strength is still preserved. Moreover, TE appears more robust for distal regions (longer delays) compared with proximal regions (smaller delays): an approximate a priori knowledge on this delay can improve the procedure. Finally, non-linear phenomena affect the assessment of connectivity, since they may significantly reduce TE estimation: information transmission between two ROIs may be weak, due to non-linear phenomena, even if a strong causal connection is present. Significance: Changes in functional connectivity during different tasks or brain conditions, might not always reflect a true change in the connecting network, but rather a change in information transmission. A limitation of the work is the use of bivariate TE. In perspective, the use of multivariate TE can improve estimation and reduce some of the problems encountered in the present study."}], "ArticleTitle": "Transfer Entropy as a Measure of Brain Connectivity: A Critical Analysis With the Help of Neural Mass Models."}, "32194389": {"mesh": [], "AbstractText": [{"section": null, "text": "Human intelligence is constituted by a multitude of cognitive functions activated either directly or indirectly by external stimuli of various kinds. Computational approaches to the cognitive sciences and to neuroscience are partly premised on the idea that computational simulations of such cognitive functions and brain operations suspected to correspond to them can help to further uncover knowledge about those functions and operations, specifically, how they might work together. These approaches are also partly premised on the idea that empirical neuroscience research, whether following on from such a simulation (as indeed simulation and empirical research are complementary) or otherwise, could help us build better artificially intelligent systems. This is based on the assumption that principles by which the brain seemingly operate, to the extent that it can be understood as computational, should at least be tested as principles for the operation of artificial systems. This paper explores some of the principles of the brain that seem to be responsible for its autonomous, problem-adaptive nature. The brain operating system (BrainOS) explicated here is an introduction to ongoing work aiming to create a robust, integrated model, combining the connectionist paradigm underlying neural networks and the symbolic paradigm underlying much else of AI. BrainOS is an automatic approach that selects the most appropriate model based on the (a) input at hand, (b) prior experience (a history of results of prior problem solving attempts), and (c) world knowledge (represented in the symbolic way and used as a means to explain its approach). It is able to accept diverse and mixed input data types, process histories and objectives, extract knowledge and infer a situational context. BrainOS is designed to be efficient through its ability to not only choose the most suitable learning model but to effectively calibrate it based on the task at hand."}], "ArticleTitle": "BrainOS: A Novel Artificial Brain-Alike Automatic Machine Learning Framework."}, "32528268": {"mesh": [], "AbstractText": [{"section": null, "text": "Systems of coupled dynamical units (e.g., oscillators or neurons) are known to exhibit complex, emergent behaviors that may be simplified through coarse-graining: a process in which one discovers coarse variables and derives equations for their evolution. Such coarse-graining procedures often require extensive experience and/or a deep understanding of the system dynamics. In this paper we present a systematic, data-driven approach to discovering \"bespoke\" coarse variables based on manifold learning algorithms. We illustrate this methodology with the classic Kuramoto phase oscillator model, and demonstrate how our manifold learning technique can successfully identify a coarse variable that is one-to-one with the established Kuramoto order parameter. We then introduce an extension of our coarse-graining methodology which enables us to learn evolution equations for the discovered coarse variables via an artificial neural network architecture templated on numerical time integrators (initial value solvers). This approach allows us to learn accurate approximations of time derivatives of state variables from sparse flow data, and hence discover useful approximate differential equation descriptions of their dynamic behavior. We demonstrate this capability by learning ODEs that agree with the known analytical expression for the Kuramoto order parameter dynamics at the continuum limit. We then show how this approach can also be used to learn the dynamics of coarse variables discovered through our manifold learning methodology. In both of these examples, we compare the results of our neural network based method to typical finite differences complemented with geometric harmonics. Finally, we present a series of computational examples illustrating how a variation of our manifold learning methodology can be used to discover sets of \"effective\" parameters, reduced parameter combinations, for multi-parameter models with complex coupling. We conclude with a discussion of possible extensions of this approach, including the possibility of obtaining data-driven effective partial differential equations for coarse-grained neuronal network behavior, as illustrated by the synchronization dynamics of Hodgkin-Huxley type neurons with a Chung-Lu network. Thus, we build an integrated suite of tools for obtaining data-driven coarse variables, data-driven effective parameters, and data-driven coarse-grained equations from detailed observations of networks of oscillators."}], "ArticleTitle": "Emergent Spaces for Coupled Oscillators."}, "31632258": {"mesh": [], "AbstractText": [{"section": null, "text": "Sensorimotor signals are integrated and processed by the cerebellar circuit to predict accurate control of actions. In order to investigate how single neuron dynamics and geometrical modular connectivity affect cerebellar processing, we have built an olivocerebellar Spiking Neural Network (SNN) based on a novel simplification algorithm for single point models (Extended Generalized Leaky Integrate and Fire, EGLIF) capturing essential non-linear neuronal dynamics (e.g., pacemaking, bursting, adaptation, oscillation and resonance). EGLIF models specifically tuned for each neuron type were embedded into an olivocerebellar scaffold reproducing realistic spatial organization and physiological convergence and divergence ratios of connections. In order to emulate the circuit involved in an eye blink response to two associated stimuli, we modeled two adjacent olivocerebellar microcomplexes with a common mossy fiber input but different climbing fiber inputs (either on or off). EGLIF-SNN model simulations revealed the emergence of fundamental response properties in Purkinje cells (burst-pause) and deep nuclei cells (pause-burst) similar to those reported in vivo. The expression of these properties depended on the specific activation of climbing fibers in the microcomplexes and did not emerge with scaffold models using simplified point neurons. This result supports the importance of embedding SNNs with realistic neuronal dynamics and appropriate connectivity and anticipates the scale-up of EGLIF-SNN and the embedding of plasticity rules required to investigate cerebellar functioning at multiple scales."}], "ArticleTitle": "Response Dynamics in an Olivocerebellar Spiking Neural Network With Non-linear Neuron Properties."}, "29946248": {"mesh": [], "AbstractText": [{"section": null, "text": "Magnetoencephalography (MEG) captures the magnetic fields generated by neuronal current sources with sensors outside the head. In MEG analysis these current sources are estimated from the measured data to identify the locations and time courses of neural activity. Since there is no unique solution to this so-called inverse problem, multiple source estimation techniques have been developed. The nulling beamformer (NB), a modified form of the linearly constrained minimum variance (LCMV) beamformer, is specifically used in the process of inferring interregional interactions and is designed to eliminate shared signal contributions, or cross-talk, between regions of interest (ROIs) that would otherwise interfere with the connectivity analyses. The nulling beamformer applies the truncated singular value decomposition (TSVD) to remove small signal contributions from a ROI to the sensor signals. However, ROIs with strong crosstalk will have high separating power in the weaker components, which may be removed by the TSVD operation. To address this issue we propose a new method, the nulling beamformer with subspace suppression (NBSS). This method, controlled by a tuning parameter, reweights the singular values of the gain matrix mapping from source to sensor space such that components with high overlap are reduced. By doing so, we are able to measure signals between nearby source locations with limited cross-talk interference, allowing for reliable cortical connectivity analysis between them. In two simulations, we demonstrated that NBSS reduces cross-talk while retaining ROIs' signal power, and has higher separating power than both the minimum norm estimate (MNE) and the nulling beamformer without subspace suppression. We also showed that NBSS successfully localized the auditory M100 event-related field in primary auditory cortex, measured from a subject undergoing an auditory localizer task, and suppressed cross-talk in a nearby region in the superior temporal sulcus."}], "ArticleTitle": "Improving the Nulling Beamformer Using Subspace Suppression."}, "31551744": {"mesh": [], "AbstractText": [{"section": null, "text": "Metastability refers to the fact that the state of a dynamical system spends a large amount of time in a restricted region of its available phase space before a transition takes place, bringing the system into another state from where it might recur into the previous one. beim Graben and Hutt (2013) suggested to use the recurrence plot (RP) technique introduced by Eckmann et al. (1987) for the segmentation of system's trajectories into metastable states using recurrence grammars. Here, we apply this recurrence structure analysis (RSA) for the first time to resting-state brain dynamics obtained from functional magnetic resonance imaging (fMRI). Brain regions are defined according to the brain hierarchical atlas (BHA) developed by Diez et al. (2015), and as a consequence, regions present high-connectivity in both structure (obtained from diffusion tensor imaging) and function (from the blood-level dependent-oxygenation-BOLD-signal). Remarkably, regions observed by Diez et al. were completely time-invariant. Here, in order to compare this static picture with the metastable systems dynamics obtained from the RSA segmentation, we determine the number of metastable states as a measure of complexity for all subjects and for region numbers varying from 3 to 100. We find RSA convergence toward an optimal segmentation of 40 metastable states for normalized BOLD signals, averaged over BHA modules. Next, we build a bistable dynamics at population level by pooling 30 subjects after Hausdorff clustering. In link with this finding, we reflect on the different modeling frameworks that can allow for such scenarios: heteroclinic dynamics, dynamics with riddled basins of attraction, multiple-timescale dynamics. Finally, we characterize the metastable states both functionally and structurally, using templates for resting state networks (RSNs) and the automated anatomical labeling (AAL) atlas, respectively."}], "ArticleTitle": "Metastable Resting State Brain Dynamics."}, "32116623": {"mesh": [], "AbstractText": [{"section": null, "text": "In this work, we propose a novel cascaded V-Nets method to segment brain tumor substructures in multimodal brain magnetic resonance imaging. Although V-Net has been successfully used in many segmentation tasks, we demonstrate that its performance could be further enhanced by using a cascaded structure and ensemble strategy. Briefly, our baseline V-Net consists of four levels with encoding and decoding paths and intra- and inter-path skip connections. Focal loss is chosen to improve performance on hard samples as well as balance the positive and negative samples. We further propose three preprocessing pipelines for multimodal magnetic resonance images to train different models. By ensembling the segmentation probability maps obtained from these models, segmentation result is further improved. In other hand, we propose to segment the whole tumor first, and then divide it into tumor necrosis, edema, and enhancing tumor. Experimental results on BraTS 2018 online validation set achieve average Dice scores of 0.9048, 0.8364, and 0.7748 for whole tumor, tumor core and enhancing tumor, respectively. The corresponding values for BraTS 2018 online testing set are 0.8761, 0.7953, and 0.7364, respectively. We also evaluate the proposed method in two additional data sets from local hospitals comprising of 28 and 28 subjects, and the best results are 0.8635, 0.8036, and 0.7217, respectively. We further make a prediction of patient overall survival by ensembling multiple classifiers for long, mid and short groups, and achieve accuracy of 0.519, mean square error of 367240 and Spearman correlation coefficient of 0.168 for BraTS 2018 online testing set."}], "ArticleTitle": "Segmenting Brain Tumor Using Cascaded V-Nets in Multimodal MR Images."}, "32982709": {"mesh": [], "AbstractText": [{"section": null, "text": "Chronotherapy is a treatment for mood disorders, including major depressive disorder, mania, and bipolar disorder (BD). Neurotransmitters associated with the pathology of mood disorders exhibit circadian rhythms. A functional deficit in the neural circuits related to mood disorders disturbs the circadian rhythm; chronotherapy is an intervention that helps resynchronize the patient's biological clock with the periodic daily cycle, leading to amelioration of symptoms. In previous reports, Hadaeghi et al. proposed a non-linear dynamic model composed of the frontal and sensory cortical neural networks and the hypothalamus to explain the relationship between deficits in neural function in the frontal cortex and the disturbed circadian rhythm/mood transitions in BD (hereinafter referred to as the Hadaeghi model). In this model, neural activity in the frontal and sensory lobes exhibits periodic behavior in the healthy state; while in BD, this neural activity is in a state of chaos-chaos intermittency; this temporal departure from the healthy periodic state disturbs the circadian pacemaker in the hypothalamus. In this study, we propose an intervention based on a feedback method called the \"reduced region of orbit\" (RRO) method to facilitate the transition of the disturbed frontal cortical neural activity underlying BD to healthy periodic activity. Our simulation was based on the Hadaeghi model. We used an RRO feedback signal based on the return-map structure of the simulated frontal and sensory lobes to induce synchronization with a relatively weak periodic signal corresponding to the healthy condition by applying feedback of appropriate strength. The RRO feedback signal induces chaotic resonance, which facilitates the transition to healthy, periodic frontal neural activity, although this synchronization is restricted to a relatively low frequency of the periodic input signal. Additionally, applying an appropriate strength of the RRO feedback signal lowered the amplitude of the periodic input signal required to induce a synchronous state compared with the periodic signal applied alone. In conclusion, through a chaotic-resonance effect induced by the RRO feedback method, the state of the disturbed frontal neural activity characteristic of BD was transformed into a state close to healthy periodic activity by relatively weak periodic perturbations. Thus, RRO feedback-modulated chronotherapy might be an innovative new type of minimally invasive chronotherapy."}], "ArticleTitle": "Transition of Neural Activity From the Chaotic Bipolar-Disorder State to the Periodic Healthy State Using External Feedback Signals."}, "27672364": {"mesh": [], "AbstractText": [{"section": null, "text": "Multiscale modeling and simulations in neuroscience is gaining scientific attention due to its growing importance and unexplored capabilities. For instance, it can help to acquire better understanding of biological phenomena that have important features at multiple scales of time and space. This includes synaptic plasticity, memory formation and modulation, homeostasis. There are several ways to organize multiscale simulations depending on the scientific problem and the system to be modeled. One of the possibilities is to simulate different components of a multiscale system simultaneously and exchange data when required. The latter may become a challenging task for several reasons. First, the components of a multiscale system usually span different spatial and temporal scales, such that rigorous analysis of possible coupling solutions is required. Then, the components can be defined by different mathematical formalisms. For certain classes of problems a number of coupling mechanisms have been proposed and successfully used. However, a strict mathematical theory is missing in many cases. Recent work in the field has not so far investigated artifacts that may arise during coupled integration of different approximation methods. Moreover, in neuroscience, the coupling of widely used numerical fixed step size solvers may lead to unexpected inefficiency. In this paper we address the question of possible numerical artifacts that can arise during the integration of a coupled system. We develop an efficient strategy to couple the components comprising a multiscale test problem in neuroscience. We introduce an efficient coupling method based on the second-order backward differentiation formula (BDF2) numerical approximation. The method uses an adaptive step size integration with an error estimation proposed by Skelboe (2000). The method shows a significant advantage over conventional fixed step size solvers used in neuroscience for similar problems. We explore different coupling strategies that define the organization of computations between system components. We study the importance of an appropriate approximation of exchanged variables during the simulation. The analysis shows a substantial impact of these aspects on the solution accuracy in the application to our multiscale neuroscientific test problem. We believe that the ideas presented in the paper may essentially contribute to the development of a robust and efficient framework for multiscale brain modeling and simulations in neuroscience. "}], "ArticleTitle": "Efficient Integration of Coupled Electrical-Chemical Systems in Multiscale Neuronal Simulations."}, "27713698": {"mesh": [], "AbstractText": [{"section": null, "text": "Stress assessment has been under study in the last years. Both biochemical and physiological markers have been used to measure stress level. In neuroscience, several studies have related modification of stress level to brain activity changes in limbic system and frontal regions, by using non-invasive techniques such as functional magnetic resonance imaging (fMRI) and electroencephalography (EEG). In particular, previous studies suggested that the exhibition or inhibition of certain brain rhythms in frontal cortical areas indicates stress. However, there is no established marker to measure stress level by EEG. In this work, we aimed to prove the usefulness of the prefrontal relative gamma power (RG) for stress assessment. We conducted a study based on stress and relaxation periods. Six healthy subjects performed the Montreal Imaging Stress Task (MIST) followed by a stay within a relaxation room while EEG and electrocardiographic signals were recorded. Our results showed that the prefrontal RG correlated with the expected stress level and with the heart rate (HR; 0.8). In addition, the difference in prefrontal RG between time periods of different stress level was statistically significant (p < 0.01). Moreover, the RG was more discriminative between stress levels than alpha asymmetry, theta, alpha, beta, and gamma power in prefrontal cortex. We propose the prefrontal RG as a marker for stress assessment. Compared with other established markers such as the HR or the cortisol, it has higher temporal resolution. Additionally, it needs few electrodes located at non-hairy head positions, thus facilitating the use of non-invasive dry wearable real-time devices for ubiquitous assessment of stress."}], "ArticleTitle": "Stress Assessment by Prefrontal Relative Gamma."}, "30034330": {"mesh": [], "AbstractText": [{"section": null, "text": "Neuronal action potentials or spikes provide a long-range, noise-resistant means of communication between neurons. As point processes single spikes contain little information in themselves, i.e., outside the context of spikes from other neurons. Moreover, they may fail to cross a synapse. A burst, which consists of a short, high frequency train of spikes, will more reliably cross a synapse, increasing the likelihood of eliciting a postsynaptic spike, depending on the specific short-term plasticity at that synapse. Both the number and the temporal pattern of spikes in a burst provide a coding space that lies within the temporal integration realm of single neurons. Bursts have been observed in many species, including the non-mammalian, and in brain regions that range from subcortical to cortical. Despite their widespread presence and potential relevance, the uncertainties of how to classify bursts seems to have limited the research into the coding possibilities for bursts. The present series of research articles provides new insights into the relevance and interpretation of bursts across different neural circuits, and new methods for their analysis. Here, we provide a succinct introduction to the history of burst coding and an overview of recent work on this topic."}], "ArticleTitle": "Neural Coding With Bursts-Current State and Future Perspectives."}, "31396067": {"mesh": [], "AbstractText": [{"section": null, "text": "A major goal of neuroscience is understanding how neurons arrange themselves into neural networks that result in behavior. Most theoretical and experimental efforts have focused on a top-down approach which seeks to identify neuronal correlates of behaviors. This has been accomplished by effectively mapping specific behaviors to distinct neural patterns, or by creating computational models that produce a desired behavioral outcome. Nonetheless, these approaches have only implicitly considered the fact that neural tissue, like any other physical system, is subjected to several restrictions and boundaries of operations. Here, we proposed a new, bottom-up conceptual paradigm: The Energy Homeostasis Principle, where the balance between energy income, expenditure, and availability are the key parameters in determining the dynamics of neuronal phenomena found from molecular to behavioral levels. Neurons display high energy consumption relative to other cells, with metabolic consumption of the brain representing 20% of the whole-body oxygen uptake, contrasting with this organ representing only 2% of the body weight. Also, neurons have specialized surrounding tissue providing the necessary energy which, in the case of the brain, is provided by astrocytes. Moreover, and unlike other cell types with high energy demands such as muscle cells, neurons have strict aerobic metabolism. These facts indicate that neurons are highly sensitive to energy limitations, with Gibb's free energy dictating the direction of all cellular metabolic processes. From this activity, the largest energy, by far, is expended by action potentials and post-synaptic potentials; therefore, plasticity can be reinterpreted in terms of their energy context. Consequently, neurons, through their synapses, impose energy demands over post-synaptic neurons in a close loop-manner, modulating the dynamics of local circuits. Subsequently, the energy dynamics end up impacting the homeostatic mechanisms of neuronal networks. Furthermore, local energy management also emerges as a neural population property, where most of the energy expenses are triggered by sensory or other modulatory inputs. Local energy management in neurons may be sufficient to explain the emergence of behavior, enabling the assessment of which properties arise in neural circuits and how. Essentially, the proposal of the Energy Homeostasis Principle is also readily testable for simple neuronal networks."}], "ArticleTitle": "The Energy Homeostasis Principle: Neuronal Energy Regulation Drives Local Network Dynamics Generating Behavior."}, "31749692": {"mesh": [], "AbstractText": [{"section": null, "text": "Increased beta-band oscillatory activity in the basal ganglia network is associated with Parkinsonian motor symptoms and is suppressed with medication and deep brain stimulation (DBS). The origins of the beta-band oscillations, however, remains unclear with both intrinsic oscillations arising within the subthalamic nucleus (STN)-external globus pallidus (GPe) network and exogenous beta-activity, originating outside the network, proposed as potential sources of the pathological activity. The aim of this study was to explore the relative contribution of autonomous oscillations and exogenous oscillatory inputs in the generation of pathological oscillatory activity in a biophysically detailed model of the parkinsonian STN-GPe network. The network model accounts for the integration of synaptic currents and their interaction with intrinsic membrane currents in dendritic structures within the STN and GPe. The model was used to investigate the development of beta-band synchrony and bursting within the STN-GPe network by changing the balance of excitation and inhibition in both nuclei, and by adding exogenous oscillatory inputs with varying phase relationships through the hyperdirect cortico-subthalamic and indirect striato-pallidal pathways. The model showed an intrinsic susceptibility to beta-band oscillations that was manifest in weak autonomously generated oscillations within the STN-GPe network and in selective amplification of exogenous beta-band synaptic inputs near the network's endogenous oscillation frequency. The frequency at which this resonance peak occurred was determined by the net level of excitatory drive to the network. Intrinsic or endogenously generated oscillations were too weak to support a pacemaker role for the STN-GPe network, however, they were considerably amplified by sparse cortical beta inputs and were further amplified by striatal beta inputs that promoted anti-phase firing of the cortex and GPe, resulting in maximum transient inhibition of STN neurons. The model elucidates a mechanism of cortical patterning of the STN-GPe network through feedback inhibition whereby intrinsic susceptibility to beta-band oscillations can lead to phase locked spiking under parkinsonian conditions. These results point to resonance of endogenous oscillations with exogenous patterning of the STN-GPe network as a mechanism of pathological synchronization, and a role for the pallido-striatal feedback loop in amplifying beta oscillations."}], "ArticleTitle": "Beta-Band Resonance and Intrinsic Oscillations in a Biophysically Detailed Model of the Subthalamic Nucleus-Globus Pallidus Network."}, "30930759": {"mesh": [], "AbstractText": [{"section": null, "text": "Connectivity and biophysical processes determine the functionality of neuronal networks. We, therefore, developed a real-time framework, called Neural Interactome,, to simultaneously visualize and interact with the structure and dynamics of such networks. Neural Interactome is a cross-platform framework, which combines graph visualization with the simulation of neural dynamics, or experimentally recorded multi neural time series, to allow application of stimuli to neurons to examine network responses. In addition, Neural Interactome supports structural changes, such as disconnection of neurons from the network (ablation feature). Neural dynamics can be explored on a single neuron level (using a zoom feature), back in time (using a review feature), and recorded (using presets feature). The development of the Neural Interactome was guided by generic concepts to be applicable to neuronal networks with different neural connectivity and dynamics. We implement the framework using a model of the nervous system of Caenorhabditis elegans (C. elegans) nematode, a model organism with resolved connectome and neural dynamics. We show that Neural Interactome assists in studying neural response patterns associated with locomotion and other stimuli. In particular, we demonstrate how stimulation and ablation help in identifying neurons that shape particular dynamics. We examine scenarios that were experimentally studied, such as touch response circuit, and explore new scenarios that did not undergo elaborate experimental studies."}], "ArticleTitle": "Neural Interactome: Interactive Simulation of a Neuronal System."}, "29937722": {"mesh": [], "AbstractText": [{"section": null, "text": "Background: Computational modeling provides an important toolset for designing and analyzing neural stimulation devices to treat neurological disorders and diseases. Modeling enables efficient exploration of large parameter spaces, where preclinical and clinical studies would be infeasible. Current commercial finite element method software packages enable straightforward calculation of the potential distributions, but it is not always clear how to implement boundary conditions to appropriately represent metal stimulating electrodes. By quantifying the effects of different electrode representations on activation thresholds for model axons, we provide recommendations for accurate and efficient modeling of neural stimulating electrodes. Methods: We quantified the effects of different representations of current sources for neural stimulation in COMSOL Multiphysics for monopolar, bipolar, and multipolar electrode designs. Results: We recommend modeling each electrode contact as a thin platinum domain, modeling the electrode substrate with the conductivity of silicone, and either using a point current source in the center of each electrode contact or using a boundary current source. Alternatively, to avoid possible numerical instabilities associated with a large range of conductivity values (i.e., platinum and silicone) and to eliminate the small mesh elements required for thin electrode contacts, the electrode substrate can be assigned the conductivity of platinum by using insulating boundaries between the substrate and surrounding medium, and within the substrate to isolate the contacts from each other. When modeling more than one contact, we recommend using superposition by solving the model once for each contact, leaving inactive contacts floating, and superposing the resulting potentials. We computed comparable errors in activation thresholds across the different implementations in a simplified model (electrode in a homogeneous, isotropic medium), and in realistic models of rat spinal cord stimulation (SCS) and human deep brain stimulation, indicating that the recommended approaches are applicable to different stimulation targets."}], "ArticleTitle": "Modeling Current Sources for Neural Stimulation in COMSOL."}, "30190674": {"mesh": [], "AbstractText": [{"section": null, "text": "Neural oscillations were established with their association with neurophysiological activities and the altered rhythmic patterns are believed to be linked directly to the progression of cognitive decline. Magnetoencephalography (MEG) is a non-invasive technique to record such neuronal activity due to excellent temporal and fair amount of spatial resolution. Single channel, connectivity as well as brain network analysis using MEG data in resting state and task-based experiments were analyzed from existing literature. Single channel analysis studies reported a less complex, more regular and predictable oscillations in Alzheimer's disease (AD) primarily in the left parietal, temporal and occipital regions. Investigations on both functional connectivity (FC) and effective (EC) connectivity analysis demonstrated a loss of connectivity in AD compared to healthy control (HC) subjects found in higher frequency bands. It has been reported from multiplex network of MEG study in AD in the affected regions of hippocampus, posterior default mode network (DMN) and occipital areas, however, conclusions cannot be drawn due to limited availability of clinical literature. Potential utilization of high spatial resolution in MEG likely to provide information related to in-depth brain functioning and underlying factors responsible for changes in neuronal waves in AD. This review is a comprehensive report to investigate diagnostic biomarkers for AD may be identified by from MEG data. It is also important to note that MEG data can also be utilized for the same pursuit in combination with other imaging modalities."}], "ArticleTitle": "A Comprehensive Review of Magnetoencephalography (MEG) Studies for Brain Functionality in Healthy Aging and Alzheimer's Disease (AD)."}, "32982710": {"mesh": [], "AbstractText": [{"section": null, "text": "Scaffolds and patterned substrates are among the most successful strategies to dictate the connectivity between neurons in culture. Here, we used numerical simulations to investigate the capacity of physical obstacles placed on a flat substrate to shape structural connectivity, and in turn collective dynamics and effective connectivity, in biologically-realistic neuronal networks. We considered &#956;-sized obstacles placed in mm-sized networks. Three main obstacle shapes were explored, namely crosses, circles and triangles of isosceles profile. They occupied either a small area fraction of the substrate or populated it entirely in a periodic manner. From the point of view of structure, all obstacles promoted short length-scale connections, shifted the in- and out-degree distributions toward lower values, and increased the modularity of the networks. The capacity of obstacles to shape distinct structural traits depended on their density and the ratio between axonal length and substrate diameter. For high densities, different features were triggered depending on obstacle shape, with crosses trapping axons in their vicinity and triangles funneling axons along the reverse direction of their tip. From the point of view of dynamics, obstacles reduced the capacity of networks to spontaneously activate, with triangles in turn strongly dictating the direction of activity propagation. Effective connectivity networks, inferred using transfer entropy, exhibited distinct modular traits, indicating that the presence of obstacles facilitated the formation of local effective microcircuits. Our study illustrates the potential of physical constraints to shape structural blueprints and remodel collective activity, and may guide investigations aimed at mimicking organizational traits of biological neuronal circuits."}], "ArticleTitle": "Impact of Physical Obstacles on the Structural and Effective Connectivity of in silico Neuronal Circuits."}, "29375357": {"mesh": [], "AbstractText": [{"section": null, "text": "A growing number of tools now allow live recordings of various signaling pathways and protein-protein interaction dynamics in time and space by ratiometric measurements, such as Bioluminescence Resonance Energy Transfer (BRET) Imaging. Accurate and reproducible analysis of ratiometric measurements has thus become mandatory to interpret quantitative imaging. In order to fulfill this necessity, we have developed an open source toolset for Fiji-BRET-Analyzer-allowing a systematic analysis, from image processing to ratio quantification. We share this open source solution and a step-by-step tutorial at https://github.com/ychastagnier/BRET-Analyzer. This toolset proposes (1) image background subtraction, (2) image alignment over time, (3) a composite thresholding method of the image used as the denominator of the ratio to refine the precise limits of the sample, (4) pixel by pixel division of the images and efficient distribution of the ratio intensity on a pseudocolor scale, and (5) quantification of the ratio mean intensity and standard variation among pixels in chosen areas. In addition to systematize the analysis process, we show that the BRET-Analyzer allows proper reconstitution and quantification of the ratiometric image in time and space, even from heterogeneous subcellular volumes. Indeed, analyzing twice the same images, we demonstrate that compared to standard analysis BRET-Analyzer precisely define the luminescent specimen limits, enlightening proficient strengths from small and big ensembles over time. For example, we followed and quantified, in live, scaffold proteins interaction dynamics in neuronal sub-cellular compartments including dendritic spines, for half an hour. In conclusion, BRET-Analyzer provides a complete, versatile and efficient toolset for automated reproducible and meaningful image ratio analysis."}], "ArticleTitle": "Image Processing for Bioluminescence Resonance Energy Transfer Measurement-BRET-Analyzer."}, "31333437": {"mesh": [], "AbstractText": [{"section": null, "text": "This article develops a model of how reactive and planned behaviors interact in real time. Controllers for both animals and animats need reactive mechanisms for exploration, and learned plans to efficiently reach goal objects once an environment becomes familiar. The SOVEREIGN model embodied these capabilities, and was tested in a 3D virtual reality environment. Neural models have characterized important adaptive and intelligent processes that were not included in SOVEREIGN. A major research program is summarized herein by which to consistently incorporate them into an enhanced model called SOVEREIGN2. Key new perceptual, cognitive, cognitive-emotional, and navigational processes require feedback networks which regulate resonant brain states that support conscious experiences of seeing, feeling, and knowing. Also included are computationally complementary processes of the mammalian neocortical What and Where processing streams, and homologous mechanisms for spatial navigation and arm movement control. These include: Unpredictably moving targets are tracked using coordinated smooth pursuit and saccadic movements. Estimates of target and present position are computed in the Where stream, and can activate approach movements. Motion cues can elicit orienting movements to bring new targets into view. Cumulative movement estimates are derived from visual and vestibular cues. Arbitrary navigational routes are incrementally learned as a labeled graph of angles turned and distances traveled between turns. Noisy and incomplete visual sensor data are transformed into representations of visual form and motion. Invariant recognition categories are learned in the What stream. Sequences of invariant object categories are stored in a cognitive working memory, whereas sequences of movement positions and directions are stored in a spatial working memory. Stored sequences trigger learning of cognitive and spatial/motor sequence categories or plans, also called list chunks, which control planned decisions and movements toward valued goal objects. Predictively successful list chunk combinations are selectively enhanced or suppressed via reinforcement learning and incentive motivational learning. Expected vs. unexpected event disconfirmations regulate these enhancement and suppressive processes. Adaptively timed learning enables attention and action to match task constraints. Social cognitive joint attention enables imitation learning of skills by learners who observe teachers from different spatial vantage points."}], "ArticleTitle": "The Embodied Brain of SOVEREIGN2: From Space-Variant Conscious Percepts During Visual Search and Navigation to Learning Invariant Object Categories and Cognitive-Emotional Plans for Acquiring Valued Goals."}, "32161530": {"mesh": [], "AbstractText": [{"section": null, "text": "Modern machine learning is based on powerful algorithms running on digital computing platforms and there is great interest in accelerating the learning process and making it more energy efficient. In this paper we present a fully autonomous probabilistic circuit for fast and efficient learning that makes no use of digital computing. Specifically we use SPICE simulations to demonstrate a clockless autonomous circuit where the required synaptic weights are read out in the form of analog voltages. This allows us to demonstrate a circuit that can be built with existing technology to emulate the Boltzmann machine learning algorithm based on gradient optimization of the maximum likelihood function. Such autonomous circuits could be particularly of interest as standalone learning devices in the context of mobile and edge computing."}], "ArticleTitle": "Probabilistic Circuits for Autonomous Learning: A Simulation Study."}, "31632259": {"mesh": [], "AbstractText": [{"section": null, "text": "Nervous systems need to detect stimulus changes based on their neuronal responses without using any additional information on the number, times, and types of stimulus changes. Here, two relatively simple, biologically realistic change point detection methods are compared with two common analysis methods. The four methods are applied to intra- and extracellularly recorded responses of a single cricket interneuron (AN2) to acoustic simulation. Solely based on these recorded responses, the methods should detect an unknown number of different types of sound intensity in- and decreases shortly after their occurrences. For this task, the methods rely on calculating an adjusting interspike interval (ISI). Both simple methods try to separate responses to intensity in- or decreases from activity during constant stimulation. The Pure-ISI method performs this task based on the distribution of the ISI, while the ISI-Ratio method uses the ratio of actual and previous ISI. These methods are compared to the frequently used Moving-Average method, which calculates mean and standard deviation of the instantaneous spike rate in a moving interval. Additionally, a classification method provides the upper limit of the change point detection performance that can be expected for the cricket interneuron responses. The classification learns the statistical properties of the actual and previous ISI during stimulus changes and constant stimulation from a training data set. The main results are: (1) The Moving-Average method requires a stable activity in a long interval to estimate the previous activity, which was not always given in our data set. (2) The Pure-ISI method can reliably detect stimulus intensity increases when the neuron bursts, but it fails to identify intensity decreases. (3) The ISI-Ratio method detects stimulus in- and decreases well, if the spike train is not too noisy. (4) The classification method shows good performance for the detection of stimulus in- and decreases. But due to the statistical learning, this method tends to confuse responses to constant stimulation with responses triggered by a stimulus change. Our results suggest that stimulus change detection does not require computationally costly mechanisms. Simple nervous systems like the cricket's could effectively apply ISI-Ratios to solve this fundamental task."}], "ArticleTitle": "Online Detection of Multiple Stimulus Changes Based on Single Neuron Interspike Intervals."}, "32038215": {"mesh": [], "AbstractText": [{"section": null, "text": "It has been suggested that cholinergic neurons shape the oscillatory activity of the thalamocortical (TC) network in behavioral and electrophysiological experiments. However, theoretical modeling demonstrating how cholinergic neuromodulation of thalamocortical rhythms during non-rapid eye movement (NREM) sleep might occur has been lacking. In this paper, we first develop a novel computational model (TC-ACH) by incorporating a cholinergic neuron population (CH) into the classical thalamo-cortical circuitry, where connections between populations are modeled in accordance with existing knowledge. The neurotransmitter acetylcholine (ACH) released by neurons in CH, which is able to change the discharge activity of thalamocortical neurons, is the primary focus of our work. Simulation results with our TC-ACH model reveal that the cholinergic projection activity is a key factor in modulating oscillation patterns in three ways: (1) transitions between different patterns of thalamocortical oscillations are dramatically modulated through diverse projection pathways; (2) the model expresses a stable spindle oscillation state with certain parameter settings for the cholinergic projection from CH to thalamus, and more spindles appear when the strength of cholinergic input from CH to thalamocortical neurons increases; (3) the duration of oscillation patterns during NREM sleep including K-complexes, spindles, and slow oscillations is longer when cholinergic input from CH to thalamocortical neurons becomes stronger. Our modeling results provide insights into the mechanisms by which the sleep state is controlled, and provide a theoretical basis for future experimental and clinical studies."}], "ArticleTitle": "Effects of Cholinergic Neuromodulation on Thalamocortical Rhythms During NREM Sleep: A Model Study."}, "33178001": {"mesh": [], "AbstractText": [{"section": null, "text": "High-frequency firing activity can be induced either naturally in a healthy brain as a result of the processing of sensory stimuli or as an uncontrolled synchronous activity characterizing epileptic seizures. As part of this work, we investigate how logic circuits that are engineered in neurons can be used to design spike filters, attenuating high-frequency activity in a neuronal network that can be used to minimize the effects of neurodegenerative disorders such as epilepsy. We propose a reconfigurable filter design built from small neuronal networks that behave as digital logic circuits. We developed a mathematical framework to obtain a transfer function derived from a linearization process of the Hodgkin-Huxley model. Our results suggest that individual gates working as the output of the logic circuits can be used as a reconfigurable filtering technique. Also, as part of the analysis, the analytical model showed similar levels of attenuation in the frequency domain when compared to computational simulations by fine-tuning the synaptic weight. The proposed approach can potentially lead to precise and tunable treatments for neurological conditions that are inspired by communication theory."}], "ArticleTitle": "Reconfigurable Filtering of Neuro-Spike Communications Using Synthetically Engineered Logic Circuits."}, "29326578": {"mesh": [], "AbstractText": [{"section": null, "text": "Hemianopic patients exhibit visual detection improvement in the blind field when audiovisual stimuli are given in spatiotemporally coincidence. Beyond this \"online\" multisensory improvement, there is evidence of long-lasting, \"offline\" effects induced by audiovisual training: patients show improved visual detection and orientation after they were trained to detect and saccade toward visual targets given in spatiotemporal proximity with auditory stimuli. These effects are ascribed to the Superior Colliculus (SC), which is spared in these patients and plays a pivotal role in audiovisual integration and oculomotor behavior. Recently, we developed a neural network model of audiovisual cortico-collicular loops, including interconnected areas representing the retina, striate and extrastriate visual cortices, auditory cortex, and SC. The network simulated unilateral V1 lesion with possible spared tissue and reproduced \"online\" effects. Here, we extend the previous network to shed light on circuits, plastic mechanisms, and synaptic reorganization that can mediate the training effects and functionally implement visual rehabilitation. The network is enriched by the oculomotor SC-brainstem route, and Hebbian mechanisms of synaptic plasticity, and is used to test different training paradigms (audiovisual/visual stimulation in eye-movements/fixed-eyes condition) on simulated patients. Results predict different training effects and associate them to synaptic changes in specific circuits. Thanks to the SC multisensory enhancement, the audiovisual training is able to effectively strengthen the retina-SC route, which in turn can foster reinforcement of the SC-brainstem route (this occurs only in eye-movements condition) and reinforcement of the SC-extrastriate route (this occurs in presence of survived V1 tissue, regardless of eye condition). The retina-SC-brainstem circuit may mediate compensatory effects: the model assumes that reinforcement of this circuit can translate visual stimuli into short-latency saccades, possibly moving the stimuli into visual detection regions. The retina-SC-extrastriate circuit is related to restitutive effects: visual stimuli can directly elicit visual detection with no need for eye movements. Model predictions and assumptions are critically discussed in view of existing behavioral and neurophysiological data, forecasting that other oculomotor compensatory mechanisms, beyond short-latency saccades, are likely involved, and stimulating future experimental and theoretical investigations."}], "ArticleTitle": "Audiovisual Rehabilitation in Hemianopia: A Model-Based Theoretical Investigation."}, "32132914": {"mesh": [], "AbstractText": [{"section": null, "text": "Neurons in many brain regions exhibit spontaneous, intrinsic rhythmic firing activity. This rhythmic firing activity may determine the way in which these neurons respond to extrinsic synaptic inputs. We hypothesized that neurons should be most responsive to inputs at the frequency of the intrinsic oscillation frequency. We addressed this question in the ventral tegmental area (VTA), a dopaminergic nucleus in the midbrain. VTA neurons have a unique propensity to exhibit spontaneous intrinsic rhythmic activity in the 1-5 Hz frequency range, which persists in the in-vitro brain slice, and form a network of weakly coupled oscillators. Here, we combine in-vitro simultaneous recording of action potentials from a 60 channel multi-electrode-array with cell-type-specific optogenetic stimulation of the VTA dopamine neurons. We investigated how VTA neurons respond to wide-band stochastic (Poisson input) as well as regular laser pulses. Strong synchrony was induced between the laser input and the spike timing of the neurons, both for regular pulse trains and Poisson pulse trains. For rhythmically pulsed input, the neurons demonstrated resonant behavior with the strongest phase locking at their intrinsic oscillation frequency, but also at half and double the intrinsic oscillation frequency. Stochastic Poisson pulse stimulation provided a more effective stimulation of the entire population, yet we observed resonance at lower frequencies (approximately half the oscillation frequency) than the neurons' intrinsic oscillation frequency. The non-linear filter characteristics of dopamine neurons could allow the VTA to predict precisely timed future rewards based on past sensory inputs, a crucial component of reward prediction error signaling. In addition, these filter characteristics could contribute to a pacemaker role for the VTA in synchronizing activity with other regions like the prefrontal cortex and the hippocampus."}], "ArticleTitle": "Resonance in the Mouse Ventral Tegmental Area Dopaminergic Network Induced by Regular and Poisson Distributed Optogenetic Stimulation in-vitro."}, "32322196": {"mesh": [], "AbstractText": [{"section": null, "text": "Accurate segmentation of different sub-regions of gliomas such as peritumoral edema, necrotic core, enhancing, and non-enhancing tumor core from multimodal MRI scans has important clinical relevance in diagnosis, prognosis and treatment of brain tumors. However, due to the highly heterogeneous appearance and shape of these tumors, segmentation of the sub-regions is challenging. Recent developments using deep learning models has proved its effectiveness in various semantic and medical image segmentation tasks, many of which are based on the U-Net network structure with symmetric encoding and decoding paths for end-to-end segmentation due to its high efficiency and good performance. In brain tumor segmentation, the 3D nature of multimodal MRI poses challenges such as memory and computation limitations and class imbalance when directly adopting the U-Net structure. In this study we aim to develop a deep learning model using a 3D U-Net with adaptations in the training and testing strategies, network structures, and model parameters for brain tumor segmentation. Furthermore, instead of picking one best model, an ensemble of multiple models trained with different hyper-parameters are used to reduce random errors from each model and yield improved performance. Preliminary results demonstrate the effectiveness of this method and achieved the 9th place in the very competitive 2018 Multimodal Brain Tumor Segmentation (BraTS) challenge. In addition, to emphasize the clinical value of the developed segmentation method, a linear model based on the radiomics features extracted from segmentation and other clinical features are developed to predict patient overall survival. Evaluation of these innovations shows high prediction accuracy in both low-grade glioma and glioblastoma patients, which achieved the 1st place in the 2018 BraTS challenge."}], "ArticleTitle": "Brain Tumor Segmentation Using an Ensemble of 3D U-Nets and Overall Survival Prediction Using Radiomic Features."}, "30687055": {"mesh": [], "AbstractText": [{"section": null, "text": "Recent electrophysiological observations related to saccadic eye movements in rhesus monkeys, suggest a prediction of the sensory consequences of movement in the Purkinje cell layer of the cerebellar oculomotor vermis (OMV). A definite encoding of real-time motion of the eye has been observed in simple-spike responses of the combined burst-pause Purkinje cell populations, organized based upon their complex-spike directional tuning. However, the underlying control mechanisms that could lead to such action encoding are still unclear. We propose a saccade control model, with emphasis on the structure of the OMV and its interaction with the extra-cerebellar components. In the simulated bilateral organization of the OMV, each caudal fastigial nucleus is arranged to receive incoming projections from combined burst-pause Purkinje cell populations. The OMV, through the caudal fastigial nuclei, interacts with the brainstem to provide adaptive saccade gain corrections that minimize the visual error in reaching a given target location. The simulation results corroborate the experimental Purkinje cell population activity patterns and their relation with saccade kinematic metrics. The Purkinje layer activity that emerges from the proposed organization, precisely predicted the speed of the eye at different target eccentricities. Simulated granular layer activity suggests no separate dynamics with respect to shaping the bilateral Purkine layer activity. We further examine the validity of the simulated OMV in maintaining the accuracy of saccadic eye movements in the presence of signal dependent variabilities, that can occur in extra-cerebellar pathways."}], "ArticleTitle": "Modeling the Encoding of Saccade Kinematic Metrics in the Purkinje Cell Layer of the Cerebellar Vermis."}, "30792635": {"mesh": [], "AbstractText": [{"section": null, "text": "Recurrent neural networks can produce ongoing state-to-state transitions without any driving inputs, and the dynamical properties of these transitions are determined by the neuronal connection strengths. Due to non-linearity, it is not clear how strongly the system dynamics is affected by discrete local changes in the connection structure, such as the removal, addition, or sign-switching of individual connections. Moreover, there are no suitable metrics to quantify structural and dynamical differences between two given networks with arbitrarily indexed neurons. In this work, we present such permutation-invariant metrics and apply them to motifs of three binary neurons with discrete ternary connection strengths, an important class of building blocks in biological networks. Using multidimensional scaling, we then study the similarity relations between all 3,411 topologically distinct motifs with regard to structure and dynamics, revealing a strong clustering and various symmetries. As expected, the structural and dynamical distance between pairs of motifs show a significant positive correlation. Strikingly, however, the key parameter controlling motif dynamics turns out to be the ratio of excitatory to inhibitory connections."}], "ArticleTitle": "Analysis of Structure and Dynamics in Three-Neuron Motifs."}, "29551968": {"mesh": [], "AbstractText": [{"section": null, "text": "Recurrent networks of spiking neurons can be in an asynchronous state characterized by low or absent cross-correlations and spike statistics which resemble those of cortical neurons. Although spatial correlations are negligible in this state, neurons can show pronounced temporal correlations in their spike trains that can be quantified by the autocorrelation function or the spike-train power spectrum. Depending on cellular and network parameters, correlations display diverse patterns (ranging from simple refractory-period effects and stochastic oscillations to slow fluctuations) and it is generally not well-understood how these dependencies come about. Previous work has explored how the single-cell correlations in a homogeneous network (excitatory and inhibitory integrate-and-fire neurons with nearly balanced mean recurrent input) can be determined numerically from an iterative single-neuron simulation. Such a scheme is based on the fact that every neuron is driven by the network noise (i.e., the input currents from all its presynaptic partners) but also contributes to the network noise, leading to a self-consistency condition for the input and output spectra. Here we first extend this scheme to homogeneous networks with strong recurrent inhibition and a synaptic filter, in which instabilities of the previous scheme are avoided by an averaging procedure. We then extend the scheme to heterogeneous networks in which (i) different neural subpopulations (e.g., excitatory and inhibitory neurons) have different cellular or connectivity parameters; (ii) the number and strength of the input connections are random (Erd&#337;s-R&#233;nyi topology) and thus different among neurons. In all heterogeneous cases, neurons are lumped in different classes each of which is represented by a single neuron in the iterative scheme; in addition, we make a Gaussian approximation of the input current to the neuron. These approximations seem to be justified over a broad range of parameters as indicated by comparison with simulation results of large recurrent networks. Our method can help to elucidate how network heterogeneity shapes the asynchronous state in recurrent neural networks."}], "ArticleTitle": "Self-Consistent Scheme for Spike-Train Power Spectra in Heterogeneous Sparse Networks."}, "33154718": {"mesh": [], "AbstractText": [{"section": null, "text": "New technologies for recording the activity of large neural populations during complex behavior provide exciting opportunities for investigating the neural computations that underlie perception, cognition, and decision-making. Non-linear state space models provide an interpretable signal processing framework by combining an intuitive dynamical system with a probabilistic observation model, which can provide insights into neural dynamics, neural computation, and development of neural prosthetics and treatment through feedback control. This brings with it the challenge of learning both latent neural state and the underlying dynamical system because neither are known for neural systems a priori. We developed a flexible online learning framework for latent non-linear state dynamics and filtered latent states. Using the stochastic gradient variational Bayes approach, our method jointly optimizes the parameters of the non-linear dynamical system, the observation model, and the black-box recognition model. Unlike previous approaches, our framework can incorporate non-trivial distributions of observation noise and has constant time and space complexity. These features make our approach amenable to real-time applications and the potential to automate analysis and experimental design in ways that testably track and modify behavior using stimuli designed to influence learning."}], "ArticleTitle": "Variational Online Learning of Neural Dynamics."}, "30930761": {"mesh": [], "AbstractText": [{"section": null, "text": "To understand the computations that underlie high-level cognitive processes we propose a framework of mechanisms that could in principle implement START, an AI program that answers questions using natural language. START organizes a sentence into a series of triplets, each containing three elements (subject, verb, object). We propose that the brain similarly defines triplets and then chunks the three elements into a spatial pattern. A complete sentence can be represented using up to 7 triplets in a working memory buffer organized by theta and gamma oscillations. This buffer can transfer information into long-term memory networks where a second chunking operation converts the serial triplets into a single spatial pattern in a network, with each triplet (with corresponding elements) represented in specialized subregions. The triplets that define a sentence become synaptically linked, thereby encoding the sentence in synaptic weights. When a question is posed, there is a search for the closest stored memory (having the greatest number of shared triplets). We have devised a search process that does not require that the question and the stored memory have the same number of triplets or have triplets in the same order. Once the most similar memory is recalled and undergoes 2-level dechunking, the sought for information can be obtained by element-by-element comparison of the key triplet in the question to the corresponding triplet in the retrieved memory. This search may require a reordering to align corresponding triplets, the use of pointers that link different triplets, or the use of semantic memory. Our framework uses 12 network processes; existing models can implement many of these, but in other cases we can only suggest neural implementations. Overall, our scheme provides the first view of how language-based question answering could be implemented by the brain."}], "ArticleTitle": "How the Brain Represents Language and Answers Questions? Using an AI System to Understand the Underlying Neurobiological Mechanisms."}, "31114493": {"mesh": [], "AbstractText": [{"section": null, "text": "Brain models music as a hierarchy of dynamical systems that encode probability distributions and complexity (i.e., entropy and uncertainty). Through musical experience over lifetime, a human is intrinsically motivated in optimizing the internalized probabilistic model for efficient information processing and the uncertainty resolution, which has been regarded as rewords. Human's behavior, however, appears to be not necessarily directing to efficiency but sometimes act inefficiently in order to explore a maximum rewards of uncertainty resolution. Previous studies suggest that the drive for novelty seeking behavior (high uncertain phenomenon) reflects human's curiosity, and that the curiosity rewards encourage humans to create and learn new regularities. That is to say, although brain generally minimizes uncertainty of music structure, we sometimes derive pleasure from music with uncertain structure due to curiosity for novelty seeking behavior by which we anticipate the resolution of uncertainty. Few studies, however, investigated how curiosity for uncertain and novelty seeking behavior modulates musical creativity. The present study investigated how the probabilistic model and the uncertainty in music fluctuate over a composer's lifetime (all of the 32 piano sonatas by Ludwig van Beethoven). In the late periods of the composer's lifetime, the transitional probabilities (TPs) of sequential patterns that ubiquitously appear in all of his music (familiar phrase) were decreased, whereas the uncertainties of the whole structure were increased. Furthermore, these findings were prominent in higher-, rather than lower-, order models of TP distribution. This may suggest that the higher-order probabilistic model is susceptible to experience and psychological phenomena over the composer's lifetime. The present study first suggested the fluctuation of uncertainty of musical structure over a composer's lifetime. It is suggested that human's curiosity for uncertain and novelty seeking behavior may modulate optimization and creativity in human's brain."}], "ArticleTitle": "Depth and the Uncertainty of Statistical Knowledge on Musical Creativity Fluctuate Over a Composer's Lifetime."}, "33071767": {"mesh": [], "AbstractText": [{"section": null, "text": "We describe an integrated theory of olfactory systems operation that incorporates experimental findings across scales, stages, and methods of analysis into a common framework. In particular, we consider the multiple stages of olfactory signal processing as a collective system, in which each stage samples selectively from its antecedents. We propose that, following the signal conditioning operations of the nasal epithelium and glomerular-layer circuitry, the plastic external plexiform layer of the olfactory bulb effects a process of category learning-the basis for extracting meaningful, quasi-discrete odor representations from the metric space of undifferentiated olfactory quality. Moreover, this early categorization process also resolves the foundational problem of how odors of interest can be recognized in the presence of strong competitive interference from simultaneously encountered background odorants. This problem is fundamentally constraining on early-stage olfactory encoding strategies and must be resolved if these strategies and their underlying mechanisms are to be understood. Multiscale general theories of olfactory systems operation are essential in order to leverage the analytical advantages of engineered approaches together with our expanding capacity to interrogate biological systems."}], "ArticleTitle": "A Systematic Framework for Olfactory Bulb Signal Transformations."}, "32499691": {"mesh": [], "AbstractText": [{"section": null, "text": "Human arm movements are highly stereotypical under a large variety of experimental conditions. This is striking due to the high redundancy of the human musculoskeletal system, which in principle allows many possible trajectories toward a goal. Many researchers hypothesize that through evolution, learning, and adaption, the human system has developed optimal control strategies to select between these possibilities. Various optimality principles were proposed in the literature that reproduce human-like trajectories in certain conditions. However, these studies often focus on a single cost function and use simple torque-driven models of motion generation, which are not consistent with human muscle-actuated motion. The underlying structure of our human system, with the use of muscle dynamics in interaction with the control principles, might have a significant influence on what optimality principles best model human motion. To investigate this hypothesis, we consider a point-to-manifold reaching task that leaves the target underdetermined. Given hypothesized motion objectives, the control input is generated using Bayesian optimization, which is a machine learning based method that trades-off exploitation and exploration. Using numerical simulations with Hill-type muscles, we show that a combination of optimality principles best predicts human point-to-manifold reaching when accounting for the muscle dynamics."}], "ArticleTitle": "Optimality Principles in Human Point-to-Manifold Reaching Accounting for Muscle Dynamics."}, "31024283": {"mesh": [], "AbstractText": [{"section": null, "text": "The paper presents a hierarchical spike timing neural network model developed in NEST simulator aimed to reproduce human decision making in simplified simulated visual navigation tasks. It includes multiple layers starting from retina photoreceptors and retinal ganglion cells (RGC) via thalamic relay including lateral geniculate nucleus (LGN), thalamic reticular nucleus (TRN), and interneurons (IN) mediating connections to the higher brain areas-visual cortex (V1), middle temporal (MT), and medial superior temporal (MTS) areas, involved in dorsal pathway processing of spatial and dynamic visual information. The last layer-lateral intraparietal cortex (LIP)-is responsible for decision making and organization of the subsequent motor response (saccade generation). We simulated two possible decision options having LIP layer with two sub-regions with mutual inhibitory connections whose increased firing rate corresponds to the perceptual decision about motor response-left or right saccade. Each stage of the model was tested by appropriately chosen stimuli corresponding to its selectivity to specific stimulus characteristics (orientation for V1, direction for MT, and expansion/contraction movement templates for MST, respectively). The overall model performance was tested with stimuli simulating optic flow patterns of forward self-motion on a linear trajectory to the left or to the right from straight ahead with a gaze in the direction of heading."}], "ArticleTitle": "Spike Timing Neural Model of Motion Perception and Decision Making."}, "32508611": {"mesh": [], "AbstractText": [{"section": null, "text": "Within computational neuroscience, the algorithmic and neural basis of structure learning remains poorly understood. Concept learning is one primary example, which requires both a type of internal model expansion process (adding novel hidden states that explain new observations), and a model reduction process (merging different states into one underlying cause and thus reducing model complexity via meta-learning). Although various algorithmic models of concept learning have been proposed within machine learning and cognitive science, many are limited to various degrees by an inability to generalize, the need for very large amounts of training data, and/or insufficiently established biological plausibility. Using concept learning as an example case, we introduce a novel approach for modeling structure learning-and specifically state-space expansion and reduction-within the active inference framework and its accompanying neural process theory. Our aim is to demonstrate its potential to facilitate a novel line of active inference research in this area. The approach we lay out is based on the idea that a generative model can be equipped with extra (hidden state or cause) \"slots\" that can be engaged when an agent learns about novel concepts. This can be combined with a Bayesian model reduction process, in which any concept learning-associated with these slots-can be reset in favor of a simpler model with higher model evidence. We use simulations to illustrate this model's ability to add new concepts to its state space (with relatively few observations) and increase the granularity of the concepts it currently possesses. We also simulate the predicted neural basis of these processes. We further show that it can accomplish a simple form of \"one-shot\" generalization to new stimuli. Although deliberately simple, these simulation results highlight ways in which active inference could offer useful resources in developing neurocomputational models of structure learning. They provide a template for how future active inference research could apply this approach to real-world structure learning problems and assess the added utility it may offer."}], "ArticleTitle": "An Active Inference Approach to Modeling Structure Learning: Concept Learning as an Example Case."}, "32848687": {"mesh": [], "AbstractText": [{"section": null, "text": "Significant progress has been made toward model-based prediction of neral tissue activation in response to extracellular electrical stimulation, but challenges remain in the accurate and efficient estimation of distributed local field potentials (LFP). Analytical methods of estimating electric fields are a first-order approximation that may be suitable for model validation, but they are computationally expensive and cannot accurately capture boundary conditions in heterogeneous tissue. While there are many appropriate numerical methods of solving electric fields in neural tissue models, there isn't an established standard for mesh geometry nor a well-known rule for handling any mismatch in spatial resolution. Moreover, the challenge of misalignment between current sources and mesh nodes in a finite-element or resistor-network method volume conduction model needs to be further investigated. Therefore, using a previously published and validated multi-scale model of the hippocampus, the authors have formulated an algorithm for LFP estimation, and by extension, bidirectional communication between discretized and numerically solved volume conduction models and biologically detailed neural circuit models constructed in NEURON. Development of this algorithm required that we assess meshes of (i) unstructured tetrahedral and grid-based hexahedral geometries as well as (ii) differing approaches for managing the spatial misalignment of current sources and mesh nodes. The resulting algorithm is validated through the comparison of Admittance Method predicted evoked potentials with analytically estimated LFPs. Establishing this method is a critical step toward closed-loop integration of volume conductor and NEURON models that could lead to substantial improvement of the predictive power of multi-scale stimulation models of cortical tissue. These models may be used to deepen our understanding of hippocampal pathologies and the identification of efficacious electroceutical treatments."}], "ArticleTitle": "Admittance Method for Estimating Local Field Potentials Generated in a Multi-Scale Neuron Model of the Hippocampus."}, "32765248": {"mesh": [], "AbstractText": [{"section": null, "text": "Growing evidence shows that top-down projections from excitatory neurons in piriform cortex selectively synapse onto local inhibitory granule cells in the main olfactory bulb, effectively gating their own inputs by controlling inhibition. An open question in olfaction is the role this feedback plays in shaping the dynamics of local circuits, and the resultant computational benefits it provides. Using rate models of neuronal firing in a network consisting of excitatory mitral and tufted cells, inhibitory granule cells and top-down piriform cortical neurons, we found that changes in the weight of feedback to inhibitory neurons generated diverse network dynamics and complex transitions between these dynamics. Changes in the weight of top-down feedback supported a number of computations, including both pattern separation and oscillatory synchrony. Additionally, the network could generate gamma oscillations though a mechanism we termed Top-down control of Inhibitory Neuron Gamma (TING). Collectively, these functions arose from a codimension-2 bifurcation in the dynamical system. Our results highlight a key role for this top-down feedback, gating inhibition to facilitate often diametrically different computations."}], "ArticleTitle": "Top-Down Control of Inhibitory Granule Cells in the Main Olfactory Bulb Reshapes Neural Dynamics Giving Rise to a Diversity of Computations."}, "30116187": {"mesh": [], "AbstractText": [{"section": null, "text": "There are several experimental studies which suggest opioids consumption forms pathological memories in different brain regions. For example it has been empirically demonstrated that the theta rhythm which appears during chronic opioid consumption is correlated with the addiction memory formation. In this paper, we present a minimal computational model that shows how opioids can change firing patterns of the neurons during acute and chronic opioid consumption and also during withdrawal periods. The model consists of a pre- and post-synaptic neuronal circuits and the astrocyte that monitors the synapses. The output circuitry consists of inhibitory interneurons and excitatory pyramidal neurons. Our simulation results demonstrate that acute opioid consumption induces synchronous patterns in the beta frequency range, while, chronic opioid consumption provokes theta frequency oscillations. This allows us to infer that the theta rhythm appeared during chronic treatment can be an indication of brain engagement in opioid-induced memory formation. Our results also suggest that changing the inputs of the interneurons and the inhibitory neuronal network is not an appropriate method for preventing the formation of pathological memory. However, the same results suggest that prevention of pathological memory formation is possible by manipulating the input of the stimulatory network and the excitatory connections in the neuronal network. They also show that during withdrawal periods, firing rate is reduced and random fluctuations are generated in the modeled neural network. The random fluctuations disappear and synchronized patterns emerge when the activities of the astrocytic transporters are decreased. These results suggest that formation of the synchronized activities can be correlated with the relapse. Our model also predicts that reduction in gliotransmitter release can eliminate the synchrony and thereby it can reduce the likelihood of the relapse occurrence."}], "ArticleTitle": "Formation of Opioid-Induced Memory and Its Prevention: A Computational Study."}, "29527159": {"mesh": [], "AbstractText": [{"section": null, "text": "Purpose: A muscle synergies model was suggested to represent a simplifying motor control mechanism by the brainstem and spinal cord. The aim of the study was to investigate the feasibility of such control mechanisms in the rehabilitation of post-stroke individuals during the execution of hand-reaching movements in multiple directions, compared to non-stroke individuals. Methods: Twelve non-stroke and 13 post-stroke individuals participated in the study. Muscle synergies were extracted from EMG data that was recorded during hand reaching tasks, using the NMF algorithm. The optimal number of synergies was evaluated in both groups using the Variance Accounted For (VAF) and the Mean Squared Error (MSE). A cross validation procedure was carried out to define a representative set of synergies. The similarity index and the K-means algorithm were applied to validate the existence of such a set of synergies, but also to compare the modulation properties of synergies for different movement directions between groups. The similarity index and hierarchical cluster analysis were also applied to compare between group synergies. Results: Four synergies were chosen to optimally capture the variances in the EMG data, with mean VAF of 0.917 &#177; 0.034 and 0.883 &#177; 0.046 of the data variances, with respective MSE of 0.007 and 0.016, in the control and study groups, respectively. The representative set of synergies was set to be extracted from movement to the center of the reaching space. Two synergies had different muscle activation balance between groups. Seven and 17 clusters partitioned the muscle synergies of the control and study groups. The control group exhibited a gradual change in the activation in the amplitude in the time domain (modulation) of synergies, as reflected by the similarity index, whereas the study group exhibited consistently significant differences between all movement directions and the representative set of synergies. The study findings support the existence of a representative set of synergies, which are modulated to execute movements in different directions. Conclusions: Post-stroke individuals differently modulate the activation of synergies to different movement directions than do non-stroke individuals. The conclusion was supported by different muscle activation balances, similarity values and different classifications of synergies among groups."}], "ArticleTitle": "Muscle Synergies Control during Hand-Reaching Tasks in Multiple Directions Post-stroke."}, "32009920": {"mesh": [], "AbstractText": [{"section": null, "text": "Behavioral studies have shown spatial working memory impairment with aging in several animal species, including humans. Persistent activity of layer 3 pyramidal dorsolateral prefrontal cortex (dlPFC) neurons during delay periods of working memory tasks is important for encoding memory of the stimulus. In vitro studies have shown that these neurons undergo significant age-related structural and functional changes, but the extent to which these changes affect neural mechanisms underlying spatial working memory is not understood fully. Here, we confirm previous studies showing impairment on the Delayed Recognition Span Task in the spatial condition (DRSTsp), and increased in vitro action potential firing rates (hyperexcitability), across the adult life span of the rhesus monkey. We use a bump attractor model to predict how empirically observed changes in the aging dlPFC affect performance on the Delayed Response Task (DRT), and introduce a model of memory retention in the DRSTsp. Persistent activity-and, in turn, cognitive performance-in both models was affected much more by hyperexcitability of pyramidal neurons than by a loss of synapses. Our DRT simulations predict that additional changes to the network, such as increased firing of inhibitory interneurons, are needed to account for lower firing rates during the DRT with aging reported in vivo. Synaptic facilitation was an essential feature of the DRSTsp model, but it did not compensate fully for the effects of the other age-related changes on DRT performance. Modeling pyramidal neuron hyperexcitability and synapse loss simultaneously led to a partial recovery of function in both tasks, with the simulated level of DRSTsp impairment similar to that observed in aging monkeys. This modeling work integrates empirical data across multiple scales, from synapse counts to cognitive testing, to further our understanding of aging in non-human primates."}], "ArticleTitle": "Network Models Predict That Pyramidal Neuron Hyperexcitability and Synapse Loss in the dlPFC Lead to Age-Related Spatial Working Memory Impairment in Rhesus Monkeys."}, "31695603": {"mesh": [], "AbstractText": [{"section": null, "text": "The somatic nervous system of the nematode worm Caenorhabditis elegans is a model for understanding the physical characteristics of the neurons and their interconnections. Its neurons show high variation in morphological attributes. This study investigates the relationship of neuronal morphology to the number of synapses per neuron. Morphology is also examined for any detectable association with neuron cell type or ganglion membership."}], "ArticleTitle": "Neuronal Morphology and Synapse Count in the Nematode Worm."}, "32508610": {"mesh": [], "AbstractText": [{"section": null, "text": "Conscious awareness plays a major role in human cognition and adaptive behavior, though its function in multisensory integration is not yet fully understood, hence, questions remain: How does the brain integrate the incoming multisensory signals with respect to different external environments? How are the roles of these multisensory signals defined to adhere to the anticipated behavioral-constraint of the environment? This work seeks to articulate a novel theory on conscious multisensory integration (CMI) that addresses the aforementioned research challenges. Specifically, the well-established contextual field (CF) in pyramidal cells and coherent infomax theory (Kay et al., 1998; Kay and Phillips, 2011) is split into two functionally distinctive integrated input fields: local contextual field (LCF) and universal contextual field (UCF). LCF defines the modulatory sensory signal coming from some other parts of the brain (in principle from anywhere in space-time) and UCF defines the outside environment and anticipated behavior (based on past learning and reasoning). Both LCF and UCF are integrated with the receptive field (RF) to develop a new class of contextually-adaptive neuron (CAN), which adapts to changing environments. The proposed theory is evaluated using human contextual audio-visual (AV) speech modeling. Simulation results provide new insights into contextual modulation and selective multisensory information amplification/suppression. The central hypothesis reviewed here suggests that the pyramidal cell, in addition to the classical excitatory and inhibitory signals, receives LCF and UCF inputs. The UCF (as a steering force or tuner) plays a decisive role in precisely selecting whether to amplify/suppress the transmission of relevant/irrelevant feedforward signals, without changing the content e.g., which information is worth paying more attention to? This, as opposed to, unconditional excitatory and inhibitory activity in existing deep neural networks (DNNs), is called conditional amplification/suppression."}], "ArticleTitle": "Conscious Multisensory Integration: Introducing a Universal Contextual Field in Biological and Deep Artificial Neural Networks."}, "31920606": {"mesh": [], "AbstractText": [{"section": null, "text": "Glioblastoma, the most frequent primary malignant brain neoplasm, is genetically diverse and classified into four transcriptomic subtypes, i. e., classical, mesenchymal, proneural, and neural. Currently, detection of transcriptomic subtype is based on ex vivo analysis of tissue that does not capture the spatial tumor heterogeneity. In view of accumulative evidence of in vivo imaging signatures summarizing molecular features of cancer, this study seeks robust non-invasive radiographic markers of transcriptomic classification of glioblastoma, based solely on routine clinically-acquired imaging sequences. A pre-operative retrospective cohort of 112 pathology-proven de novo glioblastoma patients, having multi-parametric MRI (T1, T1-Gd, T2, T2-FLAIR), collected from the Hospital of the University of Pennsylvania were included. Following tumor segmentation into distinct radiographic sub-regions, diverse imaging features were extracted and support vector machines were employed to multivariately integrate these features and derive an imaging signature of transcriptomic subtype. Extracted features included intensity distributions, volume, morphology, statistics, tumors' anatomical location, and texture descriptors for each tumor sub-region. The derived signature was evaluated against the transcriptomic subtype of surgically-resected tissue specimens, using a 5-fold cross-validation method and a receiver-operating-characteristics analysis. The proposed model was 71% accurate in distinguishing among the four transcriptomic subtypes. The accuracy (sensitivity/specificity) for distinguishing each subtype (classical, mesenchymal, proneural, neural) from the rest was equal to 88.4% (71.4/92.3), 75.9% (83.9/72.8), 82.1% (73.1/84.9), and 75.9% (79.4/74.4), respectively. The findings were also replicated in The Cancer Genomic Atlas glioblastoma dataset. The obtained imaging signature for the classical subtype was dominated by associations with features related to edge sharpness, whereas for the mesenchymal subtype had more pronounced presence of higher T2 and T2-FLAIR signal in edema, and higher volume of enhancing tumor and edema. The proneural and neural subtypes were characterized by the lower T1-Gd signal in enhancing tumor and higher T2-FLAIR signal in edema, respectively. Our results indicate that quantitative multivariate analysis of features extracted from clinically-acquired MRI may provide a radiographic biomarker of the transcriptomic profile of glioblastoma. Importantly our findings can be influential in surgical decision-making, treatment planning, and assessment of inoperable tumors."}], "ArticleTitle": "Multivariate Analysis of Preoperative Magnetic Resonance Imaging Reveals Transcriptomic Classification of de novo Glioblastoma Patients."}, "32265680": {"mesh": [], "AbstractText": [{"section": null, "text": "Image registration and segmentation are the two most studied problems in medical image analysis. Deep learning algorithms have recently gained a lot of attention due to their success and state-of-the-art results in variety of problems and communities. In this paper, we propose a novel, efficient, and multi-task algorithm that addresses the problems of image registration and brain tumor segmentation jointly. Our method exploits the dependencies between these tasks through a natural coupling of their interdependencies during inference. In particular, the similarity constraints are relaxed within the tumor regions using an efficient and relatively simple formulation. We evaluated the performance of our formulation both quantitatively and qualitatively for registration and segmentation problems on two publicly available datasets (BraTS 2018 and OASIS 3), reporting competitive results with other recent state-of-the-art methods. Moreover, our proposed framework reports significant amelioration (p < 0.005) for the registration performance inside the tumor locations, providing a generic method that does not need any predefined conditions (e.g., absence of abnormalities) about the volumes to be registered. Our implementation is publicly available online at https://github.com/TheoEst/joint_registration_tumor_segmentation."}], "ArticleTitle": "Deep Learning-Based Concurrent Brain Registration and Tumor Segmentation."}, "30349470": {"mesh": [], "AbstractText": [{"section": null, "text": "The phase-reset model of oscillatory EEG activity has received a lot of attention in the last decades for decoding different cognitive processes. Based on this model, the ERPs are assumed to be generated as a result of phase reorganization in ongoing EEG. Alignment of the phase of neuronal activities can be observed within or between different assemblies of neurons across the brain. Phase synchronization has been used to explore and understand perception, attentional binding and considering it in the domain of neuronal correlates of consciousness. The importance of the topic and its vast exploration in different domains of the neuroscience presses the need for appropriate tools and methods for measuring the level of phase synchronization of neuronal activities. Measuring the level of instantaneous phase (IP) synchronization has been used extensively in numerous studies of ERPs as well as oscillatory activity for a better understanding of the underlying cognitive binding with regard to different set of stimulations such as auditory and visual. However, the reliability of results can be challenged as a result of noise artifact in IP. Phase distortion due to environmental noise artifacts as well as different pre-processing steps on signals can lead to generation of artificial phase jumps. One of such effects presented recently is the effect of low envelope on the IP of signal. It has been shown that as the instantaneous envelope of the analytic signal approaches zero, the variations in the phase increase, effectively leading to abrupt transitions in the phase. These abrupt transitions can distort the phase synchronization results as they are not related to any neurophysiological effect. These transitions are called spurious phase variation. In this study, we present a model to remove generated artificial phase variations due to the effect of low envelope. The proposed method is based on a simplified form of a Kalman smoother, that is able to model the IP behavior in narrow-bandpassed oscillatory signals. In this work we first explain the details of the proposed Kalman smoother for modeling the dynamics of the phase variations in narrow-bandpassed signals and then evaluate it on a set of synthetic signals. Finally, we apply the model on ongoing-EEG signals to assess the removal of spurious phase variations."}], "ArticleTitle": "Reducing the Effect of Spurious Phase Variations in Neural Oscillatory Signals."}, "32009922": {"mesh": [], "AbstractText": [{"section": null, "text": "Resting-state brain activities have been extensively investigated to understand the macro-scale network architecture of the human brain using non-invasive imaging methods such as fMRI, EEG, and MEG. Previous studies revealed a mechanistic origin of resting-state networks (RSNs) using the connectome dynamics modeling approach, where the neural mass dynamics model constrained by the structural connectivity is simulated to replicate the resting-state networks measured with fMRI and/or fast synchronization transitions with EEG/MEG. However, there is still little understanding of the relationship between the slow fluctuations measured with fMRI and the fast synchronization transitions with EEG/MEG. In this study, as a first step toward evaluating experimental evidence of resting state activity at two different time scales but in a unified way, we investigate connectome dynamics models that simultaneously explain resting-state functional connectivity (rsFC) and EEG microstates. Here, we introduce empirical rsFC and microstates as evaluation criteria of simulated neuronal dynamics obtained by the Larter-Breakspear model in one cortical region connected with those in other cortical regions based on structural connectivity. We optimized the global coupling strength and the local gain parameter (variance of the excitatory and inhibitory threshold) of the simulated neuronal dynamics by fitting both rsFC and microstate spatial patterns to those of experimental ones. As a result, we found that simulated neuronal dynamics in a narrow optimal parameter range simultaneously reproduced empirical rsFC and microstates. Two parameter groups had different inter-regional interdependence. One type of dynamics was synchronized across the whole brain region, and the other type was synchronized between brain regions with strong structural connectivity. In other words, both fast synchronization transitions and slow BOLD fluctuation changed based on structural connectivity in the two parameter groups. Empirical microstates were similar to simulated microstates in the two parameter groups. Thus, fast synchronization transitions correlated with slow BOLD fluctuation based on structural connectivity yielded characteristics of microstates. Our results demonstrate that a bottom-up approach, which extends the single neuronal dynamics model based on empirical observations into a neural mass dynamics model and integrates structural connectivity, effectively reveals both macroscopic fast, and slow resting-state network dynamics."}], "ArticleTitle": "Evaluation of Resting Spatio-Temporal Dynamics of a Neural Mass Model Using Resting fMRI Connectivity and EEG Microstates."}, "32792931": {"mesh": [], "AbstractText": [{"section": null, "text": "Perspective taking is the ability to take into account what the other agent knows. This skill is not unique to humans as it is also displayed by other animals like chimpanzees. It is an essential ability for social interactions, including efficient cooperation, competition, and communication. Here we present our progress toward building artificial agents with such abilities. We implemented a perspective taking task inspired by experiments done with chimpanzees. We show that agents controlled by artificial neural networks can learn via reinforcement learning to pass simple tests that require some aspects of perspective taking capabilities. We studied whether this ability is more readily learned by agents with information encoded in allocentric or egocentric form for both their visual perception and motor actions. We believe that, in the long run, building artificial agents with perspective taking ability can help us develop artificial intelligence that is more human-like and easier to communicate with."}], "ArticleTitle": "Perspective Taking in Deep Reinforcement Learning Agents."}, "31333438": {"mesh": [], "AbstractText": [{"section": null, "text": "Auditory stream segregation is a perceptual process by which the human auditory system groups sounds from different sources into perceptually meaningful elements (e.g., a voice or a melody). The perceptual segregation of sounds is important, for example, for the understanding of speech in noisy scenarios, a particularly challenging task for listeners with a cochlear implant (CI). It has been suggested that some aspects of stream segregation may be explained by relatively basic neural mechanisms at a cortical level. During the past decades, a variety of models have been proposed to account for the data from stream segregation experiments in normal-hearing (NH) listeners. However, little attention has been given to corresponding findings in CI listeners. The present study investigated whether a neural model of sequential stream segregation, proposed to describe the behavioral effects observed in NH listeners, can account for behavioral data from CI listeners. The model operates on the stimulus features at the cortical level and includes a competition stage between the neuronal units encoding the different percepts. The competition arises from a combination of mutual inhibition, adaptation, and additive noise. The model was found to capture the main trends in the behavioral data from CI listeners, such as the larger probability of a segregated percept with increasing the feature difference between the sounds as well as the build-up effect. Importantly, this was achieved without any modification to the model's competition stage, suggesting that stream segregation could be mediated by a similar mechanism in both groups of listeners."}], "ArticleTitle": "Auditory Stream Segregation Can Be Modeled by Neural Competition in Cochlear Implant Listeners."}, "30100870": {"mesh": [], "AbstractText": [{"section": null, "text": "In synapses, calcium is required for modulating synaptic transmission, plasticity, synaptogenesis, and synaptic pruning. The regulation of calcium dynamics within neurons involves cellular mechanisms such as synaptically activated channels and pumps, calcium buffers, and calcium sequestrating organelles. Many experimental studies tend to focus on only one or a small number of these mechanisms, as technical limitations make it difficult to observe all features at once. Computational modeling enables incorporation of many of these properties together, allowing for more complete and integrated studies. However, the scale of existing detailed models is often limited to synaptic and dendritic compartments as the computational burden rapidly increases when these models are integrated in cellular or network level simulations. In this article we present a computational model of calcium dynamics at the postsynaptic spine of a CA1 pyramidal neuron, as well as a methodology that enables its implementation in multi-scale, large-scale simulations. We first present a mechanistic model that includes individually validated models of various components involved in the regulation of calcium at the spine. We validated our mechanistic model by comparing simulated calcium levels to experimental data found in the literature. We performed additional simulations with the mechanistic model to determine how the simulated calcium activity varies with respect to presynaptic-postsynaptic stimulation intervals and spine distance from the soma. We then developed an input-output (IO) model that complements the mechanistic calcium model and provide a computationally efficient representation for use in larger scale modeling studies; we show the performance of the IO model compared to the mechanistic model in terms of accuracy and speed. The models presented here help achieve two objectives. First, the mechanistic model provides a comprehensive platform to describe spine calcium dynamics based on individual contributing factors. Second, the IO model is trained on the main dynamical features of the mechanistic model and enables nonlinear spine calcium modeling on the cell and network level simulation scales. Utilizing both model representations provide a multi-level perspective on calcium dynamics, originating from the molecular interactions at spines and propagating the effects to higher levels of activity involved in network behavior."}], "ArticleTitle": "A Glutamatergic Spine Model to Enable Multi-Scale Modeling of Nonlinear Calcium Dynamics."}, "31143108": {"mesh": [], "AbstractText": [{"section": null, "text": "It is believed that Mirror Visual Feedback (MVF) increases the interlimb transfer but the exact mechanism is still a matter of debate. The aim of this study was to compare between a bimanual task (BM) and a MVF task, within functionally rather than geometrically defined cortical domains. Measure Projection Analysis (MPA) approach was applied to compare the dynamic oscillatory activity (event-related synchronization/desynchronization ERS/ERD) between and within domains. EEG was recorded in 14 healthy participants performing a BM and an MVF task with the right hand. The MPA was applied on fitted equivalent current dipoles based on independent components to define domains containing functionally similar areas. The measure of intradomain similarity was a \"signed mutual information,\" a parameter based on the coherence. Domain analysis was performed for joint tasks (BM and MVF) and for each task separately. MVF created 9 functional domains while MB task had only 4 functionally distinctive domains, two over the left hemispheres and two bilateraly. For all domains identified for BM task alone, similar domains could be identified in MVF and joint tasks analysis. In addition MVF had domains related to motor planning on the right hemisphere and to self-recognition of action. For joint tasks analysis, seven domains were identified, with similar functions for the left and the right hand with exception of a domain covering BA32 (self-recognition of action) of the left hand only. In joint task domain analysis, the ERD/ERS showed a larger difference between domains than between tasks. All domains which involved the sensory cortex had a visible beta ERS at the onset of movement, and post movement beta ERS. The frequency of ERD varied between domains. Largest difference between tasks existed in domains responsible for the awareness of action. In conclusion, functionally distinctive domains have different ERD/ERS patterns, similar for both tasks. MVF activates contralateral hemisphere in similar manner to BM movements, while at the same time also activating the ipsilateral hemisphere. Significance: Following stroke cortical activation and interhemispheric inhibition from the contralesional side is reduced. MVF creates stronger ipsilateral activity than BM, which is highly relevant of neurorehabilitation of movements."}], "ArticleTitle": "Cortical Functional Domains Show Distinctive Oscillatory Dynamic in Bimanual and Mirror Visual Feedback Tasks."}, "31293408": {"mesh": [], "AbstractText": [{"section": null, "text": "Sparse coding models of natural images and sounds have been able to predict several response properties of neurons in the visual and auditory systems. While the success of these models suggests that the structure they capture is universal across domains to some degree, it is not yet clear which aspects of this structure are universal and which vary across sensory modalities. To address this, we fit complete and highly overcomplete sparse coding models to natural images and spectrograms of speech and report on differences in the statistics learned by these models. We find several types of sparse features in natural images, which all appear in similar, approximately Laplace distributions, whereas the many types of sparse features in speech exhibit a broad range of sparse distributions, many of which are highly asymmetric. Moreover, individual sparse coding units tend to exhibit higher lifetime sparseness for overcomplete models trained on images compared to those trained on speech. Conversely, population sparseness tends to be greater for these networks trained on speech compared with sparse coding models of natural images. To illustrate the relevance of these findings to neural coding, we studied how they impact a biologically plausible sparse coding network's representations in each sensory modality. In particular, a sparse coding network with synaptically local plasticity rules learns different sparse features from speech data than are found by more conventional sparse coding algorithms, but the learned features are qualitatively the same for these models when trained on natural images."}], "ArticleTitle": "On the Sparse Structure of Natural Sounds and Natural Images: Similarities, Differences, and Implications for Neural Coding."}, "31787888": {"mesh": [], "AbstractText": [{"section": null, "text": "Insects can detect the presence of discrete objects in their visual fields based on a range of differences in spatiotemporal characteristics between the images of object and background. This includes but is not limited to relative motion. Evidence suggests that edge detection is an integral part of this capability, and this study examines the ability of a bio-inspired processing model to detect the presence of boundaries between two regions of a one-dimensional visual field, based on general differences in image dynamics. The model consists of two parts. The first is an early vision module inspired by insect visual processing, which implements adaptive photoreception, ON and OFF channels with transient and sustained characteristics, and delayed and undelayed signal paths. This is replicated for a number of photoreceptors in a small linear array. It is followed by an artificial neural network trained to discriminate the presence vs. absence of an edge based on the array output signals. Input data are derived from natural imagery and feature both static and moving edges between regions with moving texture, flickering texture, and static patterns in all possible combinations. The model can discriminate the presence of edges, stationary or moving, at rates far higher than chance. The resources required (numbers of neurons and visual signals) are realistic relative to those available in the insect second optic ganglion, where the bulk of such processing would be likely to take place."}], "ArticleTitle": "Neural Network Model for Detection of Edges Defined by Image Dynamics."}, "31998106": {"mesh": [], "AbstractText": [{"section": null, "text": "Successive patterns of activation and deactivation in local areas of the brain indicate the mechanisms of information processing in the brain. It is possible that this process can be optimized by principles, such as the maximization of mutual information and the minimization of energy consumption. In the present paper, I showed evidence for this argument by demonstrating the correlation among mutual information, the energy of the activation, and the activation patterns. Modeling the information processing based on the functional connectome datasets of the human brain, I simulated information transfer in this network structure. Evaluating the statistical quantities of the different network states, I clarified the correlation between them. First, I showed that mutual information and network energy have a close relationship, and that the values are maximized and minimized around a same network state. This implies that there is an optimal network state in the brain that is organized according to the principles regarding mutual information and energy. On the other hand, the evaluation of the network structure revealed that the characteristic network structure known as the criticality also emerges around this state. These results imply that the characteristic features of the functional network are also affected strongly by these principles. To assess the functional aspects of this state, I investigated the output activation patterns in response to random input stimuli. Measuring the redundancy of the responses in terms of the number of overlapping activation patterns, the results indicate that there is a negative correlation between mutual information and the redundancy in the patterns, suggesting that there is a trade-off between communication efficiency and robustness due to redundancy, and the principles of mutual information and network energy are important to network formation and its function in the human brain."}], "ArticleTitle": "Principles of Mutual Information Maximization and Energy Minimization Affect the Activation Patterns of Large Scale Networks in the Brain."}, "30072885": {"mesh": [], "AbstractText": [{"section": null, "text": "It has long been known that the auditory system is better suited to guide temporally precise behaviors like sensorimotor synchronization (SMS) than the visual system. Although this phenomenon has been studied for many years, the underlying neural and computational mechanisms remain unclear. Growing consensus suggests the existence of multiple, interacting, context-dependent systems, and that reduced precision in visuo-motor timing might be due to the way experimental tasks have been conceived. Indeed, the appropriateness of the stimulus for a given task greatly influences timing performance. In this review, we examine timing differences for sensorimotor synchronization and error correction with auditory and visual sequences, to inspect the underlying neural mechanisms that contribute to modality differences in timing. The disparity between auditory and visual timing likely relates to differences in the processing specialization between auditory and visual modalities (temporal vs. spatial). We propose this difference could offer potential explanation for the differing temporal abilities between modalities. We also offer suggestions as to how these sensory systems interface with motor and timing systems."}], "ArticleTitle": "Sensorimotor Synchronization With Auditory and Visual Modalities: Behavioral and Neural Differences."}, "33408622": {"mesh": [], "AbstractText": [{"section": null, "text": "Rhythmic activity in the brain fluctuates with behaviour and cognitive state, through a combination of coexisting and interacting frequencies. At large spatial scales such as those studied in human M/EEG, measured oscillatory dynamics are believed to arise primarily from a combination of cortical (intracolumnar) and corticothalamic rhythmogenic mechanisms. Whilst considerable progress has been made in characterizing these two types of neural circuit separately, relatively little work has been done that attempts to unify them into a single consistent picture. This is the aim of the present paper. We present and examine a whole-brain, connectome-based neural mass model with detailed long-range cortico-cortical connectivity and strong, recurrent corticothalamic circuitry. This system reproduces a variety of known features of human M/EEG recordings, including spectral peaks at canonical frequencies, and functional connectivity structure that is shaped by the underlying anatomical connectivity. Importantly, our model is able to capture state- (e.g., idling/active) dependent fluctuations in oscillatory activity and the coexistence of multiple oscillatory phenomena, as well as frequency-specific modulation of functional connectivity. We find that increasing the level of sensory drive to the thalamus triggers a suppression of the dominant low frequency rhythms generated by corticothalamic loops, and subsequent disinhibition of higher frequency endogenous rhythmic behaviour of intracolumnar microcircuits. These combine to yield simultaneous decreases in lower frequency and increases in higher frequency components of the M/EEG power spectrum during states of high sensory or cognitive drive. Building on this, we also explored the effect of pulsatile brain stimulation on ongoing oscillatory activity, and evaluated the impact of coexistent frequencies and state-dependent fluctuations on the response of cortical networks. Our results provide new insight into the role played by cortical and corticothalamic circuits in shaping intrinsic brain rhythms, and suggest new directions for brain stimulation therapies aimed at state-and frequency-specific control of oscillatory brain activity."}], "ArticleTitle": "A Connectome-Based, Corticothalamic Model of State- and Stimulation-Dependent Modulation of Rhythmic Neural Activity and Connectivity."}, "33013343": {"mesh": [], "AbstractText": [{"section": null, "text": "Excitation-inhibition (E-I) balanced neural networks are a classic model for modeling neural activities and functions in the cortex. The present study investigates the potential application of E-I balanced neural networks for fast signal detection in brain-inspired computation. We first theoretically analyze the response property of an E-I balanced network, and find that the asynchronous firing state of the network generates an optimal noise structure enabling the network to track input changes rapidly. We then extend the homogeneous connectivity of an E-I balanced neural network to include local neuronal connections, so that the network can still achieve fast response and meanwhile maintain spatial information in the face of spatially heterogeneous signal. Finally, we carry out simulations to demonstrate that our model works well."}], "ArticleTitle": "Excitation-Inhibition Balanced Neural Networks for Fast Signal Detection."}, "29922143": {"mesh": [], "AbstractText": [{"section": null, "text": "Crossmodal assimilation effect refers to the prominent phenomenon by which ensemble mean extracted from a sequence of task-irrelevant distractor events, such as auditory intervals, assimilates/biases the perception (such as visual interval) of the subsequent task-relevant target events in another sensory modality. In current experiments, using visual Ternus display, we examined the roles of temporal reference, materialized as the time information accumulated before the onset of target event, as well as the attentional modulation in crossmodal temporal interaction. Specifically, we examined how the global time interval, the mean auditory inter-intervals and the last interval in the auditory sequence assimilate and bias the subsequent percept of visual Ternus motion (element motion vs. group motion). We demonstrated that both the ensemble (geometric) mean and the last interval in the auditory sequence contribute to bias the percept of visual motion. Longer mean (or last) interval elicited more reports of group motion, whereas the shorter mean (or last) auditory intervals gave rise to more dominant percept of element motion. Importantly, observers have shown dynamic adaptation to the temporal reference of crossmodal assimilation: when the target visual Ternus stimuli were separated by a long gap interval after the preceding sound sequence, the assimilation effect by ensemble mean was reduced. Our findings suggested that crossmodal assimilation relies on a suitable temporal reference on adaptation level, and revealed a general temporal perceptual grouping principle underlying complex audio-visual interactions in everyday dynamic situations."}], "ArticleTitle": "Temporal Reference, Attentional Modulation, and Crossmodal Assimilation."}, "29410621": {"mesh": [], "AbstractText": [{"section": null, "text": "Synaptic plasticity is believed to be the biological substrate underlying learning and memory. One of the most widespread forms of synaptic plasticity, spike-timing-dependent plasticity (STDP), uses the spike timing information of presynaptic and postsynaptic neurons to induce synaptic potentiation or depression. An open question is how STDP organizes the connectivity patterns in neuronal circuits. Previous studies have placed much emphasis on the role of firing rate in shaping connectivity patterns. Here, we go beyond the firing rate description to develop a self-consistent linear response theory that incorporates the information of both firing rate and firing variability. By decomposing the pairwise spike correlation into one component associated with local direct connections and the other associated with indirect connections, we identify two distinct regimes regarding the network structures learned through STDP. In one regime, the contribution of the direct-connection correlations dominates over that of the indirect-connection correlations in the learning dynamics; this gives rise to a network structure consistent with the firing rate description. In the other regime, the contribution of the indirect-connection correlations dominates in the learning dynamics, leading to a network structure different from the firing rate description. We demonstrate that the heterogeneity of firing variability across neuronal populations induces a temporally asymmetric structure of indirect-connection correlations. This temporally asymmetric structure underlies the emergence of the second regime. Our study provides a new perspective that emphasizes the role of high-order statistics of spiking activity in the spike-correlation-sensitive learning dynamics."}], "ArticleTitle": "Effects of Firing Variability on Network Structures with Spike-Timing-Dependent Plasticity."}, "32153379": {"mesh": [], "AbstractText": [{"section": null, "text": "Advances in computation and neuronal modeling have enabled the study of entire neural tissue systems with an impressive degree of biological realism. These efforts have focused largely on modeling dendrites and somas while largely neglecting axons. The need for biologically realistic explicit axonal models is particularly clear for applications involving clinical and therapeutic electrical stimulation because axons are generally more excitable than other neuroanatomical subunits. While many modeling efforts can rely on existing repositories of reconstructed dendritic/somatic morphologies to study real cells or to estimate parameters for a generative model, such datasets for axons are scarce and incomplete. Those that do exist may still be insufficient to build accurate models because the increased geometric variability of axons demands a proportional increase in data. To address this need, a Ruled-Optimum Ordered Tree System (ROOTS) was developed that extends the capability of neuronal morphology generative methods to include highly branched cortical axon terminal arbors. Further, this study presents and explores a clear use-case for such models in the prediction of cortical tissue response to externally applied electric fields. The results presented herein comprise (i) a quantitative and qualitative analysis of the generative algorithm proposed, (ii) a comparison of generated fibers with those observed in histological studies, (iii) a study of the requisite spatial and morphological complexity of axonal arbors for accurate prediction of neuronal response to extracellular electrical stimulation, and (iv) an extracellular electrical stimulation strength-duration analysis to explore probable thresholds of excitation of the dentate perforant path under controlled conditions. ROOTS demonstrates a superior ability to capture biological realism in model fibers, allowing improved accuracy in predicting the impact that microscale structures and branching patterns have on spatiotemporal patterns of activity in the presence of extracellular electric fields."}], "ArticleTitle": "ROOTS: An Algorithm to Generate Biologically Realistic Cortical Axons and an Application to Electroceutical Modeling."}, "27683554": {"mesh": [], "AbstractText": [{"section": null, "text": "Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) the cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. In support of these hypotheses, we argue that a range of implementations of credit assignment through multiple layers of neurons are compatible with our current knowledge of neural circuitry, and that the brain's specialized systems can be interpreted as enabling efficient optimization for specific problem classes. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses. "}], "ArticleTitle": "Toward an Integration of Deep Learning and Neuroscience."}, "30863298": {"mesh": [], "AbstractText": [{"section": null, "text": "We propose a computational model of vision that describes the integration of cross-modal sensory information between the olfactory and visual systems in zebrafish based on the principles of the statistical extreme value theory. The integration of olfacto-retinal information is mediated by the centrifugal pathway that originates from the olfactory bulb and terminates in the neural retina. Motivation for using extreme value theory stems from physiological evidence suggesting that extremes and not the mean of the cell responses direct cellular activity in the vertebrate brain. We argue that the visual system, as measured by retinal ganglion cell responses in spikes/sec, follows an extreme value process for sensory integration and the increase in visual sensitivity from the olfactory input can be better modeled using extreme value distributions. As zebrafish maintains high evolutionary proximity to mammals, our model can be extended to other vertebrates as well."}], "ArticleTitle": "An Extreme Value Theory Model of Cross-Modal Sensory Information Integration in Modulation of Vertebrate Visual System Functions."}, "30881298": {"mesh": [], "AbstractText": [{"section": null, "text": "In natural conditions the human visual system can estimate the 3D shape of specular objects even from a single image. Although previous studies suggested that the orientation field plays a key role for 3D shape perception from specular reflections, its computational plausibility, and possible mechanisms have not been investigated. In this study, to complement the orientation field information, we first add prior knowledge that objects are illuminated from above and utilize the vertical polarity of the intensity gradient. Then we construct an algorithm that incorporates these two image cues to estimate 3D shapes from a single specular image. We evaluated the algorithm with glossy and mirrored surfaces and found that 3D shapes can be recovered with a high correlation coefficient of around 0.8 with true surface shapes. Moreover, under a specific condition, the algorithm's errors resembled those made by human observers. These findings show that the combination of the orientation field and the vertical polarity of the intensity gradient is computationally sufficient and probably reproduces essential representations used in human shape perception from specular reflections."}], "ArticleTitle": "Computational Model for Human 3D Shape Perception From a Single Specular Image."}, "28701945": {"mesh": [], "AbstractText": [{"section": null, "text": "Over 30% epileptic patients are refractory to medication, who are amenable to neurosurgical treatment. Non-invasive brain imaging technologies including video-electroencephalogram (EEG), magnetic resonance imaging (MRI), and magnetoencephalography (MEG) are widely used in presurgical assessment of epileptic patients. This review mainly discussed the current development of clinical MEG imaging as a diagnose approach, and its correlations with the golden standard intracranial electroencephalogram (iEEG). More importantly, this review discussed the possible applications of functional networks in preoperative epileptic foci localization in future studies."}], "ArticleTitle": "Magnetoencephalography in Preoperative Epileptic Foci Localization: Enlightenment from Cognitive Studies."}, "32581757": {"mesh": [], "AbstractText": [{"section": null, "text": "In this paper, we focus on the emergence of diverse neuronal oscillations arising in a mixed population of neurons with different excitability properties. These properties produce mixed mode oscillations (MMOs) characterized by the combination of large amplitudes and alternate subthreshold or small amplitude oscillations. Considering the biophysically plausible, Izhikevich neuron model, we demonstrate that various MMOs, including MMBOs (mixed mode bursting oscillations) and synchronized tonic spiking appear in a randomly connected network of neurons, where a fraction of them is in a quiescent (silent) state and the rest in self-oscillatory (firing) states. We show that MMOs and other patterns of neural activity depend on the number of oscillatory neighbors of quiescent nodes and on electrical coupling strengths. Our results are verified by constructing a reduced-order network model and supported by systematic bifurcation diagrams as well as for a small-world network. Our results suggest that, for weak couplings, MMOs appear due to the de-synchronization of a large number of quiescent neurons in the networks. The quiescent neurons together with the firing neurons produce high frequency oscillations and bursting activity. The overarching goal is to uncover a favorable network architecture and suitable parameter spaces where Izhikevich model neurons generate diverse responses ranging from MMOs to tonic spiking."}], "ArticleTitle": "Emergence of Mixed Mode Oscillations in Random Networks of Diverse Excitable Neurons: The Role of Neighbors and Electrical Coupling."}, "30555315": {"mesh": [], "AbstractText": [{"section": null, "text": "Intracellular Ca2+ dynamics in astrocytes can be triggered by neuronal activity and in turn regulate a variety of downstream processes that modulate neuronal function. In this fashion, astrocytic Ca2+ signaling is regarded as a processor of neural network activity by means of complex spatial and temporal Ca2+ dynamics. Accordingly, a key step is to understand how different patterns of neural activity translate into spatiotemporal dynamics of intracellular Ca2+ in astrocytes. Here, we introduce a minimal compartmental model for astrocytes that can qualitatively reproduce essential hierarchical features of spatiotemporal Ca2+ dynamics in astrocytes. We find that the rate of neuronal firing determines the rate of Ca2+ spikes in single individual processes as well as in the soma of the cell, while correlations of incoming neuronal activity are important in determining the rate of \"global\" Ca2+ spikes that can engulf soma and the majority of processes. Significantly, our model predicts that whether the endoplasmic reticulum is shared between soma and processes or not determines the relationship between the firing rate of somatic Ca2+ events and the rate of neural network activity. Together these results provide intuition about how neural activity in combination with inherent cellular properties shapes spatiotemporal astrocytic Ca2+ dynamics, and provide experimentally testable predictions."}], "ArticleTitle": "A Compartmental Model to Investigate Local and Global Ca2+ Dynamics in Astrocytes."}, "31417386": {"mesh": [], "AbstractText": [{"section": null, "text": "A new approach to understanding the interaction between cortical areas is provided by a mathematical analysis of biased competition, which describes many interactions between cortical areas, including those involved in top-down attention. The analysis helps to elucidate the principles of operation of such cortical systems, and in particular the parameter values within which biased competition operates. The analytic results are supported by simulations that illustrate the operation of the system with parameters selected from the analysis. The findings provide a detailed mathematical analysis of the operation of these neural systems with nodes connected by feedforward (bottom-up) and feedback (top-down) connections. The analysis provides the critical value of the top-down attentional bias that enables biased competition to operate for a range of input values to the network, and derives this as a function of all the parameters in the model. The critical value of the top-down bias depends linearly on the value of the other inputs, but the coefficients in the function reveal non-linear relations between the remaining parameters. The results provide reasons why the backprojections should not be very much weaker than the forward connections between two cortical areas. The major advantage of the analytical approach is that it discloses relations between all the parameters of the model."}], "ArticleTitle": "Analysis of Biased Competition and Cooperation for Attention in the Cerebral Cortex."}, "29209189": {"mesh": [], "AbstractText": [{"section": null, "text": "How neurons are connected in the brain to perform computation is a key issue in neuroscience. Recently, the development of calcium imaging and multi-electrode array techniques have greatly enhanced our ability to measure the firing activities of neuronal populations at single cell level. Meanwhile, the intracellular recording technique is able to measure subthreshold voltage dynamics of a neuron. Our work addresses the issue of how to combine these measurements to reveal the underlying network structure. We propose the spike-triggered regression (STR) method, which employs both the voltage trace and firing activity of the neuronal population to reconstruct the underlying synaptic connectivity. Our numerical study of the conductance-based integrate-and-fire neuronal network shows that only short data of 20 ~ 100 s is required for an accurate recovery of network topology as well as the corresponding coupling strength. Our method can yield an accurate reconstruction of a large neuronal network even in the case of dense connectivity and nearly synchronous dynamics, which many other network reconstruction methods cannot successfully handle. In addition, we point out that, for sparse networks, the STR method can infer coupling strength between each pair of neurons with high accuracy in the absence of the global information of all other neurons."}], "ArticleTitle": "Spike-Triggered Regression for Synaptic Connectivity Reconstruction in Neuronal Networks."}, "32009921": {"mesh": [], "AbstractText": [{"section": null, "text": "Dynamic Functional Connectivity (DFC) analysis is a promising approach for the characterization of brain electrophysiological activity. In this study, we investigated abnormal alterations due to mild Traumatic Brain Injury (mTBI) using DFC of the source reconstructed magnetoencephalographic (MEG) resting-state recordings. Brain activity in several well-known frequency bands was first reconstructed using beamforming of the MEG data to determine ninety anatomical brain regions of interest. A DFC graph was formulated using the imaginary part of phase-locking values, which were obtained from 30 mTBI patients and 50 healthy controls (HC). Subsequently, we estimated normalized Laplacian transformations of individual, statistically and topologically filtered quasi-static graphs. The corresponding eigenvalues of each node synchronization were then computed and through the neural-gas algorithm, we quantized the evolution of the eigenvalues resulting in distinct network microstates (NMstates). The discrimination level between the two groups was assessed using an iterative cross-validation classification scheme with features either the NMstates in each frequency band, or the combination of the so-called chronnectomics (flexibility index, occupancy time of NMstate, and Dwell time) with the complexity index over the evolution of the NMstates across all frequency bands. Classification performance based on chronnectomics showed 80% accuracy, 99% sensitivity, and 49% specificity. However, performance was much higher (accuracy: 91-97%, sensitivity: 100%, and specificity: 77-93%) when focusing on the microstates. Exploring the mean node degree within and between brain anatomical networks (default mode network, frontoparietal, occipital, cingulo-opercular, and sensorimotor), a reduced pattern occurred from lower to higher frequency bands, with statistically significant stronger degrees for the HC than the mTBI group. A higher entropic profile on the temporal evolution of the modularity index was observed for both NMstates for the mTBI group across frequencies. A significant difference in the flexibility index was observed between the two groups for the &#946; frequency band. The latter finding may support a central role of the thalamus impairment in mTBI. The current study considers a complete set of frequency-dependent connectomic markers of mTBI-caused alterations in brain connectivity that potentially could serve as markers to assess the return of an injured subject back to normality."}], "ArticleTitle": "Aberrant Whole-Brain Transitions and Dynamics of Spontaneous Network Microstates in Mild Traumatic Brain Injury."}, "32038216": {"mesh": [], "AbstractText": [{"section": null, "text": "A novel deep learning based model called Multi-Planar Spatial Convolutional Neural Network (MPS-CNN) is proposed for effective, automated segmentation of different sub-regions viz. peritumoral edema (ED), necrotic core (NCR), enhancing and non-enhancing tumor core (ET/NET), from multi-modal MR images of the brain. An encoder-decoder type CNN model is designed for pixel-wise segmentation of the tumor along three anatomical planes (axial, sagittal, and coronal) at the slice level. These are then combined, by incorporating a consensus fusion strategy with a fully connected Conditional Random Field (CRF) based post-refinement, to produce the final volumetric segmentation of the tumor and its constituent sub-regions. Concepts, such as spatial-pooling and unpooling are used to preserve the spatial locations of the edge pixels, for reducing segmentation error around the boundaries. A new aggregated loss function is also developed for effectively handling data imbalance. The MPS-CNN is trained and validated on the recent Multimodal Brain Tumor Segmentation Challenge (BraTS) 2018 dataset. The Dice scores obtained for the validation set for whole tumor (WT :NCR/NE +ET +ED), tumor core (TC:NCR/NET +ET), and enhancing tumor (ET) are 0.90216, 0.87247, and 0.82445. The proposed MPS-CNN is found to perform the best (based on leaderboard scores) for ET and TC segmentation tasks, in terms of both the quantitative measures (viz. Dice and Hausdorff). In case of the WT segmentation it also achieved the second highest accuracy, with a score which was only 1% less than that of the best performing method."}], "ArticleTitle": "Novel Volumetric Sub-region Segmentation in Brain Tumors."}, "29872388": {"mesh": [], "AbstractText": [{"section": null, "text": "Simple models of short term synaptic plasticity that incorporate facilitation and/or depression have been created in abundance for different synapse types and circumstances. The analysis of these models has included computing mutual information between a stochastic input spike train and some sort of representation of the postsynaptic response. While this approach has proven useful in many contexts, for the purpose of determining the type of process underlying a stochastic output train, it ignores the ordering of the responses, leaving an important characterizing feature on the table. In this paper we use a broader class of information measures on output only, and specifically construct hidden Markov models (HMMs) (known as epsilon machines or causal state models) to differentiate between synapse type, and classify the complexity of the process. We find that the machines allow us to differentiate between processes in a way not possible by considering distributions alone. We are also able to understand these differences in terms of the dynamics of the model used to create the output response, bringing the analysis full circle. Hence this technique provides a complimentary description of the synaptic filtering process, and potentially expands the interpretation of future experimental results."}], "ArticleTitle": "Data Driven Models of Short-Term Synaptic Plasticity."}, "32754024": {"mesh": [], "AbstractText": [{"section": null, "text": "Determination of muscle forces during motion can help to understand motor control, assess pathological movement, diagnose neuromuscular disorders, or estimate joint loads. Difficulty of in vivo measurement made computational analysis become a common alternative in which, as several muscles serve each degree of freedom, the muscle redundancy problem must be solved. Unlike static optimization (SO), synergy optimization (SynO) couples muscle activations across all time frames, thereby altering estimated muscle co-contraction. This study explores whether the use of a muscle synergy structure within an SO framework improves prediction of muscle activations during walking. A motion/force/electromyography (EMG) gait analysis was performed on five healthy subjects. A musculoskeletal model of the right leg actuated by 43 Hill-type muscles was scaled to each subject and used to calculate joint moments, muscle-tendon kinematics, and moment arms. Muscle activations were then estimated using SynO with two to six synergies and traditional SO, and these estimates were compared with EMG measurements. Synergy optimization neither improved SO prediction of experimental activation patterns nor provided SO exact matching of joint moments. Finally, synergy analysis was performed on SO estimated activations, being found that the reconstructed activations produced poor matching of experimental activations and joint moments. As conclusion, it can be said that, although SynO did not improve prediction of muscle activations during gait, its reduced dimensional control space could be beneficial for applications such as functional electrical stimulation or motion control and prediction."}], "ArticleTitle": "Do Muscle Synergies Improve Optimization Prediction of Muscle Activations During Gait?"}, "32499690": {"mesh": [], "AbstractText": [{"section": null, "text": "Our aim is to propose an efficient algorithm for enhancing the contrast of dark images based on the principle of stochastic resonance in a global feedback spiking network of integrate-and-fire neurons. By linear approximation and direct simulation, we disclose the dependence of the peak signal-to-noise ratio on the spiking threshold and the feedback coupling strength. Based on this theoretical analysis, we then develop a dynamical system algorithm for enhancing dark images. In the new algorithm, an explicit formula is given on how to choose a suitable spiking threshold for the images to be enhanced, and a more effective quantifying index, the variance of image, is used to replace the commonly used measure. Numerical tests verify the efficiency of the new algorithm. The investigation provides a good example for the application of stochastic resonance, and it might be useful for explaining the biophysical mechanism behind visual perception."}], "ArticleTitle": "Stochastic Resonance Based Visual Perception Using Spiking Neural Networks."}, "32132915": {"mesh": [], "AbstractText": [{"section": null, "text": "Natural brains perform miraculously well in learning new tasks from a small number of samples, whereas sample efficient learning is still a major open problem in the field of machine learning. Here, we raise the question, how the neural coding scheme affects sample efficiency, and make first progress on this question by proposing and analyzing a learning algorithm that uses a simple reinforce-type plasticity mechanism and does not require any gradients to learn low dimensional mappings. It harnesses three bio-plausible mechanisms, namely, population codes with bell shaped tuning curves, continous attractor mechanisms and probabilistic synapses, to achieve sample efficient learning. We show both theoretically and by simulations that population codes with broadly tuned neurons lead to high sample efficiency, whereas codes with sharply tuned neurons account for high final precision. Moreover, a dynamic adaptation of the tuning width during learning gives rise to both, high sample efficiency and high final precision. We prove a sample efficiency guarantee for our algorithm that lies within a logarithmic factor from the information theoretical optimum. Our simulations show that for low dimensional mappings, our learning algorithm achieves comparable sample efficiency to multi-layer perceptrons trained by gradient descent, although it does not use any gradients. Furthermore, it achieves competitive sample efficiency in low dimensional reinforcement learning tasks. From a machine learning perspective, these findings may inspire novel approaches to improve sample efficiency. From a neuroscience perspective, these findings suggest sample efficiency as a yet unstudied functional role of adaptive tuning curve width."}], "ArticleTitle": "Adaptive Tuning Curve Widths Improve Sample Efficient Learning."}, "29670518": {"mesh": [], "AbstractText": [{"section": null, "text": "The parieto-occipital alpha (8-13 Hz) rhythm is by far the strongest spectral fingerprint in the human brain. Almost 90 years later, its physiological origin is still far from clear. In this Research Topic I review human pharmacological studies using electroencephalography (EEG) and magnetoencephalography (MEG) that investigated the physiological mechanisms behind posterior alpha. Based on results from classical and recent experimental studies, I find a wide spectrum of drugs that modulate parieto-occipital alpha power. Alpha frequency is rarely affected, but this might be due to the range of drug dosages employed. Animal and human pharmacological findings suggest that both GABA enhancers and NMDA blockers systematically decrease posterior alpha power. Surprisingly, most of the theoretical frameworks do not seem to embrace these empirical findings and the debate on the functional role of alpha oscillations has been polarized between the inhibition vs. active poles hypotheses. Here, I speculate that the functional role of alpha might depend on physiological excitation as much as on physiological inhibition. This is supported by animal and human pharmacological work showing that GABAergic, glutamatergic, cholinergic, and serotonergic receptors in the thalamus and the cortex play a key role in the regulation of alpha power and frequency. This myriad of physiological modulations fit with the view that the alpha rhythm is a complex rhythm with multiple sources supported by both thalamo-cortical and cortico-cortical loops. Finally, I briefly discuss how future research combining experimental measurements derived from theoretical predictions based of biophysically realistic computational models will be crucial to the reconciliation of these disparate findings."}], "ArticleTitle": "On the Physiological Modulation and Potential Mechanisms Underlying Parieto-Occipital Alpha Oscillations."}, "33262697": {"mesh": [], "AbstractText": [{"section": null, "text": "Our understanding of the neurofunctional mechanisms of speech production and their pathologies is still incomplete. In this paper, a comprehensive model of speech production based on the Neural Engineering Framework (NEF) is presented. This model is able to activate sensorimotor plans based on cognitive-functional processes (i.e., generation of the intention of an utterance, selection of words and syntactic frames, generation of the phonological form and motor plan; feedforward mechanism). Since the generation of different states of the utterance are tied to different levels in the speech production hierarchy, it is shown that different forms of speech errors as well as speech disorders can arise at different levels in the production hierarchy or are linked to different levels and different modules in the speech production model. In addition, the influence of the inner feedback mechanisms on normal as well as on disordered speech is examined in terms of the model. The model uses a small number of core concepts provided by the NEF, and we show that these are sufficient to create this neurobiologically detailed model of the complex process of speech production in a manner that is, we believe, clear, efficient, and understandable."}], "ArticleTitle": "Hierarchical Sequencing and Feedforward and Feedback Control Mechanisms in Speech Production: A Preliminary Approach for Modeling Normal and Disordered Speech."}, "31333439": {"mesh": [], "AbstractText": [{"section": null, "text": "The analysis of Electroencephalographic (EEG) signals is of ulterior importance to aid in the diagnosis of mental disease and to increase our understanding of the brain. Traditionally, clinical EEG has been analyzed in terms of temporal waveforms, looking at rhythms in spontaneous activity, subjectively identifying troughs and peaks in Event-Related Potentials (ERP), or by studying graphoelements in pathological sleep stages. Additionally, the discipline of Brain Computer Interfaces (BCI) requires new methods to decode patterns from non-invasive EEG signals. This field is developing alternative communication pathways to transmit volitional information from the Central Nervous System. The technology could potentially enhance the quality of life of patients affected by neurodegenerative disorders and other mental illness. This work mimics what electroencephalographers have been doing clinically, visually inspecting, and categorizing phenomena within the EEG by the extraction of features from images of signal plots. These features are constructed based on the calculation of histograms of oriented gradients from pixels around the signal plot. It aims to provide a new objective framework to analyze, characterize and classify EEG signal waveforms. The feasibility of the method is outlined by detecting the P300, an ERP elicited by the oddball paradigm of rare events, and implementing an offline P300-based BCI Speller. The validity of the proposal is shown by offline processing a public dataset of Amyotrophic Lateral Sclerosis (ALS) patients and an own dataset of healthy subjects."}], "ArticleTitle": "Histogram of Gradient Orientations of Signal Plots Applied to P300 Detection."}, "33304259": {"mesh": [], "AbstractText": [{"section": null, "text": "Acupuncturing the ST36 acupoint can evoke the response of the sensory nervous system, which is translated into output electrical signals in the spinal dorsal root. Neural response activities, especially synchronous spike events, evoked by different acupuncture manipulations have remarkable differences. In order to identify these network collaborative activities, we analyze the underlying spike correlation in the synchronous spike event. In this paper, we adopt a log-linear model to describe network response activities evoked by different acupuncture manipulations. Then the state-space model and Bayesian theory are used to estimate network spike correlations. Two sets of simulation data are used to test the effectiveness of the estimation algorithm and the model goodness-of-fit. In addition, simulation data are also used to analyze the relationship between spike correlations and synchronous spike events. Finally, we use this method to identify network spike correlations evoked by four different acupuncture manipulations. Results show that reinforcing manipulations (twirling reinforcing and lifting-thrusting reinforcing) can evoke the third-order spike correlation but reducing manipulations (twirling reducing and lifting-thrusting reducing) does not. This is the main reason why synchronous spikes evoked by reinforcing manipulations are more abundant than reducing manipulations."}], "ArticleTitle": "Spiking Correlation Analysis of Synchronous Spikes Evoked by Acupuncture Mechanical Stimulus."}, "32116620": {"mesh": [], "AbstractText": [{"section": null, "text": "The accurate automatic segmentation of gliomas and its intra-tumoral structures is important not only for treatment planning but also for follow-up evaluations. Several methods based on 2D and 3D Deep Neural Networks (DNN) have been developed to segment brain tumors and to classify different categories of tumors from different MRI modalities. However, these networks are often black-box models and do not provide any evidence regarding the process they take to perform this task. Increasing transparency and interpretability of such deep learning techniques is necessary for the complete integration of such methods into medical practice. In this paper, we explore various techniques to explain the functional organization of brain tumor segmentation models and to extract visualizations of internal concepts to understand how these networks achieve highly accurate tumor segmentations. We use the BraTS 2018 dataset to train three different networks with standard architectures and outline similarities and differences in the process that these networks take to segment brain tumors. We show that brain tumor segmentation networks learn certain human-understandable disentangled concepts on a filter level. We also show that they take a top-down or hierarchical approach to localizing the different parts of the tumor. We then extract visualizations of some internal feature maps and also provide a measure of uncertainty with regards to the outputs of the models to give additional qualitative evidence about the predictions of these networks. We believe that the emergence of such human-understandable organization and concepts might aid in the acceptance and integration of such methods in medical diagnosis."}], "ArticleTitle": "Demystifying Brain Tumor Segmentation Networks: Interpretability and Uncertainty Analysis."}, "30809142": {"mesh": [], "AbstractText": [{"section": null, "text": "Epilepsy is one of the most common chronic neurological diseases. High-frequency oscillations (HFOs) have emerged as promising biomarkers for the epileptogenic zone. However, visual marking of HFOs is a time-consuming and laborious process. Several automated techniques have been proposed to detect HFOs, yet these are still far from being suitable for application in a clinical setting. Here, ripples and fast ripples from intracranial electroencephalograms were detected in six patients with intractable epilepsy using a convolutional neural network (CNN) method. This approach proved more accurate than using four other HFO detectors integrated in RIPPLELAB, providing a higher sensitivity (77.04% for ripples and 83.23% for fast ripples) and specificity (72.27% for ripples and 79.36% for fast ripples) for HFO detection. Furthermore, for one patient, the Cohen's kappa coefficients comparing automated detection and visual analysis results were 0.541 for ripples and 0.777 for fast ripples. Hence, our automated detector was capable of reliable estimates of ripples and fast ripples with higher sensitivity and specificity than four other HFO detectors. Our detector may be used to assist clinicians in locating epileptogenic zone in the future."}], "ArticleTitle": "Automated Detection of High-Frequency Oscillations in Epilepsy Based on a Convolutional Neural Network."}, "31019457": {"mesh": [], "AbstractText": [{"section": null, "text": "Transcranial Electrical Stimulation (TES) continues to demonstrate success as a medical intervention for individuals with neurodegenerative diseases. Despite promising results from these neuromodulation modalities, the cellular level mechanisms by which this neurotherapy operates are not fully comprehended. In particular, the effects of TES on ion channel gating and ion transport are not known. Using the Poisson-Nernst-Planck model of electrodiffusion, coupled with a Hodgkin-Huxley based model of cellular ion transport, we present a model of TES that, for the first time, integrates electric potential energy, individualized ion species, voltage-gated ion channels, and transmembrane ionic flux during TES administration. Computational simulations are executed on a biologically-inspired domain with medically-based TES treatment parameters and quantify neuron-level electrical processes resulting from this form of neurostimulation. Results confirm prior findings that show that TES polarizes the cell membrane, however, these are extended as simulations in this paper show that polarization occurs in a location specific manner, where the type and degree of polarization depends on the position on the membrane within a node of Ranvier. In addition, results demonstrate that TES causes ion channel gating variables to change in a location specific fashion and, as a result, transmembrane current from distinct ion species depends on both time and membrane location. Another simulation finding is that intracellular calcium concentrations increase significantly due to a TES-induced calcium influx. As cytosolic calcium is critical in intracellular signaling pathways that govern proper neurotransmitter secretion as well as support cell viability, this alteration in calcium homeostasis suggests a possible mechanism by which TES operates at the neuronal level to achieve neurotherapeutic success."}], "ArticleTitle": "Location Specificity of Transcranial Electrical Stimulation on Neuronal Electrodynamics: A Mathematical Model of Ion Channel Gating Dynamics and Ionic Flux Due to Neurostimulation."}, "29230172": {"mesh": [], "AbstractText": [{"section": null, "text": "In human and animal motor control several sensory organs contribute to a network of sensory pathways modulating the motion depending on the task and the phase of execution to generate daily motor tasks such as locomotion. To better understand the individual and joint contribution of reflex pathways in locomotor tasks, we developed a neuromuscular model that describes hopping movements. In this model, we consider the influence of proprioceptive length (LFB), velocity (VFB) and force feedback (FFB) pathways of a leg extensor muscle on hopping stability, performance and efficiency (metabolic effort). Therefore, we explore the space describing the blending of the monosynaptic reflex pathway gains. We call this reflex parameter space a sensor-motor map. The sensor-motor maps are used to visualize the functional contribution of sensory pathways in multisensory integration. We further evaluate the robustness of these sensor-motor maps to changes in tendon elasticity, body mass, segment length and ground compliance. The model predicted that different reflex pathway compositions selectively optimize specific hopping characteristics (e.g., performance and efficiency). Both FFB and LFB were pathways that enable hopping. FFB resulted in the largest hopping heights, LFB enhanced hopping efficiency and VFB had the ability to disable hopping. For the tested case, the topology of the sensor-motor maps as well as the location of functionally optimal compositions were invariant to changes in system designs (tendon elasticity, body mass, segment length) or environmental parameters (ground compliance). Our results indicate that different feedback pathway compositions may serve different functional roles. The topology of the sensor-motor map was predicted to be robust against changes in the mechanical system design indicating that the reflex system can use different morphological designs, which does not apply for most robotic systems (for which the control often follows a specific design). Consequently, variations in body mechanics are permitted with consistent compositions of sensory feedback pathways. Given the variability in human body morphology, such variations are highly relevant for human motor control."}], "ArticleTitle": "Sensor-Motor Maps for Describing Linear Reflex Composition in Hopping."}, "30555314": {"mesh": [], "AbstractText": [{"section": null, "text": "One curious aspect of human timing is the organization of rhythmic patterns in small integer ratios. Behavioral and neural research has shown that adjacent time intervals in rhythms tend to be perceived and reproduced as approximate fractions of small numbers (e.g., 3/2). Recent work on iterated learning and reproduction further supports this: given a randomly timed drum pattern to reproduce, participants subconsciously transform it toward small integer ratios. The mechanisms accounting for this \"attractor\" phenomenon are little understood, but might be explained by combining two theoretical frameworks from psychophysics. The scalar expectancy theory describes time interval perception and reproduction in terms of Weber's law: just detectable durational differences equal a constant fraction of the reference duration. The notion of categorical perception emphasizes the tendency to perceive time intervals in categories, i.e., \"short\" vs. \"long.\" In this piece, we put forward the hypothesis that the integer-ratio bias in rhythm perception and production might arise from the interaction of the scalar property of timing with the categorical perception of time intervals, and that neurally it can plausibly be related to oscillatory activity. We support our integrative approach with mathematical derivations to formalize assumptions and provide testable predictions. We present equations to calculate durational ratios by: (i) parameterizing the relationship between durational categories, (ii) assuming a scalar timing constant, and (iii) specifying one (of K) category of ratios. Our derivations provide the basis for future computational, behavioral, and neurophysiological work to test our model."}], "ArticleTitle": "Why Do Durations in Musical Rhythms Conform to Small Integer Ratios?"}, "28265244": {"mesh": [], "AbstractText": [{"section": null, "text": "Purpose: To examine the ability of the BOLD response to EEG spikes to assess the epileptogenicity of the lesion in patients with focal cortical dysplasia (FCD). Method: Patients with focal epilepsy and FCD who underwent 3T EEG-fMRI from 2006 to 2010 were included. Diagnosis of FCD was based on neuroradiology (MRI+), or histopathology in MRI-negative cases (MRI-). Patients underwent 120 min EEG-fMRI recording session. Spikes similar to those recorded outside the scanner were marked in the filtered EEG. The lesion (in MRI+) or the removed cortex (in MRI-) was marked on the anatomical T1 sequence, blindly to the BOLD response, after reviewing the FLAIR images. For each BOLD response we assessed the concordance with the spike field and with the lesion in MRI+ or the removed cortex in MRI-. BOLD responses were considered \"concordant\" if the maximal t-value was inside the marking. Follow-up after resection was used as gold-standard. Results: Twenty patients were included (13 MRI+, 7 MRI-), but in seven the EEG was not active or there were artifacts during acquisition. In all 13 studied patients, at least one BOLD response was concordant with the spike field; in 9/13 (69%) at least one BOLD response was concordant with the lesion: in 6/7 (86%) MRI+ and in 3/6 (50%) MRI- patients. Conclusions: Our study shows a high level of concordance between FCD and BOLD response. This data could provide useful information especially for MRI negative patients. Moreover, it shows in almost all FCD patients, a metabolic involvement of remote cortical or subcortical structures, corroborating the concept of epileptic network."}], "ArticleTitle": "Contributions of EEG-fMRI to Assessing the Epileptogenicity of Focal Cortical Dysplasia."}, "28798679": {"mesh": [], "AbstractText": [{"section": null, "text": "Counteracting the destabilizing force of gravity is usually considered to be the main purpose of postural control. However, from the consideration of the mechanical requirements for movement, we argue that posture is adjusted in view of providing impetus for movement. Thus, we show that the posture that is usually adopted in quiet standing in fact allows torque for potential movement. Moreover, when performing a movement-either voluntarily or in response to an external perturbation-we show that the postural adjustments are organized both spatially and temporally so as to provide the required torque for the movement. Thus, when movement is performed skillfully, the force of gravity is not counteracted but actually used to provide impetus to movement. This ability to move one's weight so as to exploit the torque of gravity seems to be dependent on development and skill learning, and is impaired in aging."}], "ArticleTitle": "Mobility as the Purpose of Postural Control."}, "30666194": {"mesh": [], "AbstractText": [{"section": null, "text": "Adult neurogenesis in the hippocampal dentate gyrus (DG) of mammals is known to contribute to memory encoding in many tasks. The DG also exhibits exceptionally sparse activity compared to other systems, however, whether sparseness and neurogenesis interact during memory encoding remains elusive. We implement a novel learning rule consistent with experimental findings of competition among adult-born neurons in a supervised multilayer feedforward network trained to discriminate between contexts. From this rule, the DG population partitions into neuronal ensembles each of which is biased to represent one of the contexts. This corresponds to a low dimensional representation of the contexts, whereby the fastest dimensionality reduction is achieved in sparse models. We then modify the rule, showing that equivalent representations and performance are achieved when neurons compete for synaptic stability rather than neuronal survival. Our results suggest that competition for stability in sparse models is well-suited to developing ensembles of what may be called memory engram cells."}], "ArticleTitle": "Hippocampal Neurogenesis Reduces the Dimensionality of Sparsely Coded Representations to Enhance Memory Encoding."}, "29093676": {"mesh": [], "AbstractText": [{"section": null, "text": "Musculoskeletal tissues respond to optimal mechanical signals (e.g., strains) through anabolic adaptations, while mechanical signals above and below optimal levels cause tissue catabolism. If an individual's physical behavior could be altered to generate optimal mechanical signaling to musculoskeletal tissues, then targeted strengthening and/or repair would be possible. We propose new bioinspired technologies to provide real-time biofeedback of relevant mechanical signals to guide training and rehabilitation. In this review we provide a description of how wearable devices may be used in conjunction with computational rigid-body and continuum models of musculoskeletal tissues to produce real-time estimates of localized tissue stresses and strains. It is proposed that these bioinspired technologies will facilitate a new approach to physical training that promotes tissue strengthening and/or repair through optimal tissue loading."}], "ArticleTitle": "Bioinspired Technologies to Connect Musculoskeletal Mechanobiology to the Person for Training and Rehabilitation."}, "28611618": {"mesh": [], "AbstractText": [{"section": null, "text": "Neural synchronization is believed to play an important role in different brain functions. Synchrony in cortical and subcortical circuits is frequently variable in time and not perfect. Few long intervals of desynchronized dynamics may be functionally different from many short desynchronized intervals although the average synchrony may be the same. Recent analysis of imperfect synchrony in different neural systems reported one common feature: neural oscillations may go out of synchrony frequently, but primarily for a short time interval. This study explores potential mechanisms and functional advantages of this short desynchronizations dynamics using computational neuroscience techniques. We show that short desynchronizations are exhibited in coupled neurons if their delayed rectifier potassium current has relatively large values of the voltage-dependent activation time-constant. The delayed activation of potassium current is associated with generation of quickly-rising action potential. This \"spikiness\" is a very general property of neurons. This may explain why very different neural systems exhibit short desynchronization dynamics. We also show how the distribution of desynchronization durations may be independent of the synchronization strength. Finally, we show that short desynchronization dynamics requires weaker synaptic input to reach a pre-set synchrony level. Thus, this dynamics allows for efficient regulation of synchrony and may promote efficient formation of synchronous neural assemblies."}], "ArticleTitle": "Potential Mechanisms and Functions of Intermittent Neural Synchronization."}, "32231528": {"mesh": [], "AbstractText": [{"section": null, "text": "We propose that correlations among neurons are generically strong enough to organize neural activity patterns into a discrete set of clusters, which can each be viewed as a population codeword. Our reasoning starts with the analysis of retinal ganglion cell data using maximum entropy models, showing that the population is robustly in a frustrated, marginally sub-critical, or glassy, state. This leads to an argument that neural populations in many other brain areas might share this structure. Next, we use latent variable models to show that this glassy state possesses well-defined clusters of neural activity. Clusters have three appealing properties: (i) clusters exhibit error correction, i.e., they are reproducibly elicited by the same stimulus despite variability at the level of constituent neurons; (ii) clusters encode qualitatively different visual features than their constituent neurons; and (iii) clusters can be learned by downstream neural circuits in an unsupervised fashion. We hypothesize that these properties give rise to a \"learnable\" neural code which the cortical hierarchy uses to extract increasingly complex features without supervision or reinforcement."}], "ArticleTitle": "Clustering of Neural Activity: A Design Principle for Population Codes."}, "32457589": {"mesh": [], "AbstractText": [{"section": null, "text": "The exponential time differencing (ETD) method allows using a large time step to efficiently evolve stiff systems such as Hodgkin-Huxley (HH) neural networks. For pulse-coupled HH networks, the synaptic spike times cannot be predetermined and are convoluted with neuron's trajectory itself. This presents a challenging issue for the design of an efficient numerical simulation algorithm. The stiffness in the HH equations are quite different, for example, between the spike and non-spike regions. Here, we design a second-order adaptive exponential time differencing algorithm (AETD2) for the numerical evolution of HH neural networks. Compared with the regular second-order Runge-Kutta method (RK2), our AETD2 method can use time steps one order of magnitude larger and improve computational efficiency more than ten times while excellently capturing accurate traces of membrane potentials of HH neurons. This high accuracy and efficiency can be robustly obtained and do not depend on the dynamical regimes, connectivity structure or the network size."}], "ArticleTitle": "Exponential Time Differencing Algorithm for Pulse-Coupled Hodgkin-Huxley Neural Networks."}, "32009923": {"mesh": [], "AbstractText": [{"section": null, "text": "Objective: Previous studies have shown that the performance of the famous face P300-speller was better than that of the classical row/column flashing P300-speller. Furthermore, in some studies, the brain was more active when responding to one's own face than to a famous face, and a self-face stimulus elicited larger amplitude event-related potentials (ERPs) than did a famous face. Thus, we aimed to study the role of the self-face paradigm on further improving the performance of the P300-speller system with the famous face P300-speller paradigm as the control paradigm. Methods: We designed two facial P300-speller paradigms based on the self-face and a famous face (Ming Yao, a sports star; the famous face spelling paradigm) with a neutral expression. Results: ERP amplitudes were significantly greater in the self-face than in the famous face spelling paradigm at the parietal area from 340 to 480 ms (P300), from 480 to 600 ms (P600f), and at the fronto-central area from 700 to 800 ms. Offline and online classification results showed that the self-face spelling paradigm accuracies were significantly higher than those of the famous face spelling paradigm at superposing first two times (P < 0.05). Similar results were found for information transfer rates (P < 0.05). Conclusions: The self-face spelling paradigm significantly improved the performance of the P300-speller system. This has significant practical applications for brain-computer interfaces (BCIs) and could avoid infringement issues caused by using images of other people's faces."}], "ArticleTitle": "The Self-Face Paradigm Improves the Performance of the P300-Speller System."}, "32676020": {"mesh": [], "AbstractText": [{"section": null, "text": "Neuron classification is an important component in analyzing network structure and quantifying the effect of neuron topology on signal processing. Current quantification and classification approaches rely on morphology projection onto lower-dimensional spaces. In this paper a 3D visualization and quantification tool is presented. The Density Visualization Pipeline (DVP) computes, visualizes and quantifies the density distribution, i.e., the \"mass\" of interneurons. We use the DVP to characterize and classify a set of GABAergic interneurons. Classification of GABAergic interneurons is of crucial importance to understand on the one hand their various functions and on the other hand their ubiquitous appearance in the neocortex. 3D density map visualization and projection to the one-dimensional x, y, z subspaces show a clear distinction between the studied cells, based on these metrics. The DVP can be coupled to computational studies of the behavior of neurons and networks, in which network topology information is derived from DVP information. The DVP reads common neuromorphological file formats, e.g., Neurolucida XML files, NeuroMorpho.org SWC files and plain ASCII files. Full 3D visualization and projections of the density to 1D and 2D manifolds are supported by the DVP. All routines are embedded within the visual programming IDE VRL-Studio for Java which allows the definition and rapid modification of analysis workflows."}], "ArticleTitle": "Density Visualization Pipeline: A Tool for Cellular and Network Density Visualization and Analysis."}, "32792930": {"mesh": [], "AbstractText": [{"section": null, "text": "The problem of cancer risk analysis is of great importance to health-service providers and medical researchers. In this study, we propose a novel Artificial Neural Network (ANN) algorithm based on the probabilistic framework, which aims to investigate patient patterns associated with their disease development. Compared to the traditional ANN where input features are directly extracted from raw data, the proposed probabilistic ANN manipulates original inputs according to their probability distribution. More precisely, the Na&#239;ve Bayes and Markov chain models are used to approximate the posterior distribution of the raw inputs, which provides a useful estimation of subsequent disease development. Later, this distribution information is further leveraged as additional input to train ANN. Additionally, to reduce the training cost and to boost the generalization capability, a sparse training strategy is also introduced. Experimentally, one of the largest cancer-related datasets is employed in this study. Compared to state-of-the-art methods, the proposed algorithm achieves a much better outcome, in terms of the prediction accuracy of subsequent disease development. The result also reveals the potential impact of patients' disease sequence on their future risk management."}], "ArticleTitle": "Cancer Risk Analysis Based on Improved Probabilistic Neural Network."}, "33192428": {"mesh": [], "AbstractText": [{"section": null, "text": "Subthalamic nucleus deep brain stimulation (STN-DBS) is an effective invasive treatment for advanced Parkinson's disease (PD) at present. Due to the invasiveness and cost of operations, a reliable tool is required to predict the outcome of therapy in the clinical decision-making process. This work aims to investigate whether the topological network of functional connectivity states can predict the outcome of DBS without medication. Fifty patients were recruited to extract the features of the brain related to the improvement rate of PD after STN-DBS and to train the machine learning model that can predict the therapy's effect. The functional connectivity analyses suggested that the GBRT model performed best with Pearson's correlations of r = 0.65, p = 2.58E-07 in medication-off condition. The connections between middle frontal gyrus (MFG) and inferior temporal gyrus (ITG) contribute most in the GBRT model."}], "ArticleTitle": "Connectome-Based Model Predicts Deep Brain Stimulation Outcome in Parkinson's Disease."}, "28408878": {"mesh": [], "AbstractText": [{"section": null, "text": "It is widely accepted that the basal ganglia (BG) play a key role in action selection and reinforcement learning. However, despite considerable number of studies, the BG architecture and function are not completely understood. Action selection and reinforcement learning are facilitated by the activity of dopaminergic neurons, which encode reward prediction errors when reward outcomes are higher or lower than expected. The BG are thought to select proper motor responses by gating appropriate actions, and suppressing inappropriate ones. The direct striato-nigral (GO) and the indirect striato-pallidal (NOGO) pathways have been suggested to provide the functions of BG in the two-pathway concept. Previous models confirmed the idea that these two pathways can mediate the behavioral choice, but only for a relatively small number of potential behaviors. Recent studies have provided new evidence of BG involvement in motor adaptation tasks, in which adaptation occurs in a non-error-based manner. In such tasks, there is a continuum of possible actions, each represented by a complex neuronal activity pattern. We extended the classical concept of the two-pathway BG by creating a model of BG interacting with a movement execution system, which allows for an arbitrary number of possible actions. The model includes sensory and premotor cortices, BG, a spinal cord network, and a virtual mechanical arm performing 2D reaching movements. The arm is composed of 2 joints (shoulder and elbow) controlled by 6 muscles (4 mono-articular and 2 bi-articular). The spinal cord network contains motoneurons, controlling the muscles, and sensory interneurons that receive afferent feedback and mediate basic reflexes. Given a specific goal-oriented motor task, the BG network through reinforcement learning constructs a behavior from an arbitrary number of basic actions represented by cortical activity patterns. Our study confirms that, with slight modifications, the classical two-pathway BG concept is consistent with results of previous studies, including non-error based motor adaptation experiments, pharmacological manipulations with BG nuclei, and functional deficits observed in BG-related motor disorders."}], "ArticleTitle": "Reward Based Motor Adaptation Mediated by Basal Ganglia."}, "30042668": {"mesh": [], "AbstractText": [{"section": null, "text": "During ongoing and Up state activity, cortical circuits manifest a set of dynamical features that are conserved across these states. The present work systematizes these phenomena by three notions: excitability, the ability to sustain activity without external input; balance, precise coordination of excitatory and inhibitory neuronal inputs; and stability, maintenance of activity at a steady level. Slice preparations exhibiting Up states demonstrate that balanced activity can be maintained by small local circuits. While computational models of cortical circuits have included different combinations of excitability, balance, and stability, they have done so without a systematic quantitative comparison with experimental data. Our study provides quantitative criteria for this purpose, by analyzing in-vitro and in-vivo neuronal activity and characterizing the dynamics on the neuronal and population levels. The criteria are defined with a tolerance that allows for differences between experiments, yet are sufficient to capture commonalities between persistently depolarized cortical network states and to help validate computational models of cortex. As test cases for the derived set of criteria, we analyze three widely used models of cortical circuits and find that each model possesses some of the experimentally observed features, but none satisfies all criteria simultaneously, showing that the criteria are able to identify weak spots in computational models. The criteria described here form a starting point for the systematic validation of cortical neuronal network models, which will help improve the reliability of future models, and render them better building blocks for larger models of the brain."}], "ArticleTitle": "Criteria on Balance, Stability, and Excitability in Cortical Networks for Constraining Computational Models."}, "30833896": {"mesh": [], "AbstractText": [{"section": null, "text": "This paper describes continuing research on the building of neurocognitive models of the internal mental and brain processes of children using a novel adapted combination of existing computational approaches and tools, and using electro-encephalographic (EEG) data to validate the models. The guiding working model which was pragmatically selected for investigation was the established and widely used Adaptive Control of Thought-Rational (ACT-R) modeling architecture from cognitive science. The anatomo-functional circuitry covered by ACT-R is validated by MRI-based neuroscience research. The present experimental data was obtained from a cognitive neuropsychology study involving preschool children (aged 4-6), which measured their visual selective attention and word comprehension behaviors. The collection and analysis of Event-Related Potentials (ERPs) from the EEG data allowed for the identification of sources of electrical activity known as dipoles within the cortex, using a combination of computational tools (Independent Component Analysis, FASTICA; EEG-Lab DIPFIT). The results were then used to build neurocognitive models based on Python ACT-R such that the patterns and the timings of the measured EEG could be reproduced as simplified symbolic representations of spikes, built through simplified electric-field simulations. The models simulated ultimately accounted for more than three-quarters of variations spatially and temporally in all electrical potential measurements (fit of model to dipole data expressed as R 2 ranged between 0.75 and 0.98; P < 0.0001). Implications for practical uses of the present work are discussed for learning and educational applications in non-clinical and special needs children's populations, and for the possible use of non-experts (teachers and parents)."}], "ArticleTitle": "Retooling Computational Techniques for EEG-Based Neurocognitive Modeling of Children's Data, Validity and Prospects for Learning and Education."}, "33897396": {"mesh": [], "AbstractText": [{"section": null, "text": "The relation between mental states and brain states is important in computational neuroscience, and in psychiatry in which interventions with medication are made on brain states to alter mental states. The relation between the brain and the mind has puzzled philosophers for centuries. Here a neuroscience approach is proposed in which events at the sub-neuronal, neuronal, and neuronal network levels take place simultaneously to perform a computation that can be described at a high level as a mental state, with content about the world. It is argued that as the processes at the different levels of explanation take place at the same time, they are linked by a non-causal supervenient relationship: causality can best be described in brains as operating within but not between levels. This allows the supervenient (e.g., mental) properties to be emergent, though once understood at the mechanistic levels they may seem less emergent, and expected. This mind-brain theory allows mental events to be different in kind from the mechanistic events that underlie them; but does not lead one to argue that mental events cause brain events, or vice versa: they are different levels of explanation of the operation of the computational system. This approach may provide a way of thinking about brains and minds that is different from dualism and from reductive physicalism, and which is rooted in the computational processes that are fundamental to understanding brain and mental events, and that mean that the mental and mechanistic levels are linked by the computational process being performed. Explanations at the different levels of operation may be useful in different ways. For example, if we wish to understand how arithmetic is performed in the brain, description at the mental level of the algorithm being computed will be useful. But if the brain operates to result in mental disorders, then understanding the mechanism at the neural processing level may be more useful, in for example, the treatment of psychiatric disorders."}], "ArticleTitle": "A Neuroscience Levels of Explanation Approach to the Mind and the Brain."}, "30210326": {"mesh": [], "AbstractText": [{"section": null, "text": "Time estimation is an important component of the ability to organize and plan sequences of actions as well as cognitive functions, both of which are known to be altered in dyslexia. While attention deficits are accompanied by short Time Productions (TPs), expert meditators have been reported to produce longer durations, and this seems to be related to their increased attentional resources. In the current study, we examined the effects of a month of Quadrato Motor Training (QMT), which is a structured sensorimotor training program that involves sequencing of motor responses based on verbal commands, on TP using a pre-post design. QMT has previously been found to enhance attention and EEG oscillatory activity, especially within the alpha range. For the current study, 29 adult Hebrew readers were recruited, of whom 10 dyslexic participants performed the QMT. The normal readers were randomly assigned to QMT (n = 9) or Verbal Training (VT, identical cognitive training with no overt motor component, and only verbal response, n = 10). Our results demonstrate that in contrast to the controls, longer TP in females was found following 1 month of intensive QMT in the dyslexic group, while the opposite trend occurred in control females. We suggest that this longer TP in the female dyslexics is related to their enhanced attention resulting from QMT. The current findings suggest that the combination of motor and mindful training, embedded in QMT, has a differential effect depending on gender and whether one is dyslexic or not. These results have implications for educational and contemplative neuroscience, emphasizing the connection between specifically-structured motor training, time estimation and attention."}], "ArticleTitle": "Gender-Dependent Changes in Time Production Following Quadrato Motor Training in Dyslexic and Normal Readers."}, "30687054": {"mesh": [], "AbstractText": [{"section": null, "text": "Grid cells and place cells are believed to be cellular substrates for the spatial navigation functions of hippocampus as experimental animals physically navigated in 2D and 3D spaces. However, a recent saccade study on head fixated monkey has also reported grid-like representations on saccadic trajectory while the animal scanned the images on a computer screen. We present two computational models that explain the formation of grid patterns on saccadic trajectory formed on the novel Images. The first model named Saccade Velocity Driven Oscillatory Network -Direct PCA (SVDON-DPCA) explains how grid patterns can be generated on saccadic space using Principal Component Analysis (PCA) like learning rule. The model adopts a hierarchical architecture. We extend this to a network model viz. Saccade Velocity Driven Oscillatory Network-Network PCA (SVDON-NPCA) where the direct PCA stage is replaced by a neural network that can implement PCA using a neurally plausible algorithm. This gives the leverage to study the formation of grid cells at a network level. Saccade trajectory for both models is generated based on an attention model which attends to the salient location by computing the saliency maps of the images. Both models capture the spatial characteristics of grid cells such as grid scale variation on the dorso-ventral axis of Medial Entorhinal cortex. Adding one more layer of LAHN over the SVDON-NPCA model predicts the Place cells in saccadic space, which are yet to be discovered experimentally. To the best of our knowledge, this is the first attempt to model grid cells and place cells from saccade trajectory."}], "ArticleTitle": "Saccade Velocity Driven Oscillatory Network Model of Grid Cells."}, "33013342": {"mesh": [], "AbstractText": [{"section": null, "text": "Many cognitive and behavioral tasks-such as interval timing, spatial navigation, motor control, and speech-require the execution of precisely-timed sequences of neural activation that cannot be fully explained by a succession of external stimuli. We show how repeatable and reliable patterns of spatiotemporal activity can be generated in chaotic and noisy spiking recurrent neural networks. We propose a general solution for networks to autonomously produce rich patterns of activity by providing a multi-periodic oscillatory signal as input. We show that the model accurately learns a variety of tasks, including speech generation, motor control, and spatial navigation. Further, the model performs temporal rescaling of natural spoken words and exhibits sequential neural activity commonly found in experimental data involving temporal processing. In the context of spatial navigation, the model learns and replays compressed sequences of place cells and captures features of neural activity such as the emergence of ripples and theta phase precession. Together, our findings suggest that combining oscillatory neuronal inputs with different frequencies provides a key mechanism to generate precisely timed sequences of activity in recurrent circuits of the brain."}], "ArticleTitle": "Learning Long Temporal Sequences in Spiking Networks by Multiplexing Neural Oscillations."}, "31555115": {"mesh": [], "AbstractText": [{"section": null, "text": "Previous studies have shown that task-irrelevant auditory information can provide temporal clues for the detection of visual targets and improve visual perception; such sounds are called informative sounds. The neural mechanism of the integration of informative sound and visual stimulus has been investigated extensively, using behavioral measurement or neuroimaging methods such as functional magnetic resonance imaging (fMRI) and event-related potential (ERP), but the dynamic processes of audiovisual integration cannot be characterized formally in terms of directed neuronal coupling. The present study adopts dynamic causal modeling (DCM) of fMRI data to identify changes in effective connectivity in the hierarchical brain networks that underwrite audiovisual integration and memory. This allows us to characterize context-sensitive changes in neuronal coupling and show how visual processing is contextualized by the processing of informative and uninformative sounds. Our results show that audiovisual integration with informative and uninformative sounds conforms to different optimal models in the two conditions, indicating distinct neural mechanisms of audiovisual integration. The findings also reveal that a sound is uninformative owing to low-level automatic audiovisual integration and informative owing to integration in high-level cognitive processes."}], "ArticleTitle": "Distinct Mechanism of Audiovisual Integration With Informative and Uninformative Sound in a Visual Detection Task: A DCM Study."}, "32390819": {"mesh": [], "AbstractText": [{"section": null, "text": "The perceptron learning algorithm and its multiple-layer extension, the backpropagation algorithm, are the foundations of the present-day machine learning revolution. However, these algorithms utilize a highly simplified mathematical abstraction of a neuron; it is not clear to what extent real biophysical neurons with morphologically-extended non-linear dendritic trees and conductance-based synapses can realize perceptron-like learning. Here we implemented the perceptron learning algorithm in a realistic biophysical model of a layer 5 cortical pyramidal cell with a full complement of non-linear dendritic channels. We tested this biophysical perceptron (BP) on a classification task, where it needed to correctly binarily classify 100, 1,000, or 2,000 patterns, and a generalization task, where it was required to discriminate between two \"noisy\" patterns. We show that the BP performs these tasks with an accuracy comparable to that of the original perceptron, though the classification capacity of the apical tuft is somewhat limited. We concluded that cortical pyramidal neurons can act as powerful classification devices."}], "ArticleTitle": "Perceptron Learning and Classification in a Modeled Cortical Pyramidal Cell."}, "33381018": {"mesh": [], "AbstractText": [{"section": null, "text": "A significant challenge in Glioblastoma (GBM) management is identifying pseudo-progression (PsP), a benign radiation-induced effect, from tumor recurrence, on routine imaging following conventional treatment. Previous studies have linked tumor lobar presence and laterality to GBM outcomes, suggesting that disease etiology and progression in GBM may be impacted by tumor location. Hence, in this feasibility study, we seek to investigate the following question: Can tumor location on treatment-na&#239;ve MRI provide early cues regarding likelihood of a patient developing pseudo-progression vs. tumor recurrence? In this study, 74 pre-treatment Glioblastoma MRI scans with PsP (33) and tumor recurrence (41) were analyzed. First, enhancing lesion on Gd-T1w MRI and peri-lesional hyperintensities on T2w/FLAIR were segmented by experts and then registered to a brain atlas. Using patients from the two phenotypes, we construct two atlases by quantifying frequency of occurrence of enhancing lesion and peri-lesion hyperintensities, by averaging voxel intensities across the population. Analysis of differential involvement was then performed to compute voxel-wise significant differences (p-value < 0.05) across the atlases. Statistically significant clusters were finally mapped to a structural atlas to provide anatomic localization of their location. Our results demonstrate that patients with tumor recurrence showed prominence of their initial tumor in the parietal lobe, while patients with PsP showed a multi-focal distribution of the initial tumor in the frontal and temporal lobes, insula, and putamen. These preliminary results suggest that lateralization of pre-treatment lesions toward certain anatomical areas of the brain may allow to provide early cues regarding assessing likelihood of occurrence of pseudo-progression from tumor recurrence on MRI scans."}], "ArticleTitle": "Can Tumor Location on Pre-treatment MRI Predict Likelihood of Pseudo-Progression vs. Tumor Recurrence in Glioblastoma?-A Feasibility Study."}, "31417385": {"mesh": [], "AbstractText": [{"section": null, "text": "Propagating waves with complex dynamics have been widely observed in neural population activity. To understand their formation mechanisms, we investigate a type of two-dimensional neural field model by systematically varying its recurrent excitatory and inhibitory inputs. We show that the neural field model exhibits a rich repertoire of dynamical activity states when the relevant strength of excitation and inhibition is increased, ranging from localized rotating and traveling waves to global waves. Particularly, near the transition between stable states of rotating and traveling waves, the model exhibits a bistable state; that is, both the rotating and the traveling waves can exist, and the inclusion of noise can induce spontaneous transitions between them. Furthermore, we demonstrate that when there are multiple propagating waves, they exhibit rich collective propagation dynamics with variable propagating speeds and trajectories. We use techniques from time series analysis such detrended fluctuation analysis to characterize the effect of the strength of excitation and inhibition on these collective dynamics, which range from purely random motion to motion with long-range spatiotemporal correlations. These results provide insights into the possible contribution of excitation and inhibition toward a range of previously observed spatiotemporal wave phenomena."}], "ArticleTitle": "Complex Dynamics of Propagating Waves in a Two-Dimensional Neural Field."}, "30618694": {"mesh": [], "AbstractText": [{"section": null, "text": "The insect olfactory system, which includes the antennal lobe (AL), mushroom body (MB), and ancillary structures, is a relatively simple neural system capable of learning. Its structural features, which are widespread in biological neural systems, process olfactory stimuli through a cascade of networks where large dimension shifts occur from stage to stage and where sparsity and randomness play a critical role in coding. Learning is partly enabled by a neuromodulatory reward mechanism of octopamine stimulation of the AL, whose increased activity induces synaptic weight updates in the MB through Hebbian plasticity. Enforced sparsity in the MB focuses Hebbian growth on neurons that are the most important for the representation of the learned odor. Based upon current biophysical knowledge, we have constructed an end-to-end computational firing-rate model of the Manduca sexta moth olfactory system which includes the interaction of the AL and MB under octopamine stimulation. Our model is able to robustly learn new odors, and neural firing rates in our simulations match the statistical features of in vivo firing rate data. From a biological perspective, the model provides a valuable tool for examining the role of neuromodulators, like octopamine, in learning, and gives insight into critical interactions between sparsity, Hebbian growth, and stimulation during learning. Our simulations also inform predictions about structural details of the olfactory system that are not currently well-characterized. From a machine learning perspective, the model yields bio-inspired mechanisms that are potentially useful in constructing neural nets for rapid learning from very few samples. These mechanisms include high-noise layers, sparse layers as noise filters, and a biologically-plausible optimization method to train the network based on octopamine stimulation, sparse layers, and Hebbian growth."}], "ArticleTitle": "Biological Mechanisms for Learning: A Computational Model of Olfactory Learning in the Manduca sexta Moth, With Applications to Neural Nets."}, "32372936": {"mesh": [], "AbstractText": [{"section": null, "text": "The majority of neurons in the neuronal system of the brain have a complex morphological structure, which diversifies the dynamics of neurons. In the granular layer of the cerebellum, there exists a unique cell type, the unipolar brush cell (UBC), that serves as an important relay cell for transferring information from outside mossy fibers to downstream granule cells. The distinguishing feature of the UBC is that it has a simple morphology, with only one short dendritic brush connected to its soma. Based on experimental evidence showing that UBCs exhibit a variety of dynamic behaviors, here we develop two simple models, one with a few detailed ion channels for simulation and the other one as a two-variable dynamical system for theoretical analysis, to characterize the intrinsic dynamics of UBCs. The reasonable values of the key channel parameters of the models can be determined by analysis of the stability of the resting membrane potential and the rebound firing properties of UBCs. Considered together with a large variety of synaptic dynamics installed on UBCs, we show that the simple-structured UBCs, as relay cells, can extend the range of dynamics and information from input mossy fibers to granule cells with low-frequency resonance and transfer stereotyped inputs to diverse amplitudes and phases of the output for downstream granule cells. These results suggest that neuronal computation, embedded within intrinsic ion channels and the diverse synaptic properties of single neurons without sophisticated morphology, can shape a large variety of dynamic behaviors to enhance the computational ability of local neuronal circuits."}], "ArticleTitle": "Intrinsic and Synaptic Properties Shaping Diverse Behaviors of Neural Dynamics."}, "30459584": {"mesh": [], "AbstractText": [{"section": null, "text": "One of the most controversial debates in cognitive neuroscience concerns the cortical locus of semantic knowledge and processing in the human brain. Experimental data revealed the existence of various cortical regions relevant for meaning processing, ranging from semantic hubs generally involved in semantic processing to modality-preferential sensorimotor areas involved in the processing of specific conceptual categories. Why and how the brain uses such complex organization for conceptualization can be investigated using biologically constrained neurocomputational models. Here, we improve pre-existing neurocomputational models of semantics by incorporating spiking neurons and a rich connectivity structure between the model 'areas' to mimic important features of the underlying neural substrate. Semantic learning and symbol grounding in action and perception were simulated by associative learning between co-activated neuron populations in frontal, temporal and occipital areas. As a result of Hebbian learning of the correlation structure of symbol, perception and action information, distributed cell assembly circuits emerged across various cortices of the network. These semantic circuits showed category-specific topographical distributions, reaching into motor and visual areas for action- and visually-related words, respectively. All types of semantic circuits included large numbers of neurons in multimodal connector hub areas, which is explained by cortical connectivity structure and the resultant convergence of phonological and semantic information on these zones. Importantly, these semantic hub areas exhibited some category-specificity, which was less pronounced than that observed in primary and secondary modality-preferential cortices. The present neurocomputational model integrates seemingly divergent experimental results about conceptualization and explains both semantic hubs and category-specific areas as an emergent process causally determined by two major factors: neuroanatomical connectivity structure and correlated neuronal activation during language learning."}], "ArticleTitle": "A Neurobiologically Constrained Cortex Model of Semantic Grounding With Spiking Neurons and Brain-Like Connectivity."}, "31507398": {"mesh": [], "AbstractText": [{"section": null, "text": "Purpose: Predicting patients' survival outcomes is recognized of key importance to clinicians in oncology toward determining an ideal course of treatment and patient management. This study applies radiomics analysis on pre-operative multi-parametric MRI of patients with glioblastoma from multiple institutions to identify a signature and a practical machine learning model for stratifying patients into groups based on overall survival. Methods: This study included 163 patients' data with glioblastoma, collected by BRATS 2018 Challenge from multiple institutions. In this proposed method, a set of 147 radiomics image features were extracted locally from three tumor sub-regions on standardized pre-operative multi-parametric MR images. LASSO regression was applied for identifying an informative subset of chosen features whereas a Cox model used to obtain the coefficients of those selected features. Then, a radiomics signature model of 9 features was constructed on the discovery set and it performance was evaluated for patients stratification into short- (<10 months), medium- (10-15 months), and long-survivors (>15 months) groups. Eight ML classification models, trained and then cross-validated, were tested to assess a range of survival prediction performance as a function of the choice of features. Results: The proposed mpMRI radiomics signature model had a statistically significant association with survival (P < 0.001) in the training set, but was not confirmed (P = 0.110) in the validation cohort. Its performance in the validation set had a sensitivity of 0.476 (short-), 0.231 (medium-), and 0.600 (long-survivors), and specificity of 0.667 (short-), 0.732 (medium-), and 0.794 (long-survivors). Among the tested ML classifiers, the ensemble learning model's results showed superior performance in predicting the survival classes, with an overall accuracy of 57.8% and AUC of 0.81 for short-, 0.47 for medium-, and 0.72 for long-survivors using the LASSO selected features combined with clinical factors. Conclusion: A derived GLCM feature, representing intra-tumoral inhomogeneity, was found to have a high association with survival. Clinical factors, when added to the radiomics image features, boosted the performance of the ML classification model in predicting individual glioblastoma patient's survival prognosis, which can improve prognostic quality a further step toward precision oncology."}], "ArticleTitle": "A Multi-parametric MRI-Based Radiomics Signature and a Practical ML Model for Stratifying Glioblastoma Patients Based on Survival Toward Precision Oncology."}, "30245621": {"mesh": [], "AbstractText": [{"section": null, "text": "Discrete sequential information coding is a key mechanism that transforms complex cognitive brain activity into a low-dimensional dynamical process based on the sequential switching among finite numbers of patterns. The storage size of the corresponding process is large because of the permutation capacity as a function of control signals in ensembles of these patterns. Extracting low-dimensional functional dynamics from multiple large-scale neural populations is a central problem both in neuro- and cognitive- sciences. Experimental results in the last decade represent a solid base for the creation of low-dimensional models of different cognitive functions and allow moving toward a dynamical theory of consciousness. We discuss here a methodology to build simple kinetic equations that can be the mathematical skeleton of this theory. Models of the corresponding discrete information processing can be designed using the following dynamical principles: (i) clusterization of the neural activity in space and time and formation of information patterns; (ii) robustness of the sequential dynamics based on heteroclinic chains of metastable clusters; and (iii) sensitivity of such sequential dynamics to intrinsic and external informational signals. We analyze sequential discrete coding based on winnerless competition low-frequency dynamics. Under such dynamics, entrainment, and heteroclinic coordination leads to a large variety of coding regimes that are invariant in time."}], "ArticleTitle": "Discrete Sequential Information Coding: Heteroclinic Cognitive Dynamics."}, "31031613": {"mesh": [], "AbstractText": [{"section": null, "text": "Deep neural networks (DNNs) have recently been applied successfully to brain decoding and image reconstruction from functional magnetic resonance imaging (fMRI) activity. However, direct training of a DNN with fMRI data is often avoided because the size of available data is thought to be insufficient for training a complex network with numerous parameters. Instead, a pre-trained DNN usually serves as a proxy for hierarchical visual representations, and fMRI data are used to decode individual DNN features of a stimulus image using a simple linear model, which are then passed to a reconstruction module. Here, we directly trained a DNN model with fMRI data and the corresponding stimulus images to build an end-to-end reconstruction model. We accomplished this by training a generative adversarial network with an additional loss term that was defined in high-level feature space (feature loss) using up to 6,000 training data samples (natural images and fMRI responses). The above model was tested on independent datasets and directly reconstructed image using an fMRI pattern as the input. Reconstructions obtained from our proposed method resembled the test stimuli (natural and artificial images) and reconstruction accuracy increased as a function of training-data size. Ablation analyses indicated that the feature loss that we employed played a critical role in achieving accurate reconstruction. Our results show that the end-to-end model can learn a direct mapping between brain activity and perception."}], "ArticleTitle": "End-to-End Deep Image Reconstruction From Human Brain Activity."}, "31736734": {"mesh": [], "AbstractText": [{"section": null, "text": "Resting state networks (RSNs) extracted from functional magnetic resonance imaging (fMRI) scans are believed to reflect the intrinsic organization and network structure of brain regions. Most traditional methods for computing RSNs typically assume these functional networks are static throughout the duration of a scan lasting 5-15 min. However, they are known to vary on timescales ranging from seconds to years; in addition, the dynamic properties of RSNs are affected in a wide variety of neurological disorders. Recently, there has been a proliferation of methods for characterizing RSN dynamics, yet it remains a challenge to extract reproducible time-resolved networks. In this paper, we develop a novel method based on dynamic mode decomposition (DMD) to extract networks from short windows of noisy, high-dimensional fMRI data, allowing RSNs from single scans to be resolved robustly at a temporal resolution of seconds. After validating the method on a synthetic dataset, we analyze data from 120 individuals from the Human Connectome Project and show that unsupervised clustering of DMD modes discovers RSNs at both the group (gDMD) and the single subject (sDMD) levels. The gDMD modes closely resemble canonical RSNs. Compared to established methods, sDMD modes capture individualized RSN structure that both better resembles the population RSN and better captures subject-level variation. We further leverage this time-resolved sDMD analysis to infer occupancy and transitions among RSNs with high reproducibility. This automated DMD-based method is a powerful tool to characterize spatial and temporal structures of RSNs in individual subjects."}], "ArticleTitle": "Extracting Reproducible Time-Resolved Resting State Networks Using Dynamic Mode Decomposition."}, "30297993": {"mesh": [], "AbstractText": [{"section": null, "text": "The dynamic nature of functional brain networks is being increasingly recognized in cognitive neuroscience, and methods to analyse such time-varying networks in EEG/MEG data are required. In this work, we propose a pipeline to characterize time-varying networks in single-subject EEG task-related data and further, evaluate its validity on both simulated and experimental datasets. Pre-processing is done to remove channel-wise and trial-wise differences in activity. Functional networks are estimated from short non-overlapping time windows within each \"trial,\" using a sparse-MVAR (Multi-Variate Auto-Regressive) model. Functional \"states\" are then identified by partitioning the entire space of functional networks into a small number of groups/symbols via k-means clustering.The multi-trial sequence of symbols is then described by a Markov Model (MM). We show validity of this pipeline on realistic electrode-level simulated EEG data, by demonstrating its ability to discriminate \"trials\" from two experimental conditions in a range of scenarios. We then apply it to experimental data from two individuals using a Brain-Computer Interface (BCI) via a P300 oddball task. Using just the Markov Model parameters, we obtain statistically significant discrimination between target and non-target trials. The functional networks characterizing each 'state' were also highly similar between the two individuals. This work marks the first application of the Markov Model framework to infer time-varying networks from EEG/MEG data. Due to the pre-processing, results from the pipeline are orthogonal to those from conventional ERP averaging or a typical EEG microstate analysis. The results provide powerful proof-of-concept for a Markov model-based approach to analyzing the data, paving the way for its use to track rapid changes in interaction patterns as a task is being performed. MATLAB code for the entire pipeline has been made available."}], "ArticleTitle": "Markov Model-Based Method to Analyse Time-Varying Networks in EEG Task-Related Data."}, "30618691": {"mesh": [], "AbstractText": [{"section": null, "text": "Recent neurophysiological and computational studies have proposed the hypothesis that our brain automatically codes the nth-order transitional probabilities (TPs) embedded in sequential phenomena such as music and language (i.e., local statistics in nth-order level), grasps the entropy of the TP distribution (i.e., global statistics), and predicts the future state based on the internalized nth-order statistical model. This mechanism is called statistical learning (SL). SL is also believed to contribute to the creativity involved in musical improvisation. The present study examines the interactions among local statistics, global statistics, and different levels of orders (mutual information) in musical improvisation interact. Interactions among local statistics, global statistics, and hierarchy were detected in higher-order SL models of pitches, but not lower-order SL models of pitches or SL models of rhythms. These results suggest that the information-theoretical phenomena of local and global statistics in each order may be reflected in improvisational music. The present study proposes novel methodology to evaluate musical creativity associated with SL based on information theory."}], "ArticleTitle": "Entropy, Uncertainty, and the Depth of Implicit Knowledge on Musical Creativity: Computational Study of Improvisation in Melody and Rhythm."}, "29643772": {"mesh": [], "AbstractText": [{"section": null, "text": "Periodic visual stimulation can evoke the steady-state visual potential (SSVEP) in the brain. Owing to its superior characteristics, the SSVEP has been widely used in neural engineering and cognitive neuroscience studies. However, the underlying mechanisms of the SSVEP are not well understood. In this study, we introduced a brain reconfiguration methodology to explore the possible mechanisms of the SSVEP. The EEG data from five periodic stimuli consistently indicated that the periodic visual stimulation could induce resting-state brain network reconfiguration and that the responses evoked by the stimuli were correlated to the network reconfiguration indexes. For each stimulus frequency, larger response amplitudes corresponded to higher reconfiguration indexes from the resting-state network to a stimulus-evoked network. These findings demonstrate that an external periodic visual stimulation can induce the modification of intrinsic oscillatory activities by reconfiguring resting-state activity at a network level, which could facilitate the responses evoked by the stimulus. These findings provide new insights into the response mechanisms of periodic visual stimulation."}], "ArticleTitle": "Periodic Visual Stimulation Induces Resting-State Brain Network Reconfiguration."}, "31105545": {"mesh": [], "AbstractText": [{"section": null, "text": "Epilepsy surgery is a clinical procedure that aims to remove the brain tissue responsible for the emergence of seizures, the epileptogenic zone (EZ). It is preceded by an evaluation to determine the brain tissue that must be resected. The identification of the seizure onset zone (SOZ) from intracranial EEG recordings stands as one of the key proxies for the EZ. In this study we used computational models of epilepsy to assess to what extent the SOZ may or may not represent the EZ. We considered a set of different synthetic networks (e.g., regular, small-world, random, and scale-free networks) to represent large-scale brain networks and a phenomenological network model of seizure generation. In the model, the SOZ was inferred from the seizure likelihood (SL), a measure of the propensity of single nodes to produce epileptiform dynamics, whilst a surgery corresponded to the removal of nodes and connections from the network. We used the concept of node ictogenicity (NI) to quantify the effectiveness of each node removal on reducing the network's propensity to generate seizures. This framework enabled us to systematically compare the SOZ and the seizure control achieved by each considered surgery. Specifically, we compared the distributions of SL and NI across different networks. We found that SL and NI were concordant when all nodes were similarly ictogenic, whereas when there was a small fraction of nodes with high NI, the SL was not specific at identifying these nodes. We further considered networks with heterogeneous node excitabilities, i.e., nodes with different susceptibilities of being engaged in seizure activity, to understand how such heterogeneity may affect the relationship between SL and NI. We found that while SL and NI are concordant when there is a small fraction of hyper-excitable nodes in a network that is otherwise homogeneous, they do diverge if the network is heterogeneous, such as in scale-free networks. We observe that SL is highly dependent on node excitabilities, whilst the effect of surgical resections as revealed by NI is mostly determined by network structure. Together our results suggest that the SOZ is not always a good marker of the EZ."}], "ArticleTitle": "A Model-Based Assessment of the Seizure Onset Zone Predictive Power to Inform the Epileptogenic Zone."}, "31616271": {"mesh": [], "AbstractText": [{"section": null, "text": "Humans walk, run, and change their speed in accordance with circumstances. These gaits are rhythmic motions generated by multi-articulated movements, which have specific spatiotemporal patterns. The kinematic characteristics depend on the gait and speed. In this study, we focused on the kinematic coordination of locomotor behavior to clarify the underlying mechanism for the effect of speed on the spatiotemporal kinematic patterns for each gait. In particular, we used seven elevation angles for the whole-body motion and separated the measured data into different phases depending on the foot-contact condition, that is, single-stance phase, double-stance phase, and flight phase, which have different physical constraints during locomotion. We extracted the spatiotemporal kinematic coordination patterns with singular value decomposition and investigated the effect of speed on the coordination patterns. Our results showed that most of the whole-body motion could be explained by only two sets of temporal and spatial coordination patterns in each phase. Furthermore, the temporal coordination patterns were invariant for different speeds, while the spatial coordination patterns varied. These findings will improve our understanding of human adaptation mechanisms to tune locomotor behavior for changing speed."}], "ArticleTitle": "Variant and Invariant Spatiotemporal Structures in Kinematic Coordination to Regulate Speed During Walking and Running."}, "30670958": {"mesh": [], "AbstractText": [{"section": null, "text": "Since brain structural connectivity is the foundation of its functionality, in order to understand brain abilities, studying the relation between structural and functional connectivity is essential. Several approaches have been applied to measure the role of the structural connectivity in the emergent correlation/synchronization patterns. In this study, we investigates the cross-correlation and synchronization sensitivity to coupling strength between neural regions for different topological networks. We model the neural populations by a neural mass model that express an oscillatory dynamic. The results highlight that coupling between neural ensembles leads to various cross-correlation patterns and local synchrony even on an ordered network. Moreover, as the network departs from an ordered organization to a small-world architecture, correlation patterns, and synchronization dynamics change. Interestingly, at a certain range of the synaptic strength, by fixing the structural conditions, different organized patterns are seen at the different input signals. This variety switches to a bifurcation region by increasing the synaptic strength. We show that topological variations is a major factor of synchronization behavior and lead to alterations in correlated local clusters. We found the coupling strength (between cortical areas) to be especially important at conversions of correlation and synchronization states. Since correlation patterns generate functional connections and transitions of functional connectivity have been related to cognitive operations, these diverse correlation patterns may be considered as different dynamical states corresponding to various cognitive tasks."}], "ArticleTitle": "On the Influence of Structural Connectivity on the Correlation Patterns and Network Synchronization."}, "32116618": {"mesh": [], "AbstractText": [{"section": null, "text": "Cerebrospinal fluid (CSF) and brain tissue sodium levels increase during migraine. However, little is known regarding the underlying mechanisms of sodium homeostasis disturbance in the brain during the onset and propagation of migraine. Exploring the cause of sodium dysregulation in the brain is important, since correction of the altered sodium homeostasis could potentially treat migraine. Under the hypothesis that disturbances in sodium transport mechanisms at the blood-CSF barrier (BCSFB) and/or the blood-brain barrier (BBB) are the underlying cause of the elevated CSF and brain tissue sodium levels during migraines, we developed a mechanistic, differential equation model of a rat's brain to compare the significance of the BCSFB and the BBB in controlling CSF and brain tissue sodium levels. The model includes the ventricular system, subarachnoid space, brain tissue and blood. Sodium transport from blood to CSF across the BCSFB, and from blood to brain tissue across the BBB were modeled by influx permeability coefficients P BCSFB and P BBB , respectively, while sodium movement from CSF into blood across the BCSFB, and from brain tissue to blood across the BBB were modeled by efflux permeability coefficients  P B C S F B '  and  P B B B '  , respectively. We then performed a global sensitivity analysis to investigate the sensitivity of the ventricular CSF, subarachnoid CSF and brain tissue sodium concentrations to pathophysiological variations in P BCSFB , P BBB ,  P B C S F B '  and  P B B B '  . Our results show that the ventricular CSF sodium concentration is highly influenced by perturbations of P BCSFB , and to a much lesser extent by perturbations of  P B C S F B '  . Brain tissue and subarachnoid CSF sodium concentrations are more sensitive to pathophysiological variations of P BBB and  P B B B '  than variations of P BCSFB and  P B C S F B '  within 30 min of the onset of the perturbations. However, P BCSFB is the most sensitive model parameter, followed by P BBB and  P B B B '  , in controlling brain tissue and subarachnoid CSF sodium levels within 3 h of the perturbation onset."}], "ArticleTitle": "Regulation of CSF and Brain Tissue Sodium Levels by the Blood-CSF and Blood-Brain Barriers During Migraine."}, "28066221": {"mesh": [], "AbstractText": [{"section": null, "text": "Spiking Neural Networks constitute the most promising approach to develop realistic Artificial Neural Networks (ANNs). Unlike traditional firing rate-based paradigms, information coding in spiking models is based on the precise timing of individual spikes. It has been demonstrated that spiking ANNs can be successfully and efficiently applied to multiple realistic problems solvable with traditional strategies (e.g., data classification or pattern recognition). In recent years, major breakthroughs in neuroscience research have discovered new relevant computational principles in different living neural systems. Could ANNs benefit from some of these recent findings providing novel elements of inspiration? This is an intriguing question for the research community and the development of spiking ANNs including novel bio-inspired information coding and processing strategies is gaining attention. From this perspective, in this work, we adapt the core concepts of the recently proposed Signature Neural Network paradigm-i.e., neural signatures to identify each unit in the network, local information contextualization during the processing, and multicoding strategies for information propagation regarding the origin and the content of the data-to be employed in a spiking neural network. To the best of our knowledge, none of these mechanisms have been used yet in the context of ANNs of spiking neurons. This paper provides a proof-of-concept for their applicability in such networks. Computer simulations show that a simple network model like the discussed here exhibits complex self-organizing properties. The combination of multiple simultaneous encoding schemes allows the network to generate coexisting spatio-temporal patterns of activity encoding information in different spatio-temporal spaces. As a function of the network and/or intra-unit parameters shaping the corresponding encoding modality, different forms of competition among the evoked patterns can emerge even in the absence of inhibitory connections. These parameters also modulate the memory capabilities of the network. The dynamical modes observed in the different informational dimensions in a given moment are independent and they only depend on the parameters shaping the information processing in this dimension. In view of these results, we argue that plasticity mechanisms inside individual cells and multicoding strategies can provide additional computational properties to spiking neural networks, which could enhance their capacity and performance in a wide variety of real-world tasks."}], "ArticleTitle": "Implementing Signature Neural Networks with Spiking Neurons."}, "29615886": {"mesh": [], "AbstractText": [{"section": null, "text": "The Independent Channel (IC) model is a commonly used linear balance control model in the frequency domain to analyze human balance control using system identification and parameter estimation. The IC model is a rudimentary and noise-free description of balance behavior in the frequency domain, where a stable model representation is not guaranteed. In this study, we conducted firstly time-domain simulations with added noise, and secondly robot experiments by implementing the IC model in a real-world robot (PostuRob II) to test the validity and stability of the model in the time domain and for real world situations. Balance behavior of seven healthy participants was measured during upright stance by applying pseudorandom continuous support surface rotations. System identification and parameter estimation were used to describe the balance behavior with the IC model in the frequency domain. The IC model with the estimated parameters from human experiments was implemented in Simulink for computer simulations including noise in the time domain and robot experiments using the humanoid robot PostuRob II. Again, system identification and parameter estimation were used to describe the simulated balance behavior. Time series, Frequency Response Functions, and estimated parameters from human experiments, computer simulations, and robot experiments were compared with each other. The computer simulations showed similar balance behavior and estimated control parameters compared to the human experiments, in the time and frequency domain. Also, the IC model was able to control the humanoid robot by keeping it upright, but showed small differences compared to the human experiments in the time and frequency domain, especially at high frequencies. We conclude that the IC model, a descriptive model in the frequency domain, can imitate human balance behavior also in the time domain, both in computer simulations with added noise and real world situations with a humanoid robot. This provides further evidence that the IC model is a valid description of human balance control."}], "ArticleTitle": "Evidence in Support of the Independent Channel Model Describing the Sensorimotor Control of Human Stance Using a Humanoid Robot."}, "29928197": {"mesh": [], "AbstractText": [{"section": null, "text": "Background: Parkinson's disease affects many motor processes including speech. Besides drug treatment, deep brain stimulation (DBS) in the subthalamic nucleus (STN) and globus pallidus internus (GPi) has developed as an effective therapy. Goal: We present a neural model that simulates a syllable repetition task and evaluate its performance when varying the level of dopamine in the striatum, and the level of activity reduction in the STN or GPi. Method: The Neural Engineering Framework (NEF) is used to build a model of syllable sequencing through a cortico-basal ganglia-thalamus-cortex circuit. The model is able to simulate a failing substantia nigra pars compacta (SNc), as occurs in Parkinson's patients. We simulate syllable sequencing parameterized by (i) the tonic dopamine level in the striatum and (ii) average neural activity in STN or GPi. Results: With decreased dopamine levels, the model produces syllable sequencing errors in the form of skipping and swapping syllables, repeating the same syllable, breaking and restarting in the middle of a sequence, and cessation (\"freezing\") of sequences. We also find that reducing (inhibiting) activity in either STN or GPi reduces the occurrence of syllable sequencing errors. Conclusion: The model predicts that inhibiting activity in STN or GPi can reduce syllable sequencing errors in Parkinson's patients. Since DBS also reduces syllable sequencing errors in Parkinson's patients, we therefore suggest that STN or GPi inhibition is one mechanism through which DBS reduces syllable sequencing errors in Parkinson's patients."}], "ArticleTitle": "Inhibiting Basal Ganglia Regions Reduces Syllable Sequencing Errors in Parkinson's Disease: A Computer Simulation Study."}, "31244635": {"mesh": [], "AbstractText": [{"section": null, "text": "The neurons of the olivocerebellar circuit exhibit complex electroresponsive dynamics, which are thought to play a fundamental role for network entraining, plasticity induction, signal processing, and noise filtering. In order to reproduce these properties in single-point neuron models, we have optimized the Extended-Generalized Leaky Integrate and Fire (E-GLIF) neuron through a multi-objective gradient-based algorithm targeting the desired input-output relationships. In this way, E-GLIF was tuned toward the unique input-output properties of Golgi cells, granule cells, Purkinje cells, molecular layer interneurons, deep cerebellar nuclei cells, and inferior olivary cells. E-GLIF proved able to simulate the complex cell-specific electroresponsive dynamics of the main olivocerebellar neurons including pacemaking, adaptation, bursting, post-inhibitory rebound excitation, subthreshold oscillations, resonance, and phase reset. The integration of these E-GLIF point-neuron models into olivocerebellar Spiking Neural Networks will allow to evaluate the impact of complex electroresponsive dynamics at the higher scales, up to motor behavior, in closed-loop simulations of sensorimotor tasks."}], "ArticleTitle": "Complex Electroresponsive Dynamics in Olivocerebellar Neurons Represented With Extended-Generalized Leaky Integrate and Fire Models."}, "30983984": {"mesh": [], "AbstractText": [{"section": null, "text": "We propose an approach for the detection of language expectation violations that occur in communication. We examined semantic and syntactic violations from electroencephalogram (EEG) when participants listened to spoken sentences. Previous studies have shown that such event-related potential (ERP) components as N400 and the late positivity (P600) are evoked in the auditory where semantic and syntactic anomalies occur. We used this knowledge to detect language expectation violation from single-trial EEGs by machine learning techniques. We recorded the brain activity of 18 participants while they listened to sentences that contained semantic and syntactic anomalies and identified the significant main effects of these anomalies in the ERP components. We also found that a multilayer perceptron achieved 59.5% (semantic) and 57.7% (syntactic) accuracies."}], "ArticleTitle": "Electroencephalogram-Based Single-Trial Detection of Language Expectation Violations in Listening to Speech."}, "30254579": {"mesh": [], "AbstractText": [{"section": null, "text": "We present Feasibility Theory, a conceptual and computational framework to unify today's theories of neuromuscular control. We begin by describing how the musculoskeletal anatomy of the limb, the need to control individual tendons, and the physics of a motor task uniquely specify the family of all valid muscle activations that accomplish it (its 'feasible activation space'). For our example of producing static force with a finger driven by seven muscles, computational geometry characterizes-in a complete way-the structure of feasible activation spaces as 3-dimensional polytopes embedded in 7-D. The feasible activation space for a given task is the landscape where all neuromuscular learning, control, and performance must occur. This approach unifies current theories of neuromuscular control because the structure of feasible activation spaces can be separately approximated as either low-dimensional basis functions (synergies), high-dimensional joint probability distributions (Bayesian priors), or fitness landscapes (to optimize cost functions)."}], "ArticleTitle": "Feasibility Theory Reconciles and Informs Alternative Approaches to Neuromuscular Control."}, "30374297": {"mesh": [], "AbstractText": [{"section": null, "text": "Measuring the coupling of single neuron's spiking activities to the local field potentials (LFPs) is a method to investigate neuronal synchronization. The most important synchronization measures are phase locking value (PLV), spike field coherence (SFC), pairwise phase consistency (PPC), and spike-triggered correlation matrix synchronization (SCMS). Synchronization is generally quantified using the PLV and SFC. PLV and SFC methods are either biased on the spike rates or the number of trials. To resolve these problems the PPC measure has been introduced. However, there are some shortcomings associated with the PPC measure which is unbiased only for very high spike rates. However evaluating spike-LFP phase coupling (SPC) for short trials or low number of spikes is a challenge in many studies. Lastly, SCMS measures the correlation in terms of phase in regions around the spikes inclusive of the non-spiking events which is the major difference between SCMS and SPC. This study proposes a new framework for predicting a more reliable SPC by modeling and introducing appropriate machine learning algorithms namely least squares, Lasso, and neural networks algorithms where through an initial trend of the spike rates, the ideal SPC is predicted for neurons with low spike rates. Furthermore, comparing the performance of these three algorithms shows that the least squares approach provided the best performance with a correlation of 0.99214 and R 2 of 0.9563 in the training phase, and correlation of 0.95969 and R 2 of 0.8842 in the test phase. Hence, the results show that the proposed framework significantly enhances the accuracy and provides a bias-free basis for small number of spikes for SPC as compared to the conventional methods such as PLV method. As such, it has the general ability to correct for the bias on the number of spike rates."}], "ArticleTitle": "Introducing a Comprehensive Framework to Measure Spike-LFP Coupling."}, "29445337": {"mesh": [], "AbstractText": [{"section": null, "text": "Optogenetically evoked local field potential (LFP) recorded from the medial prefrontal cortex (mPFC) of mice during basal conditions and following a systemic cocaine administration were analyzed. Blue light stimuli were delivered to mPFC through a fiber optic every 2 s and each trial was repeated 100 times. As in the previous study, we used a surrogate data method to check that nonlinearity was present in the experimental LFPs and only used the last 1.5 s of steady activity to measure the LFPs phase resetting induced by the brief 10 ms light stimulus. We found that the steady dynamics of the mPFC in response to light stimuli could be reconstructed in a three-dimensional phase space with topologically similar \"8\"-shaped attractors across different animals. Therefore, cocaine did not change the complexity of the recorded nonlinear data compared to the control case. The phase space of the reconstructed attractor is determined by the LFP time series and its temporally shifted versions by a multiple of some lag time. We also compared the change in the attractor shape between cocaine-injected and control using (1) dendrogram clustering and (2) Frechet distance. We found about 20% overlap between control and cocaine trials when classified using dendrogram method, which suggest that it may be possible to describe mathematically both data sets with the same model and slightly different model parameters. We also found that the lag times are about three times shorter for cocaine trials compared to control. As a result, although the phase space trajectories for control and cocaine may look similar, their dynamics is significantly different."}], "ArticleTitle": "Cocaine-Induced Changes in Low-Dimensional Attractors of Local Field Potentials in Optogenetic Mice."}, "28912706": {"mesh": [], "AbstractText": [{"section": null, "text": "Dynamic neural fields (DNFs) are dynamical systems models that approximate the activity of large, homogeneous, and recurrently connected neural networks based on a mean field approach. Within dynamic field theory, the DNFs have been used as building blocks in architectures to model sensorimotor embedding of cognitive processes. Typically, the parameters of a DNF in an architecture are manually tuned in order to achieve a specific dynamic behavior (e.g., decision making, selection, or working memory) for a given input pattern. This manual parameters search requires expert knowledge and time to find and verify a suited set of parameters. The DNF parametrization may be particular challenging if the input distribution is not known in advance, e.g., when processing sensory information. In this paper, we propose the autonomous adaptation of the DNF resting level and gain by a learning mechanism of intrinsic plasticity (IP). To enable this adaptation, an input and output measure for the DNF are introduced, together with a hyper parameter to define the desired output distribution. The online adaptation by IP gives the possibility to pre-define the DNF output statistics without knowledge of the input distribution and thus, also to compensate for changes in it. The capabilities and limitations of this approach are evaluated in a number of experiments."}], "ArticleTitle": "Dynamic Neural Fields with Intrinsic Plasticity."}, "28596729": {"mesh": [], "AbstractText": [{"section": null, "text": "Repeated, precise sequences of spikes are largely considered a signature of activation of cell assemblies. These repeated sequences are commonly known under the name of spatio-temporal patterns (STPs). STPs are hypothesized to play a role in the communication of information in the computational process operated by the cerebral cortex. A variety of statistical methods for the detection of STPs have been developed and applied to electrophysiological recordings, but such methods scale poorly with the current size of available parallel spike train recordings (more than 100 neurons). In this work, we introduce a novel method capable of overcoming the computational and statistical limits of existing analysis techniques in detecting repeating STPs within massively parallel spike trains (MPST). We employ advanced data mining techniques to efficiently extract repeating sequences of spikes from the data. Then, we introduce and compare two alternative approaches to distinguish statistically significant patterns from chance sequences. The first approach uses a measure known as conceptual stability, of which we investigate a computationally cheap approximation for applications to such large data sets. The second approach is based on the evaluation of pattern statistical significance. In particular, we provide an extension to STPs of a method we recently introduced for the evaluation of statistical significance of synchronous spike patterns. The performance of the two approaches is evaluated in terms of computational load and statistical power on a variety of artificial data sets that replicate specific features of experimental data. Both methods provide an effective and robust procedure for detection of STPs in MPST data. The method based on significance evaluation shows the best overall performance, although at a higher computational cost. We name the novel procedure the spatio-temporal Spike PAttern Detection and Evaluation (SPADE) analysis."}], "ArticleTitle": "Detection and Evaluation of Spatio-Temporal Spike Patterns in Massively Parallel Spike Train Data with SPADE."}, "32351376": {"mesh": [], "AbstractText": [{"section": null, "text": "[This corrects the article DOI: 10.3389/fncom.2020.00007.]."}], "ArticleTitle": "Corrigendum: A Curiosity-Based Learning Method for Spiking Neural Networks."}, "33328947": {"mesh": [], "AbstractText": [{"section": null, "text": "The topographic organization of afferents to the hippocampal CA3 subfield are well-studied, but their role in influencing the spatiotemporal dynamics of population activity is not understood. Using a large-scale, computational neuronal network model of the entorhinal-dentate-CA3 system, the effects of the perforant path, mossy fibers, and associational system on the propagation and transformation of network spiking patterns were investigated. A correlation map was constructed to characterize the spatial structure and temporal evolution of pairwise correlations which underlie the emergent patterns found in the population activity. The topographic organization of the associational system gave rise to changes in the spatial correlation structure along the longitudinal and transverse axes of the CA3. The resulting gradients may provide a basis for the known functional organization observed in hippocampus."}], "ArticleTitle": "Topographic Organization of Correlation Along the Longitudinal and Transverse Axes in Rat Hippocampal CA3 Due to Excitatory Afferents."}, "29556186": {"mesh": [], "AbstractText": [{"section": null, "text": "Noise correlations in neuronal responses can have a strong influence on the information available in large populations. In addition, the structure of noise correlations may have a great impact on the utility of different algorithms to extract this information that may depend on the specific algorithm, and hence may affect our understanding of population codes in the brain. Thus, a better understanding of the structure of noise correlations and their interplay with different readout algorithms is required. Here we use eigendecomposition to investigate the structure of noise correlations in populations of about 50-100 simultaneously recorded neurons in the primary visual cortex of anesthetized monkeys, and we relate this structure to the performance of two common decoders: the population vector and the optimal linear estimator. Our analysis reveals a non-trivial correlation structure, in which the eigenvalue spectrum is composed of several distinct large eigenvalues that represent different shared modes of fluctuation extending over most of the population, and a semi-continuous tail. The largest eigenvalue represents a uniform collective mode of fluctuation. The second and third eigenvalues typically show either a clear functional (i.e., dependent on the preferred orientation of the neurons) or spatial structure (i.e., dependent on the physical position of the neurons). We find that the number of shared modes increases with the population size, being roughly 10% of that size. Furthermore, we find that the noise in each of these collective modes grows linearly with the population. This linear growth of correlated noise power can have limiting effects on the utility of averaging neuronal responses across large populations, depending on the readout. Specifically, the collective modes of fluctuation limit the accuracy of the population vector but not of the optimal linear estimator."}], "ArticleTitle": "Relating the Structure of Noise Correlations in Macaque Primary Visual Cortex to Decoder Performance."}, "28878643": {"mesh": [], "AbstractText": [{"section": null, "text": "The classical model of basal ganglia has been refined in recent years with discoveries of subpopulations within a nucleus and previously unknown projections. One such discovery is the presence of subpopulations of arkypallidal and prototypical neurons in external globus pallidus, which was previously considered to be a primarily homogeneous nucleus. Developing a computational model of these multiple interconnected nuclei is challenging, because the strengths of the connections are largely unknown. We therefore use a genetic algorithm to search for the unknown connectivity parameters in a firing rate model. We apply a binary cost function derived from empirical firing rate and phase relationship data for the physiological and Parkinsonian conditions. Our approach generates ensembles of over 1,000 configurations, or homologies, for each condition, with broad distributions for many of the parameter values and overlap between the two conditions. However, the resulting effective weights of connections from or to prototypical and arkypallidal neurons are consistent with the experimental data. We investigate the significance of the weight variability by manipulating the parameters individually and cumulatively, and conclude that the correlation observed between the parameters is necessary for generating the dynamics of the two conditions. We then investigate the response of the networks to a transient cortical stimulus, and demonstrate that networks classified as physiological effectively suppress activity in the internal globus pallidus, and are not susceptible to oscillations, whereas parkinsonian networks show the opposite tendency. Thus, we conclude that the rates and phase relationships observed in the globus pallidus are predictive of experimentally observed higher level dynamical features of the physiological and parkinsonian basal ganglia, and that the multiplicity of solutions generated by our method may well be indicative of a natural diversity in basal ganglia networks. We propose that our approach of generating and analyzing an ensemble of multiple solutions to an underdetermined network model provides greater confidence in its predictions than those derived from a unique solution, and that projecting such homologous networks on a lower dimensional space of sensibly chosen dynamical features gives a better chance than a purely structural analysis at understanding complex pathologies such as Parkinson's disease."}], "ArticleTitle": "Homologous Basal Ganglia Network Models in Physiological and Parkinsonian Conditions."}, "29541024": {"mesh": [], "AbstractText": [{"section": null, "text": "In mental time travel (MTT) one is \"traveling\" back-and-forth in time, remembering, and imagining events. Despite intensive research regarding memory processes in the hippocampus, it was only recently shown that the hippocampus plays an essential role in encoding the temporal order of events remembered, and therefore plays an important role in MTT. Does it also encode the temporal relations of these events to the remembering self? We asked patients undergoing pre-surgical evaluation with depth electrodes penetrating the temporal lobes bilaterally toward the hippocampus to project themselves in time to a past, future, or present time-point, and then make judgments regarding various events. Classification analysis of intracranial evoked potentials revealed clear temporal dissociation in the left hemisphere between lateral-temporal electrodes, activated at ~100-300 ms, and hippocampal electrodes, activated at ~400-600 ms. This dissociation may suggest a division of labor in the temporal lobe during self-projection in time, hinting toward the different roles of the lateral-temporal cortex and the hippocampus in MTT and the temporal organization of the related events with respect to the experiencing self."}], "ArticleTitle": "Temporal Dissociation of Neocortical and Hippocampal Contributions to Mental Time Travel Using Intracranial Recordings in Humans."}, "32848686": {"mesh": [], "AbstractText": [{"section": null, "text": "Objectives: The specific intrinsic network coupling abnormalities in mild traumatic brain injury (mTBI) patients are poorly understood. Our objective is to compare the correlations among the default mode, salience, and central executive networks in patients with mTBI and healthy controls. Methods: This 2-year prospective study included 32 acute mTBI patients and 37 healthy comparisons. We calculated the functional connectivity scores among the default mode, salience, and central executive networks. Then we conducted multilevel correlation analysis to investigate component correlations, global graph, and local functional connectivity changes. Results: Patients with mTBI showed significant increased functional connectivity between the anterior part of the default mode network and the salience network compared with controls (p = 0.013, false discovery rate correction). Hyper-connectivity between the default mode and salience network was significantly positively correlated with the dimensional change card sort score in patients with mTBI (r = 0.40, p = 0.037). The average path length of mTBI patients was significantly higher than that of controls (p = 0.028). Conclusions: Aberrant functional coupling between the default mode and salience networks were identified in acute mTBI patients. Our finding has great potential to improve our understanding of the network architecture of mTBI."}], "ArticleTitle": "Aberrant Correlation Between the Default Mode and Salience Networks in Mild Traumatic Brain Injury."}, "30618692": {"mesh": [], "AbstractText": [{"section": null, "text": "A neural field model of the corticothalamic-basal ganglia system is developed that describes enhanced beta activity within subthalamic and pallidal circuits in Parkinson's disease (PD) via system resonances. A model of deep brain stimulation (DBS) of typical clinical targets, the subthalamic nucleus (STN) and globus pallidus internus (GPi), is added and studied for several distinct stimulation protocols that are used for treatment of the motor symptoms of PD and that reduce pathological beta band activity (13-30 Hz) in the corticothalamic-basal ganglia network. The resulting impact of DBS on enhanced beta activity in the STN and GPi, as well as cortico-subthalamic and cortico-pallidal coherence, are studied. Both STN-DBS and GPi-DBS are found to be effective for suppressing peak STN and GPi power in the beta band, with GPi-DBS being slightly more effective in both the STN and the GPi for all stimulus protocols tested. The largest decrease in cortico-STN coherence is observed during STN-DBS, whereas GPi-DBS is most effective for reducing cortico-GPi coherence. A reduction of the pathologically large STN connection strengths that define the parkinsonian state results in enhanced 6 Hz activity and could thus represent a compensatory mechanism that has the side effect of driving parkinsonian tremor-like oscillations. This model provides a method for systematically testing effective DBS protocols that agrees with experimental and clinical findings. Furthermore, the model suggests GPi-DBS and STN-DBS have distinct impacts on elevated synchronization between the basal ganglia and motor cortex in PD."}], "ArticleTitle": "Suppression of Parkinsonian Beta Oscillations by Deep Brain Stimulation: Determination of Effective Protocols."}, "28381996": {"mesh": [], "AbstractText": [{"section": null, "text": "Neuromechanical simulations have been used to study the spinal control of human locomotion which involves complex mechanical dynamics. So far, most neuromechanical simulation studies have focused on demonstrating the capability of a proposed control model in generating normal walking. As many of these models with competing control hypotheses can generate human-like normal walking behaviors, a more in-depth evaluation is required. Here, we conduct the more in-depth evaluation on a spinal-reflex-based control model using five representative gait disturbances, ranging from electrical stimulation to mechanical perturbation at individual leg joints and at the whole body. The immediate changes in muscle activations of the model are compared to those of humans across different gait phases and disturbance magnitudes. Remarkably similar response trends for the majority of investigated muscles and experimental conditions reinforce the plausibility of the reflex circuits of the model. However, the model's responses lack in amplitude for two experiments with whole body disturbances suggesting that in these cases the proposed reflex circuits need to be amplified by additional control structures such as location-specific cutaneous reflexes. A model that captures these selective amplifications would be able to explain both steady and reactive spinal control of human locomotion. Neuromechanical simulations that investigate hypothesized control models are complementary to gait experiments in better understanding the control of human locomotion."}], "ArticleTitle": "Evaluation of a Neuromechanical Walking Control Model Using Disturbance Experiments."}, "28588471": {"mesh": [], "AbstractText": [{"section": null, "text": "The extensive cerebral cortex and subcortical structures are considered as the major regions related to the generalized epileptiform discharges in idiopathic generalized epilepsy. However, various clinical syndromes and electroencephalogram (EEG) signs exist across generalized seizures, such as the loss of consciousness during absence seizures (AS) and the jerk of limbs during myoclonic seizures (MS). It is presumed that various functional systems affected by discharges lead to the difference in syndromes of these seizures. Twenty epileptic patients with MS, 21 patients with AS, and 21 healthy controls were recruited in this study. The functional network connectivity was analyzed based on the resting-state functional magnetic resonance imaging scans. The statistical analysis was performed in three groups to assess the difference in the functional brain networks in two types of generalized seizures. Twelve resting-state networks were identified in three groups. Both patient groups showed common abnormalities, including decreased functional connectivity in salience network (SN), cerebellum network, and primary perceptional networks and decreased connection between SN and visual network, compared with healthy controls. Interestingly, the frontal part of high-level cognitive resting-state networks showed increased functional connectivity (FC) in patients with MS, but decreased FC in patients with AS. Moreover, patients with MS showed decreased negative connections between high-level cognitive networks and primary system. The common alteration in both patient groups, including SN, might reflect a similar mechanism associated with the loss of consciousness during generalized seizures. This study provided the evidence of brain network in generalized epilepsy to understand the difference between MS and AS."}], "ArticleTitle": "Functional Network Connectivity Patterns between Idiopathic Generalized Epilepsy with Myoclonic and Absence Seizures."}, "28690509": {"mesh": [], "AbstractText": [{"section": null, "text": "Biological and artificial neural networks (ANNs) represent input signals as patterns of neural activity. In biology, neuromodulators can trigger important reorganizations of these neural representations. For instance, pairing a stimulus with the release of either acetylcholine (ACh) or dopamine (DA) evokes long lasting increases in the responses of neurons to the paired stimulus. The functional roles of ACh and DA in rearranging representations remain largely unknown. Here, we address this question using a Hebbian-learning neural network model. Our aim is both to gain a functional understanding of ACh and DA transmission in shaping biological representations and to explore neuromodulator-inspired learning rules for ANNs. We model the effects of ACh and DA on synaptic plasticity and confirm that stimuli coinciding with greater neuromodulator activation are over represented in the network. We then simulate the physiological release schedules of ACh and DA. We measure the impact of neuromodulator release on the network's representation and on its performance on a classification task. We find that ACh and DA trigger distinct changes in neural representations that both improve performance. The putative ACh signal redistributes neural preferences so that more neurons encode stimulus classes that are challenging for the network. The putative DA signal adapts synaptic weights so that they better match the classes of the task at hand. Our model thus offers a functional explanation for the effects of ACh and DA on cortical representations. Additionally, our learning algorithm yields performances comparable to those of state-of-the-art optimisation methods in multi-layer perceptrons while requiring weaker supervision signals and interacting with synaptically-local weight updates."}], "ArticleTitle": "Models of Acetylcholine and Dopamine Signals Differentially Improve Neural Representations."}, "28611617": {"mesh": [], "AbstractText": [{"section": null, "text": "In this paper, we identified factors that can affect seizure suppression via electrical stimulation by an integrative study based on experimental and computational approach. Preferentially, we analyzed the characteristics of seizure-like events (SLEs) using our previous in vitro experimental data. The results were analyzed in two groups classified according to the size of the effective region, in which the SLE was able to be completely suppressed by local stimulation. However, no significant differences were found between these two groups in terms of signal features or propagation characteristics (i.e., propagation delays, frequency spectrum, and phase synchrony). Thus, we further investigated important factors using a computational model that was capable of evaluating specific influences on effective region size. In the proposed model, signal transmission between neurons was based on two different mechanisms: synaptic transmission and the electrical field effect. We were able to induce SLEs having similar characteristics with differentially weighted adjustments for the two transmission methods in various noise environments. Although the SLEs had similar characteristics, their suppression effects differed. First of all, the suppression effect occurred only locally where directly received the stimulation effect in the high noise environment, but it occurred in the entire network in the low noise environment. Interestingly, in the same noise environment, the suppression effect was different depending on SLE propagation mechanism; only a local suppression effect was observed when the influence of the electrical field transmission was very weak, whereas a global effect was observed with a stronger electrical field effect. These results indicate that neuronal activities synchronized by a strong electrical field effect respond more sensitively to partial changes in the entire network. In addition, the proposed model was able to predict that stimulation of a seizure focus region is more effective for suppression. In conclusion, we confirmed the possibility of a computational model as a simulation tool to analyze the efficacy of deep brain stimulation (DBS) and investigated the key factors that determine the size of an effective region in seizure suppression via electrical stimulation."}], "ArticleTitle": "Prediction of the Seizure Suppression Effect by Electrical Stimulation via a Computational Modeling Approach."}, "27630556": {"mesh": [], "AbstractText": [{"section": null, "text": "Oxygen is delivered to brain tissue by a dense network of microvessels, which actively control cerebral blood flow (CBF) through vasodilation and contraction in response to changing levels of neural activity. Understanding these network-level processes is immediately relevant for (1) interpretation of functional Magnetic Resonance Imaging (fMRI) signals, and (2) investigation of neurological diseases in which a deterioration of neurovascular and neuro-metabolic physiology contributes to motor and cognitive decline. Experimental data on the structure, flow and oxygen levels of microvascular networks are needed, together with theoretical methods to integrate this information and predict physiologically relevant properties that are not directly measurable. Recent progress in optical imaging technologies for high-resolution in vivo measurement of the cerebral microvascular architecture, blood flow, and oxygenation enables construction of detailed computational models of cerebral hemodynamics and oxygen transport based on realistic three-dimensional microvascular networks. In this article, we review state-of-the-art optical microscopy technologies for quantitative in vivo imaging of cerebral microvascular structure, blood flow and oxygenation, and theoretical methods that utilize such data to generate spatially resolved models for blood flow and oxygen transport. These \"bottom-up\" models are essential for the understanding of the processes governing brain oxygenation in normal and disease states and for eventual translation of the lessons learned from animal studies to humans. "}], "ArticleTitle": "Modeling of Cerebral Oxygen Transport Based on In vivo Microscopic Imaging of Microvascular Network Structure, Blood Flow, and Oxygenation."}, "28848417": {"mesh": [], "AbstractText": [{"section": null, "text": "Deep brain stimulation (DBS) has compelling results in the desynchronization of the basal ganglia neuronal activities and thus, is used in treating the motor symptoms of Parkinson's disease (PD). Accurate definition of DBS waveform parameters could avert tissue or electrode damage, increase the neuronal activity and reduce energy cost which will prolong the battery life, hence avoiding device replacement surgeries. This study considers the use of a charge balanced Gaussian waveform pattern as a method to disrupt the firing patterns of neuronal cell activity. A computational model was created to simulate ganglia cells and their interactions with thalamic neurons. From the model, we investigated the effects of modified DBS pulse shapes and proposed a delay period between the cathodic and anodic parts of the charge balanced Gaussian waveform to desynchronize the firing patterns of the GPe and GPi cells. The results of the proposed Gaussian waveform with delay outperformed that of rectangular DBS waveforms used in in-vivo experiments. The Gaussian Delay Gaussian (GDG) waveforms achieved lower number of misses in eliciting action potential while having a lower amplitude and shorter length of delay compared to numerous different pulse shapes. The amount of energy consumed in the basal ganglia network due to GDG waveforms was dropped by 22% in comparison with charge balanced Gaussian waveforms without any delay between the cathodic and anodic parts and was also 60% lower than a rectangular charged balanced pulse with a delay between the cathodic and anodic parts of the waveform. Furthermore, by defining a Synchronization Level metric, we observed that the GDG waveform was able to reduce the synchronization of GPi neurons more effectively than any other waveform. The promising results of GDG waveforms in terms of eliciting action potential, desynchronization of the basal ganglia neurons and reduction of energy consumption can potentially enhance the performance of DBS devices."}], "ArticleTitle": "Computational Stimulation of the Basal Ganglia Neurons with Cost Effective Delayed Gaussian Waveforms."}, "28860983": {"mesh": [], "AbstractText": [{"section": null, "text": "Commonly-preserved radial convolution is a prominent characteristic of the mammalian cerebral cortex. Endeavors from multiple disciplines have been devoted for decades to explore the causes for this enigmatic structure. However, the underlying mechanisms that lead to consistent cortical convolution patterns still remain poorly understood. In this work, inspired by prior studies, we propose and evaluate a plausible theory that radial convolution during the early development of the brain is sculptured by radial structures consisting of radial glial cells (RGCs) and maturing axons. Specifically, the regionally heterogeneous development and distribution of RGCs controlled by Trnp1 regulate the convex and concave convolution patterns (gyri and sulci) in the radial direction, while the interplay of RGCs' effects on convolution and axons regulates the convex (gyral) convolution patterns. This theory is assessed by observations and measurements in literature from multiple disciplines such as neurobiology, genetics, biomechanics, etc., at multiple scales to date. Particularly, this theory is further validated by multimodal imaging data analysis and computational simulations in this study. We offer a versatile and descriptive study model that can provide reasonable explanations of observations, experiments, and simulations of the characteristic mammalian cortical folding."}], "ArticleTitle": "Radial Structure Scaffolds Convolution Patterns of Developing Cerebral Cortex."}, "28484384": {"mesh": [], "AbstractText": [{"section": null, "text": "A major function of central nervous systems is to discriminate different categories or types of sensory input. Neuronal networks accomplish such tasks by learning different sensory maps at several stages of neural hierarchy, such that different neurons fire selectively to reflect different internal or external patterns and states. The exact mechanisms of such map formation processes in the brain are not completely understood. Here we study the mechanism by which a simple recurrent/reentrant neuronal network accomplish group selection and discrimination to different inputs in order to generate sensory maps. We describe the conditions and mechanism of transition from a rhythmic epileptic state (in which all neurons fire synchronized and indiscriminately to any input) to a winner-take-all state in which only a subset of neurons fire for a specific input. We prove an analytic condition under which a stable bump solution and a winner-take-all state can emerge from the local recurrent excitation-inhibition interactions in a three-layer spiking network with distinct excitatory and inhibitory populations, and demonstrate the importance of surround inhibitory connection topology on the stability of dynamic patterns in spiking neural network."}], "ArticleTitle": "Mechanisms of Winner-Take-All and Group Selection in Neuronal Spiking Networks."}, "31507396": {"mesh": [], "AbstractText": [{"section": null, "text": "Emotion recognition using electroencephalogram (EEG) signals has attracted significant research attention. However, it is difficult to improve the emotional recognition effect across subjects. In response to this difficulty, in this study, multiple features were extracted for the formation of high-dimensional features. Based on the high-dimensional features, an effective method for cross-subject emotion recognition was then developed, which integrated the significance test/sequential backward selection and the support vector machine (ST-SBSSVM). The effectiveness of the ST-SBSSVM was validated on a dataset for emotion analysis using physiological signals (DEAP) and the SJTU Emotion EEG Dataset (SEED). With respect to high-dimensional features, the ST-SBSSVM average improved the accuracy of cross-subject emotion recognition by 12.4% on the DEAP and 26.5% on the SEED when compared with common emotion recognition methods. The recognition accuracy obtained using ST-SBSSVM was as high as that obtained using sequential backward selection (SBS) on the DEAP dataset. However, on the SEED dataset, the recognition accuracy increased by ~6% using ST-SBSSVM from that using the SBS. Using the ST-SBSSVM, ~97% (DEAP) and 91% (SEED) of the program runtime was eliminated when compared with the SBS. Compared with recent similar works, the method developed in this study for emotion recognition across all subjects was found to be effective, and its accuracy was 72% (DEAP) and 89% (SEED)."}], "ArticleTitle": "Multi-method Fusion of Cross-Subject Emotion Recognition Based on High-Dimensional EEG Features."}, "32670042": {"mesh": [], "AbstractText": [{"section": null, "text": "All vertebrate brains contain a dense matrix of thin fibers that release serotonin (5-hydroxytryptamine), a neurotransmitter that modulates a wide range of neural, glial, and vascular processes. Perturbations in the density of this matrix have been associated with a number of mental disorders, including autism and depression, but its self-organization and plasticity remain poorly understood. We introduce a model based on reflected Fractional Brownian Motion (FBM), a rigorously defined stochastic process, and show that it recapitulates some key features of regional serotonergic fiber densities. Specifically, we use supercomputing simulations to model fibers as FBM-paths in two-dimensional brain-like domains and demonstrate that the resultant steady state distributions approximate the fiber distributions in physical brain sections immunostained for the serotonin transporter (a marker for serotonergic axons in the adult brain). We suggest that this framework can support predictive descriptions and manipulations of the serotonergic matrix and that it can be further extended to incorporate the detailed physical properties of the fibers and their environment."}], "ArticleTitle": "Serotonergic Axons as Fractional Brownian Motion Paths: Insights Into the Self-Organization of Regional Densities."}, "31632257": {"mesh": [], "AbstractText": [{"section": null, "text": "Precise cerebral dynamics of action of the anesthetics are a challenge for neuroscientists. This explains why there is no gold standard for monitoring the Depth of Anesthesia (DoA) and why experimental studies may use several electroencephalogram (EEG) channels, ranging from 2 to 128 EEG-channels. Our study aimed at finding the scalp area providing valuable information about brain activity under general anesthesia (GA) to select the more optimal EEG channel to characterized the DoA. We included 30 patients undergoing elective, minor surgery under GA and used a 32-channel EEG to record their electrical brain activity. In addition, we recorded their physiological parameters and the BIS monitor. Each individual EEG channel data were processed to test their ability to differentiate awake from asleep states. Due to strict quality criteria adopted for the EEG data and the difficulties of the real-life setting of the study, only 8 patients recordings were taken into consideration in the final analysis. Using 2 classification algorithms, we identified the optimal channels to discriminate between asleep and awake states: the frontal and temporal F8 and T7 were retrieved as being the two bests channels to monitor DoA. Then, using only data from the F8 channel, we tried to minimize the number of features required to discriminate between the awake and asleep state. The best algorithm turned out to be the Gaussian Na&#239;ve Bayes (GNB) requiring only 5 features (Area Under the ROC Curve - AUC- of 0.93 &#177; 0.04). This finding may pave the way to improve the assessment of DoA by combining one EEG channel recordings with a multimodal physiological monitoring of the brain state under GA. Further work is needed to see if these results may be valid to asses the depth of sedation in ICU."}], "ArticleTitle": "Selection of the Best Electroencephalogram Channel to Predict the Depth of Anesthesia."}, "30254581": {"mesh": [], "AbstractText": [{"section": null, "text": "The dynamics of the environment where we live in and the interaction with it, predicting events, provided strong evolutionary pressures for the brain functioning to process temporal information and generate timed responses. As a result, the human brain is able to process temporal information and generate temporal patterns. Despite the clear importance of temporal processing to cognition, learning, communication and sensory, motor and emotional processing, the basal mechanisms of how animals differentiate simple intervals or provide timed responses are still under debate. The lesson we learned from the last decade of research in neuroscience is that functional and structural brain connectivity matter. Specifically, it has been accepted that the organization of the brain in interacting segregated networks enables its function. In this paper we delineate the route to a promising approach for investigating timing mechanisms. We illustrate how novel insight into timing mechanisms can come by investigating brain functioning as a multi-layer dynamical network whose clustered dynamics is bound to report the presence of metastable states. We anticipate that metastable dynamics underlie the real-time coordination necessary for the brain's dynamic functioning associated to time perception. This new point of view will help further clarifying mechanisms of neuropsychiatric disorders."}], "ArticleTitle": "Metastable States of Multiscale Brain Networks Are Keys to Crack the Timing Problem."}, "28326032": {"mesh": [], "AbstractText": [{"section": null, "text": "Blepharospasm (sometimes called \"benign essential blepharospasm,\" BEB) is one of the most common focal dystonias. It involves involuntary eyelid spasms, eye closure, and increased blinking. Despite the success of botulinum toxin injections and, in some cases, pharmacologic or surgical interventions, BEB treatments are not completely efficacious and only symptomatic. We could develop principled strategies for preventing and reversing the disease if we knew the pathogenesis of primary BEB. The objective of this study was to develop a conceptual framework and dynamic circuit hypothesis for the pathogenesis of BEB. The framework extends our overarching theory for the multifactorial pathogenesis of focal dystonias (Peterson et al., 2010) to incorporate a two-hit rodent model specifically of BEB (Schicatano et al., 1997). We incorporate in the framework three features critical to cranial motor control: (1) the joint influence of motor cortical regions and direct descending projections from one of the basal ganglia output nuclei, the substantia nigra pars reticulata, on brainstem motor nuclei, (2) nested loops composed of the trigeminal blink reflex arc and the long sensorimotor loop from trigeminal nucleus through thalamus to somatosensory cortex back through basal ganglia to the same brainstem nuclei modulating the reflex arc, and (3) abnormalities in the basal ganglia dopamine system that provide a sensorimotor learning substrate which, when combined with patterns of increased blinking, leads to abnormal sensorimotor mappings manifest as BEB. The framework explains experimental data on the trigeminal reflex blink excitability (TRBE) from Schicatano et al. and makes predictions that can be tested in new experimental animal models based on emerging genetics in dystonia, including the recently characterized striatal-specific D1R dopamine transduction alterations caused by the GNAL mutation. More broadly, the model will provide a guide for future efforts to mechanistically link multiple factors in the pathogenesis of BEB and facilitate simulations of how exogenous manipulations of the pathogenic factors could ultimately be used to prevent and reverse the disorder."}], "ArticleTitle": "A Dynamic Circuit Hypothesis for the Pathogenesis of Blepharospasm."}, "30524259": {"mesh": [], "AbstractText": [{"section": null, "text": "Neuronal networks in the brain are the structural basis of human cognitive function, and the plasticity of neuronal networks is thought to be the principal neural mechanism underlying learning and memory. Dominated by the Hebbian theory, researchers have devoted extensive effort to studying the changes in synaptic connections between neurons. However, understanding the network topology of all synaptic connections has been neglected over the past decades. Furthermore, increasing studies indicate that synaptic activities are tightly coupled with metabolic energy, and metabolic energy is a unifying principle governing neuronal activities. Therefore, the network topology of all synaptic connections may also be governed by metabolic energy. Here, by implementing a computational model, we investigate the general synaptic organization rules for neurons and neuronal networks from the perspective of energy metabolism. We find that to maintain the energy balance of individual neurons in the proposed model, the number of synaptic connections is inversely proportional to the average of the synaptic weights. This strategy may be adopted by neurons to ensure that the ability of neurons to transmit signals matches their own energy metabolism. In addition, we find that the density of neuronal networks is also an important factor in the energy balance of neuronal networks. An abnormal increase or decrease in the network density could lead to failure of energy metabolism in the neuronal network. These rules may change our view of neuronal networks in the brain and have guiding significance for the design of neuronal network models."}], "ArticleTitle": "Constraints of Metabolic Energy on the Number of Synaptic Connections of Neurons and the Density of Neuronal Networks."}, "29046631": {"mesh": [], "AbstractText": [{"section": null, "text": "The brain integrates information from different sensory modalities to generate a coherent and accurate percept of external events. Several experimental studies suggest that this integration follows the principle of Bayesian estimate. However, the neural mechanisms responsible for this behavior, and its development in a multisensory environment, are still insufficiently understood. We recently presented a neural network model of audio-visual integration (Neural Computation, 2017) to investigate how a Bayesian estimator can spontaneously develop from the statistics of external stimuli. Model assumes the presence of two unimodal areas (auditory and visual) topologically organized. Neurons in each area receive an input from the external environment, computed as the inner product of the sensory-specific stimulus and the receptive field synapses, and a cross-modal input from neurons of the other modality. Based on sensory experience, synapses were trained via Hebbian potentiation and a decay term. Aim of this work is to improve the previous model, including a more realistic distribution of visual stimuli: visual stimuli have a higher spatial accuracy at the central azimuthal coordinate and a lower accuracy at the periphery. Moreover, their prior probability is higher at the center, and decreases toward the periphery. Simulations show that, after training, the receptive fields of visual and auditory neurons shrink to reproduce the accuracy of the input (both at the center and at the periphery in the visual case), thus realizing the likelihood estimate of unimodal spatial position. Moreover, the preferred positions of visual neurons contract toward the center, thus encoding the prior probability of the visual input. Finally, a prior probability of the co-occurrence of audio-visual stimuli is encoded in the cross-modal synapses. The model is able to simulate the main properties of a Bayesian estimator and to reproduce behavioral data in all conditions examined. In particular, in unisensory conditions the visual estimates exhibit a bias toward the fovea, which increases with the level of noise. In cross modal conditions, the SD of the estimates decreases when using congruent audio-visual stimuli, and a ventriloquism effect becomes evident in case of spatially disparate stimuli. Moreover, the ventriloquism decreases with the eccentricity."}], "ArticleTitle": "Development of a Bayesian Estimator for Audio-Visual Integration: A Neurocomputational Study."}, "28484385": {"mesh": [], "AbstractText": [{"section": null, "text": "Two mathematical models are part of the foundation of Computational neurophysiology; (a) the Cable equation is used to compute the membrane potential of neurons, and, (b) volume-conductor theory describes the extracellular potential around neurons. In the standard procedure for computing extracellular potentials, the transmembrane currents are computed by means of (a) and the extracellular potentials are computed using an explicit sum over analytical point-current source solutions as prescribed by volume conductor theory. Both models are extremely useful as they allow huge simplifications of the computational efforts involved in computing extracellular potentials. However, there are more accurate, though computationally very expensive, models available where the potentials inside and outside the neurons are computed simultaneously in a self-consistent scheme. In the present work we explore the accuracy of the classical models (a) and (b) by comparing them to these more accurate schemes. The main assumption of (a) is that the ephaptic current can be ignored in the derivation of the Cable equation. We find, however, for our examples with stylized neurons, that the ephaptic current is comparable in magnitude to other currents involved in the computations, suggesting that it may be significant-at least in parts of the simulation. The magnitude of the error introduced in the membrane potential is several millivolts, and this error also translates into errors in the predicted extracellular potentials. While the error becomes negligible if we assume the extracellular conductivity to be very large, this assumption is, unfortunately, not easy to justify a priori for all situations of interest."}], "ArticleTitle": "An Evaluation of the Accuracy of Classical Models for Computing the Membrane Potential and Extracellular Potential for Neurons."}, "31001100": {"mesh": [], "AbstractText": [{"section": null, "text": "Despite the progress in understanding of neural codes, the studies of the cortico-muscular coupling still largely rely on interferential electromyographic (EMG) signal or its rectification for the assessment of motor neuron pool behavior. This assessment is non-trivial and should be used with precaution. Direct analysis of neural codes by decomposing the EMG, also known as neural decoding, is an alternative to EMG amplitude estimation. In this study, we propose a fully-deterministic hybrid surface EMG (sEMG) decomposition approach that combines the advantages of both template-based and Blind Source Separation (BSS) decomposition approaches, a.k.a. guided source separation (GSS), to identify motor unit (MU) firing patterns. We use the single-pass density-based clustering algorithm to identify possible cluster representatives in different sEMG channels. These cluster representatives are then used as initial points of modified gradient Convolution Kernel Compensation (gCKC) algorithm. Afterwards, we use the Kalman filter to reduce the noise impact and increase convergence rate of MU filter identification by gCKC. Moreover, we designed an adaptive soft-thresholding method to identify MU firing times out of estimated MU spike trains. We tested the proposed algorithm on a set of synthetic sEMG signals with known MU firing patterns. A grid of 9 &#215; 10 monopolar surface electrodes with 5-mm inter-electrode distances in both directions was simulated. Muscle excitation was set to 10, 30, and 50%. Colored Gaussian zero-mean noise with the signal-to-noise ratio (SNR) of 10, 20, and 30 dB, respectively, was added to 16 s long sEMG signals that were sampled at 4,096 Hz. Overall, 45 simulated signals were analyzed. Our decomposition approach was compared with gCKC algorithm. Overall, in our algorithm, the average numbers of identified MUs and Rate-of-Agreement (RoA) were 16.41 &#177; 4.18 MUs and 84.00 &#177; 0.06%, respectively, whereas the gCKC identified 12.10 &#177; 2.32 MUs with the average RoA of 90.78 &#177; 0.08%. Therefore, the proposed GSS method identified more MUs than the gCKC, with comparable performance. Its performance was dependent on the signal quality but not the signal complexity at different force levels. The proposed algorithm is a promising new offline tool in clinical neurophysiology."}], "ArticleTitle": "Non-invasive Decoding of the Motoneurons: A Guided Source Separation Method Based on Convolution Kernel Compensation With Clustered Initial Points."}, "30072886": {"mesh": [], "AbstractText": [{"section": null, "text": "Epilepsy is a chronic non-communicable disorder of the brain that affects individuals of all ages. It is caused by a sudden abnormal discharge of brain neurons leading to temporary dysfunction. In this regard, if seizures could be predicted a reasonable period of time before their occurrence, epilepsy patients could take precautions against them and improve their safety and quality of life. However, the potential that permutation entropy(PE) can be applied in human epilepsy prediction from intracranial electroencephalogram (iEEG) recordings remains unclear. Here, we described the novel application of PE to track the dynamical changes of human brain activity from iEEG recordings for the epileptic seizure prediction. The iEEG signals of 19 patients were obtained from the Epilepsy Centre at the University Hospital of Freiburg. After preprocessing, PE was extracted in a sliding time window, and a support vector machine (SVM) was employed to discriminate cerebral state. Then a two-step post-processing method was applied for the purpose of prediction. The results showed that we obtained an average sensitivity (SS) of 94% and false prediction rates (FPR) with 0.111 h-1. The best results with SS of 100% and FPR of 0 h-1 were achieved for some patients. The average prediction horizon was 61.93 min, leaving sufficient treatment time before a seizure. These results indicated that applying PE as a feature to extract information and SVM for classification could predict seizures, and the presented method shows great potential in clinical seizure prediction for human."}], "ArticleTitle": "Epileptic Seizure Prediction Based on Permutation Entropy."}, "30177878": {"mesh": [], "AbstractText": [{"section": null, "text": "While functional connectivity networks are often extracted from resting-state fMRI scans, they have been shown to be active during task performance as well. However, the effect of an in-scanner task on functional connectivity networks is not completely understood. While there is evidence that task-evoked positive BOLD response can alter functional connectivity networks, particularly in the primary sensorimotor cortices, the effect of task-evoked negative BOLD response on the functional connectivity of the Default mode network (DMN) is somewhat ambiguous. In this study, we aim to investigate whether task performance, which is associated with negative BOLD response in the DMN regions, alters the time-course of functional connectivity in the same regions obtained by independent component analysis (ICA). ICA has been used to effectively extract functional connectivity networks during task performance and resting-state. We first demonstrate that performing a simple visual-motor task alters the temporal time-course of the network extracted from the primary visual cortex. Then we show that despite detecting a robust task-evoked negative BOLD response in the DMN regions, a simple visual-motor task does not alter the functional connectivity of the DMN regions. Our findings suggest that different mechanisms may underlie the relationship between task-related activation/deactivation networks and the overlapping functional connectivity networks in the human large-scale brain networks."}], "ArticleTitle": "Task-Evoked Negative BOLD Response in the Default Mode Network Does Not Alter Its Functional Connectivity."}, "31849630": {"mesh": [], "AbstractText": [{"section": null, "text": "Affective human-robot interaction requires lightweight software and cheap wearable devices that could further this field. However, the estimation of emotions in real-time poses a problem that has not yet been optimized. An optimization is proposed for the emotion estimation methodology including artifact removal, feature extraction, feature smoothing, and brain pattern classification. The challenge of filtering artifacts and extracting features, while reducing processing time and maintaining high accuracy results, is attempted in this work. First, two different approaches for real-time electro-oculographic artifact removal techniques are tested and compared in terms of loss of information and processing time. Second, an emotion estimation methodology is proposed based on a set of stable and meaningful features, a carefully chosen set of electrodes, and the smoothing of the feature space. The methodology has proved to perform on real-time constraints while maintaining high accuracy on emotion estimation on the SEED database, both under subject dependent and subject independent paradigms, to test the methodology on a discrete emotional model with three affective states."}], "ArticleTitle": "Optimization of Real-Time EEG Artifact Removal and Emotion Estimation for Human-Robot Interaction Applications."}, "32038209": {"mesh": [], "AbstractText": [{"section": null, "text": "Complex environments provide structured yet variable sensory inputs. To best exploit information from these environments, organisms must evolve the ability to anticipate consequences of new stimuli, and act on these predictions. We propose an evolutionary path for neural networks, leading an organism from reactive behavior to simple proactive behavior and from simple proactive behavior to induction-based behavior. Based on earlier in-vitro and in-silico experiments, we define the conditions necessary in a network with spike-timing dependent plasticity for the organism to go from reactive to proactive behavior. Our results support the existence of specific evolutionary steps and four conditions necessary for embodied neural networks to evolve predictive and inductive abilities from an initial reactive strategy."}], "ArticleTitle": "Reactive, Proactive, and Inductive Agents: An Evolutionary Path for Biological and Artificial Spiking Networks."}, "33178002": {"mesh": [], "AbstractText": [{"section": null, "text": "Chemotherapy-induced peripheral neuropathy (CIPN) is a prevalent, painful side effect which arises due to a number of chemotherapy agents. CIPN can have a prolonged effect on quality of life. Chemotherapy treatment is often reduced or stopped altogether because of the severe pain. Currently, there are no FDA-approved treatments for CIPN partially due to its complex pathogenesis in multiple pathways involving a variety of channels, specifically, voltage-gated ion channels. One aspect of neuropathic pain in vitro is hyperexcitability in dorsal root ganglia (DRG) peripheral sensory neurons. Our study employs bifurcation theory to investigate the role of voltage-gated ion channels in inducing hyperexcitability as a consequence of spontaneous firing due to the common chemotherapy agent paclitaxel. Our mathematical investigation of a reductionist DRG neuron model comprised of sodium channel Nav1.7, sodium channel Nav1.8, delayed rectifier potassium channel, A-type transient potassium channel, and a leak channel suggests that Nav1.8 and delayed rectifier potassium channel conductances are critical for hyperexcitability of small DRG neurons. Introducing paclitaxel into the model, our bifurcation analysis predicts that hyperexcitability is highest for a medium dose of paclitaxel, which is supported by multi-electrode array (MEA) recordings. Furthermore, our findings using MEA reveal that Nav1.8 blocker A-803467 and delayed rectifier potassium enhancer L-alpha-phosphatidyl-D-myo-inositol 4,5-diphosphate, dioctanoyl (PIP2) can reduce paclitaxel-induced hyperexcitability of DRG neurons. Our approach can be readily extended and used to investigate various other contributors of hyperexcitability in CIPN."}], "ArticleTitle": "Examining Sodium and Potassium Channel Conductances Involved in Hyperexcitability of Chemotherapy-Induced Peripheral Neuropathy: A Mathematical and Cell Culture-Based Study."}, "33101000": {"mesh": [], "AbstractText": [{"section": null, "text": "Objective: To study the effect of directional deep brain stimulation (DBS) electrode configuration and vertical electrode spacing on the volume of tissue activated (VTA) in the globus pallidus, pars interna (GPi). Background: Directional DBS leads may allow clinicians to precisely direct current fields to different functional networks within traditionally targeted brain areas. Modeling the shape and size of the VTA for various monopolar or bipolar configurations can inform clinical programming strategies for GPi DBS. However, many computational models of VTA are limited by assuming tissue homogeneity. Methods: We generated a multimodal image-based detailed anatomical (MIDA) computational model with a directional DBS lead (1.5 mm or 0.5 mm vertical electrode spacing) placed with segmented contact 2 at the ventral posterolateral \"sensorimotor\" region of the GPi. The effect of tissue heterogeneity was examined by replacing the MIDA tissues with a homogeneous tissue of conductance 0.3 S/m. DBS pulses (amplitude: 1 mA, pulse width: 60 &#956;s, frequency: 130 Hz) were used to produce VTAs. The following DBS contact configurations were tested: single-segment monopole (2B-/Case+), two-segment monopole (2A-/2B-/Case+ and 2B-/3B-/Case+), ring monopole (2A-/2B-/2C-/Case+), one-cathode three-anode bipole (2B-/3A+/3B+/3C+), three-cathode three-anode bipole (2A-/2B-/2C-/3A+/3B+/3C+). Additionally, certain vertical configurations were repeated with 2 mA current amplitude. Results: Using a heterogeneous tissue model affected both the size and shape of the VTA in GPi. Electrodes with both 0.5 mm and 1.5 mm vertical spacing (1 mA) modeling showed that the single segment monopolar VTA was entirely contained within the GPi when the active electrode is placed at the posterolateral \"sensorimotor\" GPi. Two segments in a same ring and ring settings, however, produced VTAs outside of the GPi border that spread into adjacent white matter pathways, e.g., optic tract and internal capsule. Both stacked monopolar settings and vertical bipolar settings allowed activation of structures dorsal to the GPi in addition to the GPi. Modeling of the stacked monopolar settings with the DBS lead with 0.5 mm vertical electrode spacing further restricted VTAs within the GPi, but the VTA volumes were smaller compared to the equivalent settings of 1.5 mm spacing."}], "ArticleTitle": "Steering the Volume of Tissue Activated With a Directional Deep Brain Stimulation Lead in the Globus Pallidus Pars Interna: A Modeling Study With Heterogeneous Tissue Properties."}, "30154709": {"mesh": [], "AbstractText": [{"section": null, "text": "Brain signals often show rhythmic activity in the so-called gamma range (30-80 Hz), whose magnitude and center frequency are modulated by properties of the visual stimulus such as size and contrast, as well as by cognitive processes such as attention. How gamma rhythm can potentially influence cortical processing remains unclear; previous studies have proposed a scheme called phase coding, in which the intensity of the incoming stimulus is coded in the position of the spike relative to the rhythm. Using chronically implanted microelectrode arrays in the primary visual cortex (area V1) of macaques engaged in an attention task while presenting stimuli of varying contrasts, we tested whether the phase of the gamma rhythm relative to spikes varied as a function of stimulus contrast and attentional state. A previous study had found no evidence of gamma phase coding for either contrast or attention in V1, but in that study spikes and local field potential (LFP) were recorded from the same electrode, due to which spike-gamma phase estimation could have been biased. Further, the filtering operation to obtain LFP could also have biased the gamma phase. By analyzing spikes and LFP from different electrodes, we found a weak but significant effect of attention, but not stimulus contrast, on gamma phase relative to spikes. The results remained consistent even after correcting the filter induced lags, although the absolute magnitude of gamma phase shifted by up to ~15&#176;. Although we found a significant effect of attention, we argue that a small magnitude of phase shift as well as the dependence of phase angles on gamma power and center frequency limits a potential role of gamma in phase coding in V1."}], "ArticleTitle": "Effect of Stimulus Contrast and Visual Attention on Spike-Gamma Phase Relationship in Macaque Primary Visual Cortex."}, "29674961": {"mesh": [], "AbstractText": [{"section": null, "text": "We present a novel strategy for unsupervised feature learning in image applications inspired by the Spike-Timing-Dependent-Plasticity (STDP) biological learning rule. We show equivalence between rank order coding Leaky-Integrate-and-Fire neurons and ReLU artificial neurons when applied to non-temporal data. We apply this to images using rank-order coding, which allows us to perform a full network simulation with a single feed-forward pass using GPU hardware. Next we introduce a binary STDP learning rule compatible with training on batches of images. Two mechanisms to stabilize the training are also presented : a Winner-Takes-All (WTA) framework which selects the most relevant patches to learn from along the spatial dimensions, and a simple feature-wise normalization as homeostatic process. This learning process allows us to train multi-layer architectures of convolutional sparse features. We apply our method to extract features from the MNIST, ETH80, CIFAR-10, and STL-10 datasets and show that these features are relevant for classification. We finally compare these results with several other state of the art unsupervised learning methods."}], "ArticleTitle": "Unsupervised Feature Learning With Winner-Takes-All Based STDP."}, "29321737": {"mesh": [], "AbstractText": [{"section": null, "text": "The neuronal mechanisms how anesthetics lead to loss of consciousness are unclear. Thalamocortical interactions are crucially involved in conscious perception; hence the thalamocortical network might be a promising target for anesthetic modulation of neuronal information pertaining to arousal and waking behavior. General anesthetics affect the neurophysiology of the thalamus and the cortex but the exact mechanisms of how anesthetics interfere with processing thalamocortical information remain to be elucidated. Here we investigated the effect of the anesthetic agents sevoflurane and propofol on thalamocortical network activity in vitro. We used voltage-sensitive dye imaging techniques to analyze the cortical depolarization in response to stimulation of the thalamic ventrobasal nucleus in brain slices from mice. Exposure to sevoflurane globally decreased cortical depolarization in a dose-dependent manner. Sevoflurane reduced the intensity and extent of cortical depolarization and delayed thalamocortical signal propagation. In contrast, propofol neither affected area nor amplitude of cortical depolarization. However, propofol exposure resulted in regional changes in spatial distribution of maximum fluorescence intensity in deep regions of the cortex. In summary, our experiments revealed substance-specific effects on the thalamocortical network. Functional changes of the neuronal network are known to be pivotally involved in the anesthetic-induced loss of consciousness. Our findings provide further evidence that the mechanisms of anesthetic-mediated loss of consciousness are drug- and pathway-specific."}], "ArticleTitle": "Propofol and Sevoflurane Differentially Modulate Cortical Depolarization following Electric Stimulation of the Ventrobasal Thalamus."}, "28747882": {"mesh": [], "AbstractText": [{"section": null, "text": "Lehky et al. (2011) provided a statistical analysis on the responses of the recorded 674 neurons to 806 image stimuli in anterior inferotemporalm (AIT) cortex of two monkeys. In terms of kurtosis and Pareto tail index, they observed that the population sparseness of both unnormalized and normalized responses is always larger than their single-neuron selectivity, hence concluded that the critical features for individual neurons in primate AIT cortex are not very complex, but there is an indefinitely large number of them. In this work, we explore an \"inverse problem\" by simulation, that is, by simulating each neuron indeed only responds to a very limited number of stimuli among a very large number of neurons and stimuli, to assess whether the population sparseness is always larger than the single-neuron selectivity. Our simulation results show that the population sparseness exceeds the single-neuron selectivity in most cases even if the number of neurons and stimuli are much larger than several hundreds, which confirms the observations in Lehky et al. (2011). In addition, we found that the variances of the computed kurtosis and Pareto tail index are quite large in some cases, which reveals some limitations of these two criteria when used for neuron response evaluation."}], "ArticleTitle": "Comparison of IT Neural Response Statistics with Simulations."}, "32676021": {"mesh": [], "AbstractText": [{"section": null, "text": "Real-time neuron detection and neural activity extraction are critical components of real-time neural decoding. In this paper, we propose a novel real-time neuron detection and activity extraction system using a dataflow framework to provide real-time performance and adaptability to new algorithms and hardware platforms. The proposed system was evaluated on simulated calcium imaging data, calcium imaging data with manual annotation, and calcium imaging data of the anterior lateral motor cortex. We found that the proposed system accurately detected neurons and extracted neural activities in real time without any requirement for expensive, cumbersome, or special-purpose computing hardware. We expect that the system will enable cost-effective, real-time calcium imaging-based neural decoding, leading to precise neuromodulation."}], "ArticleTitle": "Real-Time Neuron Detection and Neural Signal Extraction Platform for Miniature Calcium Imaging."}, "27597822": {"mesh": [], "AbstractText": [{"section": null, "text": "Neuronal oscillations support cognitive processing. Modern views suggest that neuronal oscillations do not only reflect coordinated activity in spatially distributed networks, but also that there is interaction between the oscillations at different frequencies. For example, invasive recordings in animals and humans have found that the amplitude of fast oscillations (>40 Hz) occur non-uniformly within the phase of slower oscillations, forming the so-called cross-frequency coupling (CFC). However, the CFC patterns might be influenced by features in the signal that do not relate to underlying physiological interactions. For example, CFC estimates may be sensitive to spectral correlations due to non-sinusoidal properties of the alpha band wave morphology. To investigate this issue, we performed CFC analysis using experimental and synthetic data. The former consisted in a double-blind magnetoencephalography pharmacological study in which participants received either placebo, 0.5 or 1.5 mg of lorazepam (LZP; GABAergic enhancer) in different experimental sessions. By recording oscillatory brain activity with during rest and working memory (WM), we were able to demonstrate that posterior alpha (8-12 Hz) phase was coupled to beta-low gamma band (20-45 Hz) amplitude envelope during all sessions. Importantly, bicoherence values around the harmonics of the alpha frequency were similar both in magnitude and topographic distribution to the cross-frequency coherence (CFCoh) values observed in the alpha-phase to beta-low gamma coupling. In addition, despite the large CFCoh we found no significant cross-frequency directionality (CFD). Critically, simulations demonstrated that a sizable part of our empirical CFCoh between alpha and beta-low gamma coupling and the lack of CFD could be explained by two-three harmonics aligned in zero phase-lag produced by the physiologically characteristic alpha asymmetry in the amplitude of the peaks relative to the troughs. Furthermore, we showed that periodic signals whose waveform deviate from pure sine waves produce non-zero CFCoh with predictable CFD. Our results reveal the important role of the non-sinusoidal wave morphology on state of the art CFC metrics and we recommend caution with strong physiological interpretations of CFC and suggest basic data quality checks to enhance the mechanistic understanding of CFC. "}], "ArticleTitle": "Neuronal Oscillations with Non-sinusoidal Morphology Produce Spurious Phase-to-Amplitude Coupling and Directionality."}, "33100999": {"mesh": [], "AbstractText": [{"section": null, "text": "The aim of this study was to characterize the EEG alterations in inter-band interactions along the Alzheimer's disease (AD) continuum. For this purpose, EEG background activity from 51 healthy control subjects, 51 mild cognitive impairment patients, 50 mild AD patients, 50 moderate AD patients, and 50 severe AD patients was analyzed by means of bispectrum. Three inter-band features were extracted from bispectrum matrices: bispectral relative power (BispRP), cubic bispectral entropy (BispEn), and bispectral median frequency (BispMF). BispRP results showed an increase of delta and theta interactions with other frequency bands and the opposite behavior for alpha, beta-1, and beta-2. Delta and theta interactions, along with the rest of the spectrum, also experimented a decrease of BispEn with disease progression, suggesting these bands interact with a reduced variety of components in advanced stages of dementia. Finally, BispMF showed a consistent reduction along the AD continuum in all bands, which is reflective of an interaction of the global spectrum with lower frequency bands as the disease develops. Our results indicate a progressive decrease in inter-band interactions with the severity of the disease, especially those involving high frequency components. Since inter-band coupling oscillations are related to complex and multi-scaled brain processes, these alterations likely reflect the neurodegeneration associated with the AD continuum."}], "ArticleTitle": "Inter-band Bispectral Analysis of EEG Background Activity to Characterize Alzheimer's Disease Continuum."}, "28824408": {"mesh": [], "AbstractText": [{"section": null, "text": "The signal transformations that take place in high-level sensory regions of the brain remain enigmatic because of the many nonlinear transformations that separate responses of these neurons from the input stimuli. One would like to have dimensionality reduction methods that can describe responses of such neurons in terms of operations on a large but still manageable set of relevant input features. A number of methods have been developed for this purpose, but often these methods rely on the expansion of the input space to capture as many relevant stimulus components as statistically possible. This expansion leads to a lower effective sampling thereby reducing the accuracy of the estimated components. Alternatively, so-called low-rank methods explicitly search for a small number of components in the hope of achieving higher estimation accuracy. Even with these methods, however, noise in the neural responses can force the models to estimate more components than necessary, again reducing the methods' accuracy. Here we describe how a flexible regularization procedure, together with an explicit rank constraint, can strongly improve the estimation accuracy compared to previous methods suitable for characterizing neural responses to natural stimuli. Applying the proposed low-rank method to responses of auditory neurons in the songbird brain, we find multiple relevant components making up the receptive field for each neuron and characterize their computations in terms of logical OR and AND computations. The results highlight potential differences in how invariances are constructed in visual and auditory systems."}], "ArticleTitle": "A Low-Rank Method for Characterizing High-Level Neural Computations."}, "29230171": {"mesh": [], "AbstractText": [{"section": null, "text": "The event-related potential (ERP) is the brain response measured in electroencephalography (EEG), which reflects the process of human cognitive activity. ERP has been introduced into brain computer interfaces (BCIs) to communicate the computer with the subject's intention. Due to the low signal-to-noise ratio of EEG, most ERP studies are based on grand-averaging over many trials. Recently single-trial ERP detection attracts more attention, which enables real time processing tasks as rapid face identification. All the targets needed to be retrieved may appear only once, and there is no knowledge of target label for averaging. More interestingly, how the features contribute temporally and spatially to single-trial ERP detection has not been fully investigated. In this paper, we propose to implement a local-learning-based (LLB) feature extraction method to investigate the importance of spatial-temporal components of ERP in a task of rapid face identification using single-trial detection. Comparing to previous methods, LLB method preserves the nonlinear structure of EEG signal distribution, and analyze the importance of original spatial-temporal components via optimization in feature space. As a data-driven methods, the weighting of the spatial-temporal component does not depend on the ERP detection method. The importance weights are optimized by making the targets more different from non-targets in feature space, and regularization penalty is introduced in optimization for sparse weights. This spatial-temporal feature extraction method is evaluated on the EEG data of 15 participants in performing a face identification task using rapid serial visual presentation paradigm. Comparing with other methods, the proposed spatial-temporal analysis method uses sparser (only 10% of the total) features, and could achieve comparable performance (98%) of single-trial ERP detection as the whole features across different detection methods. The interesting finding is that the N250 is the earliest temporal component that contributes to single-trial ERP detection in face identification. And the importance of N250 components is more laterally distributed toward the left hemisphere. We show that using only the left N250 component over-performs the right N250 in the face identification task using single-trial ERP detection. The finding is also important in building a fast and efficient (fewer electrodes) BCI system for rapid face identification."}], "ArticleTitle": "Spatial-Temporal Feature Analysis on Single-Trial Event Related Potential for Rapid Face Identification."}, "31998105": {"mesh": [], "AbstractText": [{"section": null, "text": "People living with schizophrenia (SCZ) experience severe brain network deterioration. The brain is constantly fizzling with non-linear causal activities measured by electroencephalogram (EEG) and despite the variety of effective connectivity methods, only few approaches can quantify the direct non-linear causal interactions. To circumvent this problem, we are motivated to quantitatively measure the effective connectivity by multivariate transfer entropy (MTE) which has been demonstrated to be able to capture both linear and non-linear causal relationships effectively. In this work, we propose to construct the EEG effective network by MTE and further compare its performance with the Granger causal analysis (GCA) and Bivariate transfer entropy (BVTE). The simulation results quantitatively show that MTE outperformed GCA and BVTE under varied signal-to-noise conditions, edges recovered, sensitivity, and specificity. Moreover, its applications to the P300 task EEG of healthy controls (HC) and SCZ patients further clearly show the deteriorated network interactions of SCZ, compared to that of the HC. The MTE provides a novel tool to potentially deepen our knowledge of the brain network deterioration of the SCZ."}], "ArticleTitle": "Measuring the Non-linear Directed Information Flow in Schizophrenia by Multivariate Transfer Entropy."}, "31417387": {"mesh": [], "AbstractText": [{"section": null, "text": "This study compared the predictive power and robustness of texture, topological, and convolutional neural network (CNN) based image features for measuring tumors in MRI. These features were used to predict 1p/19q codeletion in the MICCAI BRATS 2017 challenge dataset. Topological data analysis (TDA) based on persistent homology had predictive performance as good as or better than texture-based features and was also less susceptible to image-based perturbations. Features from a pre-trained convolutional neural network had similar predictive performances and robustness as TDA, but also performed better using an alternative classification algorithm, k-top scoring pairs. Feature robustness can be used as a filtering technique without greatly impacting model performance and can also be used to evaluate model stability."}], "ArticleTitle": "Prediction of 1p/19q Codeletion in Diffuse Glioma Patients Using Pre-operative Multiparametric Magnetic Resonance Imaging."}, "32082133": {"mesh": [], "AbstractText": [{"section": null, "text": "Modality-invariant categorical representations, i.e., shared representation, is thought to play a key role in learning to categorize multi-modal information. We have investigated how a bimodal autoencoder can form a shared representation in an unsupervised manner with multi-modal data. We explored whether altering the depth of the network and mixing the multi-modal inputs at the input layer affect the development of the shared representations. Based on the activation of units in the hidden layers, we classified them into four different types: visual cells, auditory cells, inconsistent visual and auditory cells, and consistent visual and auditory cells. Our results show that the number and quality of the last type (i.e., shared representation) significantly differ depending on the depth of the network and are enhanced when the network receives mixed inputs as opposed to separate inputs for each modality, as occurs in typical two-stage frameworks. In the present work, we present a way to utilize information theory to understand the abstract representations formed in the hidden layers of the network. We believe that such an information theoretic approach could potentially provide insights into the development of more efficient and cost-effective ways to train neural networks using qualitative measures of the representations that cannot be captured by analyzing only the final outputs of the networks."}], "ArticleTitle": "An Information Theoretic Approach to Reveal the Formation of Shared Representations."}, "30154708": {"mesh": [], "AbstractText": [{"section": null, "text": "Despite their significant functional roles, beta-band oscillations are least understood. Synchronization in neuronal networks have attracted much attention in recent years with the main focus on transition type. Whether one obtains explosive transition or a continuous transition is an important feature of the neuronal network which can depend on network structure as well as synaptic types. In this study we consider the effect of synaptic interaction (electrical and chemical) as well as structural connectivity on synchronization transition in network models of Izhikevich neurons which spike regularly with beta rhythms. We find a wide range of behavior including continuous transition, explosive transition, as well as lack of global order. The stronger electrical synapses are more conducive to synchronization and can even lead to explosive synchronization. The key network element which determines the order of transition is found to be the clustering coefficient and not the small world effect, or the existence of hubs in a network. These results are in contrast to previous results which use phase oscillator models such as the Kuramoto model. Furthermore, we show that the patterns of synchronization changes when one goes to the gamma band. We attribute such a change to the change in the refractory period of Izhikevich neurons which changes significantly with frequency."}], "ArticleTitle": "Beta-Rhythm Oscillations and Synchronization Transition in Network Models of Izhikevich Neurons: Effect of Topology and Synaptic Type."}, "32327990": {"mesh": [], "AbstractText": [{"section": null, "text": "Biological realism of dendritic morphologies is important for simulating electrical stimulation of brain tissue. By adding point process modeling and conditional sampling to existing generation strategies, we provide a novel means of reproducing the nuanced branching behavior that occurs in different layers of granule cell dendritic morphologies. In this study, a heterogeneous Poisson point process was used to simulate branching events. Conditional distributions were then used to select branch angles depending on the orthogonal distance to the somatic plane. The proposed method was compared to an existing generation tool and a control version of the proposed method that used a homogeneous Poisson point process. Morphologies were generated with each method and then compared to a set of digitally reconstructed neurons. The introduction of a conditionally dependent branching rate resulted in the generation of morphologies that more accurately reproduced the emergent properties of dendritic material per layer, Sholl intersections, and proximal passive current flow. Conditional dependence was critically important for the generation of realistic granule cell dendritic morphologies."}], "ArticleTitle": "Generation of Granule Cell Dendritic Morphologies by Estimating the Spatial Heterogeneity of Dendritic Branching."}, "28261081": {"mesh": [], "AbstractText": [{"section": null, "text": "Central Pattern Generator (CPG) circuits are neural networks that generate rhythmic motor patterns. These circuits are typically built of half-center oscillator subcircuits with reciprocally inhibitory connections. Another common property in many CPGs is the remarkable rich spiking-bursting dynamics of their constituent cells, which balance robustness and flexibility to generate their joint coordinated rhythms. In this paper, we use conductance-based models and realistic connection topologies inspired by the crustacean pyloric CPG to address the study of asymmetry factors shaping CPG bursting rhythms. In particular, we assess the role of asymmetric maximal synaptic conductances, time constants and gap-junction connectivity to establish the regularity of half-center oscillator based CPGs. We map and characterize the synaptic parameter space that lead to regular and irregular bursting activity in these networks. The analysis indicates that asymmetric configurations display robust regular rhythms and that large regions of both regular and irregular but coordinated rhythms exist as a function of the asymmetry in the circuit. Our results show that asymmetry both in the maximal conductances and in the temporal dynamics of mutually inhibitory neurons can synergistically contribute to shape wide regimes of regular spiking-bursting activity in CPGs. Finally, we discuss how a closed-loop protocol driven by a regularity goal can be used to find and characterize regular regimes when there is not time to perform an exhaustive search, as in most experimental studies."}], "ArticleTitle": "Asymmetry Factors Shaping Regular and Irregular Bursting Rhythms in Central Pattern Generators."}, "30524260": {"mesh": [], "AbstractText": [{"section": null, "text": "Molecular switches, such as the protein kinase CaMKII, play a fundamental role in cell signaling by decoding inputs into either high or low states of activity; because the high activation state can be turned on and persist after the input ceases, these switches have earned a reputation as \"digital.\" Although this on/off, binary perspective has been valuable for understanding long timescale synaptic plasticity, accumulating experimental evidence suggests that the CaMKII switch can also control plasticity on short timescales. To investigate this idea further, a non-autonomous, nonlinear ordinary differential equation, representative of a general bistable molecular switch, is analyzed. The results suggest that switch activity in regions surrounding either the high- or low-stable states of activation could act as a reliable analog signal, whose short timescale fluctuations relative to equilibrium track instantaneous input frequency. The model makes intriguing predictions and is validated against previous work demonstrating its suitability as a minimal representation of switch dynamics; in combination with existing experimental evidence, the theory suggests a multiplexed encoding of instantaneous frequency information over short timescales, with integration of total activity over longer timescales."}], "ArticleTitle": "Analog Signaling With the \"Digital\" Molecular Switch CaMKII."}, "30723401": {"mesh": [], "AbstractText": [{"section": null, "text": "Current mainstream neural computing is based on the electricity model proposed by Hodgkin and Huxley in 1952, the core of which is ion passive transmembrane transport controlled by ion channels. However, studies on the evolutionary history of ion channels have shown that some neuronal ion channels predate the neurons. Thus, to deepen our understanding of neuronal activities, ion channel models should be applied to other cells. Expanding the scope of electrophysiological experiments from nerve to muscle, animal to plant, and metazoa to protozoa, has lead the discovery of a number of ion channels. Moreover, the properties of these newly discovered ion channels are too complex to be described by current common models. Hence this paper has presented a convenient method for estimating the distribution of ions under an electric field and established a general ionic concentration-based model of ion passive transmembrane transport that is simple but capable of explaining and simulating the complex phenomena of patch clamp experiments, is applicable to different ion channels in different cells of different species, and conforms to the current general understanding of ion channels. Finally, we designed a series of mathematical experiments, which we have compared with the results of typical electrophysiological experiments conducted on plant cells, oocytes, myocytes, cardiomyocytes, and neurocytes, to verify the model."}], "ArticleTitle": "A General Model of Ion Passive Transmembrane Transport Based on Ionic Concentration."}, "27616989": {"mesh": [], "AbstractText": [{"section": null, "text": "Despite a significant increase in efforts to identify biomarkers and endophenotypic measures of psychiatric illnesses, only a very limited amount of computational models of these markers and measures has been implemented so far. Moreover, existing computational models dealing with biomarkers typically only examine one possible mechanism in isolation, disregarding the possibility that other combinations of model parameters might produce the same network behavior (what has been termed \"multifactoriality\"). In this study we describe a step toward a computational instantiation of an endophenotypic finding for schizophrenia, namely the impairment of evoked auditory gamma and beta oscillations in schizophrenia. We explore the multifactorial nature of this impairment using an established model of primary auditory cortex, by performing an extensive search of the parameter space. We find that single network parameters contain only little information about whether the network will show impaired gamma entrainment and that different regions in the parameter space yield similar network level oscillation abnormalities. These regions in the parameter space, however, show strong differences in the underlying network dynamics. To sum up, we present a first step toward an in silico instantiation of an important biomarker of schizophrenia, which has great potential for the identification and study of disease mechanisms and for understanding of existing treatments and development of novel ones. "}], "ArticleTitle": "Multifactorial Modeling of Impairment of Evoked Gamma Range Oscillations in Schizophrenia."}, "31456678": {"mesh": [], "AbstractText": [{"section": null, "text": "Automatic segmentation of brain tumors from medical images is important for clinical assessment and treatment planning of brain tumors. Recent years have seen an increasing use of convolutional neural networks (CNNs) for this task, but most of them use either 2D networks with relatively low memory requirement while ignoring 3D context, or 3D networks exploiting 3D features while with large memory consumption. In addition, existing methods rarely provide uncertainty information associated with the segmentation result. We propose a cascade of CNNs to segment brain tumors with hierarchical subregions from multi-modal Magnetic Resonance images (MRI), and introduce a 2.5D network that is a trade-off between memory consumption, model complexity and receptive field. In addition, we employ test-time augmentation to achieve improved segmentation accuracy, which also provides voxel-wise and structure-wise uncertainty information of the segmentation result. Experiments with BraTS 2017 dataset showed that our cascaded framework with 2.5D CNNs was one of the top performing methods (second-rank) for the BraTS challenge. We also validated our method with BraTS 2018 dataset and found that test-time augmentation improves brain tumor segmentation accuracy and that the resulting uncertainty information can indicate potential mis-segmentations and help to improve segmentation accuracy."}], "ArticleTitle": "Automatic Brain Tumor Segmentation Based on Cascaded Convolutional Neural Networks With Uncertainty Estimation."}, "28149276": {"mesh": [], "AbstractText": [{"section": null, "text": "Experimental evidence indicates that neurophysiological responses to well-known meaningful sensory items and symbols (such as familiar objects, faces, or words) differ from those to matched but novel and senseless materials (unknown objects, scrambled faces, and pseudowords). Spectral responses in the high beta- and gamma-band have been observed to be generally stronger to familiar stimuli than to unfamiliar ones. These differences have been hypothesized to be caused by the activation of distributed neuronal circuits or cell assemblies, which act as long-term memory traces for learned familiar items only. Here, we simulated word learning using a biologically constrained neurocomputational model of the left-hemispheric cortical areas known to be relevant for language and conceptual processing. The 12-area spiking neural-network architecture implemented replicates physiological and connectivity features of primary, secondary, and higher-association cortices in the frontal, temporal, and occipital lobes of the human brain. We simulated elementary aspects of word learning in it, focussing specifically on semantic grounding in action and perception. As a result of spike-driven Hebbian synaptic plasticity mechanisms, distributed, stimulus-specific cell-assembly (CA) circuits spontaneously emerged in the network. After training, presentation of one of the learned \"word\" forms to the model correlate of primary auditory cortex induced periodic bursts of activity within the corresponding CA, leading to oscillatory phenomena in the entire network and spontaneous across-area neural synchronization. Crucially, Morlet wavelet analysis of the network's responses recorded during presentation of learned meaningful \"word\" and novel, senseless \"pseudoword\" patterns revealed stronger induced spectral power in the gamma-band for the former than the latter, closely mirroring differences found in neurophysiological data. Furthermore, coherence analysis of the simulated responses uncovered dissociated category specific patterns of synchronous oscillations in distant cortical areas, including indirectly connected primary sensorimotor areas. Bridging the gap between cellular-level mechanisms, neuronal-population behavior, and cognitive function, the present model constitutes the first spiking, neurobiologically, and anatomically realistic model able to explain high-frequency oscillatory phenomena indexing language processing on the basis of dynamics and competitive interactions of distributed cell-assembly circuits which emerge in the brain as a result of Hebbian learning and sensorimotor experience."}], "ArticleTitle": "A Spiking Neurocomputational Model of High-Frequency Oscillatory Brain Responses to Words and Pseudowords."}, "28491031": {"mesh": [], "AbstractText": [{"section": null, "text": "As a subtype of idiopathic generalized epilepsies, absence epilepsy is believed to be caused by pathological interactions within the corticothalamic (CT) system. Using a biophysical mean-field model of the CT system, we demonstrate here that the feed-forward inhibition (FFI) in thalamus, i.e., the pathway from the cerebral cortex (Ctx) to the thalamic reticular nucleus (TRN) and then to the specific relay nuclei (SRN) of thalamus that are also directly driven by the Ctx, may participate in controlling absence seizures. In particular, we show that increasing the excitatory Ctx-TRN coupling strength can significantly suppress typical electrical activities during absence seizures. Further, investigation demonstrates that the GABAA- and GABAB-mediated inhibitions in the TRN-SRN pathway perform combination roles in the regulation of absence seizures. Overall, these results may provide an insightful mechanistic understanding of how the thalamic FFI serves as an intrinsic regulator contributing to the control of absence seizures."}], "ArticleTitle": "Control of Absence Seizures by the Thalamic Feed-Forward Inhibition."}, "29033810": {"mesh": [], "AbstractText": [{"section": null, "text": "Cooperation and competition, as two common and opposite examples of interpersonal dynamics, are thought to be reflected by different cognitive, neural, and behavioral patterns. According to the conventional approach, they have been explored by measuring subjects' reactions during individual performance or turn-based interactions in artificial settings, that don't allow on-line, ecological enactment of real-life social exchange. Considering the importance of these factors, and accounting for the complexity of such phenomena, the hyperscanning approach emerged as a multi-subject paradigm since it allows the simultaneous recording of the brain activity from multiple participants interacting. In this view, the present paper aimed at reviewing the most significant work about cooperation and competition by EEG hyperscanning technique, which proved to be a promising tool in capturing the sudden course of social interactions. In detail, the review will consider and group different experimental tasks that have been developed so far: (1) paradigms that used rhythm, music and motor synchronization; (2) card tasks taken from the Game Theory; (3) computerized tasks; and (4) possible real-life applications. Finally, although highlighting the potential contribution of such approach, some important limitations about these paradigms will be elucidated, with a specific focus on the emotional domain."}], "ArticleTitle": "Cooperation and Competition with Hyperscanning Methods: Review and Future Application to Emotion Domain."}, "29950982": {"mesh": [], "AbstractText": [{"section": null, "text": "Background: High-frequency Deep Brain Stimulation (DBS) of the subcallosal cingulate (SCC) region is an emerging strategy for treatment-resistant depression (TRD). This study examined changes in SCC local field potentials (LFPs). The LFPs were recorded from the DBS leads following transient, unilateral stimulation at the neuroimaging-defined optimal electrode contact. The goal was identifying a putative electrophysiological measure of target engagement during implantation. Methods: Fourteen consecutive patients underwent bilateral SCC DBS lead implantation. LFP recordings were collected from all electrodes during randomized testing of stimulation on each DBS contact (eight total). Analyses evaluated changes in spectral power before and after 3 min of unilateral stimulation at the contacts that later facilitated antidepressant response, as a potential biomarker of optimal contact selection in each hemisphere. Results: Lateralized and asymmetric power spectral density changes were detected in the SCC with acute unilateral SCC stimulation at those contacts subsequently selected for chronic, therapeutic stimulation. Left stimulation induced broadband ipsilateral decreases in theta, alpha, beta and gamma bands. Right stimulation effects were restricted to ipsilateral beta and gamma decreases. These asymmetric effects contrasted with identical white matter stimulation maps used in each hemisphere. More variable ipsilateral decreases were seen with stimulation at the adjacent \"suboptimal\" contacts, but changes were not statistically different from the \"optimal\" contact in either hemisphere despite obvious differences in impacted white matter bundles. Change in theta power was, however, most robust and specific with left-sided optimal stimulation, which suggested a putative functional biomarker on the left with no such specificity inferred on the right. Conclusion: Hemisphere-specific oscillatory changes can be detected from the DBS lead with acute intraoperative testing at contacts that later engender antidepressant effects. Our approach defined potential target engagement signals for further investigation, particularly left-sided theta decreases following initial exposure to stimulation. More refined models combining tractography, bilateral SCC LFP, and cortical recordings may further improve the precision and specificity of these putative biomarkers. It may also optimize and standardize the lead implantation procedure and provide input signals for next generation closed-loop therapy and/or monitoring technologies for TRD."}], "ArticleTitle": "Initial Unilateral Exposure to Deep Brain Stimulation in Treatment-Resistant Depression Patients Alters Spectral Power in the Subcallosal Cingulate."}, "31263407": {"mesh": [], "AbstractText": [{"section": null, "text": "It is often assumed that Hebbian synaptic plasticity forms a cell assembly, a mutually interacting group of neurons that encodes memory. However, in recurrently connected networks with pure Hebbian plasticity, cell assemblies typically diverge or fade under ongoing changes of synaptic strength. Previously assumed mechanisms that stabilize cell assemblies do not robustly reproduce the experimentally reported unimodal and long-tailed distribution of synaptic strengths. Here, we show that augmenting Hebbian plasticity with experimentally observed intrinsic spine dynamics can stabilize cell assemblies and reproduce the distribution of synaptic strengths. Moreover, we posit that strong intrinsic spine dynamics impair learning performance. Our theory explains how excessively strong spine dynamics, experimentally observed in several animal models of autism spectrum disorder, impair learning associations in the brain."}], "ArticleTitle": "Intrinsic Spine Dynamics Are Critical for Recurrent Network Learning in Models With and Without Autism Spectrum Disorder."}, "28744210": {"mesh": [], "AbstractText": [{"section": null, "text": "In temporal lobe epilepsy (TLE), the variation of chemical receptor expression underlies the basis of neural network activity shifts, resulting in neuronal hyperexcitability and epileptiform discharges. However, dynamical mechanisms involved in the transitions of TLE are not fully understood, because of the neuronal diversity and the indeterminacy of network connection. Hence, based on Hodgkin-Huxley (HH) type neurons and Pinsky-Rinzel (PR) type neurons coupling with glutamatergic and GABAergic synaptic connections respectively, we propose a computational framework which contains dentate gyrus (DG) region and CA3 region. By regulating the concentration range of N-methyl-D-aspartate-type glutamate receptor (NMDAR), we demonstrate the pyramidal neuron can generate transitions from interictal to seizure discharges. This suggests that enhanced endogenous activity of NMDAR contributes to excitability in pyramidal neuron. Moreover, we conclude that excitatory discharges in CA3 region vary considerably on account of the excitatory currents produced by the excitatory pyramidal neuron. Interestingly, by changing the backprojection connection, we find that glutamatergic type backprojection can promote the dominant frequency of firings and further motivate excitatory counterpropagation from CA3 region to DG region. However, GABAergic type backprojection can reduce firing rate and block morbid counterpropagation, which may be factored into the terminations of TLE. In addition, neuronal diversity dominated network shows weak correlation with different backprojections. Our modeling and simulation studies provide new insights into the mechanisms of seizures generation and connectionism in local hippocampus, along with the synaptic mechanisms of this disease."}], "ArticleTitle": "Transition Dynamics of a Dentate Gyrus-CA3 Neuronal Network during Temporal Lobe Epilepsy."}, "30828295": {"mesh": [], "AbstractText": [{"section": null, "text": "Autism spectrum disorder (ASD) is a developmental disorder, affecting about 1% of the global population. Currently, the only clinical method for diagnosing ASD are standardized ASD tests which require prolonged diagnostic time and increased medical costs. Our objective was to explore the predictive power of personal characteristic data (PCD) from a large well-characterized dataset to improve upon prior diagnostic models of ASD. We extracted six personal characteristics (age, sex, handedness, and three individual measures of IQ) from 851 subjects in the Autism Brain Imaging Data Exchange (ABIDE) database. ABIDE is an international collaborative project that collected data from a large number of ASD patients and typical non-ASD controls from 17 research and clinical institutes. We employed this publicly available database to test nine supervised machine learning models. We implemented a cross-validation strategy to train and test those machine learning models for classification between typical non-ASD controls and ASD patients. We assessed classification performance using accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC). Of the nine models we tested using six personal characteristics, the neural network model performed the best with a mean AUC (SD) of 0.646 (0.005), followed by k-nearest neighbor with a mean AUC (SD) of 0.641 (0.004). This study established an optimal ASD classification performance with PCD as features. With additional discriminative features (e.g., neuroimaging), machine learning models may ultimately enable automated clinical diagnosis of autism."}], "ArticleTitle": "Enhancing Diagnosis of Autism With Optimized Machine Learning Models and Personal Characteristic Data."}, "31024281": {"mesh": [], "AbstractText": [{"section": null, "text": "Stabilization of the CIP (Cart Inverted Pendulum) is an analogy to stick balancing on a finger and is an example of unstable tasks that humans face in everyday life. The difficulty of the task grows exponentially with the decrease of the length of the stick and a stick length of 32 cm is considered as a human limit even for well-trained subjects. Moreover, there is a cybernetic limit related to the delay of the multimodal sensory feedback (about 230 ms) that supports a feedback stabilization strategy. We previously demonstrated that an intermittent-feedback control paradigm, originally developed for modeling the stabilization of upright standing, can be applied with success also to the CIP system, but with values of the critical parameters far from the limiting ones (stick length 50 cm and feedback delay 100 ms). The intermittent control paradigm is based on the alternation of on-phases, driven by a proportional/derivative delayed feedback controller, and off-phases, where the feedback is switched off and the motion evolves according to the intrinsic dynamics of the CIP. In its standard formulation, the switching mechanism consists of a simple threshold operator: the feedback control is switched off if the current (delayed) state vector is closer to the stable than to the unstable manifold of the off-phase and is switched on in the opposite case. Although this simple formulation is effective for explaining upright standing as well as CIP balancing, it fails in the most challenging configuration of the CIP. In this work we propose a modification of the standard intermittent control policy that focuses on the explicit selection of switching times and is based on the phase reset of the estimated state vector at each switching time and on the simulation of an approximated internal model of CIP dynamics. We demonstrate, by simulating the modified intermittent control policy, that it can match the limits of human performance, while operating near the edge of instability."}], "ArticleTitle": "Stabilization of a Cart Inverted Pendulum: Improving the Intermittent Feedback Strategy to Match the Limits of Human Performance."}, "32714173": {"mesh": [], "AbstractText": [{"section": null, "text": "Sequence learning is a fundamental cognitive function of the brain. However, the ways in which sequential information is represented and memorized are not dealt with satisfactorily by existing models. To overcome this deficiency, this paper introduces a spiking neural network based on psychological and neurobiological findings at multiple scales. Compared with existing methods, our model has four novel features: (1) It contains several collaborative subnetworks similar to those in brain regions with different cognitive functions. The individual building blocks of the simulated areas are neural functional minicolumns composed of biologically plausible neurons. Both excitatory and inhibitory connections between neurons are modulated dynamically using a spike-timing-dependent plasticity learning rule. (2) Inspired by the mechanisms of the brain's cortical-striatal loop, a dependent timing module is constructed to encode temporal information, which is essential in sequence learning but has not been processed well by traditional algorithms. (3) Goal-based and episodic retrievals can be achieved at different time scales. (4) Musical memory is used as an application to validate the model. Experiments show that the model can store a huge amount of data on melodies and recall them with high accuracy. In addition, it can remember the entirety of a melody given only an episode or the melody played at different paces."}], "ArticleTitle": "Temporal-Sequential Learning With a Brain-Inspired Spiking Neural Network and Its Application to Musical Memory."}, "30042669": {"mesh": [], "AbstractText": [{"section": null, "text": "Oscillatory phenomena are ubiquitous in the brain. Although there are oscillator-based models of brain dynamics, their universal computational properties have not been explored much unlike in the case of rate-coded and spiking neuron network models. Use of oscillator-based models is often limited to special phenomena like locomotor rhythms and oscillatory attractor-based memories. If neuronal ensembles are taken to be the basic functional units of brain dynamics, it is desirable to develop oscillator-based models that can explain a wide variety of neural phenomena. Autoencoders are a special type of feed forward networks that have been used for construction of large-scale deep networks. Although autoencoders based on rate-coded and spiking neuron networks have been proposed, there are no autoencoders based on oscillators. We propose here an oscillatory neural network model that performs the function of an autoencoder. The model is a hybrid of rate-coded neurons and neural oscillators. Input signals modulate the frequency of the neural encoder oscillators. These signals are then multiplexed using a network of rate-code neurons that has afferent Hebbian and lateral anti-Hebbian connectivity, termed as Lateral Anti Hebbian Network (LAHN). Finally the LAHN output is de-multiplexed using an output neural layer which is a combination of adaptive Hopf and Kuramoto oscillators for the signal reconstruction. The Kuramoto-Hopf combination performing demodulation is a novel way of describing a neural phase-locked loop. The proposed model is tested using both synthetic signals and real world EEG signals. The proposed model arises out of the general motivation to construct biologically inspired, oscillatory versions of some of the standard neural network models, and presents itself as an autoencoder network based on oscillatory neurons applicable to time series signals. As a demonstration, the model is applied to compression of EEG signals."}], "ArticleTitle": "An Oscillatory Neural Autoencoder Based on Frequency Modulation and Multiplexing."}, "30123120": {"mesh": [], "AbstractText": [{"section": null, "text": "The human brain is a complex system composed by several large scale intrinsic networks with distinct functions. The low frequency oscillation (LFO) signal of blood oxygen level dependent (BOLD), measured through resting-state fMRI, reflects the spontaneous neural activity of these networks. We propose to characterize these networks by applying the multiple frequency bands analysis (MFBA) to the LFO time courses (TCs) resulted from the group independent component analysis (ICA). Specifically, seven networks, including the default model network (DMN), dorsal attention network (DAN), control executive network (CEN), salience network, sensorimotor network, visual network and limbic network, are identified. After the power spectral density (PSD) analysis, the amplitude of low frequency fluctuation (ALFF) and the fractional amplitude of low frequency fluctuation (fALFF) is determined in three bands: <0.1 Hz; slow-5; and slow-4. Moreover, the MFBA method is applied to reveal the frequency-dependent alternations of fALFF for seven networks in schizotypal personality disorder (SPD). It is found that seven networks can be divided into three categories: the advanced cognitive networks, primary sensorimotor networks and limbic networks, and their fALFF successively decreases in both slow-4 and slow-5 bands. Comparing to normal control group, the fALFF of DMN, DAN and CEN in SPD tends to be higher in slow-5 band, but lower in slow-4. Higher fALFF of sensorimotor and visual networks in slow-5, higher fALFF of limbic network in both bands have been observed for SPD group. The results of ALFF are consistent with those of fALFF. The proposed MFBA method may help distinguish networks or oscillators in the human brain, reveal subtle alternations of networks through locating their dominant frequency band, and present potential to interpret the neuropathology disruptions."}], "ArticleTitle": "Multiple Frequency Bands Analysis of Large Scale Intrinsic Brain Networks and Its Application in Schizotypal Personality Disorder."}, "31780915": {"mesh": [], "AbstractText": [{"section": null, "text": "Prediction of overall survival based on multimodal MRI of brain tumor patients is a difficult problem. Although survival also depends on factors that cannot be assessed via preoperative MRI such as surgical outcome, encouraging results for MRI-based survival analysis have been published for different datasets. We assess if and how established radiomic approaches as well as novel methods can predict overall survival of brain tumor patients on the BraTS challenge dataset. This dataset consists of multimodal preoperative images of 211 glioblastoma patients from several institutions with reported resection status and known survival. In the official challenge setting, only patients with a reported gross total resection (GTR) are taken into account. We therefore evaluated previously published methods as well as different machine learning approaches on the BraTS dataset. For different types of resection status, these approaches are compared to a baseline, a linear regression on patient age only. This naive approach won the 3rd place out of 26 participants in the BraTS survival prediction challenge 2018. Previously published radiomic signatures show significant correlations and predictiveness to patient survival for patients with a reported subtotal resection. However, for patients with reported GTR, none of the evaluated approaches was able to outperform the age-only baseline in a cross-validation setting, explaining the poor performance of approaches based on radiomics in the BraTS challenge 2018."}], "ArticleTitle": "Robustness of Radiomics for Survival Prediction of Brain Tumor Patients Depending on Resection Status."}, "29962943": {"mesh": [], "AbstractText": [{"section": null, "text": "Learning of hierarchical features with spiking neurons has mostly been investigated in the database framework of standard deep learning systems. However, the properties of neuromorphic systems could be particularly interesting for learning from continuous sensor data in real-world settings. In this work, we introduce a deep spiking convolutional neural network of integrate-and-fire (IF) neurons which performs unsupervised online deep learning with spike-timing dependent plasticity (STDP) from a stream of asynchronous and continuous event-based data. In contrast to previous approaches to unsupervised deep learning with spikes, where layers were trained successively, we introduce a mechanism to train all layers of the network simultaneously. This allows approximate online inference already during the learning process and makes our architecture suitable for online learning and inference. We show that it is possible to train the network without providing implicit information about the database, such as the number of classes and the duration of stimuli presentation. By designing an STDP learning rule which depends only on relative spike timings, we make our network fully event-driven and able to operate without defining an absolute timescale of its dynamics. Our architecture requires only a small number of generic mechanisms and therefore enforces few constraints on a possible neuromorphic hardware implementation. These characteristics make our network one of the few neuromorphic architecture which could directly learn features and perform inference from an event-based vision sensor."}], "ArticleTitle": "Event-Based, Timescale Invariant Unsupervised Online Deep Learning With STDP."}, "32372938": {"mesh": [], "AbstractText": [{"section": null, "text": "Traditionally, radiologists have crudely quantified tumor extent by measuring the longest and shortest dimension by dragging a cursor between opposite boundary points across a single image rather than full segmentation of the volumetric extent. For algorithmic-based volumetric segmentation, the degree of radiologist experiential involvement varies from confirming a fully automated segmentation, to making a single drag on an image to initiate semi-automated segmentation, to making multiple drags and clicks on multiple images during interactive segmentation. An experiment was designed to test an algorithm that allows various levels of interaction. Given the ground-truth of the BraTS training data, which delimits the brain tumors of 285 patients on multi-spectral MR, a computer simulation mimicked the process that a radiologist would follow to perform segmentation with real-time interaction. Clicks and drags were placed only where needed in response to the deviation between real-time segmentation results and assumed radiologist's goal, as provided by the ground-truth. Results of accuracy for various levels of interaction are presented along with estimated elapsed time, in order to measure efficiency. Average total elapsed time, including loading the study through confirming 3D contours, was 46 s."}], "ArticleTitle": "Measuring Efficiency of Semi-automated Brain Tumor Segmentation by Simulating User Interaction."}, "29085291": {"mesh": [], "AbstractText": [{"section": null, "text": "Early in development, neural systems have primarily excitatory coupling, where even GABAergic synapses are excitatory. Many of these systems exhibit spontaneous episodes of activity that have been characterized through both experimental and computational studies. As development progress the neural system goes through many changes, including synaptic remodeling, intrinsic plasticity in the ion channel expression, and a transformation of GABAergic synapses from excitatory to inhibitory. What effect each of these, and other, changes have on the network behavior is hard to know from experimental studies since they all happen in parallel. One advantage of a computational approach is that one has the ability to study developmental changes in isolation. Here, we examine the effects of GABAergic synapse polarity change on the spontaneous activity of both a mean field and a neural network model that has both glutamatergic and GABAergic coupling, representative of a developing neural network. We find some intuitive behavioral changes as the GABAergic neurons go from excitatory to inhibitory, shared by both models, such as a decrease in the duration of episodes. We also find some paradoxical changes in the activity that are only present in the neural network model. In particular, we find that during early development the inter-episode durations become longer on average, while later in development they become shorter. In addressing this unexpected finding, we uncover a priming effect that is particularly important for a small subset of neurons, called the \"intermediate neurons.\" We characterize these neurons and demonstrate why they are crucial to episode initiation, and why the paradoxical behavioral change result from priming of these neurons. The study illustrates how even arguably the simplest of developmental changes that occurs in neural systems can present non-intuitive behaviors. It also makes predictions about neural network behavioral changes that occur during development that may be observable even in actual neural systems where these changes are convoluted with changes in synaptic connectivity and intrinsic neural plasticity."}], "ArticleTitle": "The Effects of GABAergic Polarity Changes on Episodic Neural Network Activity in Developing Neural Systems."}, "27708572": {"mesh": [], "AbstractText": [{"section": null, "text": "The study of balanced networks of excitatory and inhibitory neurons has led to several open questions. On the one hand it is yet unclear whether the asynchronous state observed in the brain is autonomously generated, or if it results from the interplay between external drivings and internal dynamics. It is also not known, which kind of network variabilities will lead to irregular spiking and which to synchronous firing states. Here we show how isolated networks of purely excitatory neurons generically show asynchronous firing whenever a minimal level of structural variability is present together with a refractory period. Our autonomous networks are composed of excitable units, in the form of leaky integrators spiking only in response to driving currents, remaining otherwise quiet. For a non-uniform network, composed exclusively of excitatory neurons, we find a rich repertoire of self-induced dynamical states. We show in particular that asynchronous drifting states may be stabilized in purely excitatory networks whenever a refractory period is present. Other states found are either fully synchronized or mixed, containing both drifting and synchronized components. The individual neurons considered are excitable and hence do not dispose of intrinsic natural firing frequencies. An effective network-wide distribution of natural frequencies is however generated autonomously through self-consistent feedback loops. The asynchronous drifting state is, additionally, amenable to an analytic solution. We find two types of asynchronous activity, with the individual neurons spiking regularly in the pure drifting state, albeit with a continuous distribution of firing frequencies. The activity of the drifting component, however, becomes irregular in the mixed state, due to the periodic driving of the synchronized component. We propose a new tool for the study of chaos in spiking neural networks, which consists of an analysis of the time series of pairs of consecutive interspike intervals. In this space, we show that a strange attractor with a fractal dimension of about 1.8 is formed in the mentioned mixed state."}], "ArticleTitle": "Drifting States and Synchronization Induced Chaos in Autonomous Networks of Excitable Neurons."}, "31133837": {"mesh": [], "AbstractText": [{"section": null, "text": "Synaptic plasticity serves as an essential mechanism underlying cognitive processes as learning and memory. For a better understanding detailed theoretical models combine experimental underpinnings of synaptic plasticity and match experimental results. However, these models are mathematically complex impeding the comprehensive investigation of their link to cognitive processes generally executed on the neuronal network level. Here, we derive a mathematical framework enabling the simplification of such detailed models of synaptic plasticity facilitating further mathematical analyses. By this framework we obtain a compact, firing-rate-dependent mathematical formulation, which includes the essential dynamics of the detailed model and, thus, of experimentally verified properties of synaptic plasticity. Amongst others, by testing our framework by abstracting the dynamics of two well-established calcium-dependent synaptic plasticity models, we derived that the synaptic changes depend on the square of the presynaptic firing rate, which is in contrast to previous assumptions. Thus, the here-presented framework enables the derivation of biologically plausible but simple mathematical models of synaptic plasticity allowing to analyze the underlying dependencies of synaptic dynamics from neuronal properties such as the firing rate and to investigate their implications in complex neuronal networks."}], "ArticleTitle": "A Theoretical Framework to Derive Simple, Firing-Rate-Dependent Mathematical Models of Synaptic Plasticity."}, "29632482": {"mesh": [], "AbstractText": [{"section": null, "text": "Pain is a subjective experience that alerts an individual to actual or potential tissue damage. Through mechanisms that are still unclear, normal physiological pain can lose its adaptive value and evolve into pathological chronic neuropathic pain. Chronic pain is a multifaceted experience that can be understood in terms of somatosensory, affective, and cognitive dimensions, each with associated symptoms and neural signals. While there have been many attempts to treat chronic pain, in this article we will argue that feedback-controlled 'closed-loop' deep brain stimulation (DBS) offers an urgent and promising route for treatment. Contemporary DBS trials for chronic pain use \"open-loop\" approaches in which tonic stimulation is delivered with fixed parameters to a single brain region. The impact of key variables such as the target brain region and the stimulation waveform is unclear, and long-term efficacy has mixed results. We hypothesize that chronic pain is due to abnormal synchronization between brain networks encoding the somatosensory, affective and cognitive dimensions of pain, and that multisite, closed-loop DBS provides an intuitive mechanism for disrupting that synchrony. By (1) identifying biomarkers of the subjective pain experience and (2) integrating these signals into a state-space representation of pain, we can create a predictive model of each patient's pain experience. Then, by establishing how stimulation in different brain regions influences individual neural signals, we can design real-time, closed-loop therapies tailored to each patient. While chronic pain is a complex disorder that has eluded modern therapies, rich historical data and state-of-the-art technology can now be used to develop a promising treatment."}], "ArticleTitle": "Closed-Loop Deep Brain Stimulation for Refractory Chronic Pain."}, "30733673": {"mesh": [], "AbstractText": [{"section": null, "text": "Acceptance of novelty depends on the receiver's emotional state. This paper proposes a novel mathematical model for predicting emotions elicited by the novelty of an event under different conditions. It models two emotion dimensions, arousal and valence, and considers different uncertainty levels. A state transition from before experiencing an event to afterwards is assumed, and a Bayesian model estimates a posterior distribution as being proportional to the product of a prior distribution and a likelihood function. Our model uses Kullback-Leibler divergence of the posterior from the prior, which we termed information gain, to represent arousal levels because it corresponds to surprise, a high-arousal emotion, upon experiencing a novel event. Based on Berlyne's hedonic function, we formalized valence as a summation of reward and aversion systems that are modeled as sigmoid functions of information gain. We derived information gain as a function of prediction errors (i.e., differences between the mean of the posterior and the peak likelihood), uncertainty (i.e., variance of the prior that is proportional to prior entropy), and noise (i.e., variance of the likelihood function). This functional model predicted an interaction effect of prediction errors and uncertainty on information gain, which we termed the arousal crossover effect. This effect means that the greater the uncertainty, the greater the information gain for a small prediction error. However, for large prediction errors, greater uncertainty means a smaller information gain. To verify this effect, we conducted an experiment with participants who watched short videos in which different percussion instruments were played. We varied uncertainty levels by using familiar and unfamiliar instruments, and we varied prediction error magnitudes by including congruent or incongruent percussive sounds in the videos. Event-related potential P300 amplitudes and subjective reports of surprise in response to the percussive sounds were used as measures of arousal levels, and the findings supported the hypothesized arousal crossover effect. The concordance between our model's predictions and our experimental results suggests that Bayesian information gain can be decomposed into uncertainty and prediction errors and is a valid measure of emotional arousal. Our model's predictions of arousal may help identify positively accepted novelty."}], "ArticleTitle": "Modeling Emotions Associated With Novelty at Variable Uncertainty Levels: A Bayesian Approach."}, "28943847": {"mesh": [], "AbstractText": [{"section": null, "text": "Our daily interaction with the world is plagued of situations in which we develop expertise through self-motivated repetition of the same task. In many of these interactions, and especially when dealing with computer and machine interfaces, we must deal with sequences of decisions and actions. For instance, when drawing cash from an ATM machine, choices are presented in a step-by-step fashion and a specific sequence of choices must be performed in order to produce the expected outcome. But, as we become experts in the use of such interfaces, is it possible to identify specific search and learning strategies? And if so, can we use this information to predict future actions? In addition to better understanding the cognitive processes underlying sequential decision making, this could allow building adaptive interfaces that can facilitate interaction at different moments of the learning curve. Here we tackle the question of modeling sequential decision-making behavior in a simple human-computer interface that instantiates a 4-level binary decision tree (BDT) task. We record behavioral data from voluntary participants while they attempt to solve the task. Using a Hidden Markov Model-based approach that capitalizes on the hierarchical structure of behavior, we then model their performance during the interaction. Our results show that partitioning the problem space into a small set of hierarchically related stereotyped strategies can potentially capture a host of individual decision making policies. This allows us to follow how participants learn and develop expertise in the use of the interface. Moreover, using a Mixture of Experts based on these stereotyped strategies, the model is able to predict the behavior of participants that master the task."}], "ArticleTitle": "Modeling Search Behaviors during the Acquisition of Expertise in a Sequential Decision-Making Task."}, "32038211": {"mesh": [], "AbstractText": [{"section": null, "text": "The resting state fMRI time series appears to have cyclic patterns, which indicates presence of cyclic interactions between different brain regions. Such interactions are not easily captured by pre-established resting state functional connectivity methods including zero-lag correlation, lagged correlation, and dynamic time warping distance. These methods formulate the functional interaction between different brain regions as similar temporal patterns within the time series. To use information related to temporal ordering, cyclicity analysis has been introduced to capture pairwise interactions between multiple time series. In this study, we compared the efficacy of cyclicity analysis with aforementioned similarity-based techniques in representing individual-level and group-level information. Additionally, we investigated how filtering and global signal regression interacted with these techniques. We obtained and analyzed fMRI data from patients with tinnitus and neurotypical controls at two different days, a week apart. For both patient and control groups, we found that the features generated by cyclicity and correlation (zero-lag and lagged) analyses were more reliable than the features generated by dynamic time warping distance in identifying individuals across visits. The reliability of all features, except those generated by dynamic time warping, improved as the global signal was regressed. Nevertheless, removing fluctuations >0.1 Hz deteriorated the reliability of all features. These observations underscore the importance of choosing appropriate preprocessing steps while evaluating different analytical methods in describing resting state functional interactivity. Further, using different machine learning techniques including support vector machines, discriminant analyses, and convolutional neural networks, our results revealed that the manifestation of the group-level information within all features was not sufficient enough to dissociate tinnitus patients from controls with high sensitivity and specificity. This necessitates further investigation regarding the representation of group-level information within different features to better identify tinnitus-related alternation in the functional organization of the brain. Our study adds to the growing body of research on developing diagnostic tools to identify neurological disorders, such as tinnitus, using resting state fMRI data."}], "ArticleTitle": "Comparing Cyclicity Analysis With Pre-established Functional Connectivity Methods to Identify Individuals and Subject Groups Using Resting State fMRI."}, "33343322": {"mesh": [], "AbstractText": [{"section": null, "text": "Electromyography (EMG)-driven musculoskeletal modeling relies on high-quality measurements of muscle electrical activity to estimate muscle forces. However, a critical challenge for practical deployment of this approach is missing EMG data from muscles that contribute substantially to joint moments. This situation may arise due to either the inability to measure deep muscles with surface electrodes or the lack of a sufficient number of EMG channels. Muscle synergy analysis (MSA) is a dimensionality reduction approach that decomposes a large number of muscle excitations into a small number of time-varying synergy excitations along with time-invariant synergy weights that define the contribution of each synergy excitation to all muscle excitations. This study evaluates how well missing muscle excitations can be predicted using synergy excitations extracted from muscles with available EMG data (henceforth called \"synergy extrapolation\" or SynX). The method was evaluated using a gait data set collected from a stroke survivor walking on an instrumented treadmill at self-selected and fastest-comfortable speeds. The evaluation process started with full calibration of a lower-body EMG-driven model using 16 measured EMG channels (collected using surface and fine wire electrodes) per leg. One fine wire EMG channel (either iliopsoas or adductor longus) was then treated as unmeasured. The synergy weights associated with the unmeasured muscle excitation were predicted by solving a nonlinear optimization problem where the errors between inverse dynamics and EMG-driven joint moments were minimized. The prediction process was performed for different synergy analysis algorithms (principal component analysis and non-negative matrix factorization), EMG normalization methods, and numbers of synergies. SynX performance was most influenced by the choice of synergy analysis algorithm and number of synergies. Principal component analysis with five or six synergies consistently predicted unmeasured muscle excitations the most accurately and with the greatest robustness to EMG normalization method. Furthermore, the associated joint moment matching accuracy was comparable to that produced by initial EMG-driven model calibration using all 16 EMG channels per leg. SynX may facilitate the assessment of human neuromuscular control and biomechanics when important EMG signals are missing."}], "ArticleTitle": "Evaluation of Synergy Extrapolation for Predicting Unmeasured Muscle Excitations from Measured Muscle Synergies."}, "32210780": {"mesh": [], "AbstractText": [{"section": null, "text": "Automatic segmentation of Multiple Sclerosis (MS) lesions from Magnetic Resonance Imaging (MRI) images is essential for clinical assessment and treatment planning of MS. Recent years have seen an increasing use of Convolutional Neural Networks (CNNs) for this task. Although these methods provide accurate segmentation, their applicability in clinical settings remains limited due to a reproducibility issue across different image domains. MS images can have highly variable characteristics across patients, MRI scanners and imaging protocols; retraining a supervised model with data from each new domain is not a feasible solution because it requires manual annotation from expert radiologists. In this work, we explore an unsupervised solution to the problem of domain shift. We present a framework, Seg-JDOT, which adapts a deep model so that samples from a source domain and samples from a target domain sharing similar representations will be similarly segmented. We evaluated the framework on a multi-site dataset, MICCAI 2016, and showed that the adaptation toward a target site can bring remarkable improvements in a model performance over standard training."}], "ArticleTitle": "Unsupervised Domain Adaptation With Optimal Transport in Multi-Site Segmentation of Multiple Sclerosis Lesions From MRI Data."}, "32754023": {"mesh": [], "AbstractText": [{"section": null, "text": "Electrical excitation of neural tissue has wide applications, but how electrical stimulation interacts with neural tissue remains to be elucidated. Here, we propose a new theory, named the Circuit-Probability theory, to reveal how this physical interaction happen. The relation between the electrical stimulation input and the neural response can be theoretically calculated. We show that many empirical models, including strength-duration relationship and linear-non-linear-Poisson model, can be theoretically explained, derived, and amended using our theory. Furthermore, this theory can explain the complex non-linear and resonant phenomena and fit in vivo experiment data. In this letter, we validated an entirely new framework to study electrical stimulation on neural tissue, which is to simulate voltage waveforms using a parallel RLC circuit first, and then calculate the excitation probability stochastically."}], "ArticleTitle": "Unveiling Stimulation Secrets of Electrical Excitation of Neural Tissue Using a Circuit Probability Theory."}, "33281591": {"mesh": [], "AbstractText": [{"section": null, "text": "Spiking Neural Networks (SNNs) are considered as the third generation of artificial neural networks, which are more closely with information processing in biological brains. However, it is still a challenge for how to train the non-differential SNN efficiently and robustly with the form of spikes. Here we give an alternative method to train SNNs by biologically-plausible structural and functional inspirations from the brain. Firstly, inspired by the significant top-down structural connections, a global random feedback alignment is designed to help the SNN propagate the error target from the output layer directly to the previous few layers. Then inspired by the local plasticity of the biological system in which the synapses are more tuned by the neighborhood neurons, a differential STDP is used to optimize local plasticity. Extensive experimental results on the benchmark MNIST (98.62%) and Fashion MNIST (89.05%) have shown that the proposed algorithm performs favorably against several state-of-the-art SNNs trained with backpropagation."}], "ArticleTitle": "GLSNN: A Multi-Layer Spiking Neural Network Based on Global Feedback Alignment and Local STDP Plasticity."}, "31920607": {"mesh": [], "AbstractText": [{"section": null, "text": "Neural spike train analysis methods are mainly used for understanding the temporal aspects of neural information processing. One approach is to measure the dissimilarity between the spike trains of a pair of neurons, often referred to as the spike train distance. The spike train distance has been often used to classify neuronal units with similar temporal patterns. Several methods to compute spike train distance have been developed so far. Intuitively, a desirable distance should be the shortest length between two objects. The Earth Mover's Distance (EMD) can compute spike train distance by measuring the shortest length between two spike trains via shifting a fraction of spikes from one spike train to another. The EMD could accurately measure spike timing differences, temporal similarity, and spikes time synchrony. It is also robust to firing rate changes. Victor and Purpura (1996) distance measures the minimum cost between two spike trains. Although it also measures the shortest path between spike trains, its output can vary with the time-scale parameter. In contrast, the EMD measures distance in a unique way by calculating the genuine shortest length between spike trains. The EMD also outperforms other existing spike train distance methods in measuring various aspects of the temporal characteristics of spike trains and in robustness to firing rate changes. The EMD can effectively measure the shortest length between spike trains without being considerably affected by the overall firing rate difference between them. Hence, it is suitable for pure temporal coding exclusively, which is a predominant premise underlying the present study."}], "ArticleTitle": "A Spike Train Distance Robust to Firing Rate Changes Based on the Earth Mover's Distance."}, "28522969": {"mesh": [], "AbstractText": [{"section": null, "text": "We introduce Equilibrium Propagation, a learning framework for energy-based models. It involves only one kind of neural computation, performed in both the first phase (when the prediction is made) and the second phase of training (after the target or prediction error is revealed). Although this algorithm computes the gradient of an objective function just like Backpropagation, it does not need a special computation or circuit for the second phase, where errors are implicitly propagated. Equilibrium Propagation shares similarities with Contrastive Hebbian Learning and Contrastive Divergence while solving the theoretical issues of both algorithms: our algorithm computes the gradient of a well-defined objective function. Because the objective function is defined in terms of local perturbations, the second phase of Equilibrium Propagation corresponds to only nudging the prediction (fixed point or stationary distribution) toward a configuration that reduces prediction error. In the case of a recurrent multi-layer supervised network, the output units are slightly nudged toward their target in the second phase, and the perturbation introduced at the output layer propagates backward in the hidden layers. We show that the signal \"back-propagated\" during this second phase corresponds to the propagation of error derivatives and encodes the gradient of the objective function, when the synaptic update corresponds to a standard form of spike-timing dependent plasticity. This work makes it more plausible that a mechanism similar to Backpropagation could be implemented by brains, since leaky integrator neural computation performs both inference and error back-propagation in our model. The only local difference between the two phases is whether synaptic changes are allowed or not. We also show experimentally that multi-layer recurrently connected networks with 1, 2, and 3 hidden layers can be trained by Equilibrium Propagation on the permutation-invariant MNIST task."}], "ArticleTitle": "Equilibrium Propagation: Bridging the Gap between Energy-Based Models and Backpropagation."}, "33328946": {"mesh": [], "AbstractText": [{"section": null, "text": "Deep neural networks (DNNs) have attained human-level performance on dozens of challenging tasks via an end-to-end deep learning strategy. Deep learning allows data representations that have multiple levels of abstraction; however, it does not explicitly provide any insights into the internal operations of DNNs. Deep learning's success is appealing to neuroscientists not only as a method for applying DNNs to model biological neural systems but also as a means of adopting concepts and methods from cognitive neuroscience to understand the internal representations of DNNs. Although general deep learning frameworks, such as PyTorch and TensorFlow, could be used to allow such cross-disciplinary investigations, the use of these frameworks typically requires high-level programming expertise and comprehensive mathematical knowledge. A toolbox specifically designed as a mechanism for cognitive neuroscientists to map both DNNs and brains is urgently needed. Here, we present DNNBrain, a Python-based toolbox designed for exploring the internal representations of DNNs as well as brains. Through the integration of DNN software packages and well-established brain imaging tools, DNNBrain provides application programming and command line interfaces for a variety of research scenarios. These include extracting DNN activation, probing and visualizing DNN representations, and mapping DNN representations onto the brain. We expect that our toolbox will accelerate scientific research by both applying DNNs to model biological neural systems and utilizing paradigms of cognitive neuroscience to unveil the black box of DNNs."}], "ArticleTitle": "DNNBrain: A Unifying Toolbox for Mapping Deep Neural Networks and Brains."}, "33123000": {"mesh": [], "AbstractText": [{"section": null, "text": "Introduction: The early and therapy-specific prediction of treatment success in major depressive disorder is of paramount importance due to high lifetime prevalence, and heterogeneity of response to standard medication and symptom expression. Hence, this study assessed the predictability of long-term antidepressant effects of escitalopram based on the short-term influence of citalopram on functional connectivity. Methods: Twenty nine subjects suffering from major depression were scanned twice with resting-state functional magnetic resonance imaging under the influence of intravenous citalopram and placebo in a randomized, double-blinded cross-over fashion. Symptom factors were identified for the Hamilton depression rating scale (HAM-D) and Beck's depression inventory (BDI) taken before and after a median of seven weeks of escitalopram therapy. Predictors were calculated from whole-brain functional connectivity, fed into robust regression models, and cross-validated. Results: Significant predictive power could be demonstrated for one HAM-D factor describing insomnia and the total score (r = 0.45-0.55). Remission and response could furthermore be predicted with an area under the receiver operating characteristic curve of 0.73 and 0.68, respectively. Functional regions with high influence on the predictor were located especially in the ventral attention, fronto-parietal, and default mode networks. Conclusion: It was shown that medication-specific antidepressant symptom improvements can be predicted using functional connectivity measured during acute pharmacological challenge as an easily assessable imaging marker. The regions with high influence have previously been related to major depression as well as the response to selective serotonin reuptake inhibitors, corroborating the advantages of the current approach of focusing on treatment-specific symptom improvements."}], "ArticleTitle": "Predicting Antidepressant Citalopram Treatment Response via Changes in Brain Functional Connectivity After Acute Intravenous Challenge."}, "31920609": {"mesh": [], "AbstractText": [{"section": null, "text": "An important challenge in segmenting real-world biomedical imaging data is the presence of multiple disease processes within individual subjects. Most adults above age 60 exhibit a variable degree of small vessel ischemic disease, as well as chronic infarcts, which will manifest as white matter hyperintensities (WMH) on brain MRIs. Subjects diagnosed with gliomas will also typically exhibit some degree of abnormal T2 signal due to WMH, rather than just due to tumor. We sought to develop a fully automated algorithm to distinguish and quantify these distinct disease processes within individual subjects' brain MRIs. To address this multi-disease problem, we trained a 3D U-Net to distinguish between abnormal signal arising from tumors vs. WMH in the 3D multi-parametric MRI (mpMRI, i.e., native T1-weighted, T1-post-contrast, T2, T2-FLAIR) scans of the International Brain Tumor Segmentation (BraTS) 2018 dataset (n training = 285, n validation = 66). Our trained neuroradiologist manually annotated WMH on the BraTS training subjects, finding that 69% of subjects had WMH. Our 3D U-Net model had a 4-channel 3D input patch (80 &#215; 80 &#215; 80) from mpMRI, four encoding and decoding layers, and an output of either four [background, active tumor (AT), necrotic core (NCR), peritumoral edematous/infiltrated tissue (ED)] or five classes (adding WMH as the fifth class). For both the four- and five-class output models, the median Dice for whole tumor (WT) extent (i.e., union of AT, ED, NCR) was 0.92 in both training and validation sets. Notably, the five-class model achieved significantly (p = 0.002) lower/better Hausdorff distances for WT extent in the training subjects. There was strong positive correlation between manually segmented and predicted volumes for WT (r = 0.96) and WMH (r = 0.89). Larger lesion volumes were positively correlated with higher/better Dice scores for WT (r = 0.33), WMH (r = 0.34), and across all lesions (r = 0.89) on a log(10) transformed scale. While the median Dice for WMH was 0.42 across training subjects with WMH, the median Dice was 0.62 for those with at least 5 cm3 of WMH. We anticipate the development of computational algorithms that are able to model multiple diseases within a single subject will be a critical step toward translating and integrating artificial intelligence systems into the heterogeneous real-world clinical workflow."}], "ArticleTitle": "Multi-Disease Segmentation of Gliomas and White Matter Hyperintensities in the BraTS Data Using a 3D Convolutional Neural Network."}, "29163117": {"mesh": [], "AbstractText": [{"section": null, "text": "Visual information in the visual cortex is processed in a hierarchical manner. Recent studies show that higher visual areas, such as V2, V3, and V4, respond more vigorously to images with naturalistic higher-order statistics than to images lacking them. This property is a functional signature of higher areas, as it is much weaker or even absent in the primary visual cortex (V1). However, the mechanism underlying this signature remains elusive. We studied this problem using computational models. In several typical hierarchical visual models including the AlexNet, VggNet, and SHMAX, this signature was found to be prominent in higher layers but much weaker in lower layers. By changing both the model structure and experimental settings, we found that the signature strongly correlated with sparse firing of units in higher layers but not with any other factors, including model structure, training algorithm (supervised or unsupervised), receptive field size, and property of training stimuli. The results suggest an important role of sparse neuronal activity underlying this special feature of higher visual areas."}], "ArticleTitle": "Deep Learning Predicts Correlation between a Functional Signature of Higher Visual Areas and Sparse Firing of Neurons."}, "31105544": {"mesh": [], "AbstractText": [{"section": null, "text": "Objective: Stimulus visual patterns, such as size, content, color, luminosity, and interval, play key roles for brain-computer interface (BCI) performance. However, the three primary colors to be intercompared as a single variable or factor on the same platform are poorly studied. In this work, we configured the visual stimulus patterns with red, green, and blue operating on a newly designed layout of the flash pattern of BCI to study the waveforms and performance of the evoked related potential (ERP). Approach: Twelve subjects participated in our experiment, and each subject was required to finish three different color sub-experiments. Four blocks of the interface were presented along the edge of the screen, and the other four were assembled in the center, aiming to investigate the problem of adjacency distraction. Repeated-measures ANOVA and Bonferroni correction were applied for statistical analysis. Main results: The averaged online accuracy was 98.44% for the red paradigm, higher than 92.71% for the green paradigm, and 93.23% for the blue paradigm. Furthermore, significant differences in online accuracy (p < 0.05) and information transfer rate (p < 0.05) were found between the red and green paradigms. Significance: The red stimulus paradigm yielded the best performance. The proposed design of ERP-based BCI was practical and effective for many potential applications."}], "ArticleTitle": "Investigation of Visual Stimulus With Various Colors and the Layout for the Oddball Paradigm in Evoked Related Potential-Based Brain-Computer Interface."}, "27812332": {"mesh": [], "AbstractText": [{"section": null, "text": "Response latency has been suggested as a possible source of information in the central nervous system when fast decisions are required. The accuracy of latency codes was studied in the past using a simplified readout algorithm termed the temporal-winner-take-all (tWTA). The tWTA is a competitive readout algorithm in which populations of neurons with a similar decision preference compete, and the algorithm selects according to the preference of the population that reaches the decision threshold first. It has been shown that this algorithm can account for accurate decisions among a small number of alternatives during short biologically relevant time periods. However, one of the major points of criticism of latency codes has been that it is unclear how can such a readout be implemented by the central nervous system. Here we show that the solution to this long standing puzzle may be rather simple. We suggest a mechanism that is based on reciprocal inhibition architecture, similar to that of the conventional winner-take-all, and show that under a wide range of parameters this mechanism is sufficient to implement the tWTA algorithm. This is done by first analyzing a rate toy model, and demonstrating its ability to discriminate short latency differences between its inputs. We then study the sensitivity of this mechanism to fine-tuning of its initial conditions, and show that it is robust to wide range of noise levels in the initial conditions. These results are then generalized to a Hodgkin-Huxley type of neuron model, using numerical simulations. Latency codes have been criticized for requiring a reliable stimulus-onset detection mechanism as a reference for measuring latency. Here we show that this frequent assumption does not hold, and that, an additional onset estimator is not needed to trigger this simple tWTA mechanism."}], "ArticleTitle": "A Readout Mechanism for Latency Codes."}, "32132913": {"mesh": [], "AbstractText": [{"section": null, "text": "Purpose: Gliomas are the most common primary brain malignancies, with varying degrees of aggressiveness and prognosis. Understanding of tumor biology and intra-tumor heterogeneity is necessary for planning personalized therapy and predicting response to therapy. Accurate tumoral and intra-tumoral segmentation on MRI is the first step toward understanding the tumor biology through computational methods. The purpose of this study was to design a segmentation algorithm and evaluate its performance on pre-treatment brain MRIs obtained from patients with gliomas. Materials and Methods: In this study, we have designed a novel 3D U-Net architecture that segments various radiologically identifiable sub-regions like edema, enhancing tumor, and necrosis. Weighted patch extraction scheme from the tumor border regions is proposed to address the problem of class imbalance between tumor and non-tumorous patches. The architecture consists of a contracting path to capture context and the symmetric expanding path that enables precise localization. The Deep Convolutional Neural Network (DCNN) based architecture is trained on 285 patients, validated on 66 patients and tested on 191 patients with Glioma from Brain Tumor Segmentation (BraTS) 2018 challenge dataset. Three dimensional patches are extracted from multi-channel BraTS training dataset to train 3D U-Net architecture. The efficacy of the proposed approach is also tested on an independent dataset of 40 patients with High Grade Glioma from our tertiary cancer center. Segmentation results are assessed in terms of Dice Score, Sensitivity, Specificity, and Hausdorff 95 distance (ITCN intra-tumoral classification network). Result: Our proposed architecture achieved Dice scores of 0.88, 0.83, and 0.75 for the whole tumor, tumor core and enhancing tumor, respectively, on BraTS validation dataset and 0.85, 0.77, 0.67 on test dataset. The results were similar on the independent patients' dataset from our hospital, achieving Dice scores of 0.92, 0.90, and 0.81 for the whole tumor, tumor core and enhancing tumor, respectively. Conclusion: The results of this study show the potential of patch-based 3D U-Net for the accurate intra-tumor segmentation. From experiments, it is observed that the weighted patch-based segmentation approach gives comparable performance with the pixel-based approach when there is a thin boundary between tumor subparts."}], "ArticleTitle": "A Novel Approach for Fully Automatic Intra-Tumor Segmentation With 3D U-Net Architecture for Gliomas."}, "29636675": {"mesh": [], "AbstractText": [{"section": null, "text": "Adaptation refers to the general phenomenon that the neural system dynamically adjusts its response property according to the statistics of external inputs. In response to an invariant stimulation, neuronal firing rates first increase dramatically and then decrease gradually to a low level close to the background activity. This prompts a question: during the adaptation, how does the neural system encode the repeated stimulation with attenuated firing rates? It has been suggested that the neural system may employ a dynamical encoding strategy during the adaptation, the information of stimulus is mainly encoded by the strong independent spiking of neurons at the early stage of the adaptation; while the weak but synchronized activity of neurons encodes the stimulus information at the later stage of the adaptation. The previous study demonstrated that short-term facilitation (STF) of electrical synapses, which increases the synchronization between neurons, can provide a mechanism to realize dynamical encoding. In the present study, we further explore whether short-term plasticity (STP) of chemical synapses, an interaction form more common than electrical synapse in the cortex, can support dynamical encoding. We build a large-size network with chemical synapses between neurons. Notably, facilitation of chemical synapses only enhances pair-wise correlations between neurons mildly, but its effect on increasing synchronization of the network can be significant, and hence it can serve as a mechanism to convey the stimulus information. To read-out the stimulus information, we consider that a downstream neuron receives balanced excitatory and inhibitory inputs from the network, so that the downstream neuron only responds to synchronized firings of the network. Therefore, the response of the downstream neuron indicates the presence of the repeated stimulation. Overall, our study demonstrates that STP of chemical synapse can serve as a mechanism to realize dynamical neural encoding. We believe that our study shed lights on the mechanism underlying the efficient neural information processing via adaptation."}], "ArticleTitle": "Dynamic Information Encoding With Dynamic Synapses in Neural Adaptation."}, "28912707": {"mesh": [], "AbstractText": [{"section": null, "text": "Muscle synergy analysis (MSA) is a mathematical technique that reduces the dimensionality of electromyographic (EMG) data. Used increasingly in biomechanics research, MSA requires methodological choices at each stage of the analysis. Differences in methodological steps affect the overall outcome, making it difficult to compare results across studies. We applied MSA to EMG data collected from individuals post-stroke identified as either responders (RES) or non-responders (nRES) on the basis of a critical post-treatment increase in walking speed. Importantly, no clinical or functional indicators identified differences between the cohort of RES and nRES at baseline. For this exploratory study, we selected the five highest RES and five lowest nRES available from a larger sample. Our goal was to assess how the methodological choices made before, during, and after MSA affect the ability to differentiate two groups with intrinsic physiologic differences based on MSA results. We investigated 30 variations in MSA methodology to determine which choices allowed differentiation of RES from nRES at baseline. Trial-to-trial variability in time-independent synergy vectors (SVs) and time-varying neural commands (NCs) were measured as a function of: (1) number of synergies computed; (2) EMG normalization method before MSA; (3) whether SVs were held constant across trials or allowed to vary during MSA; and (4) synergy analysis output normalization method after MSA. MSA methodology had a strong effect on our ability to differentiate RES from nRES at baseline. Across all 10 individuals and MSA variations, two synergies were needed to reach an average of 90% variance accounted for (VAF). Based on effect sizes, differences in SV and NC variability between groups were greatest using two synergies with SVs that varied from trial-to-trial. Differences in SV variability were clearest using unit magnitude per trial EMG normalization, while NC variability was less sensitive to EMG normalization method. No outcomes were greatly impacted by output normalization method. MSA variability for some, but not all, methods successfully differentiated intrinsic physiological differences inaccessible to traditional clinical or biomechanical assessments. Our results were sensitive to methodological choices, highlighting the need for disclosure of all aspects of MSA methodology in future studies."}], "ArticleTitle": "Methodological Choices in Muscle Synergy Analysis Impact Differentiation of Physiological Characteristics Following Stroke."}, "30254580": {"mesh": [], "AbstractText": [{"section": null, "text": "The emergence of motion sensors as a tool that provides objective motor performance data on individuals afflicted with Parkinson's disease offers an opportunity to expand the horizon of clinical care for this neurodegenerative condition. Subjective clinical scales and patient based motor diaries have limited clinometric properties and produce a glimpse rather than continuous real time perspective into motor disability. Furthermore, the expansion of machine learn algorithms is yielding novel classification and probabilistic clinical models that stand to change existing treatment paradigms, refine the application of advance therapeutics, and may facilitate the development and testing of disease modifying agents for this disease. We review the use of inertial sensors and machine learning algorithms in Parkinson's disease."}], "ArticleTitle": "Optimizing Clinical Assessments in Parkinson's Disease Through the Use of Wearable Sensors and Data Driven Modeling."}, "29163116": {"mesh": [], "AbstractText": [{"section": null, "text": "Balance control models are used to describe balance behavior in health and disease. We identified the unique contribution and relative importance of each parameter of a commonly used balance control model, the Independent Channel (IC) model, to identify which parameters are crucial to describe balance behavior. The balance behavior was expressed by transfer functions (TFs), representing the relationship between sensory perturbations and body sway as a function of frequency, in terms of amplitude (i.e., magnitude) and timing (i.e., phase). The model included an inverted pendulum controlled by a neuromuscular system, described by several parameters. Local sensitivity of each parameter was determined for both the magnitude and phase using partial derivatives. Both the intrinsic stiffness and proportional gain shape the magnitude at low frequencies (0.1-1 Hz). The derivative gain shapes the peak and slope of the magnitude between 0.5 and 0.9 Hz. The sensory weight influences the overall magnitude, and does not have any effect on the phase. The effect of the time delay becomes apparent in the phase above 0.6 Hz. The force feedback parameters and intrinsic stiffness have a small effect compared with the other parameters. All parameters shape the TF magnitude and phase and therefore play a role in the balance behavior. The sensory weight, time delay, derivative gain, and the proportional gain have a unique effect on the TFs, while the force feedback parameters and intrinsic stiffness contribute less. More insight in the unique contribution and relative importance of all parameters shows which parameters are crucial and critical to identify underlying differences in balance behavior between different patient groups."}], "ArticleTitle": "A Sensitivity Analysis of an Inverted Pendulum Balance Control Model."}, "27833545": {"mesh": [], "AbstractText": [{"section": null, "text": "Objectives: Accurate localization of epileptogenic zones (EZs) is essential for successful surgical treatment of refractory focal epilepsy. The aim of the present study is to investigate whether a dynamic network connectivity analysis based on stereo-electroencephalography (SEEG) signals is effective in localizing EZs. Methods: SEEG data were recorded from seven patients who underwent presurgical evaluation for the treatment of refractory focal epilepsy and for whom the subsequent resective surgery gave a good outcome. A time-variant multivariate autoregressive model was constructed using a Kalman filter, and the time-variant partial directed coherence was computed. This was then used to construct a dynamic directed network model of the epileptic brain. Three graph measures (in-degree, out-degree, and betweenness centrality) were used to analyze the characteristics of the dynamic network and to find the important nodes in it. Results: In all seven patients, the indicative EZs localized by the in-degree and the betweenness centrality were highly consistent with the clinically diagnosed EZs. However, the out-degree did not indicate any significant differences between nodes in the network. Conclusions: In this work, a method based on ictal SEEG signals and effective connectivity analysis localized EZs accurately. The results suggest that the in-degree and betweenness centrality may be better network characteristics to localize EZs than the out-degree."}], "ArticleTitle": "Dynamic Network Connectivity Analysis to Identify Epileptogenic Zones Based on Stereo-Electroencephalography."}, "27536230": {"mesh": [], "AbstractText": [{"section": null, "text": "Over the last decade robotics has attracted a great deal of interest from teachers and researchers as a valuable educational tool from preschool to highschool levels. The implementation of social-support behaviors in robot tutors, in particular in the emotional dimension, can make a significant contribution to learning efficiency. With the aim of contributing to the rising field of affective robot tutors we have developed ARTIE (Affective Robot Tutor Integrated Environment). We offer an architectural pattern which integrates any given educational software for primary school children with a component whose function is to identify the emotional state of the students who are interacting with the software, and with the driver of a robot tutor which provides personalized emotional pedagogical support to the students. In order to support the development of affective robot tutors according to the proposed architecture, we also provide a methodology which incorporates a technique for eliciting pedagogical knowledge from teachers, and a generic development platform. This platform contains a component for identiying emotional states by analysing keyboard and mouse interaction data, and a generic affective pedagogical support component which specifies the affective educational interventions (including facial expressions, body language, tone of voice,&#8230;) in terms of BML (a Behavior Model Language for virtual agent specification) files which are translated into actions of a robot tutor. The platform and the methodology are both adapted to primary school students. Finally, we illustrate the use of this platform to build a prototype implementation of the architecture, in which the educational software is instantiated with Scratch and the robot tutor with NAO. We also report on a user experiment we carried out to orient the development of the platform and of the prototype. We conclude from our work that, in the case of primary school students, it is possible to identify, without using intrusive and expensive identification methods, the emotions which most affect the character of educational interventions. Our work also demonstrates the feasibility of a general-purpose architecture of decoupled components, in which a wide range of educational software and robot tutors can be integrated and then used according to different educational criteria. "}], "ArticleTitle": "ARTIE: An Integrated Environment for the Development of Affective Robot Tutors."}, "30760994": {"mesh": [], "AbstractText": [{"section": null, "text": "Many organisms can time intervals flexibly on average with high accuracy but substantial variability between the trials. One of the core psychophysical features of interval timing functions relates to the signatures of this timing variability; for a given individual, the standard deviation of timed responses/time estimates is nearly proportional to their central tendency (scalar property). Many studies have aimed at elucidating the neural basis of interval timing based on the neurocomputational principles in a fashion that would explain the scalar property. Recent experimental evidence shows that there is indeed a specialized neural system for timekeeping. This system, referred to as the \"time cells,\" is composed of a group of neurons that fire sequentially as a function of elapsed time. Importantly, the time interval between consecutively firing time cell ensembles has been shown to increase with more elapsed time. However, when the subjective time is calculated by adding the distributions of time intervals between these sequentially firing time cell ensembles, the standard deviation would be compressed by the square root function. In light of this information the question becomes, \"How should the signaling between the sequentially firing time cell ensembles be for the resulting variability to increase linearly with time as required by the scalar property?\" We developed a simplified model of time cells that offers a mechanism for the synaptic communication of the sequentially firing neurons to address this ubiquitous property of interval timing. The model is composed of a single layer of time cells formulated in the form of integrate-and-fire neurons with feed-forward excitatory connections. The resulting behavior is simple neural wave activity. When this model is simulated with noisy conductances, the standard deviation of the time cell spike times increases proportionally to the mean of the spike-times. We demonstrate that this statistical property of the model outcomes is robustly observed even when the values of the key model parameters are varied."}], "ArticleTitle": "A Simplified Model of Communication Between Time Cells: Accounting for the Linearly Increasing Timing Imprecision."}, "30618690": {"mesh": [], "AbstractText": [{"section": null, "text": "Background: Convolution neural networks (CNN) is increasingly used in computer science and finds more and more applications in different fields. However, analyzing brain network with CNN is not trivial, due to the non-Euclidean characteristics of brain network built by graph theory. Method: To address this problem, we used a famous algorithm \"word2vec\" from the field of natural language processing (NLP), to represent the vertexes of graph in the node embedding space, and transform the brain network into images, which can bridge the gap between brain network and CNN. Using this model, we analyze and classify the brain network from Magnetoencephalography (MEG) data into two categories: normal controls and patients with migraine. Results: In the experiments, we applied our method on the clinical MEG dataset, and got the mean classification accuracy rate 81.25%. Conclusions: These results indicate that our method can feasibly analyze and classify the brain network, and all the abundant resources of CNN can be used on the analysis of brain network."}], "ArticleTitle": "Brain Network Analysis and Classification Based on Convolutional Neural Network."}, "30622466": {"mesh": [], "AbstractText": [{"section": null, "text": "Hough transform (HT) is one of the most well-known techniques in computer vision that has been the basis of many practical image processing algorithms. HT however is designed to work for frame-based systems such as conventional digital cameras. Recently, event-based systems such as Dynamic Vision Sensor (DVS) cameras, has become popular among researchers. Event-based cameras have a significantly high temporal resolution (1 &#956;s), but each pixel can only detect change and not color. As such, the conventional image processing algorithms cannot be readily applied to event-based output streams. Therefore, it is necessary to adapt the conventional image processing algorithms for event-based cameras. This paper provides a systematic explanation, starting from extending conventional HT to 3D HT, adaptation to event-based systems, and the implementation of the 3D HT using Spiking Neural Networks (SNNs). Using SNN enables the proposed solution to be easily realized on hardware using FPGA, without requiring CPU or additional memory. In addition, we also discuss techniques for optimal SNN-based implementation using efficient number of neurons for the required accuracy and resolution along each dimension, without increasing the overall computational complexity. We hope that this will help to reduce the gap between event-based and frame-based systems."}], "ArticleTitle": "Hough Transform Implementation For Event-Based Systems: Concepts and Challenges."}, "30072887": {"mesh": [], "AbstractText": [{"section": null, "text": "Neuroscience has long focused on finding encoding models that effectively ask \"what predicts neural spiking?\" and generalized linear models (GLMs) are a typical approach. It is often unknown how much of explainable neural activity is captured, or missed, when fitting a model. Here we compared the predictive performance of simple models to three leading machine learning methods: feedforward neural networks, gradient boosted trees (using XGBoost), and stacked ensembles that combine the predictions of several methods. We predicted spike counts in macaque motor (M1) and somatosensory (S1) cortices from standard representations of reaching kinematics, and in rat hippocampal cells from open field location and orientation. Of these methods, XGBoost and the ensemble consistently produced more accurate spike rate predictions and were less sensitive to the preprocessing of features. These methods can thus be applied quickly to detect if feature sets relate to neural activity in a manner not captured by simpler methods. Encoding models built with a machine learning approach accurately predict spike rates and can offer meaningful benchmarks for simpler models."}], "ArticleTitle": "Modern Machine Learning as a Benchmark for Fitting Neural Responses."}, "29033811": {"mesh": [], "AbstractText": [{"section": null, "text": "The purpose of this study was to evaluate the spatiotemporal Consistency of spontaneous activities in local brain regions in patients with generalized tonic-clonic seizures (GTCS). The resting-state fMRI data were acquired from nineteen patients with GTCS and twenty-two matched healthy subjects. FOur-dimensional (spatiotemporal) Consistency of local neural Activities (FOCA) metric was used to analyze the spontaneous activity in whole brain. The FOCA difference between two groups were detected using a two sample t-test analysis. Correlations between the FOCA values and features of seizures were analyzed. The findings of this study showed that patients had significantly increased FOCA in motor-related cortex regions, including bilateral supplementary motor area, paracentral lobule, precentral gyrus and left basal ganglia, as well as a substantial reduction of FOCA in regions of default mode network (DMN) and parietal lobe. In addition, several brain regions in DMN demonstrated more reduction with longer duration of epilepsy and later onset age, and the motor-related regions showed higher FOCA value in accompany with later onset age. These findings implicated the abnormality of motor-related cortical network in GTCS which were associated with the genesis and propagation of epileptiform activity. And the decreased FOCA in DMN might reflect the intrinsic disturbance of brain activity. Moreover, our study supported that the FOCA might be potential tool to investigate local brain spontaneous activity related with the epileptic activity, and to provide important insights into understanding the underlying pathophysiological mechanisms of GTCS."}], "ArticleTitle": "Altered Local Spatiotemporal Consistency of Resting-State BOLD Signals in Patients with Generalized Tonic-Clonic Seizures."}, "30131688": {"mesh": [], "AbstractText": [{"section": null, "text": "The information processing in the large scale network of the human brain is related to its cognitive functions. Due to requirements for adaptation to changing environments under biological constraints, these processes in the brain can be hypothesized to be optimized. The principles based on the information optimization are expected to play a central role in affecting the dynamics and topological structure of the brain network. Recent studies on the functional connectivity between brain regions, referred to as the functional connectome, reveal characteristics of their networks, such as self-organized criticality of brain dynamics and small-world topology. However, these important attributes are established separately, and their relations to the principle of the information optimization are unclear. Here, we show that the maximization principle of the mutual information entropy induces the optimal state, at which the small-world network topology and the criticality in the activation dynamics emerge. Our findings, based on the functional connectome analyses, show that according to the increasing mutual information entropy, the coactivation pattern converges to the state of self-organized criticality, and a phase transition of the network topology, which is responsible for the small-world topology, arises simultaneously at the same point. The coincidence of these phase transitions at the same critical point indicates that the criticality of the dynamics and the phase transition of the network topology are essentially rooted in the same phenomenon driven by the mutual information maximization. As a consequence, the two different attributes of the brain, self-organized criticality and small-world topology, can be understood within a unified perspective under the information-based principle. Thus, our study provides an insight into the mechanism underlying the information processing in the brain."}], "ArticleTitle": "Information-Based Principle Induces Small-World Topology and Self-Organized Criticality in a Large Scale Brain Network."}, "30349469": {"mesh": [], "AbstractText": [{"section": null, "text": "Success in the fine control of the nervous system depends on a deeper understanding of how neural circuits control behavior. There is, however, a wide gap between the components of neural circuits and behavior. We advance the idea that a suitable approach for narrowing this gap has to be based on a multiscale information-theoretic description of the system. We evaluate the possibility that brain-wide complex neural computations can be dissected into a hierarchy of computational motifs that rely on smaller circuit modules interacting at multiple scales. In doing so, we draw attention to the importance of formalizing the goals of stimulation in terms of neural computations so that the possible implementations are matched in scale to the underlying circuit modules."}], "ArticleTitle": "Theoretical Principles of Multiscale Spatiotemporal Control of Neuronal Networks: A Complex Systems Perspective."}, "32655389": {"mesh": [], "AbstractText": [{"section": null, "text": "[This corrects the article DOI: 10.3389/fncom.2018.00110.]."}], "ArticleTitle": "Corrigendum: A General Model of Ion Passive Transmembrane Transport Based on Ionic Concentration."}, "32351375": {"mesh": [], "AbstractText": [{"section": null, "text": "[This corrects the article DOI: 10.3389/fncom.2019.00002.]."}], "ArticleTitle": "Corrigendum: Modeling Emotions Associated With Novelty at Variable Uncertainty Levels: A Bayesian Approach."}, "29081743": {"mesh": [], "AbstractText": [{"section": null, "text": "In recent years, theory-building in motor neuroscience and our understanding of the synergistic control of the redundant human motor system has significantly profited from the emergence of a range of different mathematical approaches to analyze the structure of movement variability. Approaches such as the Uncontrolled Manifold method or the Noise-Tolerance-Covariance decomposition method allow to detect and interpret changes in movement coordination due to e.g., learning, external task constraints or disease, by analyzing the structure of within-subject, inter-trial movement variability. Whereas, for cyclical movements (e.g., locomotion), mathematical approaches exist to investigate the propagation of movement variability in time (e.g., time series analysis), similar approaches are missing for discrete, goal-directed movements, such as reaching. Here, we propose canonical correlation analysis as a suitable method to analyze the propagation of within-subject variability across different time points during the execution of discrete movements. While similar analyses have already been applied for discrete movements with only one degree of freedom (DoF; e.g., Pearson's product-moment correlation), canonical correlation analysis allows to evaluate the coupling of inter-trial variability across different time points along the movement trajectory for multiple DoF-effector systems, such as the arm. The theoretical analysis is illustrated by empirical data from a study on reaching movements under normal and disturbed proprioception. The results show increased movement duration, decreased movement amplitude, as well as altered movement coordination under ischemia, which results in a reduced complexity of movement control. Movement endpoint variability is not increased under ischemia. This suggests that healthy adults are able to immediately and efficiently adjust the control of complex reaching movements to compensate for the loss of proprioceptive information. Further, it is shown that, by using canonical correlation analysis, alterations in movement coordination that indicate changes in the control strategy concerning the use of motor redundancy can be detected, which represents an important methodical advance in the context of neuromechanics."}], "ArticleTitle": "The Propagation of Movement Variability in Time: A Methodological Approach for Discrete Movements with Multiple Degrees of Freedom."}, "33469424": {"mesh": [], "AbstractText": [{"section": null, "text": "Modeling the dynamics of neural masses is a common approach in the study of neural populations. Various models have been proven useful to describe a plenitude of empirical observations including self-sustained local oscillations and patterns of distant synchronization. We discuss the extent to which mass models really resemble the mean dynamics of a neural population. In particular, we question the validity of neural mass models if the population under study comprises a mixture of excitatory and inhibitory neurons that are densely (inter-)connected. Starting from a network of noisy leaky integrate-and-fire neurons, we formulated two different population dynamics that both fall into the category of seminal Freeman neural mass models. The derivations contained several mean-field assumptions and time scale separation(s) between membrane and synapse dynamics. Our comparison of these neural mass models with the averaged dynamics of the population reveals bounds in the fraction of excitatory/inhibitory neuron as well as overall network degree for a mass model to provide adequate estimates. For substantial parameter ranges, our models fail to mimic the neural network's dynamics proper, be that in de-synchronized or in (high-frequency) synchronized states. Only around the onset of low-frequency synchronization our models provide proper estimates of the mean potential dynamics. While this shows their potential for, e.g., studying resting state dynamics obtained by encephalography with focus on the transition region, we must accept that predicting the more general dynamic outcome of a neural network via its mass dynamics requires great care."}], "ArticleTitle": "On the Validity of Neural Mass Models."}, "29249953": {"mesh": [], "AbstractText": [{"section": null, "text": "Our eyes move constantly at a frequency of 3-5 times per second. These movements, called saccades, induce the sweeping of visual images on the retina, yet we perceive the world as stable. It has been suggested that the brain achieves this visual stability via predictive remapping of neuronal receptive field (RF). A recent experimental study disclosed details of this remapping process in the lateral intraparietal area (LIP), that is, about the time of the saccade, the neuronal RF expands along the saccadic trajectory temporally, covering the current RF (CRF), the future RF (FRF), and the region the eye will sweep through during the saccade. A cortical wave (CW) model was also proposed, which attributes the RF remapping as a consequence of neural activity propagating in the cortex, triggered jointly by a visual stimulus and the corollary discharge (CD) signal responsible for the saccade. In this study, we investigate how this CW model is learned naturally from visual experiences at the development of the brain. We build a two-layer network, with one layer consisting of LIP neurons and the other superior colliculus (SC) neurons. Initially, neuronal connections are random and non-selective. A saccade will cause a static visual image to sweep through the retina passively, creating the effect of the visual stimulus moving in the opposite direction of the saccade. According to the spiking-time-dependent-plasticity rule, the connection path in the opposite direction of the saccade between LIP neurons and the connection path from SC to LIP are enhanced. Over many such visual experiences, the CW model is developed, which generates the peri-saccadic RF remapping in LIP as observed in the experiment."}], "ArticleTitle": "Learning Peri-saccadic Remapping of Receptive Field from Experience in Lateral Intraparietal Area."}, "29636674": {"mesh": [], "AbstractText": [{"section": null, "text": "Computational models that predict the spectral sensitivities of primate cone photoreceptors have focussed only on the spectral, not spatial, dimensions. On the ecologically valid task of foraging for fruit, such models predict the M-cone (\"green\") peak spectral sensitivity 10-20 nm further from the L-cone (\"red\") sensitivity peak than it is in nature and assume their separation is limited by other visual constraints, such as the requirement of high-acuity spatial vision for closer M and L peak sensitivities. We explore the possibility that a spatio-chromatic analysis can better predict cone spectral tuning without appealing to other visual constraints. We build a computational model of the primate retina and simulate chromatic gratings of varying spatial frequencies using measured spectra. We then implement the case study of foveal processing in routinely trichromatic primates for the task of discriminating fruit and leaf spectra. We perform an exhaustive search for the configurations of M and L cone spectral sensitivities that optimally distinguish the colour patterns within these spectral images. Under such conditions, the model suggests that: (1) a long-wavelength limit is required to constrain the L cone spectral sensitivity to its natural position; (2) the optimal M cone peak spectral sensitivity occurs at ~525 nm, close to the observed position in nature (~535 nm); (3) spatial frequency has a small effect upon the spectral tuning of the cones; (4) a selective pressure toward less correlated M and L spectral sensitivities is provided by the need to reduce noise caused by the luminance variation that occurs in natural scenes."}], "ArticleTitle": "The Importance of Spatial Visual Scene Parameters in Predicting Optimal Cone Sensitivities in Routinely Trichromatic Frugivorous Old-World Primates."}, "29867423": {"mesh": [], "AbstractText": [{"section": null, "text": "The neural basis of time perception has long attracted the interests of researchers. Recently, a conceptual model consisting of neural oscillators was proposed and validated by behavioral experiments that measured the dilated duration in perception of a flickering stimulus (Hashimoto and Yotsumoto, 2015). The model proposed that flickering stimuli cause neural entrainment of oscillators, resulting in dilated time perception. In this study, we examined the oscillator-based model of time perception, by collecting electroencephalography (EEG) data during an interval-timing task. Initially, subjects observed a stimulus, either flickering at 10-Hz or constantly illuminated. The subjects then reproduced the duration of the stimulus by pressing a button. As reported in previous studies, the subjects reproduced 1.22 times longer durations for flickering stimuli than for continuously illuminated stimuli. The event-related potential (ERP) during the observation of a flicker oscillated at 10 Hz, reflecting the 10-Hz neural activity phase-locked to the flicker. Importantly, the longer reproduced duration was associated with a larger amplitude of the 10-Hz ERP component during the inter-stimulus interval, as well as during the presentation of the flicker. The correlation between the reproduced duration and the 10-Hz oscillation during the inter-stimulus interval suggested that the flicker-induced neural entrainment affected time dilation. While the 10-Hz flickering stimuli induced phase-locked entrainments at 10 Hz, we also observed event-related desynchronizations of spontaneous neural oscillations in the alpha-frequency range. These could be attributed to the activation of excitatory neurons while observing the flicker stimuli. In addition, neural activity at approximately the alpha frequency increased during the reproduction phase, indicating that flicker-induced neural entrainment persisted even after the offset of the flicker. In summary, our results suggest that the duration perception is mediated by neural oscillations, and that time dilation induced by flickering visual stimuli can be attributed to neural entrainment."}], "ArticleTitle": "The Amount of Time Dilation for Visual Flickers Corresponds to the Amount of Neural Entrainments Measured by EEG."}, "33250731": {"mesh": [], "AbstractText": [{"section": null, "text": "[This corrects the article DOI: 10.3389/fncom.2016.00112.]."}], "ArticleTitle": "Corrigendum: Spectral Entropy Based Neuronal Network Synchronization Analysis Based on Microelectrode Array Measurements."}, "28275348": {"mesh": [], "AbstractText": [{"section": null, "text": "Brain image spatial normalization and tissue segmentation rely on prior tissue probability maps. Appropriately selecting these tissue maps becomes particularly important when investigating \"unusual\" populations, such as young children or elderly subjects. When creating such priors, the disadvantage of applying more deformation must be weighed against the benefit of achieving a crisper image. We have previously suggested that statistically modeling demographic variables, instead of simply averaging images, is advantageous. Both aspects (more vs. less deformation and modeling vs. averaging) were explored here. We used imaging data from 1914 subjects, aged 13 months to 75 years, and employed multivariate adaptive regression splines to model the effects of age, field strength, gender, and data quality. Within the spm/cat12 framework, we compared an affine-only with a low- and a high-dimensional warping approach. As expected, more deformation on the individual level results in lower group dissimilarity. Consequently, effects of age in particular are less apparent in the resulting tissue maps when using a more extensive deformation scheme. Using statistically-described parameters, high-quality tissue probability maps could be generated for the whole age range; they are consistently closer to a gold standard than conventionally-generated priors based on 25, 50, or 100 subjects. Distinct effects of field strength, gender, and data quality were seen. We conclude that an extensive matching for generating tissue priors may model much of the variability inherent in the dataset which is then not contained in the resulting priors. Further, the statistical description of relevant parameters (using regression splines) allows for the generation of high-quality tissue probability maps while controlling for known confounds. The resulting CerebroMatic toolbox is available for download at http://irc.cchmc.org/software/cerebromatic.php."}], "ArticleTitle": "CerebroMatic: A Versatile Toolbox for Spline-Based MRI Template Creation."}, "30618693": {"mesh": [], "AbstractText": [{"section": null, "text": "Internal representation of far-range space in insects is well established, as it is necessary for navigation behavior. Although it is likely that insects also have an internal representation of near-range space, the behavioral evidence for the latter is much less evident. Here, we estimate the size and shape of the spatial equivalent of a near-range representation that is constituted by somatosensory sampling events. To do so, we use a large set of experimental whole-body motion capture data on unrestrained walking, climbing and searching behavior in stick insects of the species Carausius morosus to delineate 'action volumes' and 'contact volumes' for both antennae and all six legs. As these volumes are derived from recorded sampling events, they comprise a volume equivalent to a representation of coinciding somatosensory and motor activity. Accordingly, we define this volume as the peripersonal space of an insect. It is of immediate behavioral relevance, because it comprises all potential external object locations within the action range of the body. In a next step, we introduce the notion of an affordance space as that part of peripersonal space within which contact-induced spatial estimates lie within the action ranges of more than one limb. Because the action volumes of limbs overlap in this affordance space, spatial information from one limb can be used to control the movement of another limb. Thus, it gives rise to an affordance as known for contact-induced reaching movements and spatial coordination of footfall patterns in stick insects. Finally, we probe the computational properties of the experimentally derived affordance space for pairs of neighboring legs. This is done by use of artificial neural networks that map the posture of one leg into a target posture of another leg with identical foot position."}], "ArticleTitle": "Transfer of Spatial Contact Information Among Limbs and the Notion of Peripersonal Space in Insects."}, "27833546": {"mesh": [], "AbstractText": [{"section": null, "text": "This paper shortly reviews the measures used to estimate neural synchronization in experimental settings. Our focus is on multivariate measures of dependence based on the Granger causality (G-causality) principle, their applications and performance in respect of robustness to noise, volume conduction, common driving, and presence of a \"weak node.\" Application of G-causality measures to EEG, intracranial signals and fMRI time series is addressed. G-causality based measures defined in the frequency domain allow the synchronization between neural populations and the directed propagation of their electrical activity to be determined. The time-varying G-causality based measure Short-time Directed Transfer Function (SDTF) supplies information on the dynamics of synchronization and the organization of neural networks. Inspection of effective connectivity patterns indicates a modular structure of neural networks, with a stronger coupling within modules than between them. The hypothetical plausible mechanism of information processing, suggested by the identified synchronization patterns, is communication between tightly coupled modules intermitted by sparser interactions providing synchronization of distant structures."}], "ArticleTitle": "Measures of Coupling between Neural Populations Based on Granger Causality Principle."}, "29066966": {"mesh": [], "AbstractText": [{"section": null, "text": "Molecular signal transmission in cell is very crucial for information exchange. How to understand its transmission mechanism has attracted many researchers. In this paper, we prove that signal transmission problem between neural tumor molecules and drug molecules can be achieved by synchronous control. To achieve our purpose, we derive the Fokker-Plank equation by using the Langevin equation and theory of random walk, this is a model which can express the concentration change of neural tumor molecules. Second, according to the biological character that vesicles in cell can be combined with cell membrane to release the cargo which plays a role of signal transmission, we preliminarily analyzed the mechanism of tumor-drug molecular interaction. Third, we propose the view of synchronous control which means the process of vesicle docking with their target membrane is a synchronization process, and we can achieve the precise treatment of disease by using synchronous control. We believe this synchronous control mechanism is reasonable and two examples are given to illustrate the correctness of our results obtained in this paper."}], "ArticleTitle": "Signal Transmission of Biological Reaction-Diffusion System by Using Synchronization."}, "28197088": {"mesh": [], "AbstractText": [{"section": null, "text": "In this study, we used the Hodgkin-Huxley (HH) model of neurons to investigate the phase diagram of a developing single-layer neural network and that of a network consisting of two weakly coupled neural layers. These networks are noise driven and learn through the spike-timing-dependent plasticity (STDP) or the inverse STDP rules. We described how these networks transited from a non-synchronous background activity state (BAS) to a synchronous firing state (SFS) by varying the network connectivity and the learning efficacy. In particular, we studied the interaction between a SFS layer and a BAS layer, and investigated how synchronous firing dynamics was induced in the BAS layer. We further investigated the effect of the inter-layer interaction on a BAS to SFS repair mechanism by considering three types of neuron positioning (random, grid, and lognormal distributions) and two types of inter-layer connections (random and preferential connections). Among these scenarios, we concluded that the repair mechanism has the largest effect for a network with the lognormal neuron positioning and the preferential inter-layer connections."}], "ArticleTitle": "Synchronization and Inter-Layer Interactions of Noise-Driven Neural Networks."}, "29051730": {"mesh": [], "AbstractText": [{"section": null, "text": "Neuroimaging in combination with graph theory has been successful in analyzing the functional connectome. However almost all analysis are performed based on static graph theory. The derived quantitative graph measures can only describe a snap shot of the disease over time. Neurodegenerative disease evolution is poorly understood and treatment strategies are consequently only of limited efficiency. Fusing modern dynamic graph network theory techniques and modeling strategies at different time scales with pinning observability of complex brain networks will lay the foundation for a transformational paradigm in neurodegnerative diseases research regarding disease evolution at the patient level, treatment response evaluation and revealing some central mechanism in a network that drives alterations in these diseases. We model and analyze brain networks as two-time scale sparse dynamic graph networks with hubs (clusters) representing the fast sub-system and the interconnections between hubs the slow sub-system. Alterations in brain function as seen in dementia can be dynamically modeled by determining the clusters in which disturbance inputs have entered and the impact they have on the large-scale dementia dynamic system. Observing a small fraction of specific nodes in dementia networks such that the others can be recovered is accomplished by the novel concept of pinning observability. In addition, how to control this complex network seems to be crucial in understanding the progressive abnormal neural circuits in many neurodegenerative diseases. Detecting the controlling regions in the networks, which serve as key nodes to control the aberrant dynamics of the networks to a desired state and thus influence the progressive abnormal behavior, will have a huge impact in understanding and developing therapeutic solutions and also will provide useful information about the trajectory of the disease. In this paper, we present the theoretical framework and derive the necessary conditions for (1) area aggregation and time-scale modeling in brain networks and for (2) pinning observability of nodes in dynamic graph networks. Simulation examples are given to illustrate the theoretical concepts."}], "ArticleTitle": "Dynamical Graph Theory Networks Methods for the Analysis of Sparse Functional Connectivity Networks and for Determining Pinning Observability in Brain Networks."}, "29075187": {"mesh": [], "AbstractText": [{"section": null, "text": "Hippocampal place-cell sequences observed during awake immobility often represent previous experience, suggesting a role in memory processes. However, recent reports of goals being overrepresented in sequential activity suggest a role in short-term planning, although a detailed understanding of the origins of hippocampal sequential activity and of its functional role is still lacking. In particular, it is unknown which mechanism could support efficient planning by generating place-cell sequences biased toward known goal locations, in an adaptive and constructive fashion. To address these questions, we propose a model of spatial learning and sequence generation as interdependent processes, integrating cortical contextual coding, synaptic plasticity and neuromodulatory mechanisms into a map-based approach. Following goal learning, sequential activity emerges from continuous attractor network dynamics biased by goal memory inputs. We apply Bayesian decoding on the resulting spike trains, allowing a direct comparison with experimental data. Simulations show that this model (1) explains the generation of never-experienced sequence trajectories in familiar environments, without requiring virtual self-motion signals, (2) accounts for the bias in place-cell sequences toward goal locations, (3) highlights their utility in flexible route planning, and (4) provides specific testable predictions."}], "ArticleTitle": "Predictive Place-Cell Sequences for Goal-Finding Emerge from Goal Memory and the Cognitive Map: A Computational Model."}, "28736520": {"mesh": [], "AbstractText": [{"section": null, "text": "The mechanisms underlying electrophysiologically observed two-way transitions between absence and tonic-clonic epileptic seizures in cerebral cortex remain unknown. The interplay within thalamocortical network is believed to give rise to these epileptic multiple modes of activity and transitions between them. In particular, it is thought that in some areas of cortex there exists feedforward inhibition from specific relay nucleus of thalamus (TC) to inhibitory neuronal population (IN) which has even more stronger functions on cortical activities than the known feedforward excitation from TC to excitatory neuronal population (EX). Inspired by this, we proposed a modified computational model by introducing feedforward inhibitory connectivity within thalamocortical circuit, to systematically investigate the combined effects of feedforward inhibition and excitation on transitions of epileptic seizures. We first found that the feedforward excitation can induce the transition from tonic oscillation to spike and wave discharges (SWD) in cortex, i.e., the epileptic tonic-absence seizures, with the fixed weak feedforward inhibition. Thereinto, the phase of absence seizures corresponding to strong feedforward excitation can be further transformed into the clonic oscillations with the increasing of feedforward inhibition, representing the epileptic absence-clonic seizures. We also observed the other fascinating dynamical states, such as periodic 2/3/4-spike and wave discharges, reversed SWD and clonic oscillations, as well as saturated firings. More importantly, we can identify the stable parameter regions representing the tonic-clonic oscillations and SWD discharges of epileptic seizures on the 2-D plane composed of feedforward inhibition and excitation, where the physiologically plausible transition pathways between tonic-clonic and absence seizures can be figured out. These results indicate the functional role of feedforward pathways in controlling epileptic seizures and the modified thalamocortical model may provide a guide for future efforts to mechanistically link feedforward pathways in the pathogenesis of epileptic seizures."}], "ArticleTitle": "Combined Effects of Feedforward Inhibition and Excitation in Thalamocortical Circuit on the Transitions of Epileptic Seizures."}, "28867999": {"mesh": [], "AbstractText": [{"section": null, "text": "The neuronal synchronous discharging may cause an epileptic seizure. Currently, most of the studies conducted to investigate the mechanism of epilepsy are based on EEGs or functional magnetic resonance imaging (fMRI) recorded during the ictal discharging or the resting-state, and few studies have probed into the dynamic patterns during the inter-ictal discharging that are much easier to record in clinical applications. Here, we propose a time-varying network analysis based on adaptive directed transfer function to uncover the dynamic brain network patterns during the inter-ictal discharging. In addition, an algorithm based on the time-varying outflow of information derived from the network analysis is developed to detect the epileptogenic zone. The analysis performed revealed the time-varying network patterns during different stages of inter-ictal discharging; the epileptogenic zone was activated prior to the discharge onset then worked as the source to propagate the activity to other brain regions. Consistence between the epileptogenic zones detected by our proposed approach and the actual epileptogenic zones proved that time-varying network analysis could not only reveal the underlying neural mechanism of epilepsy, but also function as a useful tool in detecting the epileptogenic zone based on the EEGs in the inter-ictal discharging."}], "ArticleTitle": "Time-Varying Networks of Inter-Ictal Discharging Reveal Epileptogenic Zone."}, "29497372": {"mesh": [], "AbstractText": [{"section": null, "text": "Resting state functional MRI (rs-fMRI) is an imaging technique that allows the spontaneous activity of the brain to be measured. Measures of functional connectivity highly depend on the quality of the BOLD signal data processing. In this study, our aim was to study the influence of preprocessing steps and their order of application on small-world topology and their efficiency in resting state fMRI data analysis using graph theory. We applied the most standard preprocessing steps: slice-timing, realign, smoothing, filtering, and the tCompCor method. In particular, we were interested in how preprocessing can retain the small-world economic properties and how to maximize the local and global efficiency of a network while minimizing the cost. Tests that we conducted in 54 healthy subjects showed that the choice and ordering of preprocessing steps impacted the graph measures. We found that the csr (where we applied realignment, smoothing, and tCompCor as a final step) and the scr (where we applied realignment, tCompCor and smoothing as a final step) strategies had the highest mean values of global efficiency (eg) . Furthermore, we found that the fscr strategy (where we applied realignment, tCompCor, smoothing, and filtering as a final step), had the highest mean local efficiency (el) values. These results confirm that the graph theory measures of functional connectivity depend on the ordering of the processing steps, with the best results being obtained using smoothing and tCompCor as the final steps for global efficiency with additional filtering for local efficiency."}], "ArticleTitle": "The Influence of Preprocessing Steps on Graph Theory Measures Derived from Resting State fMRI."}, "29375358": {"mesh": [], "AbstractText": [{"section": null, "text": "Spike Timing-Dependent Plasticity has been found to assume many different forms. The classic STDP curve, with one potentiating and one depressing window, is only one of many possible curves that describe synaptic learning using the STDP mechanism. It has been shown experimentally that STDP curves may contain multiple LTP and LTD windows of variable width, and even inverted windows. The underlying STDP mechanism that is capable of producing such an extensive, and apparently incompatible, range of learning curves is still under investigation. In this paper, it is shown that STDP originates from a combination of two dynamic Hebbian cross-correlations of local activity at the synapse. The correlation of the presynaptic activity with the local postsynaptic activity is a robust and reliable indicator of the discrepancy between the presynaptic neuron and the postsynaptic neuron's activity. The second correlation is between the local postsynaptic activity with dendritic activity which is a good indicator of matching local synaptic and dendritic activity. We show that this simple time-independent learning rule can give rise to many forms of the STDP learning curve. The rule regulates synaptic strength without the need for spike matching or other supervisory learning mechanisms. Local differences in dendritic activity at the synapse greatly affect the cross-correlation difference which determines the relative contributions of different neural activity sources. Dendritic activity due to nearby synapses, action potentials, both forward and back-propagating, as well as inhibitory synapses will dynamically modify the local activity at the synapse, and the resulting STDP learning rule. The dynamic Hebbian learning rule ensures furthermore, that the resulting synaptic strength is dynamically stable, and that interactions between synapses do not result in local instabilities. The rule clearly demonstrates that synapses function as independent localized computational entities, each contributing to the global activity, not in a simply linear fashion, but in a manner that is appropriate to achieve local and global stability of the neuron and the entire dendritic structure."}], "ArticleTitle": "Dynamic Hebbian Cross-Correlation Learning Resolves the Spike Timing Dependent Plasticity Conundrum."}, "30123119": {"mesh": [], "AbstractText": [{"section": null, "text": "An all optical, non-destructive method for monitoring neural activity has been proposed and its performance in detection has been analyzed computationally. The proposed method is based on excitation of Surface Plasmon Resonance (SPR) through the structure of optical fibers. The sensor structure consists of a multimode optical fiber where, the cladding of fiber has been removed and thin film of gold structure has been deposited on the surface. Impinging the laser light with appropriate wavelength inside the fiber and based on the total internal reflection, the evanescent wave will excite surface plasmons in the gold thin film. The absorption of light by surface plasmons in the gold structure is severely dependent on the dielectric properties at its vicinity. The electrical activity of neural cells (action potential) can modulate the dielectric properties at its vicinity and hence can modify the absorption of light inside the optical fiber. We have computationally analyzed the performance of the proposed sensor with different available geometries using Finite Element Method (FEM). In this regard, we have shown that the optical response of proposed sensor will track the action potential of the neuron at its vicinity. Based on different geometrical structure, the sensor has absorption in different regions of visible spectrum."}], "ArticleTitle": "Recording Neural Activity Based on Surface Plasmon Resonance by Optical Fibers-A Computational Analysis."}, "33192429": {"mesh": [], "AbstractText": [{"section": null, "text": "[This corrects the article DOI: 10.3389/fncom.2019.00049.]."}], "ArticleTitle": "Corrigendum: The Energy Homeostasis Principle: Neuronal Energy Regulation Drives Local Network Dynamics Generating Behavior."}, "29467642": {"mesh": [], "AbstractText": [{"section": null, "text": "The FitzHugh-Nagumo model is improved to consider the effect of the electromagnetic induction on single neuron. On the basis of investigating the Hopf bifurcation behavior of the improved model, stochastic resonance in the stochastic version is captured near the bifurcation point. It is revealed that a weak harmonic oscillation in the electromagnetic disturbance can be amplified through stochastic resonance, and it is the cooperative effect of random transition between the resting state and the large amplitude oscillating state that results in the resonant phenomenon. Using the noise dependence of the mean of interburst intervals, we essentially suggest a biologically feasible clue for detecting weak signal by means of neuron model with subcritical Hopf bifurcation. These observations should be helpful in understanding the influence of the magnetic field to neural electrical activity."}], "ArticleTitle": "Subcritical Hopf Bifurcation and Stochastic Resonance of Electrical Activities in Neuron under Electromagnetic Induction."}, "28487644": {"mesh": [], "AbstractText": [{"section": null, "text": "Most cortical inhibitory cell types exclusively express one of three genes, parvalbumin, somatostatin and 5HT3a. We conjecture that these three inhibitory neuron types possess distinct roles in visual contextual processing based on two observations. First, they have distinctive synaptic sources and targets over different spatial extents and from different areas. Second, the visual responses of cortical neurons are affected not only by local cues, but also by visual context. We use modeling to relate structural information to function in primary visual cortex (V1) of the mouse, and investigate their role in contextual visual processing. Our findings are three-fold. First, the inhibition mediated by parvalbumin positive (PV) cells mediates local processing and could underlie their role in boundary detection. Second, the inhibition mediated by somatostatin-positive (SST) cells facilitates longer range spatial competition among receptive fields. Third, non-specific top-down modulation to interneurons expressing vasoactive intestinal polypeptide (VIP), a subclass of 5HT3a neurons, can selectively enhance V1 responses."}], "ArticleTitle": "A Computational Analysis of the Function of Three Inhibitory Cell Types in Contextual Visual Processing."}, "28798680": {"mesh": [], "AbstractText": [{"section": null, "text": "We present a network model of striatum, which generates \"winnerless\" dynamics typical for a network of sparse, unidirectionally connected inhibitory units. We observe that these dynamics, while interesting and a good match to normal striatal electrophysiological recordings, are fragile. Specifically, we find that randomly initialized networks often show dynamics more resembling \"winner-take-all,\" and relate this \"unhealthy\" model activity to dysfunctional physiological and anatomical phenotypes in the striatum of Huntington's disease animal models. We report plasticity as a potent mechanism to refine randomly initialized networks and create a healthy winnerless dynamic in our model, and we explore perturbations to a healthy network, modeled on changes observed in Huntington's disease, such as neuron cell death and increased bidirectional connectivity. We report the effect of these perturbations on the conversion risk of the network to an unhealthy state. Finally we discuss the relationship between structural and functional phenotypes observed at the level of simulated network dynamics as a promising means to model disease progression in different patient populations."}], "ArticleTitle": "Striatal Network Models of Huntington's Disease Dysfunction Phenotypes."}, "29527158": {"mesh": [], "AbstractText": [{"section": null, "text": "Bacteria are easily characterizable model organisms with an impressively complicated set of abilities. Among them is quorum sensing, a cell-cell signaling system that may have a common evolutionary origin with eukaryotic cell-cell signaling. The two systems are behaviorally similar, but quorum sensing in bacteria is more easily studied in depth than cell-cell signaling in eukaryotes. Because of this comparative ease of study, bacterial dynamics are also more suited to direct interpretation than eukaryotic dynamics, e.g., those of the neuron. Here we review literature on neuron-like qualities of bacterial colonies and biofilms, including ion-based and hormonal signaling, and a phenomenon similar to the graded action potential. This suggests that bacteria could be used to help create more accurate and detailed biological models in neuroscientific research. More speculatively, bacterial systems may be considered an analog for neurons in biologically based computational research, allowing models to better harness the tremendous ability of biological organisms to process information and make decisions."}], "ArticleTitle": "Is Smaller Better? A Proposal to Use Bacteria For Neuroscientific Modeling."}, "27803659": {"mesh": [], "AbstractText": [{"section": null, "text": "Even without external random input, cortical networks in vivo sustain asynchronous irregular firing with low firing rate. In addition to detailed balance between excitatory and inhibitory activities, recent theoretical studies have revealed that another feature commonly observed in cortical networks, i.e., long-tailed distribution of excitatory synapses implying coexistence of many weak and a few extremely strong excitatory synapses, plays an essential role in realizing the self-sustained activity in recurrent networks of biologically plausible spiking neurons. The previous studies, however, have not considered highly non-random features of the synaptic connectivity, namely, bidirectional connections between cortical neurons are more common than expected by chance and strengths of synapses are positively correlated between pre- and postsynaptic neurons. The positive correlation of synaptic connections may destabilize asynchronous activity of networks with the long-tailed synaptic distribution and induce pathological synchronized firing among neurons. It remains unclear how the cortical network avoids such pathological synchronization. Here, we demonstrate that introduction of the correlated connections indeed gives rise to synchronized firings in a cortical network model with the long-tailed distribution. By using a simplified feed-forward network model of spiking neurons, we clarify the underlying mechanism of the synchronization. We then show that the synchronization can be efficiently suppressed by highly heterogeneous distribution, typically a lognormal distribution, of inhibitory-to-excitatory connection strengths in a recurrent network model of cortical neurons."}], "ArticleTitle": "Effective Suppression of Pathological Synchronization in Cortical Networks by Highly Heterogeneous Distribution of Inhibitory Connections."}, "28420975": {"mesh": [], "AbstractText": [{"section": null, "text": "During force production, hand muscle activity is known to be coherent with activity in primary motor cortex, specifically in the beta-band (15-30 Hz) frequency range. It is not clear, however, if this coherence reflects the control strategy selected by the nervous system for a given task, or if it instead reflects an intrinsic property of cortico-spinal communication. Here, we measured corticomuscular and intermuscular coherence between muscles of index finger and thumb while a two-finger pinch grip of identical net force was applied to objects which were either stable (allowing synergistic activation of finger muscles) or unstable (requiring individuated finger control). We found that beta-band corticomuscular coherence with the first dorsal interosseous (FDI) and abductor pollicis brevis (APB) muscles, as well as their beta-band coherence with each other, was significantly reduced when individuated control of the thumb and index finger was required. We interpret these findings to show that beta-band coherence is reflective of a synergistic control strategy in which the cortex binds task-related motor neurons into functional units."}], "ArticleTitle": "Beta Band Corticomuscular Drive Reflects Muscle Coordination Strategies."}, "29765314": {"mesh": [], "AbstractText": [{"section": null, "text": "Long-term potentiation (LTP) is a specific form of activity-dependent synaptic plasticity that is a leading mechanism of learning and memory in mammals. The properties of cooperativity, input specificity, and associativity are essential for LTP; however, the underlying mechanisms are unclear. Here, based on experimentally observed phenomena, we introduce a computational model of synaptic plasticity in a pyramidal cell to explore the mechanisms responsible for the cooperativity, input specificity, and associativity of LTP. The model is based on molecular processes involved in synaptic plasticity and integrates gene expression involved in the regulation of neuronal activity. In the model, we introduce a local positive feedback loop of protein synthesis at each synapse, which is essential for bimodal response and synapse specificity. Bifurcation analysis of the local positive feedback loop of brain-derived neurotrophic factor (BDNF) signaling illustrates the existence of bistability, which is the basis of LTP induction. The local bifurcation diagram provides guidance for the realization of LTP, and the projection of whole system trajectories onto the two-parameter bifurcation diagram confirms the predictions obtained from bifurcation analysis. Moreover, model analysis shows that pre- and postsynaptic components are required to achieve the three properties of LTP. This study provides insights into the mechanisms underlying the cooperativity, input specificity, and associativity of LTP, and the further construction of neural networks for learning and memory."}], "ArticleTitle": "Underlying Mechanisms of Cooperativity, Input Specificity, and Associativity of Long-Term Potentiation Through a Positive Feedback of Local Protein Synthesis."}, "30344485": {"mesh": [], "AbstractText": [{"section": null, "text": "Humans have flexible control over cognitive functions depending on the context. Several studies suggest that the prefrontal cortex (PFC) controls this cognitive flexibility, but the detailed underlying mechanisms remain unclear. Recent developments in machine learning techniques allow simple PFC models written as a recurrent neural network to perform various behavioral tasks like humans and animals. Computational modeling allows the estimation of neuronal parameters that are crucial for performing the tasks, which cannot be observed by biologic experiments. To identify salient neural-network features for flexible cognition tasks, we compared four PFC models using a context-dependent integration task. After training the neural networks with the task, we observed highly plastic synapses localized to a small neuronal population in all models. In three of the models, the neuronal units containing these highly plastic synapses contributed most to the performance. No common tendencies were observed in the distribution of synaptic strengths among the four models. These results suggest that task-dependent plastic synaptic changes are more important for accomplishing flexible cognitive tasks than the structures of the constructed synaptic networks."}], "ArticleTitle": "Task-Related Synaptic Changes Localized to Small Neuronal Population in Recurrent Neural Network Cortical Models."}, "28659781": {"mesh": [], "AbstractText": [{"section": null, "text": "Finding accurate reduced descriptions for large, complex, dynamically evolving networks is a crucial enabler to their simulation, analysis, and ultimately design. Here, we propose and illustrate a systematic and powerful approach to obtaining good collective coarse-grained observables-variables successfully summarizing the detailed state of such networks. Finding such variables can naturally lead to successful reduced dynamic models for the networks. The main premise enabling our approach is the assumption that the behavior of a node in the network depends (after a short initial transient) on the node identity: a set of descriptors that quantify the node properties, whether intrinsic (e.g., parameters in the node evolution equations) or structural (imparted to the node by its connectivity in the particular network structure). The approach creates a natural link with modeling and \"computational enabling technology\" developed in the context of Uncertainty Quantification. In our case, however, we will not focus on ensembles of different realizations of a problem, each with parameters randomly selected from a distribution. We will instead study many coupled heterogeneous units, each characterized by randomly assigned (heterogeneous) parameter value(s). One could then coin the term Heterogeneity Quantification for this approach, which we illustrate through a model dynamic network consisting of coupled oscillators with one intrinsic heterogeneity (oscillator individual frequency) and one structural heterogeneity (oscillator degree in the undirected network). The computational implementation of the approach, its shortcomings and possible extensions are also discussed."}], "ArticleTitle": "Coarse-Grained Descriptions of Dynamics for Networks with Both Intrinsic and Structural Heterogeneities."}, "28932190": {"mesh": [], "AbstractText": [{"section": null, "text": "The muscle synergy hypothesis assumes that individual muscle synergies are independent of each other and voluntarily controllable. However, this assumption has not been empirically tested. This study tested if human subjects can voluntarily activate individual muscle synergies extracted by non-negative matrix factorization (NMF), the standard mathematical method for synergy extraction. We defined the activation of a single muscle synergy as the generation of a muscle activity pattern vector parallel to the single muscle synergy vector. Subjects performed an isometric force production task with their right hand, and the 13 muscle activity patterns associated with their elbow and shoulder movements were measured. We extracted muscle synergies during the task using electromyogram (EMG) data and the NMF method with varied numbers of muscle synergies. The number (N) of muscle synergies was determined by using the variability accounted for (VAF, NVAF ) and the coefficient of determination (CD, NCD ). An additional muscle synergy model with NAD was also considered. We defined a conventional muscle synergy as the muscle synergy extracted by the NVAF , NCD , and NAD . We also defined an extended muscle synergy as the muscle synergy extracted by the NEX > NAD . To examine whether the individual muscle synergy was voluntarily activatable or not, we calculated the index of independent activation, which reflects similarities between a selected single muscle synergy and the current muscle activation pattern of the subject. Subjects were visually feed-backed the index of independent activation, then instructed to generate muscle activity patterns similar to the conventional and extended muscle synergies. As a result, an average of 90.8% of the muscle synergy extracted by the NVAF was independently activated. However, the proportion of activatable muscle synergies extracted by NCD and NAD was lower. These results partly support the assumption of the muscle synergy hypothesis, i.e., that the conventional method can extract voluntarily and independently activatable muscle synergies by using the appropriate index of reconstruction. Moreover, an average of 25.5% of the extended muscle synergy was significantly activatable. This result suggests that the CNS can use extended muscle synergies to perform voluntary movements."}], "ArticleTitle": "Empirical Evaluation of Voluntarily Activatable Muscle Synergies."}, "28539881": {"mesh": [], "AbstractText": [{"section": null, "text": "Noise correlations are a common feature of neural responses and have been observed in many cortical areas across different species. These correlations can influence information processing by enhancing or diminishing the quality of the neural code, but the origin of these correlations is still a matter of controversy. In this computational study we explore the hypothesis that noise correlations are the result of local recurrent excitatory and inhibitory connections. We simulated two-dimensional networks of adaptive spiking neurons with local connection patterns following Gaussian kernels. Noise correlations decay with distance between neurons but are only observed if the range of excitatory connections is smaller than the range of inhibitory connections (\"Mexican hat\" connectivity) and if the connection strengths are sufficiently strong. These correlations arise from a moving blob-like structure of evoked activity, which is absent if inhibitory interactions have a smaller range (\"inverse Mexican hat\" connectivity). Spatially structured external inputs fixate these blobs to certain locations and thus effectively reduce noise correlations. We further investigated the influence of these network configurations on stimulus encoding. On the one hand, the observed correlations diminish information about a stimulus encoded by a network. On the other hand, correlated activity allows for more precise encoding of stimulus information if the decoder has only access to a limited amount of neurons."}], "ArticleTitle": "The Influence of Mexican Hat Recurrent Connectivity on Noise Correlations and Stimulus Encoding."}, "32063839": {"mesh": [], "AbstractText": [{"section": null, "text": "Chronic Fatigue Syndrome (CFS) is a debilitating condition estimated to impact at least 1 million individuals in the United States, however there persists controversy about its existence. Machine learning algorithms have become a powerful methodology for evaluating multi-regional areas of fMRI activation that can classify disease phenotype from sedentary control. Uncovering objective biomarkers such as an fMRI pattern is important for lending credibility to diagnosis of CFS. fMRI scans were evaluated for 69 patients (38 CFS and 31 Control) taken before (Day 1) and after (Day 2) a submaximal exercise test while undergoing the n-back memory paradigm. A predictive model was created by grouping fMRI voxels into the Automated Anatomical Labeling (AAL) atlas, splitting the data into a training and testing dataset, and feeding these inputs into a logistic regression to evaluate differences between CFS and control. Model results were cross-validated 10 times to ensure accuracy. Model results were able to differentiate CFS from sedentary controls at a 80% accuracy on Day 1 and 76% accuracy on Day 2 (Table 3). Recursive features selection identified 29 ROI's that significantly distinguished CFS from control on Day 1 and 28 ROI's on Day 2 with 10 regions of overlap shared with Day 1 (Figure 3). These 10 shared regions included the putamen, inferior frontal gyrus, orbital (F3O), supramarginal gyrus (SMG), temporal pole; superior temporal gyrus (T1P) and caudate ROIs. This study was able to uncover a pattern of activated neurological regions that differentiated CFS from Control. This pattern provides a first step toward developing fMRI as a diagnostic biomarker and suggests this methodology could be emulated for other disorders. We concluded that a logistic regression model performed on fMRI data significantly differentiated CFS from Control."}], "ArticleTitle": "A Machine Learning Approach to the Differentiation of Functional Magnetic Resonance Imaging Data of Chronic Fatigue Syndrome (CFS) From a Sedentary Control."}, "28663729": {"mesh": [], "AbstractText": [{"section": null, "text": "Understanding the relation between (sensory) stimuli and the activity of neurons (i.e., \"the neural code\") lies at heart of understanding the computational properties of the brain. However, quantifying the information between a stimulus and a spike train has proven to be challenging. We propose a new (in vitro) method to measure how much information a single neuron transfers from the input it receives to its output spike train. The input is generated by an artificial neural network that responds to a randomly appearing and disappearing \"sensory stimulus\": the hidden state. The sum of this network activity is injected as current input into the neuron under investigation. The mutual information between the hidden state on the one hand and spike trains of the artificial network or the recorded spike train on the other hand can easily be estimated due to the binary shape of the hidden state. The characteristics of the input current, such as the time constant as a result of the (dis)appearance rate of the hidden state or the amplitude of the input current (the firing frequency of the neurons in the artificial network), can independently be varied. As an example, we apply this method to pyramidal neurons in the CA1 of mouse hippocampi and compare the recorded spike trains to the optimal response of the \"Bayesian neuron\" (BN). We conclude that like in the BN, information transfer in hippocampal pyramidal cells is non-linear and amplifying: the information loss between the artificial input and the output spike train is high if the input to the neuron (the firing of the artificial network) is not very informative about the hidden state. If the input to the neuron does contain a lot of information about the hidden state, the information loss is low. Moreover, neurons increase their firing rates in case the (dis)appearance rate is high, so that the (relative) amount of transferred information stays constant."}], "ArticleTitle": "Estimating the Information Extracted by a Single Spiking Neuron from a Continuous Input Time Series."}, "28769779": {"mesh": [], "AbstractText": [{"section": null, "text": "Many hippocampal cell types are characterized by a progressive increase in scale along the dorsal-to-ventral axis, such as in the cases of head-direction, grid and place cells. Also located in the medial entorhinal cortex (MEC), border cells would be expected to benefit from such scale modulations. However, this phenomenon has not been experimentally observed. Grid cells in the MEC of mammals integrate velocity related signals to map the environment with characteristic hexagonal tessellation patterns. Due to the noisy nature of these input signals, path integration processes tend to accumulate errors as animals explore the environment, leading to a loss of grid-like activity. It has been suggested that border-to-grid cells' associations minimize the accumulated grid cells' error when rodents explore enclosures. Thus, the border-grid interaction for error minimization is a suitable scenario to study the effects of border cell scaling within the context of spatial representation. In this study, we computationally address the question of (i) border cells' scale from the perspective of their role in maintaining the regularity of grid cells' firing fields, as well as (ii) what are the underlying mechanisms of grid-border associations relative to the scales of both grid and border cells. Our results suggest that for optimal contribution to grid cells' error minimization, border cells should express smaller firing fields relative to those of the associated grid cells, which is consistent with the hypothesis of border cells functioning as spatial anchoring signals."}], "ArticleTitle": "Size Matters: How Scaling Affects the Interaction between Grid and Border Cells."}, "29238299": {"mesh": [], "AbstractText": [{"section": null, "text": "Hierarchical temporal memory (HTM) provides a theoretical framework that models several key computational principles of the neocortex. In this paper, we analyze an important component of HTM, the HTM spatial pooler (SP). The SP models how neurons learn feedforward connections and form efficient representations of the input. It converts arbitrary binary input patterns into sparse distributed representations (SDRs) using a combination of competitive Hebbian learning rules and homeostatic excitability control. We describe a number of key properties of the SP, including fast adaptation to changing input statistics, improved noise robustness through learning, efficient use of cells, and robustness to cell death. In order to quantify these properties we develop a set of metrics that can be directly computed from the SP outputs. We show how the properties are met using these metrics and targeted artificial simulations. We then demonstrate the value of the SP in a complete end-to-end real-world HTM system. We discuss the relationship with neuroscience and previous studies of sparse coding. The HTM spatial pooler represents a neurally inspired algorithm for learning sparse representations from noisy data streams in an online fashion."}], "ArticleTitle": "The HTM Spatial Pooler-A Neocortical Algorithm for Online Sparse Distributed Coding."}, "28649195": {"mesh": [], "AbstractText": [{"section": null, "text": "The ability for cortical neurons to adapt their input/output characteristics and information processing capabilities ultimately relies on the interplay between synaptic plasticity, synapse location, and the nonlinear properties of the dendrite. Collectively, they shape both the strengths and spatial arrangements of convergent afferent inputs to neuronal dendrites. Recent experimental and theoretical studies support a clustered plasticity model, a view that synaptic plasticity promotes the formation of clusters or hotspots of synapses sharing similar properties. We have previously shown that spike timing-dependent plasticity (STDP) can lead to synaptic efficacies being arranged into spatially segregated clusters. This effectively partitions the dendritic tree into a tessellated imprint which we have called a dendritic mosaic. Here, using a biophysically detailed neuron model of a reconstructed layer 2/3 pyramidal cell and STDP learning, we investigated the impact of altered STDP balance on forming such a spatial organization. We show that cluster formation and extend depend on several factors, including the balance between potentiation and depression, the afferents' mean firing rate and crucially on the dendritic morphology. We find that STDP balance has an important role to play for this emergent mode of spatial organization since any imbalances lead to severe degradation- and in some case even destruction- of the mosaic. Our model suggests that, over a broad range of of STDP parameters, synaptic plasticity shapes the spatial arrangement of synapses, favoring the formation of clustered efficacy engrams."}], "ArticleTitle": "Modulating STDP Balance Impacts the Dendritic Mosaic."}, "29093675": {"mesh": [], "AbstractText": [{"section": null, "text": "Active inference is a corollary of the Free Energy Principle that prescribes how self-organizing biological agents interact with their environment. The study of active inference processes relies on the definition of a generative probabilistic model and a description of how a free energy functional is minimized by neuronal message passing under that model. This paper presents a tutorial introduction to specifying active inference processes by Forney-style factor graphs (FFG). The FFG framework provides both an insightful representation of the probabilistic model and a biologically plausible inference scheme that, in principle, can be automatically executed in a computer simulation. As an illustrative example, we present an FFG for a deep temporal active inference process. The graph clearly shows how policy selection by expected free energy minimization results from free energy minimization per se, in an appropriate generative policy model."}], "ArticleTitle": "A Factor Graph Description of Deep Temporal Active Inference."}, "28082892": {"mesh": [], "AbstractText": [{"section": null, "text": "A fundamental question concerning representation of the visual world in our brain is how a cortical cell responds when presented with more than a single stimulus. We find supportive evidence that most cells presented with a pair of stimuli respond predominantly to one stimulus at a time, rather than a weighted average response. Traditionally, the firing rate is assumed to be a weighted average of the firing rates to the individual stimuli (response-averaging model) (Bundesen et al., 2005). Here, we also evaluate a probability-mixing model (Bundesen et al., 2005), where neurons temporally multiplex the responses to the individual stimuli. This provides a mechanism by which the representational identity of multiple stimuli in complex visual scenes can be maintained despite the large receptive fields in higher extrastriate visual cortex in primates. We compare the two models through analysis of data from single cells in the middle temporal visual area (MT) of rhesus monkeys when presented with two separate stimuli inside their receptive field with attention directed to one of the two stimuli or outside the receptive field. The spike trains were modeled by stochastic point processes, including memory effects of past spikes and attentional effects, and statistical model selection between the two models was performed by information theoretic measures as well as the predictive accuracy of the models. As an auxiliary measure, we also tested for uni- or multimodality in interspike interval distributions, and performed a correlation analysis of simultaneously recorded pairs of neurons, to evaluate population behavior."}], "ArticleTitle": "Neurons in Primate Visual Cortex Alternate between Responses to Multiple Stimuli in Their Receptive Field."}, "33390922": {"mesh": [], "AbstractText": [{"section": null, "text": "A fundamental neuroscience question is how memories are maintained from days to a lifetime, given turnover of proteins that underlie expression of long-term synaptic potentiation (LTP) or \"tag\" synapses as eligible for LTP. A likely solution relies on synaptic positive feedback loops, prominently including persistent activation of Ca2+/calmodulin kinase II (CaMKII) and self-activated synthesis of protein kinase M &#950; (PKM&#950;). Data also suggest positive feedback based on recurrent synaptic reactivation within neuron assemblies, or engrams, is necessary to maintain memories. The relative importance of these mechanisms is controversial. To explore the likelihood that each mechanism is necessary or sufficient to maintain memory, we simulated maintenance of LTP with a simplified model incorporating persistent kinase activation, synaptic tagging, and preferential reactivation of strong synapses, and analyzed implications of recent data. We simulated three model variants, each maintaining LTP with one feedback loop: autonomous, self-activated PKM&#950; synthesis (model variant I); self-activated CamKII (model variant II); and recurrent reactivation of strengthened synapses (model variant III). Variant I predicts that, for successful maintenance of LTP, either 1) PKM&#950; contributes to synaptic tagging, or 2) a low constitutive tag level persists during maintenance independent of PKM&#950;, or 3) maintenance of LTP is independent of tagging. Variant II maintains LTP and suggests persistent CaMKII activation could maintain PKM&#950; activity, a feedforward interaction not previously considered. However, we note data challenging the CaMKII feedback loop. In Variant III synaptic reactivation drives, and thus predicts, recurrent or persistent activation of CamKII and other necessary kinases, plausibly contributing to persistent elevation of PKM&#950; levels. Reactivation is thus predicted to sustain recurrent rounds of synaptic tagging and incorporation of plasticity-related proteins. We also suggest (model variant IV) that synaptic reactivation and autonomous kinase activation could synergistically maintain LTP. We propose experiments that could discriminate these maintenance mechanisms."}], "ArticleTitle": "Comparing Theories for the Maintenance of Late LTP and Long-Term Memory: Computational Analysis of the Roles of Kinase Feedback Pathways and Synaptic Reactivation."}, "28713258": {"mesh": [], "AbstractText": [{"section": null, "text": "How the brain reconstitutes consciousness and cognition after a major perturbation like general anesthesia is an important question with significant neuroscientific and clinical implications. Recent empirical studies in animals and humans suggest that the recovery of consciousness after anesthesia is not random but ordered. Emergence patterns have been classified as progressive and abrupt transitions from anesthesia to consciousness, with associated differences in duration and electroencephalogram (EEG) properties. We hypothesized that the progressive and abrupt emergence patterns from the unconscious state are associated with, respectively, continuous and discontinuous synchronization transitions in functional brain networks. The discontinuous transition is explainable with the concept of explosive synchronization, which has been studied almost exclusively in network science. We used the Kuramato model, a simple oscillatory network model, to simulate progressive and abrupt transitions in anatomical human brain networks acquired from diffusion tensor imaging (DTI) of 82 brain regions. To facilitate explosive synchronization, distinct frequencies for hub nodes with a large frequency disassortativity (i.e., higher frequency nodes linking with lower frequency nodes, or vice versa) were applied to the brain network. In this simulation study, we demonstrated that both progressive and abrupt transitions follow distinct synchronization processes at the individual node, cluster, and global network levels. The characteristic synchronization patterns of brain regions that are \"progressive and earlier\" or \"abrupt but delayed\" account for previously reported behavioral responses of gradual and abrupt emergence from the unconscious state. The characteristic network synchronization processes observed at different scales provide new insights into how regional brain functions are reconstituted during progressive and abrupt emergence from the unconscious state. This theoretical approach also offers a principled explanation of how the brain reconstitutes consciousness and cognitive functions after physiologic (sleep), pharmacologic (anesthesia), and pathologic (coma) perturbations."}], "ArticleTitle": "Relationship of Topology, Multiscale Phase Synchronization, and State Transitions in Human Brain Networks."}, "29666576": {"mesh": [], "AbstractText": [{"section": null, "text": "The modular control hypothesis suggests that motor commands are built from precoded modules whose specific combined recruitment can allow the performance of virtually any motor task. Despite considerable experimental support, this hypothesis remains tentative as classical findings of reduced dimensionality in muscle activity may also result from other constraints (biomechanical couplings, data averaging or low dimensionality of motor tasks). Here we assessed the effectiveness of modularity in describing muscle activity in a comprehensive experiment comprising 72 distinct point-to-point whole-body movements during which the activity of 30 muscles was recorded. To identify invariant modules of a temporal and spatial nature, we used a space-by-time decomposition of muscle activity that has been shown to encompass classical modularity models. To examine the decompositions, we focused not only on the amount of variance they explained but also on whether the task performed on each trial could be decoded from the single-trial activations of modules. For the sake of comparison, we confronted these scores to the scores obtained from alternative non-modular descriptions of the muscle data. We found that the space-by-time decomposition was effective in terms of data approximation and task discrimination at comparable reduction of dimensionality. These findings show that few spatial and temporal modules give a compact yet approximate representation of muscle patterns carrying nearly all task-relevant information for a variety of whole-body reaching movements."}], "ArticleTitle": "Space-by-Time Modular Decomposition Effectively Describes Whole-Body Muscle Activity During Upright Reaching in Various Directions."}, "29674960": {"mesh": [], "AbstractText": [{"section": null, "text": "Dynamics of a homogeneous neural population interacting with active extracellular medium were considered. The corresponding mathematical model was tuned specifically to describe the behavior of interneurons with tonic GABA conductance under the action of non-stationary ambient GABA. The feedback provided by the GABA mediated transmembrane current enriched the repertoire of population activity by enabling the oscillatory behavior. This behavior appeared in the form of relaxation oscillations which can be considered as a specific type of brainwaves."}], "ArticleTitle": "Emergence of Relaxation Oscillations in Neurons Interacting With Non-stationary Ambient GABA."}, "30279653": {"mesh": [], "AbstractText": [{"section": null, "text": "Repeating spatiotemporal spike patterns exist and carry information. Here we investigated how a single spiking neuron can optimally respond to one given pattern (localist coding), or to either one of several patterns (distributed coding, i.e., the neuron's response is ambiguous but the identity of the pattern could be inferred from the response of multiple neurons), but not to random inputs. To do so, we extended a theory developed in a previous paper (Masquelier, 2017), which was limited to localist coding. More specifically, we computed analytically the signal-to-noise ratio (SNR) of a multi-pattern-detector neuron, using a threshold-free leaky integrate-and-fire (LIF) neuron model with non-plastic unitary synapses and homogeneous Poisson inputs. Surprisingly, when increasing the number of patterns, the SNR decreases slowly, and remains acceptable for several tens of independent patterns. In addition, we investigated whether spike-timing-dependent plasticity (STDP) could enable a neuron to reach the theoretical optimal SNR. To this aim, we simulated a LIF equipped with STDP, and repeatedly exposed it to multiple input spike patterns, embedded in equally dense Poisson spike trains. The LIF progressively became selective to every repeating pattern with no supervision, and stopped discharging during the Poisson spike trains. Furthermore, tuning certain STDP parameters, the resulting pattern detectors were optimal. Tens of independent patterns could be learned by a single neuron using a low adaptive threshold, in contrast with previous studies, in which higher thresholds led to localist coding only. Taken together these results suggest that coincidence detection and STDP are powerful mechanisms, fully compatible with distributed coding. Yet we acknowledge that our theory is limited to single neurons, and thus also applies to feed-forward networks, but not to recurrent ones."}], "ArticleTitle": "Optimal Localist and Distributed Coding of Spatiotemporal Spike Patterns Through STDP and Coincidence Detection."}, "28790909": {"mesh": [], "AbstractText": [{"section": null, "text": "Subthreshold fluctuations in neuronal membrane potential traces contain nonlinear components, and employing nonlinear models might improve the statistical inference. We propose a new strategy to estimate synaptic conductances, which has been tested using in silico data and applied to in vivo recordings. The model is constructed to capture the nonlinearities caused by subthreshold activated currents, and the estimation procedure can discern between excitatory and inhibitory conductances using only one membrane potential trace. More precisely, we perform second order approximations of biophysical models to capture the subthreshold nonlinearities, resulting in quadratic integrate-and-fire models, and apply approximate maximum likelihood estimation where we only suppose that conductances are stationary in a 50-100 ms time window. The results show an improvement compared to existent procedures for the models tested here."}], "ArticleTitle": "Estimation of Synaptic Conductances in Presence of Nonlinear Effects Caused by Subthreshold Ionic Currents."}, "28798678": {"mesh": [], "AbstractText": [{"section": null, "text": "The striatum is the primary input nucleus for the basal ganglia, and receives glutamatergic afferents from the cortex. Under the hypothesis that basal ganglia perform action selection, these cortical afferents encode potential \"action requests.\" Previous studies have suggested the striatum may utilize a mutually inhibitory network of medium spiny neurons (MSNs) to filter these requests so that only those of high salience are selected. However, the mechanisms enabling the striatum to perform clean, rapid switching between distinct actions that form part of a learned action sequence are still poorly understood. Substance P (SP) and enkephalin are neuropeptides co-released with GABA in MSNs preferentially expressing D1 or D2 dopamine receptors respectively. SP has a facilitatory effect on subsequent glutamatergic inputs to target MSNs, while enkephalin has an inhibitory effect. Blocking the action of SP in the striatum is also known to affect behavioral transitions. We constructed phenomenological models of the effects of SP and enkephalin, and integrated these into a hybrid model of basal ganglia comprising a spiking striatal microcircuit and rate-coded populations representing other major structures. We demonstrated that diffuse neuropeptide connectivity enhanced the selection of unordered action requests, and that for true action sequences, where action semantics define a fixed structure, a patterning of the SP connectivity reflecting this ordering enhanced selection of actions presented in the correct sequential order and suppressed incorrect ordering. We also showed that selective pruning of SP connections allowed context-sensitive inhibition of specific undesirable requests that otherwise interfered with selection of an action group. Our model suggests that the interaction of SP and enkephalin enhances the contrast between selection and rejection of action requests, and that patterned SP connectivity in the striatum allows the \"chunking\" of actions and improves selection of sequences. Efficient execution of action sequences may therefore result from a combination of ordered cortical inputs and patterned neuropeptide connectivity within striatum."}], "ArticleTitle": "Striatal Neuropeptides Enhance Selection and Rejection of Sequential Actions."}, "29467641": {"mesh": [], "AbstractText": [{"section": null, "text": "The coexistence of neuronal activity regimes has been reported under normal and pathological conditions. Such multistability could enhance the flexibility of the nervous system and has many implications for motor control, memory, and decision making. Multistability is commonly promoted by neuromodulation targeting specific membrane ionic currents. Here, we investigated how modulation of different ionic currents could affect the neuronal propensity for bistability. We considered a leech heart interneuron model. It exhibits bistability of bursting and silence in a narrow range of the leak current parameters, conductance (gleak ) and reversal potential (Eleak ). We assessed the propensity for bistability of the model by using bifurcation diagrams. On the diagram (gleak , Eleak ), we mapped bursting and silent regimes. For the canonical value of Eleak we determined the range of gleak which supported the bistability. We use this range as an index of propensity for bistability. We investigated how this index was affected by alterations of ionic currents. We systematically changed their conductances, one at a time, and built corresponding bifurcation diagrams in parameter planes of the maximal conductance of a given current and the leak conductance. We found that conductance of only one current substantially affected the index of propensity; the increase of the maximal conductance of the hyperpolarization-activated cationic current increased the propensity index. The second conductance with the strongest effect was the conductance of the low-threshold fast Ca2+ current; its reduction increased the propensity index although the effect was about two times smaller in magnitude. Analyzing the model with both changes applied simultaneously, we found that the diagram (gleak , Eleak ) showed a progressively expanded area of bistability of bursting and silence."}], "ArticleTitle": "Propensity for Bistability of Bursting and Silence in the Leech Heart Interneuron."}, "29209191": {"mesh": [], "AbstractText": [{"section": null, "text": "Precise spike timing is considered to play a fundamental role in communications and signal processing in biological neural networks. Understanding the mechanism of spike timing adjustment would deepen our understanding of biological systems and enable advanced engineering applications such as efficient computational architectures. However, the biological mechanisms that adjust and maintain spike timing remain unclear. Existing algorithms adopt a supervised approach, which adjusts the axonal conduction delay and synaptic efficacy until the spike timings approximate the desired timings. This study proposes a spike timing-dependent learning model that adjusts the axonal conduction delay and synaptic efficacy in both unsupervised and supervised manners. The proposed learning algorithm approximates the Expectation-Maximization algorithm, and classifies the input data encoded into spatio-temporal spike patterns. Even in the supervised classification, the algorithm requires no external spikes indicating the desired spike timings unlike existing algorithms. Furthermore, because the algorithm is consistent with biological models and hypotheses found in existing biological studies, it could capture the mechanism underlying biological delay learning."}], "ArticleTitle": "Conduction Delay Learning Model for Unsupervised and Supervised Classification of Spatio-Temporal Spike Patterns."}, "28620291": {"mesh": [], "AbstractText": [{"section": null, "text": "Neuronal networks are often characterized by their spiking and bursting statistics. Previously, we introduced an adaptive burst analysis method which enhances the analysis power for neuronal networks with highly varying firing dynamics. The adaptation is based on single channels analyzing each element of a network separately. Such kind of analysis was adequate for the assessment of local behavior, where the analysis focuses on the neuronal activity in the vicinity of a single electrode. However, the assessment of the whole network may be hampered, if parts of the network are analyzed using different rules. Here, we test how using multiple channels and measurement time points affect adaptive burst detection. The main emphasis is, if network-wide adaptive burst detection can provide new insights into the assessment of network activity. Therefore, we propose a modification to the previously introduced inter-spike interval (ISI) histogram based cumulative moving average (CMA) algorithm to analyze multiple spike trains simultaneously. The network size can be freely defined, e.g., to include all the electrodes in a microelectrode array (MEA) recording. Additionally, the method can be applied on a series of measurements on the same network to pool the data for statistical analysis. Firstly, we apply both the original CMA-algorithm and our proposed network-wide CMA-algorithm on artificial spike trains to investigate how the modification changes the burst detection. Thereafter, we use the algorithms on MEA data of spontaneously active chemically manipulated in vitro rat cortical networks. Moreover, we compare the synchrony of the detected bursts introducing a new burst synchrony measure. Finally, we demonstrate how the bursting statistics can be used to classify networks by applying k-means clustering to the bursting statistics. The results show that the proposed network wide adaptive burst detection provides a method to unify the burst definition in the whole network and thus improves the assessment and classification of the neuronal activity, e.g., the effects of different pharmaceuticals. The results indicate that the novel method is adaptive enough to be usable on networks with different dynamics, and it is especially feasible when comparing the behavior of differently spiking networks, for example in developing networks."}], "ArticleTitle": "Network-Wide Adaptive Burst Detection Depicts Neuronal Activity with Improved Accuracy."}, "30809141": {"mesh": [], "AbstractText": [{"section": null, "text": "A new computational framework implementing asynchronous neural dynamics is used to address the duality between synchronous vs. asynchronous processes, and their possible relation to conscious vs. unconscious behaviors. Extending previous results on modeling the first three levels of animal awareness, this formalism is used here to produce the execution traces of parallel threads that implement these models. Running simulations demonstrate how sensory stimuli associated with a population of excitatory neurons inhibit in turn other neural assemblies i.e., a kind of neuronal asynchronous wiring/unwiring process that is reflected in the progressive trimming of execution traces. Whereas, reactive behaviors relying on configural learning produce vanishing traces, the learning of a rule and its later application produce persistent traces revealing potential synchronous roots of animal awareness. In contrast, to previous formalisms that use analytical and/or statistical methods to search for patterns existing in a brain, this new framework proposes a tool for studying the emergence of brain structures that might be associated with higher level cognitive capabilities."}], "ArticleTitle": "Symbolic Modeling of Asynchronous Neural Dynamics Reveals Potential Synchronous Roots for the Emergence of Awareness."}, "32477087": {"mesh": [], "AbstractText": [{"section": null, "text": "Recently DCNN (Deep Convolutional Neural Network) has been advocated as a general and promising modeling approach for neural object representation in primate inferotemporal cortex. In this work, we show that some inherent non-uniqueness problem exists in the DCNN-based modeling of image object representations. This non-uniqueness phenomenon reveals to some extent the theoretical limitation of this general modeling approach, and invites due attention to be taken in practice."}], "ArticleTitle": "Non-uniqueness Phenomenon of Object Representation in Modeling IT Cortex by Deep Convolutional Neural Network (DCNN)."}, "30210325": {"mesh": [], "AbstractText": [{"section": null, "text": "We analyse statistical and information-theoretical properties of EEG microstate sequences, as seen through the lens of five different clustering algorithms. Microstate sequences are computed for n = 20 resting state EEG recordings during wakeful rest. The input for all clustering algorithms is the set of EEG topographic maps obtained at local maxima of the spatial variance. This data set is processed by two classical microstate clustering algorithms (1) atomize and agglomerate hierarchical clustering (AAHC) and (2) a modified K-means algorithm, as well as by (3) K-medoids, (4) principal component analysis (PCA) and (5) fast independent component analysis (Fast-ICA). Using this technique, EEG topographies can be substituted with microstate labels by competitive fitting based on spatial correlation, resulting in a symbolic, non-metric time series, the microstate sequence. Microstate topographies and symbolic time series are further analyzed statistically, including static and dynamic properties. Static properties, which do not contain information about temporal dependencies of the microstate sequence include the maximum similarity of microstate maps within and between the tested clustering algorithms, the global explained variance and the Shannon entropy of the microstate sequences. Dynamic properties are sensitive to temporal correlations between the symbols and include the mixing time of the microstate transition matrix, the entropy rate of the microstate sequences and the location of the first local maximum of the autoinformation function. We also test the Markov property of microstate sequences, the time stationarity of the transition matrix and detect periodicities by means of time-lagged mutual information. Finally, possible long-range correlations of microstate sequences are assessed via Hurst exponent estimation. We find that while static properties partially reflect properties of the clustering algorithms, information-theoretical quantities are largely invariant with respect to the clustering method used. As each clustering algorithm has its own profile of computational speed, ease of implementation, determinism vs. stochasticity and theoretical underpinnings, our results convey a positive message concerning the free choice of method and the comparability of results obtained from different algorithms. The invariance of these quantities implies that the tested properties are algorithm-independent, inherent features of resting state EEG derived microstate sequences."}], "ArticleTitle": "EEG Microstate Sequences From Different Clustering Algorithms Are Information-Theoretically Invariant."}, "27917120": {"mesh": [], "AbstractText": [{"section": null, "text": "The generation of pain signals from primary afferent neurons is explained by a labeled-line code. However, this notion cannot apply in a simple way to cutaneous C-fibers, which carry signals from a variety of receptors that respond to various stimuli including agonist chemicals. To represent the discharge patterns of C-fibers according to different agonist chemicals, we have developed a quantitative approach using three consecutive spikes. By using this method, the generation of pain in response to chemical stimuli is shown to be dependent on the temporal aspect of the spike trains. Furthermore, under pathological conditions, gamma-aminobutyric acid resulted in pain behavior without change of spike number but with an altered discharge pattern. Our results suggest that information about the agonist chemicals may be encoded in specific temporal patterns of signals in C-fibers, and nociceptive sensation may be influenced by the extent of temporal summation originating from the temporal patterns."}], "ArticleTitle": "Analysis of Nociceptive Information Encoded in the Temporal Discharge Patterns of Cutaneous C-Fibers."}, "29643773": {"mesh": [], "AbstractText": [{"section": null, "text": "The brain uses a mixture of distributed and modular organization to perform computations and generate appropriate actions. While the principles under which the brain might perform computations using modular systems have been more amenable to modeling, the principles by which the brain might make choices using distributed principles have not been explored. Our goal in this perspective is to delineate some of those distributed principles using a neural network method and use its results as a lens through which to reconsider some previously published neurophysiological data. To allow for direct comparison with our own data, we trained the neural network to perform binary risky choices. We find that value correlates are ubiquitous and are always accompanied by non-value information, including spatial information (i.e., no pure value signals). Evaluation, comparison, and selection were not distinct processes; indeed, value signals even in the earliest stages contributed directly, albeit weakly, to action selection. There was no place, other than at the level of action selection, at which dimensions were fully integrated. No units were specialized for specific offers; rather, all units encoded the values of both offers in an anti-correlated format, thus contributing to comparison. Individual network layers corresponded to stages in a continuous rotation from input to output space rather than to functionally distinct modules. While our network is likely to not be a direct reflection of brain processes, we propose that these principles should serve as hypotheses to be tested and evaluated for future studies."}], "ArticleTitle": "Using a Simple Neural Network to Delineate Some Principles of Distributed Economic Choice."}, "29922142": {"mesh": [], "AbstractText": [{"section": null, "text": "Effective connectivity measures the pattern of causal interactions between brain regions. Traditionally, these patterns of causality are inferred from brain recordings using either non-parametric, i.e., model-free, or parametric, i.e., model-based, approaches. The latter approaches, when based on biophysically plausible models, have the advantage that they may facilitate the interpretation of causality in terms of underlying neural mechanisms. Recent biophysically plausible neural network models of recurrent microcircuits have shown the ability to reproduce well the characteristics of real neural activity and can be applied to model interacting cortical circuits. Unfortunately, however, it is challenging to invert these models in order to estimate effective connectivity from observed data. Here, we propose to use a classification-based method to approximate the result of such complex model inversion. The classifier predicts the pattern of causal interactions given a multivariate timeseries as input. The classifier is trained on a large number of pairs of multivariate timeseries and the respective pattern of causal interactions, which are generated by simulation from the neural network model. In simulated experiments, we show that the proposed method is much more accurate in detecting the causal structure of timeseries than current best practice methods. Additionally, we present further results to characterize the validity of the neural network model and the ability of the classifier to adapt to the generative model of the data."}], "ArticleTitle": "Classification-Based Prediction of Effective Connectivity Between Timeseries With a Realistic Cortical Network Model."}, "27932971": {"mesh": [], "AbstractText": [{"section": null, "text": "Phase-amplitude coupling (PAC) plays an important role in neural communication and computation. Interestingly, recent studies have indicated the presence of ubiquitous PAC phenomenon even during the resting state. Despite the importance of PAC phenomenon, estimation of significant physiological PAC is challenging because of the lack of appropriate surrogate measures to control false positives caused by non-physiological PAC. Therefore, in the present study, we evaluated PAC phenomenon during resting-state magnetoencephalography (MEG) signal and considered various surrogate measures and computational approaches widely used in the literature in addition to proposing new ones. We evaluated PAC phenomenon over the entire length of the MEG signal and for multiple shorter time segments. The results indicate that the extent of PAC phenomenon mainly depends on the surrogate measures and PAC computational methods used, as well as the evaluation approach. After a careful and critical evaluation, we found that resting-state MEG signals failed to exhibit ubiquitous PAC phenomenon, contrary to what has been suggested previously."}], "ArticleTitle": "Evaluation of Phase-Amplitude Coupling in Resting State Magnetoencephalographic Signals: Effect of Surrogates and Evaluation Approach."}, "27642281": {"mesh": [], "AbstractText": [{"section": null, "text": "View-invariant object recognition is a challenging problem that has attracted much attention among the psychology, neuroscience, and computer vision communities. Humans are notoriously good at it, even if some variations are presumably more difficult to handle than others (e.g., 3D rotations). Humans are thought to solve the problem through hierarchical processing along the ventral stream, which progressively extracts more and more invariant visual features. This feed-forward architecture has inspired a new generation of bio-inspired computer vision systems called deep convolutional neural networks (DCNN), which are currently the best models for object recognition in natural images. Here, for the first time, we systematically compared human feed-forward vision and DCNNs at view-invariant object recognition task using the same set of images and controlling the kinds of transformation (position, scale, rotation in plane, and rotation in depth) as well as their magnitude, which we call \"variation level.\" We used four object categories: car, ship, motorcycle, and animal. In total, 89 human subjects participated in 10 experiments in which they had to discriminate between two or four categories after rapid presentation with backward masking. We also tested two recent DCNNs (proposed respectively by Hinton's group and Zisserman's group) on the same tasks. We found that humans and DCNNs largely agreed on the relative difficulties of each kind of variation: rotation in depth is by far the hardest transformation to handle, followed by scale, then rotation in plane, and finally position (much easier). This suggests that DCNNs would be reasonable models of human feed-forward vision. In addition, our results show that the variation levels in rotation in depth and scale strongly modulate both humans' and DCNNs' recognition performances. We thus argue that these variations should be controlled in the image datasets used in vision research. "}], "ArticleTitle": "Humans and Deep Networks Largely Agree on Which Kinds of Variation Make Object Recognition Harder."}, "30271337": {"mesh": [], "AbstractText": [{"section": null, "text": "Whether an event-related potential (ERP), N170, related to facial recognition was modulated by emotion has always been a controversial issue. Some researchers considered the N170 to be independent of emotion, whereas a recent study has shown the opposite view. In the current study, electroencephalogram (EEG) recordings while responding to facial pictures with emotion were utilized to investigate whether the N170 was modulated by emotion. We found that there was a significant difference between ERP trials with positive and negative emotions of around 170 ms at the occipitotemporal electrodes (i.e., N170). Then, we further proposed the application of the single-trial N170 as a feature for the classification of facial emotion, which could avoid the fact that ERPs were obtained by averaging most of the time while ignoring the trial-to-trial variation. In order to find an optimal classifier for emotional classification with single-trial N170 as a feature, three types of classifiers, namely, linear discriminant analysis (LDA), L1-regularized logistic regression (L1LR), and support vector machine with radial basis function (RBF-SVM), were comparatively investigated. The results showed that the single-trial N170 could be used as a classification feature to successfully distinguish positive emotion from negative emotion. L1-regularized logistic regression classifiers showed a good generalization, whereas LDA showed a relatively poor generalization. Moreover, when compared with L1LR, the RBF-SVM required more time to optimize the parameters during the classification, which became an obstacle while applying it to the online operating system of brain-computer interfaces (BCIs). The findings suggested that face-related N170 could be affected by facial expression and that the single-trial N170 could be a biomarker used to monitor the emotional states of subjects for the BCI domain."}], "ArticleTitle": "Classification for Single-Trial N170 During Responding to Facial Picture With Emotion."}, "30319383": {"mesh": [], "AbstractText": [{"section": null, "text": "As a sophisticated computing unit, the pyramidal neuron requires sufficient metabolic energy to fuel its powerful computational capabilities. However, the majority of previous works focus on nonlinear integration and energy consumption in individual pyramidal neurons but seldom on the effects of metabolic energy on synaptic transmission and dendritic integration. Here, we developed biologically plausible models to simulate the synaptic transmission and dendritic integration of pyramidal neurons, exploring the relations between synaptic transmission and metabolic energy and between dendritic integration and metabolic energy. We find that synaptic energy not only drives synaptic vesicle cycle, but also participates in the regulation of this cycle. Release probability of synapses adapts to synaptic energy levels by regulating the speed of synaptic vesicle cycle. Besides, we also find that to match neural energy levels, only a part of the synapses receive presynaptic signals during a given period so that neurons have a low action potential frequency. That is, the number of simultaneously active synapses over a period of time should be adapted to neural energy levels."}], "ArticleTitle": "Effects of Metabolic Energy on Synaptic Transmission and Dendritic Integration in Pyramidal Neurons."}, "33469425": {"mesh": [], "AbstractText": [{"section": null, "text": "Brain-Computer Interface (BCI) systems enable an alternative communication channel for severely-motor disabled patients to interact with their environment using no muscular movements. In recent years, the importance of research into non-gaze dependent brain-computer interface paradigms has been increasing, in contrast to the most frequently studied BCI-based speller paradigm (i.e., row-column presentation, RCP). Several visual modifications that have already been validated under the RCP paradigm for communication purposes have not been validated under the most extended non-gaze dependent rapid serial visual presentation (RSVP) paradigm. Thus, in the present study, three different sets of stimuli were assessed under RSVP, with the following communication features: white letters (WL), famous faces (FF), neutral pictures (NP). Eleven healthy subjects participated in this experiment, in which the subjects had to go through a calibration phase, an online phase and, finally, a subjective questionnaire completion phase. The results showed that the FF and NP stimuli promoted better performance in the calibration and online phases, being slightly better in the FF paradigm. Regarding the subjective questionnaires, again both FF and NP were preferred by the participants in contrast to the WL stimuli, but this time the NP stimuli scored slightly higher. These findings suggest that the use of FF and NP for RSVP-based spellers could be beneficial to increase information transfer rate in comparison to the most frequently used letter-based stimuli and could represent a promising communication system for individuals with altered ocular-motor function."}], "ArticleTitle": "Performance Analysis With Different Types of Visual Stimuli in a BCI-Based Speller Under an RSVP Paradigm."}, "28082889": {"mesh": [], "AbstractText": [{"section": null, "text": "Deep neural networks (DNNs) provide useful models of visual representational transformations. We present a method that enables a DNN (student) to learn from the internal representational spaces of a reference model (teacher), which could be another DNN or, in the future, a biological brain. Representational spaces of the student and the teacher are characterized by representational distance matrices (RDMs). We propose representational distance learning (RDL), a stochastic gradient descent method that drives the RDMs of the student to approximate the RDMs of the teacher. We demonstrate that RDL is competitive with other transfer learning techniques for two publicly available benchmark computer vision datasets (MNIST and CIFAR-100), while allowing for architectural differences between student and teacher. By pulling the student's RDMs toward those of the teacher, RDL significantly improved visual classification performance when compared to baseline networks that did not use transfer learning. In the future, RDL may enable combined supervised training of deep neural networks using task constraints (e.g., images and category labels) and constraints from brain-activity measurements, so as to build models that replicate the internal representational spaces of biological brains."}], "ArticleTitle": "Representational Distance Learning for Deep Neural Networks."}, "28458635": {"mesh": [], "AbstractText": [{"section": null, "text": "Multi-electrode arrays (MEA) are increasingly used to investigate spontaneous neuronal network activity. The recorded signals comprise several distinct components: Apart from artifacts without biological significance, one can distinguish between spikes (action potentials) and subthreshold fluctuations (local fields potentials). Here we aim to develop a theoretical model that allows for a compact and robust characterization of subthreshold fluctuations in terms of a Gaussian statistical field theory in two spatial and one temporal dimension. What is usually referred to as the driving noise in the context of statistical physics is here interpreted as a representation of the neural activity. Spatial and temporal correlations of this activity give valuable information about the connectivity in the neural tissue. We apply our methods on a dataset obtained from MEA-measurements in an acute hippocampal brain slice from a rat. Our main finding is that the empirical correlation functions indeed obey the logarithmic behavior that is a general feature of theoretical models of this kind. We also find a clear correlation between the activity and the occurrence of spikes. Another important insight is the importance of correctly separating out certain artifacts from the data before proceeding with the analysis."}], "ArticleTitle": "Analysis and Modeling of Subthreshold Neural Multi-Electrode Array Data by Statistical Field Theory."}, "32733225": {"mesh": [], "AbstractText": [{"section": null, "text": "[This corrects the article DOI: 10.3389/fncom.2020.00031.]."}], "ArticleTitle": "Erratum: Contextual Integration in Cortical and Convolutional Neural Networks."}, "29209190": {"mesh": [], "AbstractText": [{"section": null, "text": "Feature extraction is an important step in the process of electroencephalogram (EEG) signal classification. The authors propose a \"pattern recognition\" approach that discriminates EEG signals recorded during different cognitive conditions. Wavelet based feature extraction such as, multi-resolution decompositions into detailed and approximate coefficients as well as relative wavelet energy were computed. Extracted relative wavelet energy features were normalized to zero mean and unit variance and then optimized using Fisher's discriminant ratio (FDR) and principal component analysis (PCA). A high density EEG dataset validated the proposed method (128-channels) by identifying two classifications: (1) EEG signals recorded during complex cognitive tasks using Raven's Advance Progressive Metric (RAPM) test; (2) EEG signals recorded during a baseline task (eyes open). Classifiers such as, K-nearest neighbors (KNN), Support Vector Machine (SVM), Multi-layer Perceptron (MLP), and Na&#239;ve Bayes (NB) were then employed. Outcomes yielded 99.11% accuracy via SVM classifier for coefficient approximations (A5) of low frequencies ranging from 0 to 3.90 Hz. Accuracy rates for detailed coefficients were 98.57 and 98.39% for SVM and KNN, respectively; and for detailed coefficients (D5) deriving from the sub-band range (3.90-7.81 Hz). Accuracy rates for MLP and NB classifiers were comparable at 97.11-89.63% and 91.60-81.07% for A5 and D5 coefficients, respectively. In addition, the proposed approach was also applied on public dataset for classification of two cognitive tasks and achieved comparable classification results, i.e., 93.33% accuracy with KNN. The proposed scheme yielded significantly higher classification performances using machine learning classifiers compared to extant quantitative feature extraction. These results suggest the proposed feature extraction method reliably classifies EEG signals recorded during cognitive tasks with a higher degree of accuracy."}], "ArticleTitle": "Classification of EEG Signals Based on Pattern Recognition Approach."}, "30186130": {"mesh": [], "AbstractText": [{"section": null, "text": "Motor system uses muscle synergies as a modular organization to simplify the control of movements. Motor cortical impairments, such as stroke and spinal cord injuries, disrupt the orchestration of the muscle synergies and result in abnormal movements. In this paper, the alterations of muscle synergies in subacute stroke survivors were examined during the voluntary reaching movement. We collected electromyographic (EMG) data from 35 stroke survivors, ranging from Brunnstrom Stage III to VI, and 25 age-matched control subjects. Muscle synergies were extracted from the activity of 7 upper-limb muscles via nonnegative matrix factorization under the criterion of 95% variance accounted for. By comparing the structure of muscle synergies and the similarity of activation coefficients across groups, we can validate the increasing activation of pectoralis major muscle and the decreasing activation of elbow extensor of triceps in stroke groups. Furthermore, the similarity of muscle synergies was significantly correlated with the Brunnstrom Stage (R = 0.52, p < 0.01). The synergies of stroke survivors at Brunnstrom Stage IV-III gradually diverged from those of control group, but the activation coefficients remained the same after stroke, irrespective of the recovery level."}], "ArticleTitle": "Alterations of Muscle Synergies During Voluntary Arm Reaching Movement in Subacute Stroke Survivors at Different Levels of Impairment."}, "27790111": {"mesh": [], "AbstractText": [{"section": null, "text": "Several studies have explored brain computer interface (BCI) systems based on auditory stimuli, which could help patients with visual impairments. Usability and user satisfaction are important considerations in any BCI. Although background music can influence emotion and performance in other task environments, and many users may wish to listen to music while using a BCI, auditory, and other BCIs are typically studied without background music. Some work has explored the possibility of using polyphonic music in auditory BCI systems. However, this approach requires users with good musical skills, and has not been explored in online experiments. Our hypothesis was that an auditory BCI with background music would be preferred by subjects over a similar BCI without background music, without any difference in BCI performance. We introduce a simple paradigm (which does not require musical skill) using percussion instrument sound stimuli and background music, and evaluated it in both offline and online experiments. The result showed that subjects preferred the auditory BCI with background music. Different performance measures did not reveal any significant performance effect when comparing background music vs. no background. Since the addition of background music does not impair BCI performance but is preferred by users, auditory (and perhaps other) BCIs should consider including it. Our study also indicates that auditory BCIs can be effective even if the auditory channel is simultaneously otherwise engaged."}], "ArticleTitle": "Effects of Background Music on Objective and Subjective Performance Measures in an Auditory BCI."}, "27721750": {"mesh": [], "AbstractText": [{"section": null, "text": "The striatum is an important subcortical structure with extensive connections to other regions of the brain. These connections are believed to play important roles in behaviors such as reward-related processes and impulse control, which show significant sex differences. However, little is known about sex differences in the striatum-projected fiber connectivity. The current study examined sex differences between 50 Chinese males and 79 Chinese females in their fiber connections between the striatum and nine selected cortical and subcortical regions. Despite overall similarities, males showed stronger fiber connections between the left caudate and rostral cingulate cortex, between the right putamen and the lateral orbitofrontal cortex, between the bilateral putamen and the ventro-lateral prefrontal cortex, and between the right caudate and the ventro-lateral prefrontal cortex, whereas females showed stronger fiber connections between the right putamen and the dorsolateral prefrontal cortex, between bilateral caudate and hippocampus, and between the left putamen and hippocampus. These findings help us to understand sex differences in the striatum-projected fiber connections and their implications for sex differences in behaviors."}], "ArticleTitle": "Sex Differences in Fiber Connection between the Striatum and Subcortical and Cortical Regions."}, "28450833": {"mesh": [], "AbstractText": [{"section": null, "text": "Predicting the movements, ground reaction forces and neuromuscular activity during gait can be a valuable asset to the clinical rehabilitation community, both to understand pathology, as well as to plan effective intervention. In this work we use an optimal control method to generate predictive simulations of pathological gait in the sagittal plane. We construct a patient-specific model corresponding to a 7-year old child with gait abnormalities and identify the optimal spring characteristics of an ankle-foot orthosis that minimizes muscle effort. Our simulations include the computation of foot-ground reaction forces, as well as the neuromuscular dynamics using computationally efficient muscle torque generators and excitation-activation equations. The optimal control problem (OCP) is solved with a direct multiple shooting method. The solution of this problem is physically consistent synthetic neural excitation commands, muscle activations and whole body motion. Our simulations produced similar changes to the gait characteristics as those recorded on the patient. The orthosis-equipped model was able to walk faster with more extended knees. Notably, our approach can be easily tuned to simulate weakened muscles, produces physiologically realistic ground reaction forces and smooth muscle activations and torques, and can be implemented on a standard workstation to produce results within a few hours. These results are an important contribution toward bridging the gap between research methods in computational neuromechanics and day-to-day clinical rehabilitation."}], "ArticleTitle": "Optimal Control Based Stiffness Identification of an Ankle-Foot Orthosis Using a Predictive Walking Model."}, "28487645": {"mesh": [], "AbstractText": [{"section": null, "text": "Children with cerebral palsy (CP) often develop reduced passive range of motion with age. The determining factor underlying this process is believed to be progressive development of contracture in skeletal muscle that likely changes the biomechanics of the joints. Consequently, to identify the underlying mechanisms, we modeled the mechanical characteristics of the forearm flexors acting across the wrist joint. We investigated skeletal muscle strength (Grippit&#174;) and passive stiffness and viscosity of the forearm flexors in 15 typically developing (TD) children (10 boys/5 girls, mean age 12 years, range 8-18 yrs) and nine children with CP Nine children (6 boys/3 girls, mean age 11 &#177; 3 years (yrs), range 7-15 yrs) using the NeuroFlexor&#174; apparatus. The muscle stiffness we estimate and report is the instantaneous mechanical response of the tissue that is independent of reflex activity. Furthermore, we assessed cross-sectional area of the flexor carpi radialis (FCR) muscle using ultrasound. Age and body weight did not differ significantly between the two groups. Children with CP had a significantly weaker (-65%, p < 0.01) grip and had smaller cross-sectional area (-43%, p < 0.01) of the FCR muscle. Passive stiffness of the forearm muscles in children with CP was increased 2-fold (p < 0.05) whereas viscosity did not differ significantly between CP and TD children. FCR cross-sectional area correlated to age (R2 = 0.58, p < 0.01), body weight (R2 = 0.92, p < 0.0001) and grip strength (R2 = 0.82, p < 0.0001) in TD children but only to grip strength (R2 = 0.60, p < 0.05) in children with CP. We conclude that children with CP have weaker, thinner, and stiffer forearm flexors as compared to typically developing children."}], "ArticleTitle": "Forearm Flexor Muscles in Children with Cerebral Palsy Are Weak, Thin and Stiff."}, "30622467": {"mesh": [], "AbstractText": [{"section": null, "text": "Cortical networks both in vivo and in vitro sustain asynchronous irregular firings with extremely low frequency. To realize such self-sustained activity in neural network models, balance between excitatory and inhibitory activities is known to be one of the keys. In addition, recent theoretical studies have revealed that another feature commonly observed in cortical networks, i.e., sparse but strong connections and dense weak connections, plays an essential role. The previous studies, however, have not thoroughly considered the cooperative dynamics between a network of such heterogeneous synaptic connections and intrinsic noise. The noise stimuli, representing inherent nature of the neuronal activities, e.g., variability of presynaptic discharges, should be also of significant importance for sustaining the irregular firings in cortical networks. Here, we numerically demonstrate that highly heterogeneous distribution, typically a lognormal type, of excitatory-to-excitatory connections, reduces the amount of noise required to sustain the network firing activities. In the sense that noise consumes an energy resource, the heterogeneous network receiving less amount of noise stimuli is considered to realize an efficient dynamics in cortex. A noise-driven network of bi-modally distributed synapses further shows that many weak and a few very strong synapses are the key feature of the synaptic heterogeneity, supporting the network firing activity."}], "ArticleTitle": "Highly Heterogeneous Excitatory Connections Require Less Amount of Noise to Sustain Firing Activities in Cortical Networks."}, "28377709": {"mesh": [], "AbstractText": [{"section": null, "text": "The recent \"deep learning revolution\" in artificial neural networks had strong impact and widespread deployment for engineering applications, but the use of deep learning for neurocomputational modeling has been so far limited. In this article we argue that unsupervised deep learning represents an important step forward for improving neurocomputational models of perception and cognition, because it emphasizes the role of generative learning as opposed to discriminative (supervised) learning. As a case study, we present a series of simulations investigating the emergence of neural coding of visual space for sensorimotor transformations. We compare different network architectures commonly used as building blocks for unsupervised deep learning by systematically testing the type of receptive fields and gain modulation developed by the hidden neurons. In particular, we compare Restricted Boltzmann Machines (RBMs), which are stochastic, generative networks with bidirectional connections trained using contrastive divergence, with autoencoders, which are deterministic networks trained using error backpropagation. For both learning architectures we also explore the role of sparse coding, which has been identified as a fundamental principle of neural computation. The unsupervised models are then compared with supervised, feed-forward networks that learn an explicit mapping between different spatial reference frames. Our simulations show that both architectural and learning constraints strongly influenced the emergent coding of visual space in terms of distribution of tuning functions at the level of single neurons. Unsupervised models, and particularly RBMs, were found to more closely adhere to neurophysiological data from single-cell recordings in the primate parietal cortex. These results provide new insights into how basic properties of artificial neural networks might be relevant for modeling neural information processing in biological systems."}], "ArticleTitle": "The Role of Architectural and Learning Constraints in Neural Network Models: A Case Study on Visual Space Coding."}, "27990109": {"mesh": [], "AbstractText": [{"section": null, "text": "We present a computational model by which ensembles of regularly spiking neurons can encode different time intervals through synchronous firing. We show that a neuron responding to a large population of convergent inputs has the potential to learn to produce an appropriately-timed output via spike-time dependent plasticity. We explain why temporal variability of this population synchrony increases with increasing time intervals. We also show that the scalar property of timing and its violation at short intervals can be explained by the spike-wise accumulation of jitter in the inter-spike intervals of timing neurons. We explore how the challenge of encoding longer time intervals can be overcome and conclude that this may involve a switch to a different population of neurons with lower firing rate, with the added effect of producing an earlier bias in response. Experimental data on human timing performance show features in agreement with the model's output."}], "ArticleTitle": "Timing Intervals Using Population Synchrony and Spike Timing Dependent Plasticity."}, "28232796": {"mesh": [], "AbstractText": [{"section": null, "text": "Communication between cortical sites is mediated by long-range synaptic connections. However, these connections are relatively static, while everyday cognitive tasks demand a fast and flexible routing of information in the brain. Synchronization of activity between distant cortical sites has been proposed as the mechanism underlying such a dynamic communication structure. Here, we study how oscillatory activity affects the excitability and input-output relation of local cortical circuits and how it alters the transmission of information between cortical circuits. To this end, we develop model circuits showing fast oscillations by the PING mechanism, of which the oscillatory characteristics can be altered. We identify conditions for synchronization between two brain circuits and show that the level of intercircuit coherence and the phase difference is set by the frequency difference between the intrinsic oscillations. We show that the susceptibility of the circuits to inputs, i.e., the degree of change in circuit output following input pulses, is not uniform throughout the oscillation period and that both firing rate, frequency and power are differentially modulated by inputs arriving at different phases. As a result, an appropriate phase difference between the circuits is critical for the susceptibility windows of the circuits in the network to align and for information to be efficiently transferred. We demonstrate that changes in synchrony and phase difference can be used to set up or abolish information transfer in a network of cortical circuits."}], "ArticleTitle": "Phase Difference between Model Cortical Areas Determines Level of Information Transfer."}, "28775686": {"mesh": [], "AbstractText": [{"section": null, "text": "Probing a bullfrog retina with spatially uniform light pulses of correlated stochastic intervals, we calculate the mutual information between the spiking output at the ganglion cells measured with multi-electrode array (MEA) and the interval of the stimulus at a time shift later. The time-integrated information from the output about the future stimulus is maximized when the mean interval of the stimulus is within the dynamic range of the well-established anticipative phenomena of omitted-stimulus responses for the retina. The peak position of the mutual information as a function of the time shift is typically negative considering the processing delay of the retina. However, the peak position can become positive for long enough correlation time of the stimulus when the pulse intervals are generated by a Hidden Markovian model (HMM). This is indicative of a predictive behavior of the retina which is possible only when the hidden variable of the HMM can be recovered from the history of the stimulus for a prediction of its future. We verify that stochastic intervals of the same mean, variance, and correlation time do not result in the same predictive behavior of the retina when they are generated by an Ornstein-Uhlenbeck (OU) process, which is strictly Markovian."}], "ArticleTitle": "Characterization of Predictive Behavior of a Retina by Mutual Information."}, "28507514": {"mesh": [], "AbstractText": [{"section": null, "text": "In a previous study we developed a Machine Learning procedure for the automatic identification and classification of spontaneous cord dorsum potentials (CDPs). This study further supported the proposal that in the anesthetized cat, the spontaneous CDPs recorded from different lumbar spinal segments are generated by a distributed network of dorsal horn neurons with structured (non-random) patterns of functional connectivity and that these configurations can be changed to other non-random and stable configurations after the noceptive stimulation produced by the intradermic injection of capsaicin in the anesthetized cat. Here we present a study showing that the sequence of identified forms of the spontaneous CDPs follows a Markov chain of at least order one. That is, the system has memory in the sense that the spontaneous activation of dorsal horn neuronal ensembles producing the CDPs is not independent of the most recent activity. We used this markovian property to build a procedure to identify portions of signals as belonging to a specific functional state of connectivity among the neuronal networks involved in the generation of the CDPs. We have tested this procedure during acute nociceptive stimulation produced by the intradermic injection of capsaicin in intact as well as spinalized preparations. Altogether, our results indicate that CDP sequences cannot be generated by a renewal stochastic process. Moreover, it is possible to describe some functional features of activity in the cord dorsum by modeling the CDP sequences as generated by a Markov order one stochastic process. Finally, these Markov models make possible to determine the functional state which produced a CDP sequence. The proposed identification procedures appear to be useful for the analysis of the sequential behavior of the ongoing CDPs recorded from different spinal segments in response to a variety of experimental procedures including the changes produced by acute nociceptive stimulation. They are envisaged as a useful tool to examine alterations of the patterns of functional connectivity between dorsal horn neurons under normal and different pathological conditions, an issue of potential clinical concern."}], "ArticleTitle": "Markovian Analysis of the Sequential Behavior of the Spontaneous Spinal Cord Dorsum Potentials Induced by Acute Nociceptive Stimulation in the Anesthetized Cat."}, "28018202": {"mesh": [], "AbstractText": [{"section": null, "text": "This paper furthers our attempts to resolve two major controversies-whether gamma synchrony plays a role in cognition, and whether cortical columns are functionally important. We have previously argued that the configuration of cortical cells that emerges in development is that which maximizes the magnitude of synchronous oscillation and minimizes metabolic cost. Here we analyze the separate effects in development of minimization of axonal lengths, and of early Hebbian learning and selective distribution of resources to growing synapses, by showing in simulations that these effects are partially antagonistic, but their interaction during development produces accurate anatomical and functional properties for both columnar and non-columnar cortex. The resulting embryonic anatomical order can provide a cortex-wide scaffold for postnatal learning that is dimensionally consistent with the representation of moving sensory objects, and, as learning progressively overwrites the embryonic order, further associations also occur in a dimensionally consistent framework. The role ascribed to cortical synchrony does not demand specific frequency, amplitude or phase variation of pulses to mediate \"feature linking.\" Instead, the concerted interactions of pulse synchrony with short-term synaptic dynamics, and synaptic resource competition can further explain cortical information processing in analogy to Hopfield networks and quantum computation."}], "ArticleTitle": "Further Work on the Shaping of Cortical Development and Function by Synchrony and Metabolic Competition."}, "30061819": {"mesh": [], "AbstractText": [{"section": null, "text": "The interplay of reinforcement learning and memory is at the core of several recent neural network models, such as the Attention-Gated MEmory Tagging (AuGMEnT) model. While successful at various animal learning tasks, we find that the AuGMEnT network is unable to cope with some hierarchical tasks, where higher-level stimuli have to be maintained over a long time, while lower-level stimuli need to be remembered and forgotten over a shorter timescale. To overcome this limitation, we introduce a hybrid AuGMEnT, with leaky (or short-timescale) and non-leaky (or long-timescale) memory units, that allows the exchange of low-level information while maintaining high-level one. We test the performance of the hybrid AuGMEnT network on two cognitive reference tasks, sequence prediction and 12AX."}], "ArticleTitle": "Multi-Timescale Memory Dynamics Extend Task Repertoire in a Reinforcement Learning Network With Attention-Gated Memory."}, "28769778": {"mesh": [], "AbstractText": [{"section": null, "text": "Constructing a robust emotion-aware analytical framework using non-invasively recorded electroencephalogram (EEG) signals has gained intensive attentions nowadays. However, as deploying a laboratory-oriented proof-of-concept study toward real-world applications, researchers are now facing an ecological challenge that the EEG patterns recorded in real life substantially change across days (i.e., day-to-day variability), arguably making the pre-defined predictive model vulnerable to the given EEG signals of a separate day. The present work addressed how to mitigate the inter-day EEG variability of emotional responses with an attempt to facilitate cross-day emotion classification, which was less concerned in the literature. This study proposed a robust principal component analysis (RPCA)-based signal filtering strategy and validated its neurophysiological validity and machine-learning practicability on a binary emotion classification task (happiness vs. sadness) using a five-day EEG dataset of 12 subjects when participated in a music-listening task. The empirical results showed that the RPCA-decomposed sparse signals (RPCA-S) enabled filtering off the background EEG activity that contributed more to the inter-day variability, and predominately captured the EEG oscillations of emotional responses that behaved relatively consistent along days. Through applying a realistic add-day-in classification validation scheme, the RPCA-S progressively exploited more informative features (from 12.67 &#177; 5.99 to 20.83 &#177; 7.18) and improved the cross-day binary emotion-classification accuracy (from 58.31 &#177; 12.33% to 64.03 &#177; 8.40%) as trained the EEG signals from one to four recording days and tested against one unseen subsequent day. The original EEG features (prior to RPCA processing) neither achieved the cross-day classification (the accuracy was around chance level) nor replicated the encouraging improvement due to the inter-day EEG variability. This result demonstrated the effectiveness of the proposed method and may shed some light on developing a realistic emotion-classification analytical framework alleviating day-to-day variability."}], "ArticleTitle": "Improving Cross-Day EEG-Based Emotion Classification Using Robust Principal Component Analysis."}, "28970791": {"mesh": [], "AbstractText": [{"section": null, "text": "Changes in intracellular Na+ concentration ([Na+]i) are rarely taken into account when neuronal activity is examined. As opposed to Ca2+, [Na+]i dynamics are strongly affected by longitudinal diffusion, and therefore they are governed by the morphological structure of the neurons, in addition to the localization of influx and efflux mechanisms. Here, we examined [Na+]i dynamics and their effects on neuronal computation in three multi-compartmental neuronal models, representing three distinct cell types: accessory olfactory bulb (AOB) mitral cells, cortical layer V pyramidal cells, and cerebellar Purkinje cells. We added [Na+]i as a state variable to these models, and allowed it to modulate the Na+ Nernst potential, the Na+-K+ pump current, and the Na+-Ca2+ exchanger rate. Our results indicate that in most cases [Na+]i dynamics are significantly slower than [Ca2+]i dynamics, and thus may exert a prolonged influence on neuronal computation in a neuronal type specific manner. We show that [Na+]i dynamics affect neuronal activity via three main processes: reduction of EPSP amplitude in repeatedly active synapses due to reduction of the Na+ Nernst potential; activity-dependent hyperpolarization due to increased activity of the Na+-K+ pump; specific tagging of active synapses by extended Ca2+ elevation, intensified by concurrent back-propagating action potentials or complex spikes. Thus, we conclude that [Na+]i dynamics should be considered whenever synaptic plasticity, extensive synaptic input, or bursting activity are examined."}], "ArticleTitle": "The Slow Dynamics of Intracellular Sodium Concentration Increase the Time Window of Neuronal Integration: A Simulation Study."}, "28373838": {"mesh": [], "AbstractText": [{"section": null, "text": "An exciting avenue of neuroscientific research involves quantifying the time-varying properties of functional connectivity networks. As a result, many methods have been proposed to estimate the dynamic properties of such networks. However, one of the challenges associated with such methods involves the interpretation and visualization of high-dimensional, dynamic networks. In this work, we employ graph embedding algorithms to provide low-dimensional vector representations of networks, thus facilitating traditional objectives such as visualization, interpretation and classification. We focus on linear graph embedding methods based on principal component analysis and regularized linear discriminant analysis. The proposed graph embedding methods are validated through a series of simulations and applied to fMRI data from the Human Connectome Project."}], "ArticleTitle": "Decoding Time-Varying Functional Connectivity Networks via Linear Graph Embedding Methods."}, "28690508": {"mesh": [], "AbstractText": [{"section": null, "text": "Experimental measurements of pairwise connection probability of pyramidal neurons together with the distribution of synaptic weights have been used to construct randomly connected model networks. However, several experimental studies suggest that both wiring and synaptic weight structure between neurons show statistics that differ from random networks. Here we study a network containing a subset of neurons which we call weight-hub neurons, that are characterized by strong inward synapses. We propose a connectivity structure for excitatory neurons that contain assemblies of densely connected weight-hub neurons, while the pairwise connection probability and synaptic weight distribution remain consistent with experimental data. Simulations of such a network with generalized integrate-and-fire neurons display regular and irregular slow oscillations akin to experimentally observed up/down state transitions in the activity of cortical neurons with a broad distribution of pairwise spike correlations. Moreover, stimulation of a model network in the presence or absence of assembly structure exhibits responses similar to light-evoked responses of cortical layers in optogenetically modified animals. We conclude that a high connection probability into and within assemblies of excitatory weight-hub neurons, as it likely is present in some but not all cortical layers, changes the dynamics of a layer of cortical microcircuitry significantly."}], "ArticleTitle": "Cortical Dynamics in Presence of Assemblies of Densely Connected Weight-Hub Neurons."}, "29123477": {"mesh": [], "AbstractText": [{"section": null, "text": "The damage of dopaminergic neurons that innervate the striatum has been considered to be the proximate cause of Parkinson's disease (PD). In the dopamine-denervated state, the loss of dendritic spines and the decrease of dendritic length may prevent medium spiny neuron (MSN) from receiving too much excitatory stimuli from the cortex, thereby reducing the symptom of Parkinson's disease. However, the reduction in dendritic spine density obtained by different experiments is significantly different. We developed a biological-based network computational model to quantify the effect of dendritic spine loss and dendrites tree degeneration on basal ganglia (BG) signal regulation. Through the introduction of error index (EI), which was used to measure the attenuation of the signal, we explored the amount of dendritic spine loss and dendritic trees degradation required to restore the normal regulatory function of the network, and found that there were two ranges of dendritic spine loss that could reduce EI to normal levels in the case of dopamine at a certain level, this was also true for dendritic trees. However, although these effects were the same, the mechanisms of these two cases were significant difference. Using the method of phase diagram analysis, we gained insight into the mechanism of signal degradation. Furthermore, we explored the role of cortex in MSN morphology changes dopamine depletion-induced and found that proper adjustments to cortical activity do stop the loss in dendritic spines induced by dopamine depleted. These results suggested that modifying cortical drive onto MSN might provide a new idea on clinical therapeutic strategies for Parkinson's disease."}], "ArticleTitle": "The Effects of Medium Spiny Neuron Morphologcial Changes on Basal Ganglia Network under External Electric Field: A Computational Modeling Study."}, "29780316": {"mesh": [], "AbstractText": [{"section": null, "text": "Evidence suggests that layer 5 pyramidal neurons can be divided into functional zones with unique afferent connectivity and membrane characteristics that allow for post-synaptic integration of feedforward and feedback inputs. To assess the existence of these zones and their interaction, we characterized the resonance properties of a biophysically-realistic compartmental model of a neocortical layer 5 pyramidal neuron. Consistent with recently published theoretical and empirical findings, our model was configured to have a \"hot zone\" in distal apical dendrite and apical tuft where both high- and low-threshold Ca2+ ionic conductances had densities 1-2 orders of magnitude higher than anywhere else in the apical dendrite. We simulated injection of broad spectrum sinusoidal currents with linearly increasing frequency to calculate the input impedance of individual compartments, the transfer impedance between the soma and key compartments within the dendritic tree, and a dimensionless term we introduce called resonance quality. We show that input resonance analysis distinguished at least four distinct zones within the model based on properties of their frequency preferences: basal dendrite which displayed little resonance; soma/proximal apical dendrite which displayed resonance at 5-23 Hz, strongest at 5-10 Hz and hyperpolarized/resting membrane potentials; distal apical dendrite which displayed resonance at 8-19 Hz, strongest at 10 Hz and depolarized membrane potentials; and apical tuft which displayed a weak resonance largely between 8 and 10 Hz across a wide range of membrane potentials. Transfer resonance analysis revealed that changes in subthreshold electrical coupling were found to modulate the transfer resonant frequency of signals transmitted from distal apical dendrite and apical tuft to the soma, which would impact the frequencies that individual neurons are expected to respond to and reinforce. Furthermore, eliminating the hot zone was found to reduce amplification of resonance within the model, which contributes to reduced excitability when perisomatic and distal apical regions receive coincident stimulating current injections. These results indicate that the interactions between different functional zones should be considered in a more complete understanding of neuronal integration. Resonance analysis may therefore be a useful tool for assessing the integration of inputs across the entire neuronal membrane."}], "ArticleTitle": "Resonance Analysis as a Tool for Characterizing Functional Division of Layer 5 Pyramidal Neurons."}, "28082888": {"mesh": [], "AbstractText": [{"section": null, "text": "Steady state visual evoked potentials (SSVEPs) are steady state oscillatory potentials elicited in the electroencephalogram (EEG) by flicker stimulation. The frequency of these responses maches the frequency of the stimulation and of its harmonics and subharmonics. In this study, we investigated the origin of the harmonic and subharmonic components of SSVEPs, which are not well understood. We applied both sine and square wave visual stimulation at 5 and 15 Hz to human subjects and analyzed the properties of the fundamental responses and harmonically related components. In order to interpret the results, we used the well-established neural mass model that consists of interacting populations of excitatory and inhibitory cortical neurons. In our study, this model provided a simple explanation for the origin of SSVEP spectra, and showed that their harmonic and subharmonic components are a natural consequence of the nonlinear properties of neuronal populations and the resonant properties of the modeled network. The model also predicted multiples of subharmonic responses, which were subsequently confirmed using experimental data."}], "ArticleTitle": "Nonlinear Origin of SSVEP Spectra-A Combined Experimental and Modeling Study."}, "28424606": {"mesh": [], "AbstractText": [{"section": null, "text": "We investigate a discrete-time network model composed of excitatory and inhibitory neurons and dynamic synapses with the aim at revealing dynamical properties behind oscillatory phenomena possibly related to brain functions. We use a stochastic neural network model to derive the corresponding macroscopic mean field dynamics, and subsequently analyze the dynamical properties of the network. In addition to slow and fast oscillations arising from excitatory and inhibitory networks, respectively, we show that the interaction between these two networks generates phase-amplitude cross-frequency coupling (CFC), in which multiple different frequency components coexist and the amplitude of the fast oscillation is modulated by the phase of the slow oscillation. Furthermore, we clarify the detailed properties of the oscillatory phenomena by applying the bifurcation analysis to the mean field model, and accordingly show that the intermittent and the continuous CFCs can be characterized by an aperiodic orbit on a closed curve and one on a torus, respectively. These two CFC modes switch depending on the coupling strength from the excitatory to inhibitory networks, via the saddle-node cycle bifurcation of a one-dimensional torus in map (MT1SNC), and may be associated with the function of multi-item representation. We believe that the present model might have potential for studying possible functional roles of phase-amplitude CFC in the cerebral cortex."}], "ArticleTitle": "Bifurcation Analysis on Phase-Amplitude Cross-Frequency Coupling in Neural Networks with Dynamic Synapses."}, "27713697": {"mesh": [], "AbstractText": [{"section": null, "text": "The discovery of stimulus induced synchronization in the visual cortex suggested the possibility that the relations among low-level stimulus features are encoded by the temporal relationship between neuronal discharges. In this framework, temporal coherence is considered a signature of perceptual grouping. This insight triggered a large number of experimental studies which sought to investigate the relationship between temporal coordination and cognitive functions. While some core predictions derived from the initial hypothesis were confirmed, these studies, also revealed a rich dynamical landscape beyond simple coherence whose role in signal processing is still poorly understood. In this paper, a framework is presented which establishes links between the various manifestations of cortical dynamics by assigning specific coding functions to low-dimensional dynamic features such as synchronized oscillations and phase shifts on the one hand and high-dimensional non-linear, non-stationary dynamics on the other. The data serving as basis for this synthetic approach have been obtained with chronic multisite recordings from the visual cortex of anesthetized cats and from monkeys trained to solve cognitive tasks. It is proposed that the low-dimensional dynamics characterized by synchronized oscillations and large-scale correlations are substates that represent the results of computations performed in the high-dimensional state-space provided by recurrently coupled networks."}], "ArticleTitle": "Does the Cerebral Cortex Exploit High-Dimensional, Non-linear Dynamics for Information Processing?"}, "28553219": {"mesh": [], "AbstractText": [{"section": null, "text": "Perceptual decision making can be described as a process of accumulating evidence to a bound which has been formalized within drift-diffusion models (DDMs). Recently, an equivalent Bayesian model has been proposed. In contrast to standard DDMs, this Bayesian model directly links information in the stimulus to the decision process. Here, we extend this Bayesian model further and allow inter-trial variability of two parameters following the extended version of the DDM. We derive parameter distributions for the Bayesian model and show that they lead to predictions that are qualitatively equivalent to those made by the extended drift-diffusion model (eDDM). Further, we demonstrate the usefulness of the extended Bayesian model (eBM) for the analysis of concrete behavioral data. Specifically, using Bayesian model selection, we find evidence that including additional inter-trial parameter variability provides for a better model, when the model is constrained by trial-wise stimulus features. This result is remarkable because it was derived using just 200 trials per condition, which is typically thought to be insufficient for identifying variability parameters in DDMs. In sum, we present a Bayesian analysis, which provides for a novel and promising analysis of perceptual decision making experiments."}], "ArticleTitle": "A Bayesian Reformulation of the Extended Drift-Diffusion Model in Perceptual Decision Making."}, "30386226": {"mesh": [], "AbstractText": [{"section": null, "text": "Objective: We investigate the design of deep recurrent neural networks for detecting sleep stages from single channel EEG signals recorded at home by non-expert users. We report the effect of data set size, architecture choices, regularization, and personalization on the classification performance. Methods: We evaluated 58 different architectures and training configurations using three-fold cross validation. Results: A network consisting of convolutional (CONV) layers and long short term memory (LSTM) layers can achieve an agreement with a human annotator of Cohen's Kappa of ~0.73 using a training data set of 19 subjects. Regularization and personalization do not lead to a performance gain. Conclusion: The optimal neural network architecture achieves a performance that is very close to the previously reported human inter-expert agreement of Kappa 0.75. Significance: We give the first detailed account of CONV/LSTM network design process for EEG sleep staging in single channel home based setting."}], "ArticleTitle": "Recurrent Deep Neural Networks for Real-Time Sleep Stage Classification From Single Channel EEG."}, "29367844": {"mesh": [], "AbstractText": [{"section": null, "text": "The quantity of music content is rapidly increasing and automated affective tagging of music video clips can enable the development of intelligent retrieval, music recommendation, automatic playlist generators, and music browsing interfaces tuned to the users' current desires, preferences, or affective states. To achieve this goal, the field of affective computing has emerged, in particular the development of so-called affective brain-computer interfaces, which measure the user's affective state directly from measured brain waves using non-invasive tools, such as electroencephalography (EEG). Typically, conventional features extracted from the EEG signal have been used, such as frequency subband powers and/or inter-hemispheric power asymmetry indices. More recently, the coupling between EEG and peripheral physiological signals, such as the galvanic skin response (GSR), have also been proposed. Here, we show the importance of EEG amplitude modulations and propose several new features that measure the amplitude-amplitude cross-frequency coupling per EEG electrode, as well as linear and non-linear connections between multiple electrode pairs. When tested on a publicly available dataset of music video clips tagged with subjective affective ratings, support vector classifiers trained on the proposed features were shown to outperform those trained on conventional benchmark EEG features by as much as 6, 20, 8, and 7% for arousal, valence, dominance and liking, respectively. Moreover, fusion of the proposed features with EEG-GSR coupling features showed to be particularly useful for arousal (feature-level fusion) and liking (decision-level fusion) prediction. Together, these findings show the importance of the proposed features to characterize human affective states during music clip watching."}], "ArticleTitle": "Electroencephalography Amplitude Modulation Analysis for Automated Affective Tagging of Music Video Clips."}, "29760656": {"mesh": [], "AbstractText": [{"section": null, "text": "Compared to computer vision systems, the human visual system is more fast and accurate. It is well accepted that V1 neurons can well encode contour information. There are plenty of computational models about contour detection based on the mechanism of the V1 neurons. Multiple-cue inhibition operator is one well-known model, which is based on the mechanism of V1 neurons' non-classical receptive fields. However, this model is time-consuming and noisy. To solve these two problems, we propose an improved model which integrates some additional other mechanisms of the primary vision system. Firstly, based on the knowledge that the salient contours only occupy a small portion of the whole image, the prior filtering is introduced to decrease the running time. Secondly, based on the physiological finding that nearby neurons often have highly correlated responses and thus include redundant information, we adopt the uniform samplings to speed up the algorithm. Thirdly, sparse coding is introduced to suppress the unwanted noises. Finally, to validate the performance, we test it on Berkeley Segmentation Data Set. The results show that the improved model can decrease running time as well as keep the accuracy of the contour detection."}], "ArticleTitle": "A Fast Contour Detection Model Inspired by Biological Mechanisms in Primary Vision System."}, "29375356": {"mesh": [], "AbstractText": [{"section": null, "text": "Accurate classification of either patients with Alzheimer's disease (AD) or patients with mild cognitive impairment (MCI), the prodromal stage of AD, from cognitively unimpaired (CU) individuals is important for clinical diagnosis and adequate intervention. The current study focused on distinguishing AD or MCI from CU based on the multi-feature kernel supervised within-Class-similar discriminative dictionary learning algorithm (MKSCDDL), which we introduced in a previous study, demonstrating that MKSCDDL had superior performance in face recognition. Structural magnetic resonance imaging (sMRI), fluorodeoxyglucose (FDG) positron emission tomography (PET), and florbetapir-PET data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database were all included for classification of AD vs. CU, MCI vs. CU, as well as AD vs. MCI (113 AD patients, 110 MCI patients, and 117 CU subjects). By adopting MKSCDDL, we achieved a classification accuracy of 98.18% for AD vs. CU, 78.50% for MCI vs. CU, and 74.47% for AD vs. MCI, which in each instance was superior to results obtained using several other state-of-the-art approaches (MKL, JRC, mSRC, and mSCDDL). In addition, testing time results outperformed other high quality methods. Therefore, the results suggested that the MKSCDDL procedure is a promising tool for assisting early diagnosis of diseases using neuroimaging data."}], "ArticleTitle": "Classification of Alzheimer's Disease, Mild Cognitive Impairment, and Cognitively Unimpaired Individuals Using Multi-feature Kernel Discriminant Dictionary Learning."}, "33551783": {"mesh": [], "AbstractText": [{"section": null, "text": "Recent whole-brain calcium imaging recordings of the nematode C. elegans have demonstrated that the neural activity associated with behavior is dominated by dynamics on a low-dimensional manifold that can be clustered according to behavioral states. Previous models of C. elegans dynamics have either been linear models, which cannot support the existence of multiple fixed points in the system, or Markov-switching models, which do not describe how control signals in C. elegans neural dynamics can produce switches between stable states. It remains unclear how a network of neurons can produce fast and slow timescale dynamics that control transitions between stable states in a single model. We propose a global, nonlinear control model which is minimally parameterized and captures the state transitions described by Markov-switching models with a single dynamical system. The model is fit by reproducing the timeseries of the dominant PCA mode in the calcium imaging data. Long and short time-scale changes in transition statistics can be characterized via changes in a single parameter in the control model. Some of these macro-scale transitions have experimental correlates to single neuro-modulators that seem to act as biological controls, allowing this model to generate testable hypotheses about the effect of these neuro-modulators on the global dynamics. The theory provides an elegant characterization of control in the neuron population dynamics in C. elegans. Moreover, the mathematical structure of the nonlinear control framework provides a paradigm that can be generalized to more complex systems with an arbitrary number of behavioral states."}], "ArticleTitle": "Nonlinear Control in the Nematode C. elegans."}, "33013339": {"mesh": [], "AbstractText": [{"section": null, "text": "Humans organize sequences of events into a single overall experience, and evaluate the aggregated experience as a whole, such as a generally pleasant dinner, movie, or trip. However, such evaluations are potentially computationally taxing, and so our brains must employ heuristics (i.e., approximations). For example, the peak-end rule hypothesis suggests that we average the peaks and end of a sequential event vs. integrating every moment. However, there is no general model to test viable hypotheses quantitatively. Here, we propose a general model and test among multiple specific ones, while also examining the role of working memory. The models were tested with a novel picture-rating task. We first compared averaging across entire sequences vs. the peak-end heuristic. Correlation tests indicated that averaging prevailed, with peak and end both still having significant prediction power. Given this, we developed generalized order-dependent and relative-preference-dependent models to subsume averaging, peak and end. The combined model improved the prediction power. However, based on limitations of relative-preference-including imposing a potentially arbitrary ranking among preferences-we introduced an absolute-preference-dependent model, which successfully explained the remembered utilities. Yet, because using all experiences in a sequence requires too much memory as real-world settings scale, we then tested \"windowed\" models, i.e., evaluation within a specified window. The windowed (absolute) preference-dependent (WP) model explained the empirical data with long sequences better than without windowing. However, because fixed-windowed models harbor their own limitations-including an inability to capture peak-event influences beyond a fixed window-we then developed discounting models. With (absolute) preference-dependence added to the discounting rate, the results showed that the discounting model reflected the actual working memory of the participants, and that the preference-dependent discounting (PD) model described different features from the WP model. Taken together, we propose a combined WP-PD model as a means by which people evaluate experiences, suggesting preference-dependent working-memory as a significant factor underlying our evaluations."}], "ArticleTitle": "Retrospective Evaluation of Sequential Events and the Influence of Preference-Dependent Working Memory: A Computational Examination."}, "33967728": {"mesh": [], "AbstractText": [{"section": null, "text": "We derive a theoretical construct that allows for the characterisation of both scalable and scale free systems within the dynamic causal modelling (DCM) framework. We define a dynamical system to be \"scalable\" if the same equation of motion continues to apply as the system changes in size. As an example of such a system, we simulate planetary orbits varying in size and show that our proposed methodology can be used to recover Kepler's third law from the timeseries. In contrast, a \"scale free\" system is one in which there is no characteristic length scale, meaning that images of such a system are statistically unchanged at different levels of magnification. As an example of such a system, we use calcium imaging collected in murine cortex and show that the dynamical critical exponent, as defined in renormalization group theory, can be estimated in an empirical biological setting. We find that a task-relevant region of the cortex is associated with higher dynamical critical exponents in task vs. spontaneous states and vice versa for a task-irrelevant region."}], "ArticleTitle": "Neural Systems Under Change of Scale."}, "30483087": {"mesh": [], "AbstractText": [{"section": null, "text": "It has been suggested that musical creativity is mainly formed by implicit knowledge. However, the types of spectro-temporal features and depth of the implicit knowledge forming individualities of improvisation are unknown. This study, using various-order Markov models on implicit statistical learning, investigated spectro-temporal statistics among musicians. The results suggested that lower-order models on implicit knowledge represented general characteristics shared among musicians, whereas higher-order models detected specific characteristics unique to each musician. Second, individuality may essentially be formed by pitch but not rhythm, whereas the rhythms may allow the individuality of pitches to strengthen. Third, time-course variation of musical creativity formed by implicit knowledge and uncertainty (i.e., entropy) may occur in a musician's lifetime. Individuality of improvisational creativity may be formed by deeper but not superficial implicit knowledge of pitches, and that the rhythms may allow the individuality of pitches to strengthen. Individualities of the creativity may shift over a musician's lifetime via experience and training."}], "ArticleTitle": "Musical Creativity and Depth of Implicit Knowledge: Spectral and Temporal Individualities in Improvisation."}, "28469569": {"mesh": [], "AbstractText": [{"section": null, "text": "Deep brain stimulation (DBS) can play a crucial role in the modulation of absence seizures, yet relevant biophysical mechanisms are not completely established. In this paper, on the basis of a biophysical mean-field model, we investigate a typical absence epilepsy activity by introducing slow kinetics of GABAB receptors on thalamus reticular nucleus (TRN). We find that the region of spike and slow-wave discharges (SWDs) can be reduced greatly when we add the DBS to TRN. Furthermore, we systematically explore how the corresponding stimulation parameters including frequency, amplitude and positive input duration suppress the SWDs under certain conditions. It is shown that the SWDs can be controlled as key stimulation parameters are suitably chosen. The results in this paper can be helpful for researchers to understand the thalamus stimulation in treating epilepsy patients, and provide theoretical basis for future experimental and clinical studies."}], "ArticleTitle": "Eliminating Absence Seizures through the Deep Brain Stimulation to Thalamus Reticular Nucleus."}, "29910720": {"mesh": [], "AbstractText": [{"section": null, "text": "Obtaining good quality image features is of remarkable importance for most computer vision tasks. It has been demonstrated that the first layers of the human visual cortex are devoted to feature detection. The need for these features has made line, segment, and corner detection one of the most studied topics in computer vision. HT3D is a recent variant of the Hough transform for the combined detection of corners and line segments in images. It uses a 3D parameter space that enables the detection of segments instead of whole lines. This space also encloses canonical configurations of image corners, transforming corner detection into a pattern search problem. Spiking neural networks (SNN) have previously been proposed for multiple image processing tasks, including corner and line detection using the Hough transform. Following these ideas, this paper presents and describes in detail a model to implement HT3D as a Spiking Neural Network for corner detection. The results obtained from a thorough testing of its implementation using real images evince the correctness of the Spiking Neural Network HT3D implementation. Such results are comparable to those obtained with the regular HT3D implementation, which are in turn superior to other corner detection algorithms."}], "ArticleTitle": "A Spiking Neural Model of HT3D for Corner Detection."}, "28824409": {"mesh": [], "AbstractText": [{"section": null, "text": "Purpose: Driving fatigue has become one of the important causes of road accidents, there are many researches to analyze driver fatigue. EEG is becoming increasingly useful in the measuring fatigue state. Manual interpretation of EEG signals is impossible, so an effective method for automatic detection of EEG signals is crucial needed. Method: In order to evaluate the complex, unstable, and non-linear characteristics of EEG signals, four feature sets were computed from EEG signals, in which fuzzy entropy (FE), sample entropy (SE), approximate Entropy (AE), spectral entropy (PE), and combined entropies (FE + SE + AE + PE) were included. All these feature sets were used as the input vectors of AdaBoost classifier, a boosting method which is fast and highly accurate. To assess our method, several experiments including parameter setting and classifier comparison were conducted on 28 subjects. For comparison, Decision Trees (DT), Support Vector Machine (SVM) and Naive Bayes (NB) classifiers are used. Results: The proposed method (combination of FE and AdaBoost) yields superior performance than other schemes. Using FE feature extractor, AdaBoost achieves improved area (AUC) under the receiver operating curve of 0.994, error rate (ERR) of 0.024, Precision of 0.969, Recall of 0.984, F1 score of 0.976, and Matthews correlation coefficient (MCC) of 0.952, compared to SVM (ERR at 0.035, Precision of 0.957, Recall of 0.974, F1 score of 0.966, and MCC of 0.930 with AUC of 0.990), DT (ERR at 0.142, Precision of 0.857, Recall of 0.859, F1 score of 0.966, and MCC of 0.716 with AUC of 0.916) and NB (ERR at 0.405, Precision of 0.646, Recall of 0.434, F1 score of 0.519, and MCC of 0.203 with AUC of 0.606). It shows that the FE feature set and combined feature set outperform other feature sets. AdaBoost seems to have better robustness against changes of ratio of test samples for all samples and number of subjects, which might therefore aid in the real-time detection of driver fatigue through the classification of EEG signals. Conclusion: By using combination of FE features and AdaBoost classifier to detect EEG-based driver fatigue, this paper ensured confidence in exploring the inherent physiological mechanisms and wearable application."}], "ArticleTitle": "Automated Detection of Driver Fatigue Based on AdaBoost Classifier with EEG Signals."}, "29201003": {"mesh": [], "AbstractText": [{"section": null, "text": "To investigate the effect of electromagnetic induction on the electrical activity of neuron, the variable for magnetic flow is used to improve Hindmarsh-Rose neuron model. Simultaneously, due to the existence of time-delay when signals are propagated between neurons or even in one neuron, it is important to study the role of time-delay in regulating the electrical activity of the neuron. For this end, a four-variable neuron model is proposed to investigate the effects of electromagnetic induction and time-delay. Simulation results suggest that the proposed neuron model can show multiple modes of electrical activity, which is dependent on the time-delay and external forcing current. It means that suitable discharge mode can be obtained by selecting the time-delay or external forcing current, which could be helpful for further investigation of electromagnetic radiation on biological neuronal system."}], "ArticleTitle": "Electrical Activity in a Time-Delay Four-Variable Neuron Model under Electromagnetic Induction."}, "28066225": {"mesh": [], "AbstractText": [{"section": null, "text": "Detecting the existence of temporally coordinated spiking activity, and its role in information processing in the cortex, has remained a major challenge for neuroscience research. Different methods and approaches have been suggested to test whether the observed synchronized events are significantly different from those expected by chance. To analyze the simultaneous spike trains for precise spike correlation, these methods typically model the spike trains as a Poisson process implying that the generation of each spike is independent of all the other spikes. However, studies have shown that neural spike trains exhibit dependence among spike sequences, such as the absolute and relative refractory periods which govern the spike probability of the oncoming action potential based on the time of the last spike, or the bursting behavior, which is characterized by short epochs of rapid action potentials, followed by longer episodes of silence. Here we investigate non-renewal processes with the inter-spike interval distribution model that incorporates spike-history dependence of individual neurons. For that, we use the Monte Carlo method to estimate the full shape of the coincidence count distribution and to generate false positives for coincidence detection. The results show that compared to the distributions based on homogeneous Poisson processes, and also non-Poisson processes, the width of the distribution of joint spike events changes. Non-renewal processes can lead to both heavy tailed or narrow coincidence distribution. We conclude that small differences in the exact autostructure of the point process can cause large differences in the width of a coincidence distribution. Therefore, manipulations of the autostructure for the estimation of significance of joint spike events seem to be inadequate."}], "ArticleTitle": "Serial Spike Time Correlations Affect Probability Distribution of Joint Spike Events."}, "30013471": {"mesh": [], "AbstractText": [{"section": null, "text": "Some previous studies have shown that chaotic dynamics in the balanced state, i.e., one with balanced excitatory and inhibitory inputs into cortical neurons, is the underlying mechanism for the irregularity of neural activity. In this work, we focus on networks of current-based integrate-and-fire neurons with delta-pulse coupling. While we show that the balanced state robustly persists in this system within a broad range of parameters, we mathematically prove that the largest Lyapunov exponent of this type of neuronal networks is negative. Therefore, the irregular firing activity can exist in the system without the chaotic dynamics. That is the irregularity of balanced neuronal networks need not arise from chaos."}], "ArticleTitle": "The Dynamics of Balanced Spiking Neuronal Networks Under Poisson Drive Is Not Chaotic."}, "33967729": {"mesh": [], "AbstractText": [{"section": null, "text": "One of the most fundamental questions in the field of neuroscience is the emergence of synchronous behaviour in the brain, such as phase, anti-phase, and shift-phase synchronisation. In this work, we investigate how the connectivity between brain areas can influence the phase angle and the neuronal synchronisation. To do this, we consider brain areas connected by means of excitatory and inhibitory synapses, in which the neuron dynamics is given by the adaptive exponential integrate-and-fire model. Our simulations suggest that excitatory and inhibitory connections from one area to another play a crucial role in the emergence of these types of synchronisation. Thus, in the case of unidirectional interaction, we observe that the phase angles of the neurons in the receiver area depend on the excitatory and inhibitory synapses which arrive from the sender area. Moreover, when the neurons in the sender area are synchronised, the phase angle variability of the receiver area can be reduced for some conductance values between the areas. For bidirectional interactions, we find that phase and anti-phase synchronisation can emerge due to excitatory and inhibitory connections. We also verify, for a strong inhibitory-to-excitatory interaction, the existence of silent neuronal activities, namely a large number of excitatory neurons that remain in silence for a long time."}], "ArticleTitle": "Emergence of Neuronal Synchronisation in Coupled Areas."}, "28943846": {"mesh": [], "AbstractText": [{"section": null, "text": "In this paper, we investigated the problem of computer-aided diagnosis of Attention Deficit Hyperactivity Disorder (ADHD) using machine learning techniques. With the ADHD-200 dataset, we developed a Support Vector Machine (SVM) model to classify ADHD patients from typically developing controls (TDCs), using the regional brain volumes as predictors. Conventionally, the volume of a brain region was considered to be an anatomical feature and quantified using structural magnetic resonance images. One major contribution of the present study was that we had initially proposed to measure the regional brain volumes using fMRI images. Brain volumes measured from fMRI images were denoted as functional volumes, which quantified the volumes of brain regions that were actually functioning during fMRI imaging. We compared the predictive power of functional volumes with that of regional brain volumes measured from anatomical images, which were denoted as anatomical volumes. The former demonstrated higher discriminative power than the latter for the classification of ADHD patients vs. TDCs. Combined with our two-step feature selection approach which integrated prior knowledge with the recursive feature elimination (RFE) algorithm, our SVM classification model combining functional volumes and demographic characteristics achieved a balanced accuracy of 67.7%, which was 16.1% higher than that of a relevant model published previously in the work of Sato et al. Furthermore, our classifier highlighted 10 brain regions that were most discriminative in distinguishing between ADHD patients and TDCs. These 10 regions were mainly located in occipital lobe, cerebellum posterior lobe, parietal lobe, frontal lobe, and temporal lobe. Our present study using functional images will likely provide new perspectives about the brain regions affected by ADHD."}], "ArticleTitle": "A Computational Model for the Automatic Diagnosis of Attention Deficit Hyperactivity Disorder Based on Functional Brain Volume."}, "30356859": {"mesh": [], "AbstractText": [{"section": null, "text": "It is often assumed that the spinal control of human locomotion combines feed-forward central pattern generation with sensory feedback via muscle reflexes. However, the actual contribution of each component to the generation and stabilization of gait is not well understood, as direct experimental evidence for either is difficult to obtain. We here investigate the relative contribution of the two components to gait stability in a simulation model of human walking. Specifically, we hypothesize that a simple linear combination of feedback and feed-forward control at the level of the spinal cord improves the reaction to unexpected step down perturbations. In previous work, we found preliminary evidence supporting this hypothesis when studying a very reduced model of rebounding behaviors. In the present work, we investigate if the evidence extends to a more realistic model of human walking. We revisit a model that has previously been published and relies on spinal feedback control to generate walking. We extend the control of this model with a feed-forward muscle activation pattern. The feed-forward pattern is recorded from the unperturbed feedback control output. We find that the improvement in the robustness of the walking model with respect to step down perturbations depends on the ratio between the two strategies and on the muscle to which they are applied. The results suggest that combining feed-forward and feedback control is not guaranteed to improve locomotion, as the beneficial effects are dependent on the muscle and its function during walking."}], "ArticleTitle": "The Benefit of Combining Neuronal Feedback and Feed-Forward Control for Robustness in Step Down Perturbations of Simulated Human Walking Depends on the Muscle Function."}, "29922141": {"mesh": [], "AbstractText": [{"section": null, "text": "Asynchrony among synaptic inputs may prevent a neuron from responding to behaviorally relevant sensory stimuli. For example, \"octopus cells\" are monaural neurons in the auditory brainstem of mammals that receive input from auditory nerve fibers (ANFs) representing a broad band of sound frequencies. Octopus cells are known to respond with finely timed action potentials at the onset of sounds despite the fact that due to the traveling wave delay in the cochlea, synaptic input from the auditory nerve is temporally diffuse. This paper provides a proof of principle that the octopus cells' dendritic delay may provide compensation for this input asynchrony, and that synaptic weights may be adjusted by a spike-timing dependent plasticity (STDP) learning rule. This paper used a leaky integrate and fire model of an octopus cell modified to include a \"rate threshold,\" a property that is known to create the appropriate onset response in octopus cells. Repeated audio click stimuli were passed to a realistic auditory nerve model which provided the synaptic input to the octopus cell model. A genetic algorithm was used to find the parameters of the STDP learning rule that reproduced the microscopically observed synaptic connectivity. With these selected parameter values it was shown that the STDP learning rule was capable of adjusting the values of a large number of input synaptic weights, creating a configuration that compensated the traveling wave delay of the cochlea."}], "ArticleTitle": "Compensation for Traveling Wave Delay Through Selection of Dendritic Delays Using Spike-Timing-Dependent Plasticity in a Model of the Auditory Brainstem."}, "29946249": {"mesh": [], "AbstractText": [{"section": null, "text": "This paper introduces a new system for dynamic visual recognition that combines bio-inspired hardware with a brain-like spiking neural network. The system is designed to take data from a dynamic vision sensor (DVS) that simulates the functioning of the human retina by producing an address event output (spike trains) based on the movement of objects. The system then convolutes the spike trains and feeds them into a brain-like spiking neural network, called NeuCube, which is organized in a three-dimensional manner, representing the organization of the primary visual cortex. Spatio-temporal patterns of the data are learned during a deep unsupervised learning stage, using spike-timing-dependent plasticity. In a second stage, supervised learning is performed to train the network for classification tasks. The convolution algorithm and the mapping into the network mimic the function of retinal ganglion cells and the retinotopic organization of the visual cortex. The NeuCube architecture can be used to visualize the deep connectivity inside the network before, during, and after training and thereby allows for a better understanding of the learning processes. The method was tested on the benchmark MNIST-DVS dataset and achieved a classification accuracy of 92.90%. The paper discusses advantages and limitations of the new method and concludes that it is worth exploring further on different datasets, aiming for advances in dynamic computer vision and multimodal systems that integrate visual, aural, tactile, and other kinds of information in a biologically plausible way."}], "ArticleTitle": "A Retinotopic Spiking Neural Network System for Accurate Recognition of Moving Objects Using NeuCube and Dynamic Vision Sensors."}, "28638335": {"mesh": [], "AbstractText": [{"section": null, "text": "Achieved motor movement can be estimated using both sensory and motor signals. The value of motor signals for estimating movement should depend critically on the stereotypy or predictability of the resulting actions. As predictability increases, motor signals become more reliable indicators of achieved movement, so weight attributed to sensory signals should decrease accordingly. Here we describe a method to quantify this predictability for head movement during human locomotion by measuring head motion with an inertial measurement unit (IMU), and calculating the variance explained by the mean movement over one stride, i.e., a metric similar to the coefficient of determination. Predictability exhibits differences across activities, being most predictable during running, and changes over the course of a stride, being least predictable around the time of heel-strike and toe-off. In addition to quantifying predictability, we relate this metric to sensory-motor weighting via a statistically optimal model based on two key assumptions: (1) average head movement provides a conservative estimate of the efference copy prediction, and (2) noise on sensory signals scales with signal magnitude. The model suggests that differences in predictability should lead to changes in the weight attributed to vestibular sensory signals for estimating head movement. In agreement with the model, prior research reports that vestibular perturbations have greatest impact at the time points and during activities where high vestibular weight is predicted. Thus, we propose a unified explanation for time-and activity-dependent modulation of vestibular effects that was lacking previously. Furthermore, the proposed predictability metric constitutes a convenient general method for quantifying any kind of kinematic variability. The probabilistic model is also general; it applies to any situation in which achieved movement is estimated from both motor signals and zero-mean sensory signals with signal-dependent noise."}], "ArticleTitle": "Quantification of Head Movement Predictability and Implications for Suppression of Vestibular Input during Locomotion."}, "28713259": {"mesh": [], "AbstractText": [{"section": null, "text": "The EEG rhythmic activities of the somato-sensory cortex reveal event-related desynchronization (ERD) or event-related synchronization (ERS) in beta band (14-30 Hz) as subjects perform certain tasks or react to specific stimuli. Data reported for imagination of movement support the hypothesis that activation of one sensorimotor area (SMA) can be accompanied by deactivation of the other. In order to improve our understanding of beta ERD/ERS generation, two neural mass models (NMM) of a cortical column taken from Wendling et al. (2002) were interconnected to simulate the transmission of information from one cortex to the other. The results show that the excitation of one cortex leads to inhibition of the other and vice versa, enforcing the Theory of Inhibition. This behavior strongly depends on the initial working point (WP) of the neural populations (between the linear and the upper saturation region of a sigmoidal function) and on how the cortical activation or deactivation can move the WP in the upper saturation region ERD or in the linear region ERS, respectively."}], "ArticleTitle": "Transcallosal Inhibition during Motor Imagery: Analysis of a Neural Mass Model."}, "29089882": {"mesh": [], "AbstractText": [{"section": null, "text": "Non-linear behaviors of a single neuron described by Fitzhugh-Nagumo (FHN) neuron model, with external electromagnetic radiation considered, is investigated. It is discovered that with external electromagnetic radiation in form of a cosine function, the mode selection of membrane potential occurs among periodic, quasi-periodic, and chaotic motions as increasing the frequency of external transmembrane current, which is selected as a sinusoidal function. When the frequency is small or large enough, periodic, and quasi-periodic motions are captured alternatively. Otherwise, when frequency is in interval 0.778 < &#969; < 2.208, chaotic motion characterizes the main behavior type. The mechanism of mode transition from quasi-periodic to chaotic motion is also observed when varying the amplitude of external electromagnetic radiation. The frequency apparently plays a more important role in determining the system behavior."}], "ArticleTitle": "A Route to Chaotic Behavior of Single Neuron Exposed to External Electromagnetic Radiation."}, "30574083": {"mesh": [], "AbstractText": [{"section": null, "text": "The human nervous system is an ensemble of connected neuronal networks. Modeling and system identification of the human nervous system helps us understand how the brain processes sensory input and controls responses at the systems level. This study aims to propose an advanced approach based on a hierarchical neural network and non-linear system identification method to model neural activity in the nervous system in response to an external somatosensory input. The proposed approach incorporates basic concepts of Non-linear AutoRegressive Moving Average Model with eXogenous input (NARMAX) and neural network to acknowledge non-linear closed-loop neural interactions. Different from the commonly used polynomial NARMAX method, the proposed approach replaced the polynomial non-linear terms with a hierarchical neural network. The hierarchical neural network is built based on known neuroanatomical connections and corresponding transmission delays in neural pathways. The proposed method is applied to an experimental dataset, where cortical activities from ten young able-bodied individuals are extracted from electroencephalographic signals while applying mechanical perturbations to their wrist joint. The results yielded by the proposed method were compared with those obtained by the polynomial NARMAX and Volterra methods, evaluated by the variance accounted for (VAF). Both the proposed and polynomial NARMAX methods yielded much better modeling results than the Volterra model. Furthermore, the proposed method modeled cortical responded with a mean VAF of 69.35% for a three-step ahead prediction, which is significantly better than the VAF from a polynomial NARMAX model (mean VAF 47.09%). This study provides a novel approach for precise modeling of cortical responses to sensory input. The results indicate that the incorporation of knowledge of neuroanatomical connections in building a realistic model greatly improves the performance of system identification of the human nervous system."}], "ArticleTitle": "A Novel Approach for Modeling Neural Responses to Joint Perturbations Using the NARMAX Method and a Hierarchical Neural Network."}, "27536232": {"mesh": [], "AbstractText": [{"section": null, "text": "This article is based on the assumption of musical power to change the listener's mood. The paper studies the outcome of two experiments on the regulation of emotional states in a series of participants who listen to different auditions. The present research focuses on note value, an important musical cue related to rhythm. The influence of two concepts linked to note value is analyzed separately and discussed together. The two musical cues under investigation are tempo and rhythmic unit. The participants are asked to label music fragments by using opposite meaningful words belonging to four semantic scales, namely \"Tension\" (ranging from Relaxing to Stressing), \"Expressiveness\" (Expressionless to Expressive), \"Amusement\" (Boring to Amusing) and \"Attractiveness\" (Pleasant to Unpleasant). The participants also have to indicate how much they feel certain basic emotions while listening to each music excerpt. The rated emotions are \"Happiness,\" \"Surprise,\" and \"Sadness.\" This study makes it possible to draw some interesting conclusions about the associations between note value and emotions. "}], "ArticleTitle": "Influence of Tempo and Rhythmic Unit in Musical Emotion Regulation."}, "27807414": {"mesh": [], "AbstractText": [{"section": null, "text": "Long-term potentiation (LTP) of synaptic strength is strongly implicated in learning and memory. On the other hand, depotentiation, the reversal of synaptic strength from potentiated LTP state to the pre-LTP level, is required in extinction of the obsolete memory. A generic tristable system, which couples the phosphatase and kinase switches, exclusively explains how moderate and high elevation of intracellular calcium concentration triggers long-term depression (LTD) and LTP, respectively. The present study, introducing calcium influx and calcium release from internal store into the tristable system, further show that significant elevation of cytoplasmic calcium concentration switches activation of both kinase and phosphatase to their basal states, thereby depotentiate the synaptic strength. A phase-plane analysis of the combined model was employed to explain the previously reported depotentiation in experiments and predict a threshold-like effect with calcium concentration. The results not only reveal a mechanism of NMDAR- and mGluR-dependent depotentiation, but also predict further experiments about the role of internal calcium store in induction of depotentiation and extinction of established memories."}], "ArticleTitle": "Depotentiation from Potentiated Synaptic Strength in a Tristable System of Coupled Phosphatase and Kinase."}, "29643771": {"mesh": [], "AbstractText": [{"section": null, "text": "A system consisting of interconnected networks, or a network of networks (NoN), appears diversely in many real-world systems, including the brain. In this study, we consider NoNs consisting of heterogeneous phase oscillators and investigate how the topology of subnetworks affects the global synchrony of the network. The degree of synchrony and the effect of subnetwork topology are evaluated based on the Kuramoto order parameter and the minimum coupling strength necessary for the order parameter to exceed a threshold value, respectively. In contrast to an isolated network in which random connectivity is favorable for achieving synchrony, NoNs synchronize with weaker interconnections when the degree distribution of subnetworks is heterogeneous, suggesting the major role of the high-degree nodes. We also investigate a case in which subnetworks with different average natural frequencies are coupled to show that direct coupling of subnetworks with the largest variation is effective for synchronizing the whole system. In real-world NoNs like the brain, the balance of synchrony and asynchrony is critical for its function at various spatial resolutions. Our work provides novel insights into the topological basis of coordinated dynamics in such networks."}], "ArticleTitle": "Effective Subnetwork Topology for Synchronizing Interconnected Networks of Coupled Phase Oscillators."}, "29910719": {"mesh": [], "AbstractText": [{"section": null, "text": "The design of novel inhibitors to target BACE1 with reduced cytotoxicity effects is a promising approach to treat Alzheimer's disease (AD). Multiple clinical drugs and antibodies such as AZD3293 and Solanezumab are being tested to investigate their therapeutical potential against AD. The current study explores the binding pattern of AZD3293 and Solanezumab against their target proteins such as &#946;-secretase (BACE1) and mid-region amyloid-beta (A&#946;) (PDBIDs: 2ZHV & 4XXD), respectively using molecular docking and dynamic simulation (MD) approaches. The molecular docking results show that AZD3293 binds within the active region of BACE1 by forming hydrogen bonds against Asp32 and Lys107 with distances 2.95 and 2.68 &#197;, respectively. However, the heavy chain of Solanezumab interacts with Lys16 and Asp23 of amyloid beta having bond length 2.82, 2.78, and 3.00 &#197;, respectively. The dynamic cross correlations and normal mode analyses show that BACE1 depicted good residual correlated motions and fluctuations, as compared to Solanezumab. Using MD, the Root Mean Square Deviation and Fluctuation (RMSD/F) graphs show that AZD3293 residual fluctuations and RMSD value (0.2 nm) was much better compared to Solanezumab (0.7 nm). Moreover, the radius of gyration (Rg) results also depicts the significance of AZD3293 docked complex compared to Solanezumab through residual compactness. Our comparative results show that AZD3293 is a better therapeutic agent for treating AD than Solanezumab."}], "ArticleTitle": "Molecular Docking and Dynamic Simulation of AZD3293 and Solanezumab Effects Against BACE1 to Treat Alzheimer's Disease."}, "33551782": {"mesh": [], "AbstractText": [{"section": null, "text": "Cerebral (\"brain\") organoids are high-fidelity in vitro cellular models of the developing brain, which makes them one of the go-to methods to study isolated processes of tissue organization and its electrophysiological properties, allowing to collect invaluable data for in silico modeling neurodevelopmental processes. Complex computer models of biological systems supplement in vivo and in vitro experimentation and allow researchers to look at things that no laboratory study has access to, due to either technological or ethical limitations. In this paper, we present the Biological Cellular Neural Network Modeling (BCNNM) framework designed for building dynamic spatial models of neural tissue organization and basic stimulus dynamics. The BCNNM uses a convenient predicate description of sequences of biochemical reactions and can be used to run complex models of multi-layer neural network formation from a single initial stem cell. It involves processes such as proliferation of precursor cells and their differentiation into mature cell types, cell migration, axon and dendritic tree formation, axon pathfinding and synaptogenesis. The experiment described in this article demonstrates a creation of an in silico cerebral organoid-like structure, constituted of up to 1 million cells, which differentiate and self-organize into an interconnected system with four layers, where the spatial arrangement of layers and cells are consistent with the values of analogous parameters obtained from research on living tissues. Our in silico organoid contains axons and millions of synapses within and between the layers, and it comprises neurons with high density of connections (more than 10). In sum, the BCNNM is an easy-to-use and powerful framework for simulations of neural tissue development that provides a convenient way to design a variety of tractable in silico experiments."}], "ArticleTitle": "BCNNM: A Framework for in silico Neural Tissue Development Modeling."}, "28123365": {"mesh": [], "AbstractText": [{"section": null, "text": "Here we apply a control theoretic view of movement to the behavior of human locomotion with the goal of using perturbations to learn about subtask control. Controlling one's speed and maintaining upright posture are two critical subtasks, or underlying functions, of human locomotion. How the nervous system simultaneously controls these two subtasks was investigated in this study. Continuous visual and mechanical perturbations were applied concurrently to subjects (n = 20) as probes to investigate these two subtasks during treadmill walking. Novel application of harmonic transfer function (HTF) analysis to human motor behavior was used, and these HTFs were converted to the time-domain based representation of phase-dependent impulse response functions (&#981;IRFs). These &#981;IRFs were used to identify the mapping from perturbation inputs to kinematic and electromyographic (EMG) outputs throughout the phases of the gait cycle. Mechanical perturbations caused an initial, passive change in trunk orientation and, at some phases of stimulus presentation, a corrective trunk EMG and orientation response. Visual perturbations elicited a trunk EMG response prior to a trunk orientation response, which was subsequently followed by an anterior-posterior displacement response. This finding supports the notion that there is a temporal hierarchy of functional subtasks during locomotion in which the control of upper-body posture precedes other subtasks. Moreover, the novel analysis we apply has the potential to probe a broad range of rhythmic behaviors to better understand their neural control."}], "ArticleTitle": "Using a System Identification Approach to Investigate Subtask Control during Human Locomotion."}, "28634449": {"mesh": [], "AbstractText": [{"section": null, "text": "Muscle synergies calculated from electromyography (EMG) data identify weighted groups of muscles activated together during functional tasks. Research has shown that fewer synergies are required to describe EMG data of individuals with neurologic impairments. When considering potential clinical applications of synergies, understanding how EMG data processing impacts results and clinical interpretation is important. The aim of this study was to evaluate how EMG signal processing impacts synergy outputs during gait. We evaluated the impacts of two common processing steps for synergy analyses: low pass (LP) filtering and unit variance scaling. We evaluated EMG data collected during barefoot walking from five muscles of 113 children with cerebral palsy (CP) and 73 typically-developing (TD) children. We applied LP filters to the EMG data with cutoff frequencies ranging from 4 to 40 Hz (reflecting the range reported in prior synergy research). We also evaluated the impact of normalizing EMG amplitude by unit variance. We found that the total variance accounted for (tVAF) by a given number of synergies was sensitive to LP filter choice and decreased in both TD and CP groups with increasing LP cutoff frequency (e.g., 9.3 percentage points change for one synergy between 4 and 40 Hz). This change in tVAF can alter the number of synergies selected for further analyses. Normalizing tVAF to a z-score (e.g., dynamic motor control index during walking, walk-DMC) reduced sensitivity to LP cutoff. Unit variance scaling caused comparatively small changes in tVAF. Synergy weights and activations were impacted less than tVAF by LP filter choice and unit variance normalization. These results demonstrate that EMG signal processing methods impact outputs of synergy analysis and z-score based measures can assist in reporting and comparing results across studies and clinical centers."}], "ArticleTitle": "Electromyography Data Processing Impacts Muscle Synergies during Gait for Unimpaired Children and Children with Cerebral Palsy."}, "29209192": {"mesh": [], "AbstractText": [{"section": null, "text": "Electrical activities are ubiquitous neuronal bioelectric phenomena, which have many different modes to encode the expression of biological information, and constitute the whole process of signal propagation between neurons. Therefore, we focus on the electrical activities of neurons, which is also causing widespread concern among neuroscientists. In this paper, we mainly investigate the electrical activities of the Morris-Lecar (M-L) model with electromagnetic radiation or Gaussian white noise, which can restore the authenticity of neurons in realistic neural network. First, we explore dynamical response of the whole system with electromagnetic induction (EMI) and Gaussian white noise. We find that there are slight differences in the discharge behaviors via comparing the response of original system with that of improved system, and electromagnetic induction can transform bursting or spiking state to quiescent state and vice versa. Furthermore, we research bursting transition mode and the corresponding periodic solution mechanism for the isolated neuron model with electromagnetic induction by using one-parameter and bi-parameters bifurcation analysis. Finally, we analyze the effects of Gaussian white noise on the original system and coupled system, which is conducive to understand the actual discharge properties of realistic neurons."}], "ArticleTitle": "Response of Electrical Activity in an Improved Neuron Model under Electromagnetic Radiation and Noise."}, "33762919": {"mesh": [], "AbstractText": [{"section": null, "text": "Directed acyclic graphs or Bayesian networks that are popular in many AI-related sectors for probabilistic inference and causal reasoning can be mapped to probabilistic circuits built out of probabilistic bits (p-bits), analogous to binary stochastic neurons of stochastic artificial neural networks. In order to satisfy standard statistical results, individual p-bits not only need to be updated sequentially but also in order from the parent to the child nodes, necessitating the use of sequencers in software implementations. In this article, we first use SPICE simulations to show that an autonomous hardware Bayesian network can operate correctly without any clocks or sequencers, but only if the individual p-bits are appropriately designed. We then present a simple behavioral model of the autonomous hardware illustrating the essential characteristics needed for correct sequencer-free operation. This model is also benchmarked against SPICE simulations and can be used to simulate large-scale networks. Our results could be useful in the design of hardware accelerators that use energy-efficient building blocks suited for low-level implementations of Bayesian networks. The autonomous massively parallel operation of our proposed stochastic hardware has biological relevance since neural dynamics in brain is also stochastic and autonomous by nature."}], "ArticleTitle": "Hardware Design for Autonomous Bayesian Networks."}, "33854425": {"mesh": [], "AbstractText": [{"section": null, "text": "Traumatic brain injury is a devastating public health problem, the eighth leading cause of death across the world. To improve our understanding of how injury at the cellular scale affects neural circuit function, we developed a protocol to precisely injure individual neurons within an in vitro neural network. We used high speed calcium imaging to estimate alterations in neural activity and connectivity that occur followed targeted microtrauma. Our studies show that mechanically injured neurons inactivate following microtrauma and eventually re-integrate into the network. Single neuron re-integration is dependent on its activity prior to injury and initial connections in the network: more active and integrated neurons are more resistant to microtrauma and more likely to re-integrate into the network. Micromechanical injury leads to neuronal death 6 h post-injury in a subset of both injured and uninjured neurons. Interestingly, neural activity and network participation after injury were associated with survival in linear discriminate analysis (77.3% correct prediction, Wilks' Lambda = 0.838). Based on this observation, we modulated neuronal activity to rescue neurons after microtrauma. Inhibition of neuronal activity provided much greater survivability than did activation of neurons (ANOVA, p < 0.01 with post-hoc Tukey HSD, p < 0.01). Rescue of neurons by blocking activity in the post-acute period is partially mediated by mitochondrial energetics, as we observed silencing neurons after micromechanical injury led to a significant reduction in mitochondrial calcium accumulation. Overall, the present study provides deeper insight into the propagation of injury within networks, demonstrating that together the initial activity, network structure, and post-injury activity levels contribute to the progressive changes in a neural circuit after mechanical trauma."}], "ArticleTitle": "Regional Neurodegeneration in vitro: The Protective Role of Neural Activity."}, "28133449": {"mesh": [], "AbstractText": [{"section": null, "text": "This article investigates the application of optimal feedback control to trajectory planning in voluntary human arm movements. A nonlinear model predictive controller (NMPC) with a finite prediction horizon was used as the optimal feedback controller to predict the hand trajectory planning and execution of planar reaching tasks. The NMPC is completely predictive, and motion tracking or electromyography data are not required to obtain the limb trajectories. To present this concept, a two degree of freedom musculoskeletal planar arm model actuated by three pairs of antagonist muscles was used to simulate the human arm dynamics. This study is based on the assumption that the nervous system minimizes the muscular effort during goal-directed movements. The effects of prediction horizon length on the trajectory, velocity profile, and muscle activities of a reaching task are presented. The NMPC predictions of the hand trajectory to reach fixed and moving targets are in good agreement with the trajectories found by dynamic optimization and those from experiments. However, the hand velocity and muscle activations predicted by NMPC did not agree as well with experiments or with those found from dynamic optimization."}], "ArticleTitle": "Predictive Simulation of Reaching Moving Targets Using Nonlinear Model Predictive Control."}, "28579954": {"mesh": [], "AbstractText": [{"section": null, "text": "Dynamic joint stiffness is a dynamic, nonlinear relationship between the position of a joint and the torque acting about it, which can be used to describe the biomechanics of the joint and associated limb(s). This paper models and quantifies changes in ankle dynamic stiffness and its individual elements, intrinsic and reflex stiffness, in healthy human subjects during isometric, time-varying (TV) contractions of the ankle plantarflexor muscles. A subspace, linear parameter varying, parallel-cascade (LPV-PC) algorithm was used to identify the model from measured input position perturbations and output torque data using voluntary torque as the LPV scheduling variable (SV). Monte-Carlo simulations demonstrated that the algorithm is accurate, precise, and robust to colored measurement noise. The algorithm was then used to examine stiffness changes associated with TV isometric contractions. The SV was estimated from the Soleus EMG using a Hammerstein model of EMG-torque dynamics identified from unperturbed trials. The LPV-PC algorithm identified (i) a non-parametric LPV impulse response function (LPV IRF) for intrinsic stiffness and (ii) a LPV-Hammerstein model for reflex stiffness consisting of a LPV static nonlinearity followed by a time-invariant state-space model of reflex dynamics. The results demonstrated that: (a) intrinsic stiffness, in particular ankle elasticity, increased significantly and monotonically with activation level; (b) the gain of the reflex pathway increased from rest to around 10-20% of subject's MVC and then declined; and (c) the reflex dynamics were second order. These findings suggest that in healthy human ankle, reflex stiffness contributes most at low muscle contraction levels, whereas, intrinsic contributions monotonically increase with activation level."}], "ArticleTitle": "Linear Parameter Varying Identification of Dynamic Joint Stiffness during Time-Varying Voluntary Contractions."}, "28659782": {"mesh": [], "AbstractText": [{"section": null, "text": "The lack of a formal link between neural network structure and its emergent function has hampered our understanding of how the brain processes information. We have now come closer to describing such a link by taking the direction of synaptic transmission into account, constructing graphs of a network that reflect the direction of information flow, and analyzing these directed graphs using algebraic topology. Applying this approach to a local network of neurons in the neocortex revealed a remarkably intricate and previously unseen topology of synaptic connectivity. The synaptic network contains an abundance of cliques of neurons bound into cavities that guide the emergence of correlated activity. In response to stimuli, correlated activity binds synaptically connected neurons into functional cliques and cavities that evolve in a stereotypical sequence toward peak complexity. We propose that the brain processes stimuli by forming increasingly complex functional cliques and cavities."}], "ArticleTitle": "Cliques of Neurons Bound into Cavities Provide a Missing Link between Structure and Function."}, "28659783": {"mesh": [], "AbstractText": [{"section": null, "text": "The neural dynamics of the nematode Caenorhabditis elegans are experimentally low-dimensional and may be understood as long-timescale transitions between multiple low-dimensional attractors. Previous modeling work has found that dynamic models of the worm's full neuronal network are capable of generating reasonable dynamic responses to certain inputs, even when all neurons are treated as identical save for their connectivity. This study investigates such a model of C. elegans neuronal dynamics, finding that a wide variety of multistable responses are generated in response to varied inputs. Specifically, we generate bifurcation diagrams for all possible single-neuron inputs, showing the existence of fixed points and limit cycles for different input regimes. The nature of the dynamical response is seen to vary according to the type of neuron receiving input; for example, input into sensory neurons is more likely to drive a bifurcation in the system than input into motor neurons. As a specific example we consider compound input into the neuron pairs PLM and ASK, discovering bistability of a limit cycle and a fixed point. The transient timescales in approaching each of these states are much longer than any intrinsic timescales of the system. This suggests consistency of our model with the characterization of dynamics in neural systems as long-timescale transitions between discrete, low-dimensional attractors corresponding to behavioral states."}], "ArticleTitle": "Multistability and Long-Timescale Transients Encoded by Network Structure in a Model of C. elegans Connectome Dynamics."}, "28955216": {"mesh": [], "AbstractText": [{"section": null, "text": "The blurred gray/white matter junction is an important feature of focal cortical dysplasia (FCD) lesions. FCD is the main cause of epilepsy and can be detected through magnetic resonance (MR) imaging. Several earlier studies have focused on computing the gradient magnitude of the MR image and used the resulting map to model the blurred gray/white matter junction. However, gradient magnitude cannot quantify the blurred gray/white matter junction. Therefore, we proposed a novel algorithm called local directional probability optimization (LDPO) for detecting and quantifying the width of the gray/white matter boundary (GWB) within the lesional areas. The proposed LDPO method mainly consists of the following three stages: (1) introduction of a hidden Markov random field-expectation-maximization algorithm to compute the probability images of brain tissues in order to obtain the GWB region; (2) generation of local directions from gray matter (GM) to white matter (WM) passing through the GWB, considering the GWB to be an electric potential field; (3) determination of the optimal local directions for any given voxel of GWB, based on iterative searching of the neighborhood. This was then used to measure the width of the GWB. The proposed LDPO method was tested on real MR images of patients with FCD lesions. The results indicated that the LDPO method could quantify the GWB width. On the GWB width map, the width of the blurred GWB in the lesional region was observed to be greater than that in the non-lesional regions. The proposed GWB width map produced higher F-scores in terms of detecting the blurred GWB within the FCD lesional region as compared to that of FCD feature maps, indicating better trade-off between precision and recall."}], "ArticleTitle": "Local Directional Probability Optimization for Quantification of Blurred Gray/White Matter Junction in Magnetic Resonance Image."}, "29114215": {"mesh": [], "AbstractText": [{"section": null, "text": "We here introduce and study the properties, via computer simulation, of a candidate automated approach to algorithmic reconstruction of dense neural morphology, based on simulated data of the kind that would be obtained via two emerging molecular technologies-expansion microscopy (ExM) and in-situ molecular barcoding. We utilize a convolutional neural network to detect neuronal boundaries from protein-tagged plasma membrane images obtained via ExM, as well as a subsequent supervoxel-merging pipeline guided by optical readout of information-rich, cell-specific nucleic acid barcodes. We attempt to use conservative imaging and labeling parameters, with the goal of establishing a baseline case that points to the potential feasibility of optical circuit reconstruction, leaving open the possibility of higher-performance labeling technologies and algorithms. We find that, even with these conservative assumptions, an all-optical approach to dense neural morphology reconstruction may be possible via the proposed algorithmic framework. Future work should explore both the design-space of chemical labels and barcodes, as well as algorithms, to ultimately enable routine, high-performance optical circuit reconstruction."}], "ArticleTitle": "Feasibility of 3D Reconstruction of Neural Morphology Using Expansion Microscopy and Barcode-Guided Agglomeration."}, "29946250": {"mesh": [], "AbstractText": [{"section": null, "text": "Two important stimulus features represented within the rodent barrel cortex are velocity and angular direction of whisker deflection. Each cortical barrel receives information from thalamocortical (TC) cells that relay information from a single whisker, and TC input is decoded by barrel regular-spiking (RS) cells through a feedforward inhibitory architecture (with inhibition delivered by cortical fast-spiking or FS cells). TC cells encode deflection velocity through population synchrony, while deflection direction is encoded through the distribution of spike counts across the TC population. Barrel RS cells encode both deflection direction and velocity with spike rate, and are divided into functional domains by direction preference. Following repetitive whisker stimulation, system adaptation causes a weakening of synaptic inputs to RS cells and diminishes RS cell spike responses, though evidence suggests that stimulus discrimination may improve following adaptation. In this work, I construct a model of the TC, FS, and RS cells comprising a single barrel system-the model incorporates realistic synaptic connectivity and dynamics and simulates both angular direction (through the spatial pattern of TC activation) and velocity (through synchrony of the TC population spikes) of a deflection of the primary whisker, and I use the model to examine direction and velocity selectivity of barrel RS cells before and after adaptation. I find that velocity and direction selectivity of individual RS cells (measured over multiple trials) sharpens following adaptation, but stimulus discrimination using a simple linear classifier by the RS population response during a single trial (a more biologically meaningful measure than single cell discrimination over multiple trials) exhibits strikingly different behavior-velocity discrimination is similar both before and after adaptation, while direction classification improves substantially following adaptation. This is the first model, to my knowledge, that simulates both whisker deflection velocity and angular direction and examines the ability of the RS population response to pinpoint both stimulus features within the context of adaptation."}], "ArticleTitle": "Effects of Adaptation on Discrimination of Whisker Deflection Velocity and Angular Direction in a Model of the Barrel Cortex."}, "33776675": {"mesh": [], "AbstractText": [{"section": null, "text": "The increasingly popular application of AI runs the risk of amplifying social bias, such as classifying non-white faces as animals. Recent research has largely attributed this bias to the training data implemented. However, the underlying mechanism is poorly understood; therefore, strategies to rectify the bias are unresolved. Here, we examined a typical deep convolutional neural network (DCNN), VGG-Face, which was trained with a face dataset consisting of more white faces than black and Asian faces. The transfer learning result showed significantly better performance in identifying white faces, similar to the well-known social bias in humans, the other-race effect (ORE). To test whether the effect resulted from the imbalance of face images, we retrained the VGG-Face with a dataset containing more Asian faces, and found a reverse ORE that the newly-trained VGG-Face preferred Asian faces over white faces in identification accuracy. Additionally, when the number of Asian faces and white faces were matched in the dataset, the DCNN did not show any bias. To further examine how imbalanced image input led to the ORE, we performed a representational similarity analysis on VGG-Face's activation. We found that when the dataset contained more white faces, the representation of white faces was more distinct, indexed by smaller in-group similarity and larger representational Euclidean distance. That is, white faces were scattered more sparsely in the representational face space of the VGG-Face than the other faces. Importantly, the distinctiveness of faces was positively correlated with identification accuracy, which explained the ORE observed in the VGG-Face. In summary, our study revealed the mechanism underlying the ORE in DCNNs, which provides a novel approach to studying AI ethics. In addition, the face multidimensional representation theory discovered in humans was also applicable to DCNNs, advocating for future studies to apply more cognitive theories to understand DCNNs' behavior."}], "ArticleTitle": "Multidimensional Face Representation in a Deep Convolutional Neural Network Reveals the Mechanism Underlying AI Racism."}, "28744211": {"mesh": [], "AbstractText": [{"section": null, "text": "It has been suggested that at least two mechanisms mediate disparity processing, one for coarse and one for fine disparities. Here we analyze individual differences in our previously measured normative dataset on the disparity sensitivity as a function of spatial frequency of 61 observers to assess the tuning of the spatial frequency channels underlying disparity sensitivity for oblique corrugations (Reynaud et al., 2015). Inter-correlations and factor analysis of the population data revealed two spatial frequency channels for disparity sensitivity: one tuned to high spatial frequencies and one tuned to low spatial frequencies. Our results confirm that disparity is encoded by spatial frequency channels of different sensitivities tuned to different ranges of corrugation frequencies."}], "ArticleTitle": "Characterization of Spatial Frequency Channels Underlying Disparity Sensitivity by Factor Analysis of Population Data."}, "28066223": {"mesh": [], "AbstractText": [{"section": null, "text": "Chunking refers to a phenomenon whereby individuals group items together when performing a memory task to improve the performance of sequential memory. In this work, we build a bio-plausible hierarchical chunking of sequential memory (HCSM) model to explain why such improvement happens. We address this issue by linking hierarchical chunking with synaptic plasticity and neuromorphic engineering. We uncover that a chunking mechanism reduces the requirements of synaptic plasticity since it allows applying synapses with narrow dynamic range and low precision to perform a memory task. We validate a hardware version of the model through simulation, based on measured memristor behavior with narrow dynamic range in neuromorphic circuits, which reveals how chunking works and what role it plays in encoding sequential memory. Our work deepens the understanding of sequential memory and enables incorporating it for the investigation of the brain-inspired computing on neuromorphic architecture."}], "ArticleTitle": "Hierarchical Chunking of Sequential Memory on Neuromorphic Architecture with Reduced Synaptic Plasticity."}, "27909405": {"mesh": [], "AbstractText": [{"section": null, "text": "Early diagnosis of dementia is critical for assessing disease progression and potential treatment. State-or-the-art machine learning techniques have been increasingly employed to take on this diagnostic task. In this study, we employed Generalized Matrix Learning Vector Quantization (GMLVQ) classifiers to discriminate patients with Mild Cognitive Impairment (MCI) from healthy controls based on their cognitive skills. Further, we adopted a \"Learning with privileged information\" approach to combine cognitive and fMRI data for the classification task. The resulting classifier operates solely on the cognitive data while it incorporates the fMRI data as privileged information (PI) during training. This novel classifier is of practical use as the collection of brain imaging data is not always possible with patients and older participants. MCI patients and healthy age-matched controls were trained to extract structure from temporal sequences. We ask whether machine learning classifiers can be used to discriminate patients from controls and whether differences between these groups relate to individual cognitive profiles. To this end, we tested participants in four cognitive tasks: working memory, cognitive inhibition, divided attention, and selective attention. We also collected fMRI data before and after training on a probabilistic sequence learning task and extracted fMRI responses and connectivity as features for machine learning classifiers. Our results show that the PI guided GMLVQ classifiers outperform the baseline classifier that only used the cognitive data. In addition, we found that for the baseline classifier, divided attention is the only relevant cognitive feature. When PI was incorporated, divided attention remained the most relevant feature while cognitive inhibition became also relevant for the task. Interestingly, this analysis for the fMRI GMLVQ classifier suggests that (1) when overall fMRI signal is used as inputs to the classifier, the post-training session is most relevant; and (2) when the graph feature reflecting underlying spatiotemporal fMRI pattern is used, the pre-training session is most relevant. Taken together these results suggest that brain connectivity before training and overall fMRI signal after training are both diagnostic of cognitive skills in MCI."}], "ArticleTitle": "Classifying Cognitive Profiles Using Machine Learning with Privileged Information in Mild Cognitive Impairment."}, "33469423": {"mesh": [], "AbstractText": [{"section": null, "text": "Cardiovascular diseases (CVDs) are the leading cause of death today. The current identification method of the diseases is analyzing the Electrocardiogram (ECG), which is a medical monitoring technology recording cardiac activity. Unfortunately, looking for experts to analyze a large amount of ECG data consumes too many medical resources. Therefore, the method of identifying ECG characteristics based on machine learning has gradually become prevalent. However, there are some drawbacks to these typical methods, requiring manual feature recognition, complex models, and long training time. This paper proposes a robust and efficient 12-layer deep one-dimensional convolutional neural network on classifying the five micro-classes of heartbeat types in the MIT- BIH Arrhythmia database. The five types of heartbeat features are classified, and wavelet self-adaptive threshold denoising method is used in the experiments. Compared with BP neural network, random forest, and other CNN networks, the results show that the model proposed in this paper has better performance in accuracy, sensitivity, robustness, and anti-noise capability. Its accurate classification effectively saves medical resources, which has a positive effect on clinical practice."}], "ArticleTitle": "A Study on Arrhythmia via ECG Signal Classification Using the Convolutional Neural Network."}, "29422842": {"mesh": [], "AbstractText": [{"section": null, "text": "One of the current challenges in human motor rehabilitation is the robust application of Brain-Machine Interfaces to assistive technologies such as powered lower limb exoskeletons. Reliable decoding of motor intentions and accurate timing of the robotic device actuation is fundamental to optimally enhance the patient's functional improvement. Several studies show that it may be possible to extract motor intentions from electroencephalographic (EEG) signals. These findings, although notable, suggests that current techniques are still far from being systematically applied to an accurate real-time control of rehabilitation or assistive devices. Here we propose the estimation of spinal primitives of multi-muscle control from EEG, using electromyography (EMG) dimensionality reduction as a solution to increase the robustness of the method. We successfully apply this methodology, both to healthy and incomplete spinal cord injury (SCI) patients, to identify muscle contraction during periodical knee extension from the EEG. We then introduce a novel performance metric, which accurately evaluates muscle primitive activations."}], "ArticleTitle": "Estimation of Neuromuscular Primitives from EEG Slow Cortical Potentials in Incomplete Spinal Cord Injury Individuals for a New Class of Brain-Machine Interfaces."}, "29867424": {"mesh": [], "AbstractText": [{"section": null, "text": "Effective and accurate diagnosis of Alzheimer's disease (AD), as well as its early stage (mild cognitive impairment, MCI), has attracted more and more attention recently. Researchers have constructed threshold brain function networks and extracted various features for the classification of brain diseases. However, in the construction of the brain function network, the selection of threshold is very important, and the unreasonable setting will seriously affect the final classification results. To address this issue, in this paper, we propose a minimum spanning tree (MST) classification framework to identify Alzheimer's disease (AD), MCI, and normal controls (NCs). The proposed method mainly uses the MST method, graph-based Substructure Pattern mining (gSpan), and graph kernel Principal Component Analysis (graph kernel PCA). Specifically, MST is used to construct the brain functional connectivity network; gSpan, to extract features; and subnetwork selection and graph kernel PCA, to select features. Finally, the support vector machine is used to perform classification. We evaluate our method on MST brain functional networks of 21 AD, 25 MCI, and 22 NC subjects. The experimental results show that our proposed method achieves classification accuracy of 98.3, 91.3, and 77.3%, for MCI vs. NC, AD vs. NC, and AD vs. MCI, respectively. The results show our proposed method can achieve significantly improved classification performance compared to other state-of-the-art methods."}], "ArticleTitle": "Classification of Alzheimer's Disease, Mild Cognitive Impairment, and Normal Controls With Subnetwork Selection and Graph Kernel Principal Component Analysis Based on Minimum Spanning Tree Brain Functional Network."}, "33833674": {"mesh": [], "AbstractText": [{"section": null, "text": "In modern computational modeling, neuroscientists need to reproduce long-lasting activity of large-scale networks, where neurons are described by highly complex mathematical models. These aspects strongly increase the computational load of the simulations, which can be efficiently performed by exploiting parallel systems to reduce the processing times. Graphics Processing Unit (GPU) devices meet this need providing on desktop High Performance Computing. In this work, authors describe a novel Granular layEr Simulator development implemented on a multi-GPU system capable of reconstructing the cerebellar granular layer in a 3D space and reproducing its neuronal activity. The reconstruction is characterized by a high level of novelty and realism considering axonal/dendritic field geometries, oriented in the 3D space, and following convergence/divergence rates provided in literature. Neurons are modeled using Hodgkin and Huxley representations. The network is validated by reproducing typical behaviors which are well-documented in the literature, such as the center-surround organization. The reconstruction of a network, whose volume is 600 &#215; 150 &#215; 1,200 &#956;m3 with 432,000 granules, 972 Golgi cells, 32,399 glomeruli, and 4,051 mossy fibers, takes 235 s on an Intel i9 processor. The 10 s activity reproduction takes only 4.34 and 3.37 h exploiting a single and multi-GPU desktop system (with one or two NVIDIA RTX 2080 GPU, respectively). Moreover, the code takes only 3.52 and 2.44 h if run on one or two NVIDIA V100 GPU, respectively. The relevant speedups reached (up to ~38&#215; in the single-GPU version, and ~55&#215; in the multi-GPU) clearly demonstrate that the GPU technology is highly suitable for realistic large network simulations."}], "ArticleTitle": "Granular layEr Simulator: Design and Multi-GPU Simulation of the Cerebellar Granular Layer."}, "30534065": {"mesh": [], "AbstractText": [{"section": null, "text": "Despite substantial efforts, it remains difficult to identify reliable neuroanatomic biomarkers of autism spectrum disorder (ASD) based on magnetic resonance imaging (MRI) and diffusion tensor imaging (DTI). Studies which use standard statistical methods to approach this task have been hampered by numerous challenges, many of which are innate to the mathematical formulation and assumptions of general linear models (GLM). Although the potential of alternative approaches such as machine learning (ML) to identify robust neuroanatomic correlates of psychiatric disease has long been acknowledged, few studies have attempted to evaluate the abilities of ML to identify structural brain abnormalities associated with ASD. Here we use a sample of 110 ASD patients and 83 typically developing (TD) volunteers (95 females) to assess the suitability of support vector machines (SVMs, a robust type of ML) as an alternative to standard statistical inference for identifying structural brain features which can reliably distinguish ASD patients from TD subjects of either sex, thereby facilitating the study of the interaction between ASD diagnosis and sex. We find that SVMs can perform these tasks with high accuracy and that the neuroanatomic correlates of ASD identified using SVMs overlap substantially with those found using conventional statistical methods. Our results confirm and establish SVMs as powerful ML tools for the study of ASD-related structural brain abnormalities. Additionally, they provide novel insights into the volumetric, morphometric, and connectomic correlates of this epidemiologically significant disorder."}], "ArticleTitle": "Support Vector Machines, Multidimensional Scaling and Magnetic Resonance Imaging Reveal Structural Brain Abnormalities Associated With the Interaction Between Autism Spectrum Disorder and Sex."}, "28588470": {"mesh": [], "AbstractText": [{"section": null, "text": "Purpose: This study is to exam self-esteem related brain morphometry on brain magnetic resonance (MR) images using multilevel-features-based classification method. Method: The multilevel region of interest (ROI) features consist of two types of features: (i) ROI features, which include gray matter volume, white matter volume, cerebrospinal fluid volume, cortical thickness, and cortical surface area, and (ii) similarity features, which are based on similarity calculation of cortical thickness between ROIs. For each feature type, a hybrid feature selection method, comprising of filter-based and wrapper-based algorithms, is used to select the most discriminating features. ROI features and similarity features are integrated by using multi-kernel support vector machines (SVMs) with appropriate weighting factor. Results: The classification performance is improved by using multilevel ROI features with an accuracy of 96.66%, a specificity of 96.62%, and a sensitivity of 95.67%. The most discriminating ROI features that are related to self-esteem spread over occipital lobe, frontal lobe, parietal lobe, limbic lobe, temporal lobe, and central region, mainly involving white matter and cortical thickness. The most discriminating similarity features are distributed in both the right and left hemisphere, including frontal lobe, occipital lobe, limbic lobe, parietal lobe, and central region, which conveys information of structural connections between different brain regions. Conclusion: By using ROI features and similarity features to exam self-esteem related brain morphometry, this paper provides a pilot evidence that self-esteem is linked to specific ROIs and structural connections between different brain regions."}], "ArticleTitle": "Examining Brain Morphometry Associated with Self-Esteem in Young Adults Using Multilevel-ROI-Features-Based Classification Method."}, "27660609": {"mesh": [], "AbstractText": [{"section": null, "text": "Sleep and wakefulness are characterized by distinct states of thalamocortical network oscillations. The complex interplay of ionic conductances within the thalamo-reticular-cortical network give rise to these multiple modes of activity and a rapid transition exists between these modes. To better understand this transition, we constructed a simplified computational model based on physiological recordings and physiologically realistic parameters of a three-neuron network containing a thalamocortical cell, a thalamic reticular neuron, and a corticothalamic cell. The network can assume multiple states of oscillatory activity, resembling sleep, wakefulness, and the transition between these two. We found that during the transition period, but not during other states, thalamic and cortical neurons displayed chaotic dynamics, based on the presence of strange attractors, estimation of positive Lyapunov exponents and the presence of a fractal dimension in the spike trains. These dynamics were quantitatively dependent on certain features of the network, such as the presence of corticothalamic feedback and the strength of inhibition between the thalamic reticular nucleus and thalamocortical neurons. These data suggest that chaotic dynamics facilitate a rapid transition between sleep and wakefulness and produce a series of experimentally testable predictions to further investigate the events occurring during the sleep-wake transition period. "}], "ArticleTitle": "Presence of a Chaotic Region at the Sleep-Wake Transition in a Simplified Thalamocortical Circuit Model."}, "28197089": {"mesh": [], "AbstractText": [{"section": null, "text": "Dreaming is generally thought to be generated by spontaneous brain activity during sleep with patterns common to waking experience. This view is supported by a recent study demonstrating that dreamed objects can be predicted from brain activity during sleep using statistical decoders trained with stimulus-induced brain activity. However, it remains unclear whether and how visual image features associated with dreamed objects are represented in the brain. In this study, we used a deep neural network (DNN) model for object recognition as a proxy for hierarchical visual feature representation, and DNN features for dreamed objects were analyzed with brain decoding of fMRI data collected during dreaming. The decoders were first trained with stimulus-induced brain activity labeled with the feature values of the stimulus image from multiple DNN layers. The decoders were then used to decode DNN features from the dream fMRI data, and the decoded features were compared with the averaged features of each object category calculated from a large-scale image database. We found that the feature values decoded from the dream fMRI data positively correlated with those associated with dreamed object categories at mid- to high-level DNN layers. Using the decoded features, the dreamed object category could be identified at above-chance levels by matching them to the averaged features for candidate categories. The results suggest that dreaming recruits hierarchical visual feature representations associated with objects, which may support phenomenal aspects of dream experience."}], "ArticleTitle": "Hierarchical Neural Representation of Dreamed Objects Revealed by Brain Decoding with Deep Neural Network Features."}, "33488373": {"mesh": [], "AbstractText": [{"section": null, "text": "Healthy brain function is marked by neuronal network dynamics at or near the critical phase, which separates regimes of instability and stasis. A failure to remain at this critical point can lead to neurological disorders such as epilepsy, which is associated with pathological synchronization of neuronal oscillations. Using full Hodgkin-Huxley (HH) simulations on a Small-World Network, we are able to generate synthetic electroencephalogram (EEG) signals with intervals corresponding to seizure (ictal) or non-seizure (interictal) states that can occur based on the hyperexcitability of the artificial neurons and the strength and topology of the synaptic connections between them. These interictal simulations can be further classified into scale-free critical phases and disjoint subcritical exponential phases. By changing the HH parameters, we can model seizures due to a variety of causes, including traumatic brain injury (TBI), congenital channelopathies, and idiopathic etiologies, as well as the effects of anticonvulsant drugs. The results of this work may be used to help identify parameters from actual patient EEG or electrocorticographic (ECoG) data associated with ictogenesis, as well as generating simulated data for training machine-learning seizure prediction algorithms."}], "ArticleTitle": "Critical and Ictal Phases in Simulated EEG Signals on a Small-World Network."}, "27999537": {"mesh": [], "AbstractText": [{"section": null, "text": "Coupling between cortical oscillations and muscle activity facilitates neuronal communication during motor control. The linear part of this coupling, known as corticomuscular coherence, has received substantial attention, even though neuronal communication underlying motor control has been demonstrated to be highly nonlinear. A full assessment of corticomuscular coupling, including the nonlinear part, is essential to understand the neuronal communication within the sensorimotor system. In this study, we applied the recently developed n:m coherence method to assess nonlinear corticomuscular coupling during isotonic wrist flexion. The n:m coherence is a generalized metric for quantifying nonlinear cross-frequency coupling as well as linear iso-frequency coupling. By using independent component analysis (ICA) and equivalent current dipole source localization, we identify four sensorimotor related brain areas based on the locations of the dipoles, i.e., the contralateral primary sensorimotor areas, supplementary motor area (SMA), prefrontal area (PFA) and posterior parietal cortex (PPC). For all these areas, linear coupling between electroencephalogram (EEG) and electromyogram (EMG) is present with peaks in the beta band (15-35 Hz), while nonlinear coupling is detected with both integer (1:2, 1:3, 1:4) and non-integer (2:3) harmonics. Significant differences between brain areas is shown in linear coupling with stronger coherence for the primary sensorimotor areas and motor association cortices (SMA, PFA) compared to the sensory association area (PPC); but not for the nonlinear coupling. Moreover, the detected nonlinear coupling is similar to previously reported nonlinear coupling of cortical activity to somatosensory stimuli. We suggest that the descending motor pathways mainly contribute to linear corticomuscular coupling, while nonlinear coupling likely originates from sensory feedback."}], "ArticleTitle": "Nonlinear Coupling between Cortical Oscillations and Muscle Activity during Isotonic Wrist Flexion."}, "27799906": {"mesh": [], "AbstractText": [{"section": null, "text": "Resting-state and task-related recordings are characterized by oscillatory brain activity and widely distributed networks of synchronized oscillatory circuits. Electroencephalographic recordings (EEG) were used to assess network structure and network dynamics during resting state with eyes open and closed, and auditory oddball performance through phase synchronization between EEG channels. For this assessment, we constructed a hyper-frequency network (HFN) based on within- and cross-frequency coupling (WFC and CFC, respectively) at 10 oscillation frequencies ranging between 2 and 20 Hz. We found that CFC generally differentiates between task conditions better than WFC. CFC was the highest during resting state with eyes open. Using a graph-theoretical approach (GTA), we found that HFNs possess small-world network (SWN) topology with a slight tendency to random network characteristics. Moreover, analysis of the temporal fluctuations of HFNs revealed specific network topology dynamics (NTD), i.e., temporal changes of different graph-theoretical measures such as strength, clustering coefficient, characteristic path length (CPL), local, and global efficiency determined for HFNs at different time windows. The different topology metrics showed significant differences between conditions in the mean and standard deviation of these metrics both across time and nodes. In addition, using an artificial neural network approach, we found stimulus-related dynamics that varied across the different network topology metrics. We conclude that functional connectivity dynamics (FCD), or NTD, which was found using the HFN approach during rest and stimulus processing, reflects temporal and topological changes in the functional organization and reorganization of neuronal cell assemblies."}], "ArticleTitle": "Structure and Topology Dynamics of Hyper-Frequency Networks during Rest and Auditory Oddball Performance."}, "33679356": {"mesh": [], "AbstractText": [{"section": null, "text": "Given the rapid development of light weight EEG devices which we have witnessed the past decade, it is reasonable to ask to which extent neuroscience could now be taken outside the lab. In this study, we have designed an EEG paradigm well suited for deployment \"in the wild.\" The paradigm is tested in repeated recordings on 20 subjects, on eight different occasions (4 in the laboratory, 4 in the subject's own home). By calculating the inter subject, intra subject and inter location variance, we find that the inter location variation for this paradigm is considerably less than the inter subject variation. We believe the paradigm is representative of a large group of other relevant paradigms. This means that given the positive results in this study, we find that if a research paradigm would benefit from being performed in less controlled environments, we expect limited problems in doing so."}], "ArticleTitle": "EEGs Vary Less Between Lab and Home Locations Than They Do Between People."}, "31379546": {"mesh": [], "AbstractText": [{"section": null, "text": "[This corrects the article DOI: 10.3389/fncom.2019.00035.]."}], "ArticleTitle": "Corrigendum: Complex Electroresponsive Dynamics in Olivocerebellar Neurons Represented With Extended-Generalized Leaky Integrate and Fire Models."}, "33867963": {"mesh": [], "AbstractText": [{"section": null, "text": "The optimal organization for functional segregation and integration in brain is made evident by the \"small-world\" feature of functional connectivity (FC) networks and is further supported by the loss of this feature that has been described in many types of brain disease. However, it remains unknown how such optimally organized FC networks arise from the brain's structural constrains. On the other hand, an emerging literature suggests that brain function may be supported by critical neural dynamics, which is believed to facilitate information processing in brain. Though previous investigations have shown that the critical dynamics plays an important role in understanding the relation between whole brain structural connectivity and functional connectivity, it is not clear if the critical dynamics could be responsible for the optimal FC network configuration in human brains. Here, we show that the long-range temporal correlations (LRTCs) in the resting state fMRI blood-oxygen-level-dependent (BOLD) signals are significantly correlated with the topological matrices of the FC brain network. Using structure-dynamics-function modeling approach that incorporates diffusion tensor imaging (DTI) data and simple cellular automata dynamics, we showed that the critical dynamics could optimize the whole brain FC network organization by, e.g., maximizing the clustering coefficient while minimizing the characteristic path length. We also demonstrated with a more detailed excitation-inhibition neuronal network model that loss of local excitation-inhibition (E/I) balance causes failure of critical dynamics, therefore disrupting the optimal FC network organization. The results highlighted the crucial role of the critical dynamics in forming an optimal organization of FC networks in the brain and have potential application to the understanding and modeling of abnormal FC configurations in neuropsychiatric disorders."}], "ArticleTitle": "Optimal Organization of Functional Connectivity Networks for Segregation and Integration With Large-Scale Critical Dynamics in Human Brains."}, "33488374": {"mesh": [], "AbstractText": [{"section": null, "text": "Functional microcircuits are useful for studying interactions among neural dynamics of neighboring neurons during cognition and emotion. A functional microcircuit is a group of neurons that are spatially close, and that exhibit synchronized neural activities. For computational analysis, functional microcircuits are represented by graphs, which pose special challenges when applied as input to machine learning algorithms. Graph embedding, which involves the conversion of graph data into low dimensional vector spaces, is a general method for addressing these challenges. In this paper, we discuss limitations of conventional graph embedding methods that make them ill-suited to the study of functional microcircuits. We then develop a novel graph embedding framework, called Weighted Graph Embedding with Vertex Identity Awareness (WGEVIA), that overcomes these limitations. Additionally, we introduce a dataset, called the five vertices dataset, that helps in assessing how well graph embedding methods are suited to functional microcircuit analysis. We demonstrate the utility of WGEVIA through extensive experiments involving real and simulated microcircuit data."}], "ArticleTitle": "WGEVIA: A Graph Level Embedding Method for Microcircuit Data."}, "33679358": {"mesh": [], "AbstractText": [{"section": null, "text": "Over the past decade there has been a growing interest in the development of parallel hardware systems for simulating large-scale networks of spiking neurons. Compared to other highly-parallel systems, GPU-accelerated solutions have the advantage of a relatively low cost and a great versatility, thanks also to the possibility of using the CUDA-C/C++ programming languages. NeuronGPU is a GPU library for large-scale simulations of spiking neural network models, written in the C++ and CUDA-C++ programming languages, based on a novel spike-delivery algorithm. This library includes simple LIF (leaky-integrate-and-fire) neuron models as well as several multisynapse AdEx (adaptive-exponential-integrate-and-fire) neuron models with current or conductance based synapses, different types of spike generators, tools for recording spikes, state variables and parameters, and it supports user-definable models. The numerical solution of the differential equations of the dynamics of the AdEx models is performed through a parallel implementation, written in CUDA-C++, of the fifth-order Runge-Kutta method with adaptive step-size control. In this work we evaluate the performance of this library on the simulation of a cortical microcircuit model, based on LIF neurons and current-based synapses, and on balanced networks of excitatory and inhibitory neurons, using AdEx or Izhikevich neuron models and conductance-based or current-based synapses. On these models, we will show that the proposed library achieves state-of-the-art performance in terms of simulation time per second of biological activity. In particular, using a single NVIDIA GeForce RTX 2080 Ti GPU board, the full-scale cortical-microcircuit model, which includes about 77,000 neurons and 3 &#183; 108 connections, can be simulated at a speed very close to real time, while the simulation time of a balanced network of 1,000,000 AdEx neurons with 1,000 connections per neuron was about 70 s per second of biological activity."}], "ArticleTitle": "Fast Simulations of Highly-Connected Spiking Cortical Models Using GPUs."}, "33692678": {"mesh": [], "AbstractText": [{"section": null, "text": "Human not only can effortlessly recognize objects, but also characterize object categories into semantic concepts with a nested hierarchical structure. One dominant view is that top-down conceptual guidance is necessary to form such hierarchy. Here we challenged this idea by examining whether deep convolutional neural networks (DCNNs) could learn relations among objects purely based on bottom-up perceptual experience of objects through training for object categorization. Specifically, we explored representational similarity among objects in a typical DCNN (e.g., AlexNet), and found that representations of object categories were organized in a hierarchical fashion, suggesting that the relatedness among objects emerged automatically when learning to recognize them. Critically, the emerged relatedness of objects in the DCNN was highly similar to the WordNet in human, implying that top-down conceptual guidance may not be a prerequisite for human learning the relatedness among objects. In addition, the developmental trajectory of the relatedness among objects during training revealed that the hierarchical structure was constructed in a coarse-to-fine fashion, and evolved into maturity before the establishment of object recognition ability. Finally, the fineness of the relatedness was greatly shaped by the demand of tasks that the DCNN performed, as the higher superordinate level of object classification was, the coarser the hierarchical structure of the relatedness emerged. Taken together, our study provides the first empirical evidence that semantic relatedness of objects emerged as a by-product of object recognition in DCNNs, implying that human may acquire semantic knowledge on objects without explicit top-down conceptual guidance."}], "ArticleTitle": "Semantic Relatedness Emerges in Deep Convolutional Neural Networks Designed for Object Recognition."}, "30087604": {"mesh": [], "AbstractText": [{"section": null, "text": "Visual perception involves continuously choosing the most prominent inputs while suppressing others. Neuroscientists induce visual competitions in various ways to study why and how the brain makes choices of what to perceive. Recently deep neural networks (DNNs) have been used as models of the ventral stream of the visual system, due to similarities in both accuracy and hierarchy of feature representation. In this study we created non-dynamic visual competitions for humans by briefly presenting mixtures of two images. We then tested feed-forward DNNs with similar mixtures and examined their behavior. We found that both humans and DNNs tend to perceive only one image when presented with a mixture of two. We revealed image parameters which predict this perceptual dominance and compared their predictability for the two visual systems. Our findings can be used to both improve DNNs as models, as well as potentially improve their performance by imitating biological behaviors."}], "ArticleTitle": "Perceptual Dominance in Brief Presentations of Mixed Images: Human Perception vs. Deep Neural Networks."}, "27807415": {"mesh": [], "AbstractText": [{"section": null, "text": "Highlights We develop computer-aided diagnosis system for unilateral hearing loss detection in structural magnetic resonance imaging.Wavelet entropy is introduced to extract image global features from brain images. Directed acyclic graph is employed to endow support vector machine an ability to handle multi-class problems.The developed computer-aided diagnosis system achieves an overall accuracy of 95.1% for this three-class problem of differentiating left-sided and right-sided hearing loss from healthy controls. Aim: Sensorineural hearing loss (SNHL) is correlated to many neurodegenerative disease. Now more and more computer vision based methods are using to detect it in an automatic way. Materials: We have in total 49 subjects, scanned by 3.0T MRI (Siemens Medical Solutions, Erlangen, Germany). The subjects contain 14 patients with right-sided hearing loss (RHL), 15 patients with left-sided hearing loss (LHL), and 20 healthy controls (HC). Method: We treat this as a three-class classification problem: RHL, LHL, and HC. Wavelet entropy (WE) was selected from the magnetic resonance images of each subjects, and then submitted to a directed acyclic graph support vector machine (DAG-SVM). Results: The 10 repetition results of 10-fold cross validation shows 3-level decomposition will yield an overall accuracy of 95.10% for this three-class classification problem, higher than feedforward neural network, decision tree, and naive Bayesian classifier. Conclusions: This computer-aided diagnosis system is promising. We hope this study can attract more computer vision method for detecting hearing loss."}], "ArticleTitle": "Wavelet Entropy and Directed Acyclic Graph Support Vector Machine for Detection of Patients with Unilateral Hearing Loss in MRI Scanning."}, "27660610": {"mesh": [], "AbstractText": [{"section": null, "text": "Despite an abundance of computational models for learning of synaptic weights, there has been relatively little research on structural plasticity, i.e., the creation and elimination of synapses. Especially, it is not clear how structural plasticity works in concert with spike-timing-dependent plasticity (STDP) and what advantages their combination offers. Here we present a fairly large-scale functional model that uses leaky integrate-and-fire neurons, STDP, homeostasis, recurrent connections, and structural plasticity to learn the input encoding, the relation between inputs, and to infer missing inputs. Using this model, we compare the error and the amount of noise in the network's responses with and without structural plasticity and the influence of structural plasticity on the learning speed of the network. Using structural plasticity during learning shows good results for learning the representation of input values, i.e., structural plasticity strongly reduces the noise of the response by preventing spikes with a high error. For inferring missing inputs we see similar results, with responses having less noise if the network was trained using structural plasticity. Additionally, using structural plasticity with pruning significantly decreased the time to learn weights suitable for inference. Presumably, this is due to the clearer signal containing less spikes that misrepresent the desired value. Therefore, this work shows that structural plasticity is not only able to improve upon the performance using STDP without structural plasticity but also speeds up learning. Additionally, it addresses the practical problem of limited resources for connectivity that is not only apparent in the mammalian neocortex but also in computer hardware or neuromorphic (brain-inspired) hardware by efficiently pruning synapses without losing performance. "}], "ArticleTitle": "Structural Plasticity Denoises Responses and Improves Learning Speed."}, "28878644": {"mesh": [], "AbstractText": [{"section": null, "text": "A new hyperbolic-type memristor emulator is presented and its frequency-dependent pinched hysteresis loops are analyzed by numerical simulations and confirmed by hardware experiments. Based on the emulator, a novel hyperbolic-type memristor based 3-neuron Hopfield neural network (HNN) is proposed, which is achieved through substituting one coupling-connection weight with a memristive synaptic weight. It is numerically shown that the memristive HNN has a dynamical transition from chaotic, to periodic, and further to stable point behaviors with the variations of the memristor inner parameter, implying the stabilization effect of the hyperbolic-type memristor on the chaotic HNN. Of particular interest, it should be highly stressed that for different memristor inner parameters, different coexisting behaviors of asymmetric attractors are emerged under different initial conditions, leading to the existence of multistable oscillation states in the memristive HNN. Furthermore, by using commercial discrete components, a nonlinear circuit is designed and PSPICE circuit simulations and hardware experiments are performed. The results simulated and captured from the realization circuit are consistent with numerical simulations, which well verify the facticity of coexisting asymmetric attractors' behaviors."}], "ArticleTitle": "Coexisting Behaviors of Asymmetric Attractors in Hyperbolic-Type Memristor based Hopfield Neural Network."}, "27242501": {"mesh": [], "AbstractText": [{"section": null, "text": "Placebo exhibits beneficial effects on pain perception in human experimental studies. Most of these studies demonstrate that placebo significantly decreased neural activities in pain modulatory brain regions and pain-evoked potentials. This study examined placebo analgesia-related effects on spontaneous brain oscillations. We examined placebo effects on four order-fixed 20-min conditions in two sessions: isotonic saline-induced control conditions (with/without placebo) followed by hypertonic saline-induced tonic muscle pain conditions (with/without placebo) in 19 subjects using continuous electroencephalography (EEG) recording. Placebo treatment exerted significant analgesic effects in 14 placebo responders, as subjective intensity of pain perception decreased. Frequency analyses were performed on whole continuous EEG data, data during pain perception rating and data after rating. The results in the first two cases revealed that placebo induced significant increases and a trend toward significant increases in the amplitude of alpha oscillation during tonic muscle pain compared to control conditions in frontal-central regions of the brain, respectively. Placebo-induced decreases in the subjective intensity of pain perception significantly and positively correlated with the increases in the amplitude of alpha oscillations during pain conditions. In conclusion, the modulation effect of placebo treatment was captured when the pain perception evaluating period was included. The strong correlation between the placebo effect on reported pain perception and alpha amplitude suggest that alpha oscillations in frontal-central regions serve as a cortical oscillatory basis of the placebo effect on tonic muscle pain. These results provide important evidence for the investigation of objective indicators of the placebo effect. "}], "ArticleTitle": "Placebo Analgesia Changes Alpha Oscillations Induced by Tonic Muscle Pain: EEG Frequency Analysis Including Data during Pain Evaluation."}, "27980532": {"mesh": [], "AbstractText": [{"section": null, "text": "[This corrects the article on p. 45 in vol. 10, PMID: 27242501.]."}], "ArticleTitle": "Corrigendum: Placebo Analgesia Changes Alpha Oscillations Induced by Tonic Muscle Pain: EEG Frequency Analysis Including Data during Pain Evaluation."}, "33613221": {"mesh": [], "AbstractText": [{"section": null, "text": "Cognitive control and decision-making rely on the interplay of medial and lateral prefrontal cortex (mPFC/lPFC), particularly for circumstances in which correct behavior requires integrating and selecting among multiple sources of interrelated information. While the interaction between mPFC and lPFC is generally acknowledged as a crucial circuit in adaptive behavior, the nature of this interaction remains open to debate, with various proposals suggesting complementary roles in (i) signaling the need for and implementing control, (ii) identifying and selecting appropriate behavioral policies from a candidate set, and (iii) constructing behavioral schemata for performance of structured tasks. Although these proposed roles capture salient aspects of conjoint mPFC/lPFC function, none are sufficiently well-specified to provide a detailed account of the continuous interaction of the two regions during ongoing behavior. A recent computational model of mPFC and lPFC, the Hierarchical Error Representation (HER) model, places the regions within the framework of hierarchical predictive coding, and suggests how they interact during behavioral periods preceding and following salient events. In this manuscript, we extend the HER model to incorporate real-time temporal dynamics and demonstrate how the extended model is able to capture single-unit neurophysiological, behavioral, and network effects previously reported in the literature. Our results add to the wide range of results that can be accounted for by the HER model, and provide further evidence for predictive coding as a unifying framework for understanding PFC function and organization."}], "ArticleTitle": "Interactions of Medial and Lateral Prefrontal Cortex in Hierarchical Predictive Coding."}, "28344550": {"mesh": [], "AbstractText": [{"section": null, "text": "In this article, we describe and analyze the chaotic behavior of a conductance-based neuronal bursting model. This is a model with a reduced number of variables, yet it retains biophysical plausibility. Inspired by the activity of cold thermoreceptors, the model contains a persistent Sodium current, a Calcium-activated Potassium current and a hyperpolarization-activated current (Ih) that drive a slow subthreshold oscillation. Driven by this oscillation, a fast subsystem (fast Sodium and Potassium currents) fires action potentials in a periodic fashion. Depending on the parameters, this model can generate a variety of firing patterns that includes bursting, regular tonic and polymodal firing. Here we show that the transitions between different firing patterns are often accompanied by a range of chaotic firing, as suggested by an irregular, non-periodic firing pattern. To confirm this, we measure the maximum Lyapunov exponent of the voltage trajectories, and the Lyapunov exponent and Lempel-Ziv's complexity of the ISI time series. The four-variable slow system (without spiking) also generates chaotic behavior, and bifurcation analysis shows that this is often originated by period doubling cascades. Either with or without spikes, chaos is no longer generated when the Ih is removed from the system. As the model is biologically plausible with biophysically meaningful parameters, we propose it as a useful tool to understand chaotic dynamics in neurons."}], "ArticleTitle": "Hyperpolarization-Activated Current Induces Period-Doubling Cascades and Chaos in a Cold Thermoreceptor Model."}, "29358914": {"mesh": [], "AbstractText": [{"section": null, "text": "A dynamic system showing stable rhythmic activity can be represented by the dynamics of phase oscillators. This would provide a useful mathematical framework through which one can understand the system's dynamic properties. A recent study proposed a Bayesian approach capable of extracting the underlying phase dynamics directly from time-series data of a system showing rhythmic activity. Here we extended this method to spike data that otherwise provide only limited phase information. To determine how this method performs with spike data, we applied it to simulated spike data generated by a realistic neuronal network model. We then compared the estimated dynamics obtained based on the spike data with the dynamics theoretically derived from the model. The method successfully extracted the modeled phase dynamics, particularly the interaction function, when the amount of available data was sufficiently large. Furthermore, the method was able to infer synaptic connections based on the estimated interaction function. Thus, the method was found to be applicable to spike data and practical for understanding the dynamic properties of rhythmic neural systems."}], "ArticleTitle": "Bayesian Estimation of Phase Dynamics Based on Partially Sampled Spikes Generated by Realistic Model Neurons."}, "33584233": {"mesh": [], "AbstractText": [{"section": null, "text": "Every year thousands of patients are diagnosed with a glioma, a type of malignant brain tumor. MRI plays an essential role in the diagnosis and treatment assessment of these patients. Neural networks show great potential to aid physicians in the medical image analysis. This study investigated the creation of synthetic brain T1-weighted (T1), post-contrast T1-weighted (T1CE), T2-weighted (T2), and T2 Fluid Attenuated Inversion Recovery (Flair) MR images. These synthetic MR (synMR) images were assessed quantitatively with four metrics. The synMR images were also assessed qualitatively by an authoring physician with notions that synMR possessed realism in its portrayal of structural boundaries but struggled to accurately depict tumor heterogeneity. Additionally, this study investigated the synMR images created by generative adversarial network (GAN) to overcome the lack of annotated medical image data in training U-Nets to segment enhancing tumor, whole tumor, and tumor core regions on gliomas. Multiple two-dimensional (2D) U-Nets were trained with original BraTS data and differing subsets of the synMR images. Dice similarity coefficient (DSC) was used as the loss function during training as well a quantitative metric. Additionally, Hausdorff Distance 95% CI (HD) was used to judge the quality of the contours created by these U-Nets. The model performance was improved in both DSC and HD when incorporating synMR in the training set. In summary, this study showed the ability to generate high quality Flair, T2, T1, and T1CE synMR images using GAN. Using synMR images showed encouraging results to improve the U-Net segmentation performance and shows potential to address the scarcity of annotated medical images."}], "ArticleTitle": "Improvement of Multiparametric MR Image Segmentation by Augmenting the Data With Generative Adversarial Networks for Glioma Patients."}, "27594831": {"mesh": [], "AbstractText": [{"section": null, "text": "Technical advances, particularly the integration of wearable and embedded sensors, facilitate tracking of physiological responses in a less intrusive way. Currently, there are many devices that allow gathering biometric measurements from human beings, such as EEG Headsets or Health Bracelets. The massive data sets generated by tracking of EEG and physiology may be used, among other things, to infer knowledge about human moods and emotions. Apart from direct biometric signal measurement, eye tracking systems are nowadays capable of determining the point of gaze of the users when interacting in ICT environments, which provides an added value research on many different areas, such as psychology or marketing. We present a process in which devices for eye tracking, biometric, and EEG signal measurements are synchronously used for studying both basic and complex emotions. We selected the least intrusive devices for different signal data collection given the study requirements and cost constraints, so users would behave in the most natural way possible. On the one hand, we have been able to determine basic emotions participants were experiencing by means of valence and arousal. On the other hand, a complex emotion such as empathy has also been detected. To validate the usefulness of this approach, a study involving forty-four people has been carried out, where they were exposed to a series of affective stimuli while their EEG activity, biometric signals, and eye position were synchronously recorded to detect self-regulation. The hypothesis of the work was that people who self-regulated would show significantly different results when analyzing their EEG data. Participants were divided into two groups depending on whether Electro Dermal Activity (EDA) data indicated they self-regulated or not. The comparison of the results obtained using different machine learning algorithms for emotion recognition shows that using EEG activity alone as a predictor for self-regulation does not allow properly determining whether a person in self-regulation its emotions while watching affective stimuli. However, adequately combining different data sources in a synchronous way to detect emotions makes it possible to overcome the limitations of single detection methods. "}], "ArticleTitle": "Method for Improving EEG Based Emotion Recognition by Combining It with Synchronized Biometric and Eye Tracking Technologies in a Non-invasive and Low Cost Way."}, "27904469": {"mesh": [], "AbstractText": [{"section": null, "text": "[This corrects the article on p. 85 in vol. 10, PMID: 27594831.]."}], "ArticleTitle": "Corrigendum: Method for Improving EEG Based Emotion Recognition by Combining It with Synchronized Biometric and Eye Tracking Technologies in a Non-invasive and Low Cost Way."}, "27683555": {"mesh": [], "AbstractText": [{"section": null, "text": "The present study aims to identify early cognitive impairment through the efficient use of therapies that can improve the quality of daily life and prevent disease progress. We propose a methodology based on the hypothesis that the dissociation between oral semantic expression and the physical expressions, facial gestures, or emotions transmitted in a person's tone of voice is a possible indicator of cognitive impairment. Experiments were carried out with phrases, analyzing the semantics of the message, and the tone of the voice of patients through unstructured interviews in healthy people and patients at an early Alzheimer's stage. The results show that the dissociation in cognitive impairment was an effective indicator, arising from patterns of inconsistency between the analyzed elements. Although the results of our study are encouraging, we believe that further studies are necessary to confirm that this dissociation is a probable indicator of cognitive impairment. "}], "ArticleTitle": "The Dissociation between Polarity, Semantic Orientation, and Emotional Tone as an Early Indicator of Cognitive Impairment."}, "33679359": {"mesh": [], "AbstractText": [{"section": null, "text": "The convolutional neural networks (CNNs) are a powerful tool of image classification that has been widely adopted in applications of automated scene segmentation and identification. However, the mechanisms underlying CNN image classification remain to be elucidated. In this study, we developed a new approach to address this issue by investigating transfer of learning in representative CNNs (AlexNet, VGG, ResNet-101, and Inception-ResNet-v2) on classifying geometric shapes based on local/global features or invariants. While the local features are based on simple components, such as orientation of line segment or whether two lines are parallel, the global features are based on the whole object such as whether an object has a hole or whether an object is inside of another object. Six experiments were conducted to test two hypotheses on CNN shape classification. The first hypothesis is that transfer of learning based on local features is higher than transfer of learning based on global features. The second hypothesis is that the CNNs with more layers and advanced architectures have higher transfer of learning based global features. The first two experiments examined how the CNNs transferred learning of discriminating local features (square, rectangle, trapezoid, and parallelogram). The other four experiments examined how the CNNs transferred learning of discriminating global features (presence of a hole, connectivity, and inside/outside relationship). While the CNNs exhibited robust learning on classifying shapes, transfer of learning varied from task to task, and model to model. The results rejected both hypotheses. First, some CNNs exhibited lower transfer of learning based on local features than that based on global features. Second the advanced CNNs exhibited lower transfer of learning on global features than that of the earlier models. Among the tested geometric features, we found that learning of discriminating inside/outside relationship was the most difficult to be transferred, indicating an effective benchmark to develop future CNNs. In contrast to the \"ImageNet\" approach that employs natural images to train and analyze the CNNs, the results show proof of concept for the \"ShapeNet\" approach that employs well-defined geometric shapes to elucidate the strengths and limitations of the computation in CNN image classification. This \"ShapeNet\" approach will also provide insights into understanding visual information processing the primate visual systems."}], "ArticleTitle": "Transfer of Learning in the Convolutional Neural Networks on Classifying Geometric Shapes Based on Local or Global Invariants."}, "33664661": {"mesh": [], "AbstractText": [{"section": null, "text": "Understanding and producing embedded sequences according to supra-regular grammars in language has always been considered a high-level cognitive function of human beings, named \"syntax barrier\" between humans and animals. However, some neurologists recently showed that macaques could be trained to produce embedded sequences involving supra-regular grammars through a well-designed experiment paradigm. Via comparing macaques and preschool children's experimental results, they claimed that human uniqueness might only lie in the speed and learning strategy resulting from the chunking mechanism. Inspired by their research, we proposed a Brain-inspired Sequence Production Spiking Neural Network (SP-SNN) to model the same production process, followed by memory and learning mechanisms of the multi-brain region cooperation. After experimental verification, we demonstrated that SP-SNN could also handle embedded sequence production tasks, striding over the \"syntax barrier.\" SP-SNN used Population-Coding and STDP mechanism to realize working memory, Reward-Modulated STDP mechanism for acquiring supra-regular grammars. Therefore, SP-SNN needs to simultaneously coordinate short-term plasticity (STP) and long-term plasticity (LTP) mechanisms. Besides, we found that the chunking mechanism indeed makes a difference in improving our model's robustness. As far as we know, our work is the first one toward the \"syntax barrier\" in the SNN field, providing the computational foundation for further study of related underlying animals' neural mechanisms in the future."}], "ArticleTitle": "Brain Inspired Sequences Production by Spiking Neural Networks With Reward-Modulated STDP."}, "33935674": {"mesh": [], "AbstractText": [{"section": null, "text": "The brain is a non-linear dynamical system with a self-restoration process, which protects itself from external damage but is often a bottleneck for clinical treatment. To treat the brain to induce the desired functionality, formulation of a self-restoration process is necessary for optimal brain control. This study proposes a computational model for the brain's self-restoration process following the free-energy and degeneracy principles. Based on this model, a computational framework for brain control is established. We posited that the pre-treatment brain circuit has long been configured in response to the environmental (the other neural populations') demands on the circuit. Since the demands persist even after treatment, the treated circuit's response to the demand may gradually approximate the pre-treatment functionality. In this framework, an energy landscape of regional activities, estimated from resting-state endogenous activities by a pairwise maximum entropy model, is used to represent the pre-treatment functionality. The approximation of the pre-treatment functionality occurs via reconfiguration of interactions among neural populations within the treated circuit. To establish the current framework's construct validity, we conducted various simulations. The simulations suggested that brain control should include the self-restoration process, without which the treatment was not optimal. We also presented simulations for optimizing repetitive treatments and optimal timing of the treatment. These results suggest a plausibility of the current framework in controlling the non-linear dynamical brain with a self-restoration process."}], "ArticleTitle": "A Computational Framework for Controlling the Self-Restorative Brain Based on the Free Energy and Degeneracy Principles."}, "33613220": {"mesh": [], "AbstractText": [{"section": null, "text": "Liquid state machine (LSM) is a type of recurrent spiking network with a strong relationship to neurophysiology and has achieved great success in time series processing. However, the computational cost of simulations and complex dynamics with time dependency limit the size and functionality of LSMs. This paper presents a large-scale bioinspired LSM with modular topology. We integrate the findings on the visual cortex that specifically designed input synapses can fit the activation of the real cortex and perform the Hough transform, a feature extraction algorithm used in digital image processing, without additional cost. We experimentally verify that such a combination can significantly improve the network functionality. The network performance is evaluated using the MNIST dataset where the image data are encoded into spiking series by Poisson coding. We show that the proposed structure can not only significantly reduce the computational complexity but also achieve higher performance compared to the structure of previous reported networks of a similar size. We also show that the proposed structure has better robustness against system damage than the small-world and random structures. We believe that the proposed computationally efficient method can greatly contribute to future applications of reservoir computing."}], "ArticleTitle": "Computational Efficiency of a Modular Reservoir Network for Image Recognition."}, "33584234": {"mesh": [], "AbstractText": [{"section": null, "text": "Orientation selectivity, as an emergent property of neurons in the visual cortex, is of critical importance in the processing of visual information. Characterizing the orientation selectivity based on neuronal firing activities or local field potentials (LFPs) is a hot topic of current research. In this paper, we used cross-frequency coupling and least absolute shrinkage and selection operator (LASSO) to predict the grating orientations in V1 and V4 of two rhesus monkeys. The experimental data were recorded by utilizing two chronically implanted multi-electrode arrays, which were placed, respectively, in V1 and V4 of two rhesus monkeys performing a selective visual attention task. The phase-amplitude coupling (PAC) and amplitude-amplitude coupling (AAC) were employed to characterize the cross-frequency coupling of LFPs under sinusoidal grating stimuli with different orientations. Then, a LASSO logistic regression model was constructed to predict the grating orientation based on the strength of PAC and AAC. Moreover, the cross-validation method was used to evaluate the performance of the model. It was found that the average accuracy of the prediction based on the combination of PAC and AAC was 73.9%, which was higher than the predicting accuracy with PAC or AAC separately. In conclusion, a LASSO logistic regression model was introduced in this study, which can predict the grating orientations with relatively high accuracy by using PAC and AAC together. Our results suggest that the principle behind the LASSO model is probably an alternative direction to explore the mechanism for generating orientation selectivity."}], "ArticleTitle": "Predicting Grating Orientations With Cross-Frequency Coupling and Least Absolute Shrinkage and Selection Operator in V1 and V4 of Rhesus Monkeys."}, "33716702": {"mesh": [], "AbstractText": [{"section": null, "text": "Background: The rapid serial visual presentation (RSVP) paradigm is a high-speed paradigm of brain-computer interface (BCI) applications. The target stimuli evoke event-related potential (ERP) activity of odd-ball effect, which can be used to detect the onsets of targets. Thus, the neural control can be produced by identifying the target stimulus. However, the ERPs in single trials vary in latency and length, which makes it difficult to accurately discriminate the targets against their neighbors, the near-non-targets. Thus, it reduces the efficiency of the BCI paradigm. Methods: To overcome the difficulty of ERP detection against their neighbors, we proposed a simple but novel ternary classification method to train the classifiers. The new method not only distinguished the target against all other samples but also further separated the target, near-non-target, and other, far-non-target samples. To verify the efficiency of the new method, we performed the RSVP experiment. The natural scene pictures with or without pedestrians were used; the ones with pedestrians were used as targets. Magnetoencephalography (MEG) data of 10 subjects were acquired during presentation. The SVM and CNN in EEGNet architecture classifiers were used to detect the onsets of target. Results: We obtained fairly high target detection scores using SVM and EEGNet classifiers based on MEG data. The proposed ternary classification method showed that the near-non-target samples can be discriminated from others, and the separation significantly increased the ERP detection scores in the EEGNet classifier. Moreover, the visualization of the new method suggested the different underling of SVM and EEGNet classifiers in ERP detection of the RSVP experiment. Conclusion: In the RSVP experiment, the near-non-target samples contain separable ERP activity. The ERP detection scores can be increased using classifiers of the EEGNet model, by separating the non-target into near- and far-targets based on their delay against targets."}], "ArticleTitle": "Target Detection Using Ternary Classification During a Rapid Serial Visual Presentation Task Using Magnetoencephalography Data."}, "28555102": {"mesh": [], "AbstractText": [{"section": null, "text": "Hebbian changes of excitatory synapses are driven by and enhance correlations between pre- and postsynaptic neuronal activations, forming a positive feedback loop that can lead to instability in simulated neural networks. Because Hebbian learning may occur on time scales of seconds to minutes, it is conjectured that some form of fast stabilization of neural firing is necessary to avoid runaway of excitation, but both the theoretical underpinning and the biological implementation for such homeostatic mechanism are to be fully investigated. Supported by analytical and computational arguments, we show that a Hebbian spike-timing-dependent metaplasticity rule, accounts for inherently-stable, quick tuning of the total input weight of a single neuron in the general scenario of asynchronous neural firing characterized by UP and DOWN states of activity."}], "ArticleTitle": "A Model of Fast Hebbian Spike Latency Normalization."}, "33912021": {"mesh": [], "AbstractText": [{"section": null, "text": "Experimental studies support the notion of spike-based neuronal information processing in the brain, with neural circuits exhibiting a wide range of temporally-based coding strategies to rapidly and efficiently represent sensory stimuli. Accordingly, it would be desirable to apply spike-based computation to tackling real-world challenges, and in particular transferring such theory to neuromorphic systems for low-power embedded applications. Motivated by this, we propose a new supervised learning method that can train multilayer spiking neural networks to solve classification problems based on a rapid, first-to-spike decoding strategy. The proposed learning rule supports multiple spikes fired by stochastic hidden neurons, and yet is stable by relying on first-spike responses generated by a deterministic output layer. In addition to this, we also explore several distinct, spike-based encoding strategies in order to form compact representations of presented input data. We demonstrate the classification performance of the learning rule as applied to several benchmark datasets, including MNIST. The learning rule is capable of generalizing from the data, and is successful even when used with constrained network architectures containing few input and hidden layer neurons. Furthermore, we highlight a novel encoding strategy, termed \"scanline encoding,\" that can transform image data into compact spatiotemporal patterns for subsequent network processing. Designing constrained, but optimized, network structures and performing input dimensionality reduction has strong implications for neuromorphic applications."}], "ArticleTitle": "Supervised Learning With First-to-Spike Decoding in Multilayer Spiking Neural Networks."}, "33597856": {"mesh": [], "AbstractText": [{"section": null, "text": "Insects search for and find odor sources as their basic behaviors, such as when looking for food or a mate. This has motivated research to describe how they achieve such behavior under turbulent odor plumes with a small number of neurons. Among different insects, the silk moth has been studied owing to its clear motor response to olfactory input. In past studies, the \"programmed behavior\" of the silk moth has been modeled as the average duration of a sequence of maneuvers based on the duration of periods without odor hits. However, this model does not fully represent the fine variations in their behavior. In this study, we used silk moth olfactory search trajectories from an experimental virtual reality device. We achieved an accurate input by using optogenetic silk moths that react to blue light. We then modeled such trajectories as a probabilistic learning agent with a belief of possible source locations. We found that maneuvers mismatching the programmed behavior are related to larger entropy decrease, that is, they are more likely to increase the certainty of the belief. This implies that silkmoths include some stochasticity in their search policy to balance the exploration and exploitation of olfactory information by matching or mismatching the programmed behavior model. We believe that this information-theoretic representation of insect behavior is important for the future implementation of olfactory searches in artificial agents such as robots."}], "ArticleTitle": "Identification of Exploration and Exploitation Balance in the Silkmoth Olfactory Search Behavior by Information-Theoretic Modeling."}, "33505262": {"mesh": [], "AbstractText": [{"section": null, "text": "Topological data analyses are widely used for describing and conceptualizing large volumes of neurobiological data, e.g., for quantifying spiking outputs of large neuronal ensembles and thus understanding the functions of the corresponding networks. Below we discuss an approach in which convergent topological analyses produce insights into how information may be processed in mammalian hippocampus-a brain part that plays a key role in learning and memory. The resulting functional model provides a unifying framework for integrating spiking data at different timescales and following the course of spatial learning at different levels of spatiotemporal granularity. This approach allows accounting for contributions from various physiological phenomena into spatial cognition-the neuronal spiking statistics, the effects of spiking synchronization by different brain waves, the roles played by synaptic efficacies and so forth. In particular, it is possible to demonstrate that networks with plastic and transient synaptic architectures can encode stable cognitive maps, revealing the characteristic timescales of memory processing."}], "ArticleTitle": "From Topological Analyses to Functional Modeling: The Case of Hippocampus."}, "33679357": {"mesh": [], "AbstractText": [{"section": null, "text": "Multiple mechanisms contribute to the generation, propagation, and coordination of the rhythmic patterns necessary for locomotion in Caenorhabditis elegans. Current experiments have focused on two possibilities: pacemaker neurons and stretch-receptor feedback. Here, we focus on whether it is possible that a chain of multiple network rhythmic pattern generators in the ventral nerve cord also contribute to locomotion. We use a simulation model to search for parameters of the anatomically constrained ventral nerve cord circuit that, when embodied and situated, can drive forward locomotion on agar, in the absence of pacemaker neurons or stretch-receptor feedback. Systematic exploration of the space of possible solutions reveals that there are multiple configurations that result in locomotion that is consistent with certain aspects of the kinematics of worm locomotion on agar. Analysis of the best solutions reveals that gap junctions between different classes of motorneurons in the ventral nerve cord can play key roles in coordinating the multiple rhythmic pattern generators."}], "ArticleTitle": "A Neuromechanical Model of Multiple Network Rhythmic Pattern Generators for Forward Locomotion in C. elegans."}, "28878642": {"mesh": [], "AbstractText": [{"section": null, "text": "Large-scale neuromorphic hardware platforms, specialized computer systems for energy efficient simulation of spiking neural networks, are being developed around the world, for example as part of the European Human Brain Project (HBP). Due to conceptual differences, a universal performance analysis of these systems in terms of runtime, accuracy and energy efficiency is non-trivial, yet indispensable for further hard- and software development. In this paper we describe a scalable benchmark based on a spiking neural network implementation of the binary neural associative memory. We treat neuromorphic hardware and software simulators as black-boxes and execute exactly the same network description across all devices. Experiments on the HBP platforms under varying configurations of the associative memory show that the presented method allows to test the quality of the neuron model implementation, and to explain significant deviations from the expected reference output."}], "ArticleTitle": "Binary Associative Memories as a Benchmark for Spiking Neuromorphic Hardware."}, "33967727": {"mesh": [], "AbstractText": [{"section": null, "text": "Working memory is closely involved in various cognitive activities, but its neural mechanism is still under exploration. The mainstream view has long been that persistent activity is the neural basis of working memory, but recent experiments have observed that activity-silent memory can also be correctly recalled. The underlying mechanism of activity-silent memory is considered to be an alternative scheme that rejects the theory of persistent activity. We propose a working memory model based on spike-timing-dependent plasticity (STDP). Different from models based on spike-rate coding, our model adopts temporal patterns of action potentials to represent information, so it can flexibly encode new memory representation. The model can work in both persistent and silent states, i.e., it is compatible with both of these seemingly conflicting neural mechanisms. We conducted a simulation experiment, and the results are similar to the real experimental results, which suggests that our model is plausible in biology."}], "ArticleTitle": "A Computational Model of Working Memory Based on Spike-Timing-Dependent Plasticity."}, "27486395": {"mesh": [], "AbstractText": [{"section": null, "text": "The amazing imitation capabilities of songbirds show that they can memorize sensory sequences and transform them into motor activities which in turn generate the original sound sequences. This suggests that the bird's brain can learn (1) to reliably reproduce spatio-temporal sensory representations and (2) to transform them into corresponding spatio-temporal motor activations by using an inverse mapping. Neither the synaptic mechanisms nor the network architecture enabling these two fundamental aspects of imitation learning are known. We propose an architecture of coupled neuronal modules that mimick areas in the song bird and show that a unique synaptic plasticity mechanism can serve to learn both, sensory sequences in a recurrent neuronal network, as well as an inverse model that transforms the sensory memories into the corresponding motor activations. The proposed membrane potential dependent learning rule together with the architecture that includes basic features of the bird's brain represents the first comprehensive account of bird imitation learning based on spiking neurons. "}], "ArticleTitle": "A Comprehensive Account of Sound Sequence Imitation in the Songbird."}, "33732127": {"mesh": [], "AbstractText": [{"section": null, "text": "Recurrent cortical networks provide reservoirs of states that are thought to play a crucial role for sequential information processing in the brain. However, classical reservoir computing requires manual adjustments of global network parameters, particularly of the spectral radius of the recurrent synaptic weight matrix. It is hence not clear if the spectral radius is accessible to biological neural networks. Using random matrix theory, we show that the spectral radius is related to local properties of the neuronal dynamics whenever the overall dynamical state is only weakly correlated. This result allows us to introduce two local homeostatic synaptic scaling mechanisms, termed flow control and variance control, that implicitly drive the spectral radius toward the desired value. For both mechanisms the spectral radius is autonomously adapted while the network receives and processes inputs under working conditions. We demonstrate the effectiveness of the two adaptation mechanisms under different external input protocols. Moreover, we evaluated the network performance after adaptation by training the network to perform a time-delayed XOR operation on binary sequences. As our main result, we found that flow control reliably regulates the spectral radius for different types of input statistics. Precise tuning is however negatively affected when interneural correlations are substantial. Furthermore, we found a consistent task performance over a wide range of input strengths/variances. Variance control did however not yield the desired spectral radii with the same precision, being less consistent across different input strengths. Given the effectiveness and remarkably simple mathematical form of flow control, we conclude that self-consistent local control of the spectral radius via an implicit adaptation scheme is an interesting and biological plausible alternative to conventional methods using set point homeostatic feedback controls of neural firing."}], "ArticleTitle": "Local Homeostatic Regulation of the Spectral Radius of Echo-State Networks."}, "33510629": {"mesh": [], "AbstractText": [{"section": null, "text": "Humans quickly and accurately learn new visual concepts from sparse data, sometimes just a single example. The impressive performance of artificial neural networks which hierarchically pool afferents across scales and positions suggests that the hierarchical organization of the human visual system is critical to its accuracy. These approaches, however, require magnitudes of order more examples than human learners. We used a benchmark deep learning model to show that the hierarchy can also be leveraged to vastly improve the speed of learning. We specifically show how previously learned but broadly tuned conceptual representations can be used to learn visual concepts from as few as two positive examples; reusing visual representations from earlier in the visual hierarchy, as in prior approaches, requires significantly more examples to perform comparably. These results suggest techniques for learning even more efficiently and provide a biologically plausible way to learn new visual concepts from few examples."}], "ArticleTitle": "Leveraging Prior Concept Learning Improves Generalization From Few Examples in Computational Models of Human Object Recognition."}, "33867962": {"mesh": [], "AbstractText": [{"section": null, "text": "Gamma and theta oscillations have been functionally associated with cognitive processes, such as learning and memory. Synaptic conductances play an important role in the generation of intrinsic network rhythmicity, but few studies have examined the effects of voltage-gated ion channels (VGICs) on these rhythms. In this report, we have used a pyramidal-interneuron-gamma (PING) network consisting of excitatory pyramidal cells and two types of inhibitory interneurons. We have constructed a conductance-based neural network incorporating a persistent sodium current (I NaP ), a delayed rectifier potassium current (I KDR ), a inactivating potassium current (I A ) and a hyperpolarization-activated current (I H ). We have investigated the effects of several conductances on network theta and gamma frequency oscillations. Variation of all conductances of interest changed network rhythmicity. Theta power was altered by all conductances tested. Gamma rhythmogenesis was dependent on I A and I H . The I KDR currents in excitatory pyramidal cells as well as both types of inhibitory interneurons were essential for theta rhythmogenesis and altered gamma rhythm properties. Increasing I NaP suppressed both gamma and theta rhythms. Addition of noise did not alter these patterns. Our findings suggest that VGICs strongly affect brain network rhythms. Further investigations in vivo will be of great interest, including potential effects on neural function and cognition."}], "ArticleTitle": "Effects of Several Classes of Voltage-Gated Ion Channel Conductances on Gamma and Theta Oscillations in a Hippocampal Microcircuit Model."}, "33732128": {"mesh": [], "AbstractText": [{"section": null, "text": "Traditional synaptic plasticity experiments and models depend on tight temporal correlations between pre- and postsynaptic activity. These tight temporal correlations, on the order of tens of milliseconds, are incompatible with significantly longer behavioral time scales, and as such might not be able to account for plasticity induced by behavior. Indeed, recent findings in hippocampus suggest that rapid, bidirectional synaptic plasticity which modifies place fields in CA1 operates at behavioral time scales. These experimental results suggest that presynaptic activity generates synaptic eligibility traces both for potentiation and depression, which last on the order of seconds. These traces can be converted to changes in synaptic efficacies by the activation of an instructive signal that depends on naturally occurring or experimentally induced plateau potentials. We have developed a simple mathematical model that is consistent with these observations. This model can be fully analyzed to find the fixed points of induced place fields and how these fixed points depend on system parameters such as the size and shape of presynaptic place fields, the animal's velocity during induction, and the parameters of the plasticity rule. We also make predictions about the convergence time to these fixed points, both for induced and pre-existing place fields."}], "ArticleTitle": "Behavioral Time Scale Plasticity of Place Fields: Mathematical Analysis."}, "30364814": {"mesh": [], "AbstractText": [{"section": null, "text": "[This corrects the article DOI: 10.3389/fncom.2018.00061.]."}], "ArticleTitle": "Corrigendum: Recording Neural Activity Based on Surface Plasmon Resonance by Optical Fibers-A Computational Analysis."}, "33897398": {"mesh": [], "AbstractText": [{"section": null, "text": "Autism spectrum disorder (ASD) is a heterogenous neurodevelopmental disorder which is characterized by impaired communication, and limited social interactions. The shortcomings of current clinical approaches which are based exclusively on behavioral observation of symptomology, and poor understanding of the neurological mechanisms underlying ASD necessitates the identification of new biomarkers that can aid in study of brain development, and functioning, and can lead to accurate and early detection of ASD. In this paper, we developed a deep-learning model called ASD-SAENet for classifying patients with ASD from typical control subjects using fMRI data. We designed and implemented a sparse autoencoder (SAE) which results in optimized extraction of features that can be used for classification. These features are then fed into a deep neural network (DNN) which results in superior classification of fMRI brain scans more prone to ASD. Our proposed model is trained to optimize the classifier while improving extracted features based on both reconstructed data error and the classifier error. We evaluated our proposed deep-learning model using publicly available Autism Brain Imaging Data Exchange (ABIDE) dataset collected from 17 different research centers, and include more than 1,035 subjects. Our extensive experimentation demonstrate that ASD-SAENet exhibits comparable accuracy (70.8%), and superior specificity (79.1%) for the whole dataset as compared to other methods. Further, our experiments demonstrate superior results as compared to other state-of-the-art methods on 12 out of the 17 imaging centers exhibiting superior generalizability across different data acquisition sites and protocols. The implemented code is available on GitHub portal of our lab at: https://github.com/pcdslab/ASD-SAENet."}], "ArticleTitle": "ASD-SAENet: A Sparse Autoencoder, and Deep-Neural Network Model for Detecting Autism Spectrum Disorder (ASD) Using fMRI Data."}, "33746728": {"mesh": [], "AbstractText": [{"section": null, "text": "Reinforcement learning is a paradigm that can account for how organisms learn to adapt their behavior in complex environments with sparse rewards. To partition an environment into discrete states, implementations in spiking neuronal networks typically rely on input architectures involving place cells or receptive fields specified ad hoc by the researcher. This is problematic as a model for how an organism can learn appropriate behavioral sequences in unknown environments, as it fails to account for the unsupervised and self-organized nature of the required representations. Additionally, this approach presupposes knowledge on the part of the researcher on how the environment should be partitioned and represented and scales poorly with the size or complexity of the environment. To address these issues and gain insights into how the brain generates its own task-relevant mappings, we propose a learning architecture that combines unsupervised learning on the input projections with biologically motivated clustered connectivity within the representation layer. This combination allows input features to be mapped to clusters; thus the network self-organizes to produce clearly distinguishable activity patterns that can serve as the basis for reinforcement learning on the output projections. On the basis of the MNIST and Mountain Car tasks, we show that our proposed model performs better than either a comparable unclustered network or a clustered network with static input projections. We conclude that the combination of unsupervised learning and clustered connectivity provides a generic representational substrate suitable for further computation."}], "ArticleTitle": "Unsupervised Learning and Clustered Connectivity Enhance Reinforcement Learning in Spiking Neural Networks."}, "33643016": {"mesh": [], "AbstractText": [{"section": null, "text": "The capacity to produce and understand written language is a uniquely human skill that exists on a continuum, and foundational to other facets of human cognition. Multivariate classifiers based on support vector machines (SVM) have provided much insight into the networks underlying reading skill beyond what traditional univariate methods can tell us. Shallow models like SVM require large amounts of data, and this problem is compounded when functional connections, which increase exponentially with network size, are predictors of interest. Data reduction using independent component analyses (ICA) mitigates this problem, but conventionally assumes linear relationships. Multilayer feedforward networks, in contrast, readily find optimal low-dimensional encodings of complex patterns that include complex nonlinear or conditional relationships. Samples of poor and highly-skilled young readers were selected from two open access data sets using rhyming and mental multiplication tasks, respectively. Functional connectivity was computed for the rhyming task within a functionally-defined reading network and used to train multilayer feedforward classifier models to simultaneously associate functional connectivity patterns with lexicality (word vs. pseudoword) and reading skill (poor vs. highly-skilled). Classifiers identified validation set lexicality with significantly better than chance accuracy, and reading skill with near-ceiling accuracy. Critically, a series of replications used pre-trained rhyming-task models to classify reading skill from mental multiplication task participants' connectivity with near-ceiling accuracy. The novel deep learning approach presented here provides the clearest demonstration to date that reading-skill dependent functional connectivity within the reading network influences brain processing dynamics across cognitive domains."}], "ArticleTitle": "The Connectivity Fingerprints of Highly-Skilled and Disordered Reading Persist Across Cognitive Domains."}, "34012388": {"mesh": [], "AbstractText": [{"section": null, "text": "The collective electrophysiological dynamics of the brain as a result of sleep-related biological drives in Drosophila are investigated in this paper. Based on the Huber-Braun thermoreceptor model, the conductance-based neurons model is extended to a coupled neural network to analyze the local field potential (LFP). The LFP is calculated by using two different metrics: the mean value and the distance-dependent LFP. The distribution of neurons around the electrodes is assumed to have a circular or grid distribution on a two-dimensional plane. Regardless of which method is used, qualitatively similar results are obtained that are roughly consistent with the experimental data. During wake, the LFP has an irregular or a regular spike. However, the LFP becomes regular bursting during sleep. To further analyze the results, wavelet analysis and raster plots are used to examine how the LFP frequencies changed. The synchronization of neurons under different network structures is also studied. The results demonstrate that there are obvious oscillations at approximately 8 Hz during sleep that are absent during wake. Different time series of the LFP can be obtained under different network structures and the density of the network will also affect the magnitude of the potential. As the number of coupled neurons increases, the neural network becomes easier to synchronize, but the sleep and wake time described by the LFP spectrogram do not change. Moreover, the parameters that affect the durations of sleep and wake are analyzed."}], "ArticleTitle": "Collective Dynamics of Neural Networks With Sleep-Related Biological Drives in Drosophila."}, "33603655": {"mesh": [], "AbstractText": [{"section": null, "text": "The globus pallidus internus and the subthalamic nucleus are common targets for deep brain stimulation to alleviate symptoms of Parkinson's disease and dystonia. In the rodent models, however, their direct targeting is hindered by the relatively large dimensions of applied electrodes. To reduce the neurological damage, the electrodes are usually implanted cranial to the nuclei, thus exposing the non-targeted brain regions to large electric fields and, in turn, possible undesired stimulation effects. In this numerical study, we analyze the spread of the fields for the conventional electrodes and several modifications. As a result, we present a relatively simple electrode design that allows an efficient focalization of the stimulating field in the inferiorly located nuclei."}], "ArticleTitle": "Numerical Study on Electrode Design for Rodent Deep Brain Stimulation With Implantations Cranial to Targeted Nuclei."}, "33574746": {"mesh": [], "AbstractText": [{"section": null, "text": "Deep convolutional neural networks (DCNN) nowadays can match human performance in challenging complex tasks, but it remains unknown whether DCNNs achieve human-like performance through human-like processes. Here we applied a reverse-correlation method to make explicit representations of DCNNs and humans when performing face gender classification. We found that humans and a typical DCNN, VGG-Face, used similar critical information for this task, which mainly resided at low spatial frequencies. Importantly, the prior task experience, which the VGG-Face was pre-trained to process faces at the subordinate level (i.e., identification) as humans do, seemed necessary for such representational similarity, because AlexNet, a DCNN pre-trained to process objects at the basic level (i.e., categorization), succeeded in gender classification but relied on a completely different representation. In sum, although DCNNs and humans rely on different sets of hardware to process faces, they can use a similar and implementation-independent representation to achieve the same computation goal."}], "ArticleTitle": "Implementation-Independent Representation for Deep Convolutional Neural Networks and Humans in Processing Faces."}, "33967726": {"mesh": [], "AbstractText": [{"section": null, "text": "In this paper we present a Competitive Rate-Based Algorithm (CRBA) that approximates operation of a Competitive Spiking Neural Network (CSNN). CRBA is based on modeling of the competition between neurons during a sample presentation, which can be reduced to ranking of the neurons based on a dot product operation and the use of a discrete Expectation Maximization algorithm; the latter is equivalent to the spike time-dependent plasticity rule. CRBA's performance is compared with that of CSNN on the MNIST and Fashion-MNIST datasets. The results show that CRBA performs on par with CSNN, while using three orders of magnitude less computational time. Importantly, we show that the weights and firing thresholds learned by CRBA can be used to initialize CSNN's parameters that results in its much more efficient operation."}], "ArticleTitle": "CRBA: A Competitive Rate-Based Algorithm Based on Competitive Spiking Neural Networks."}, "33776676": {"mesh": [], "AbstractText": [{"section": null, "text": "Among many artificial neural networks, the research on Spike Neural Network (SNN), which mimics the energy-efficient signal system in the brain, is drawing much attention. Memristor is a promising candidate as a synaptic component for hardware implementation of SNN, but several non-ideal device properties are making it challengeable. In this work, we conducted an SNN simulation by adding a device model with a non-linear weight update to test the impact on SNN performance. We found that SNN has a strong tolerance for the device non-linearity and the network can keep the accuracy high if a device meets one of the two conditions: 1. symmetric LTP and LTD curves and 2. positive non-linearity factors for both LTP and LTD. The reason was analyzed in terms of the balance between network parameters as well as the variability of weight. The results are considered to be a piece of useful prior information for the future implementation of emerging device-based neuromorphic hardware."}], "ArticleTitle": "Spiking Neural Network (SNN) With Memristor Synapses Having Non-linear Weight Update."}, "33584235": {"mesh": [], "AbstractText": [{"section": null, "text": "[This corrects the article DOI: 10.3389/fncom.2020.00006.]."}], "ArticleTitle": "Corrigendum: Demystifying Brain Tumor Segmentation Networks: Interpretability and Uncertainty Analysis."}, "27536231": {"mesh": [], "AbstractText": [{"section": null, "text": "Accumulating evidence reveals that neuronal oscillations with various frequency bands in the brain have different physiological functions. However, the frequency band divisions in rats were typically based on empirical spectral distribution from limited channels information. In the present study, functionally relevant frequency bands across vigilance states and brain regions were identified using factor analysis based on 9 channels EEG signals recorded from multiple brain areas in rats. We found that frequency band divisions varied both across vigilance states and brain regions. In particular, theta oscillations during REM sleep were subdivided into two bands, 5-7 and 8-11 Hz corresponding to the tonic and phasic stages, respectively. The spindle activities of SWS were different along the anterior-posterior axis, lower oscillations (~16 Hz) in frontal regions and higher in parietal (~21 Hz). The delta and theta activities co-varied in the visual and auditory cortex during wakeful rest. In addition, power spectra of beta oscillations were significantly decreased in association cortex during REM sleep compared with wakeful rest. These results provide us some new insights into understand the brain oscillations across vigilance states, and also indicate that the spatial factor should not be ignored when considering the frequency band divisions in rats. "}], "ArticleTitle": "EEG Bands of Wakeful Rest, Slow-Wave and Rapid-Eye-Movement Sleep at Different Brain Areas in Rats."}, "27504086": {"mesh": [], "AbstractText": [{"section": null, "text": "Mechanisms underlying the emergence and plasticity of representational discontinuities in the mammalian primary somatosensory cortical representation of the hand are investigated in a computational model. The model consists of an input lattice organized as a three-digit hand forward-connected to a lattice of cortical columns each of which contains a paired excitatory and inhibitory cell. Excitatory and inhibitory synaptic plasticity of feedforward and lateral connection weights is implemented as a simple covariance rule and competitive normalization. Receptive field properties are computed independently for excitatory and inhibitory cells and compared within and across columns. Within digit representational zones intracolumnar excitatory and inhibitory receptive field extents are concentric, single-digit, small, and unimodal. Exclusively in representational boundary-adjacent zones, intracolumnar excitatory and inhibitory receptive field properties diverge: excitatory cell receptive fields are single-digit, small, and unimodal; and the paired inhibitory cell receptive fields are bimodal, double-digit, and large. In simulated syndactyly (webbed fingers), boundary-adjacent intracolumnar receptive field properties reorganize to within-representation type; divergent properties are reacquired following syndactyly release. This study generates testable hypotheses for assessment of cortical laminar-dependent receptive field properties and plasticity within and between cortical representational zones. For computational studies, present results suggest that concurrent excitatory and inhibitory plasticity may underlie novel emergent properties. "}], "ArticleTitle": "Emergent Spatial Patterns of Excitatory and Inhibitory Synaptic Strengths Drive Somatotopic Representational Discontinuities and their Plasticity in a Computational Model of Primary Sensory Cortical Area 3b."}, "27471461": {"mesh": [], "AbstractText": [{"section": null, "text": "The olfactory bulb processes inputs from olfactory receptor neurons (ORNs) through two levels: the glomerular layer at the site of input, and the granule cell level at the site of output to the olfactory cortex. The sequence of action of these two levels has not yet been examined. We analyze this issue using a novel computational framework that is scaled up, in three-dimensions (3D), with realistic representations of the interactions between layers, activated by simulated natural odors, and constrained by experimental and theoretical analyses. We suggest that the postulated functions of glomerular circuits have as their primary role transforming a complex and disorganized input into a contrast-enhanced and normalized representation, but cannot provide for synchronization of the distributed glomerular outputs. By contrast, at the granule cell layer, the dendrodendritic interactions mediate temporal decorrelation, which we show is dependent on the preceding contrast enhancement by the glomerular layer. The results provide the first insights into the successive operations in the olfactory bulb, and demonstrate the significance of the modular organization around glomeruli. This layered organization is especially important for natural odor inputs, because they activate many overlapping glomeruli. "}], "ArticleTitle": "Glomerular and Mitral-Granule Cell Microcircuits Coordinate Temporal and Spatial Information Processing in the Olfactory Bulb."}, "27462216": {"mesh": [], "AbstractText": [{"section": null, "text": "Affective brain-computer interfaces (BCI) harness Neuroscience knowledge to develop affective interaction from first principles. In this article, we explore affective engagement with a virtual agent through Neurofeedback (NF). We report an experiment where subjects engage with a virtual agent by expressing positive attitudes towards her under a NF paradigm. We use for affective input the asymmetric activity in the dorsolateral prefrontal cortex (DL-PFC), which has been previously found to be related to the high-level affective-motivational dimension of approach/avoidance. The magnitude of left-asymmetric DL-PFC activity, measured using functional near infrared spectroscopy (fNIRS) and treated as a proxy for approach, is mapped onto a control mechanism for the virtual agent's facial expressions, in which action units (AUs) are activated through a neural network. We carried out an experiment with 18 subjects, which demonstrated that subjects are able to successfully engage with the virtual agent by controlling their mental disposition through NF, and that they perceived the agent's responses as realistic and consistent with their projected mental disposition. This interaction paradigm is particularly relevant in the case of affective BCI as it facilitates the volitional activation of specific areas normally not under conscious control. Overall, our contribution reconciles a model of affect derived from brain metabolic data with an ecologically valid, yet computationally controllable, virtual affective communication environment. "}], "ArticleTitle": "Affective Interaction with a Virtual Character Through an fNIRS Brain-Computer Interface."}, "27445782": {"mesh": [], "AbstractText": [{"section": null, "text": "As high-resolution functional magnetic resonance imaging (fMRI) and fMRI of cortical layers become more widely used, the question how well high-resolution fMRI signals reflect the underlying neural processing, and how to interpret laminar fMRI data becomes more and more relevant. High-resolution fMRI has shown laminar differences in cerebral blood flow (CBF), volume (CBV), and neurovascular coupling. Features and processes that were previously lumped into a single voxel become spatially distinct at high resolution. These features can be vascular compartments such as veins, arteries, and capillaries, or cortical layers and columns, which can have differences in metabolism. Mesoscopic models of the blood oxygenation level dependent (BOLD) response therefore need to be expanded, for instance, to incorporate laminar differences in the coupling between neural activity, metabolism and the hemodynamic response. Here we discuss biological and methodological factors that affect the modeling and interpretation of high-resolution fMRI data. We also illustrate with examples from neuropharmacology and the negative BOLD response how combining BOLD with CBF- and CBV-based fMRI methods can provide additional information about neurovascular coupling, and can aid modeling and interpretation of high-resolution fMRI. "}], "ArticleTitle": "fMRI at High Spatial Resolution: Implications for BOLD-Models."}, "27378897": {"mesh": [], "AbstractText": [{"section": null, "text": "We consider a network of coupled excitatory and inhibitory theta neurons which is capable of supporting stable spatially-localized \"bump\" solutions. We randomly add long-range and simultaneously remove short-range connections within the network to form a small-world network and investigate the effects of this rewiring on the existence and stability of the bump solution. We consider two limits in which continuum equations can be derived; bump solutions are fixed points of these equations. We can thus use standard numerical bifurcation analysis to determine the stability of these bumps and to follow them as parameters (such as rewiring probabilities) are varied. We find that under some rewiring schemes bumps are quite robust, whereas in other schemes they can become unstable via Hopf bifurcation or even be destroyed in saddle-node bifurcations. "}], "ArticleTitle": "Bumps in Small-World Networks."}, "30294268": {"mesh": [], "AbstractText": [{"section": null, "text": "The mechanism by which deep brain stimulation (DBS) improves dystonia is not understood, partly heterogeneity of the underlying disorders leads to differing effects of stimulation in different locations. Similarity between the effects of DBS and the effects of lesions has led to biophysical models of blockade or reduced transmission of involuntary activity in individual cells in the pathways responsible for dystonia. Here, we expand these theories by modeling the effect of DBS on populations of neurons. We emphasize the important observation that the DBS signal itself causes surprisingly few side effects and does not normally appear in the electromyographic signal. We hypothesize that, at the population level, massively synchronous rhythmic firing caused by DBS is only poorly transmitted through downstream populations. However, the high frequency of stimulation overwhelms incoming dystonic activity, thereby substituting an ineffectively transmitted exogenous signal for the endogenous abnormal signal. Changes in sensitivity can occur not only at the site of stimulation, but also at downstream sites due to synaptic and homeostatic plasticity mechanisms. The mechanism is predicted to depend strongly on the stimulation frequency. We provide preliminary data from simultaneous multichannel recordings in basal ganglia and thalamus in children with secondary dystonia. We also provide illustrative simulations of the effect of stimulation frequency on the transmission of the DBS pulses through sequential populations of neurons in the dystonia pathway. Our experimental results and model provide a new hypothesis and computational framework consistent with the clinical features of DBS in childhood acquired dystonia."}], "ArticleTitle": "A Computational Model of Deep-Brain Stimulation for Acquired Dystonia in Children."}, "27471462": {"mesh": [], "AbstractText": [{"section": null, "text": "This work focuses on finding the most discriminatory or representative features that allow to classify commercials according to negative, neutral and positive effectiveness based on the Ace Score index. For this purpose, an experiment involving forty-seven participants was carried out. In this experiment electroencephalography (EEG), electrocardiography (ECG), Galvanic Skin Response (GSR) and respiration data were acquired while subjects were watching a 30-min audiovisual content. This content was composed by a submarine documentary and nine commercials (one of them the ad under evaluation). After the signal pre-processing, four sets of features were extracted from the physiological signals using different state-of-the-art metrics. These features computed in time and frequency domains are the inputs to several basic and advanced classifiers. An average of 89.76% of the instances was correctly classified according to the Ace Score index. The best results were obtained by a classifier consisting of a combination between AdaBoost and Random Forest with automatic selection of features. The selected features were those extracted from GSR and HRV signals. These results are promising in the audiovisual content evaluation field by means of physiological signal processing. "}], "ArticleTitle": "A Comparison of Physiological Signal Analysis Techniques and Classifiers for Automatic Emotional Evaluation of Audiovisual Contents."}, "27445779": {"mesh": [], "AbstractText": [{"section": null, "text": "Many diurnal photoreceptors encode vast real-world light changes effectively, but how this performance originates from photon sampling is unclear. A 4-module biophysically-realistic fly photoreceptor model, in which information capture is limited by the number of its sampling units (microvilli) and their photon-hit recovery time (refractoriness), can accurately simulate real recordings and their information content. However, sublinear summation in quantum bump production (quantum-gain-nonlinearity) may also cause adaptation by reducing the bump/photon gain when multiple photons hit the same microvillus simultaneously. Here, we use a Random Photon Absorption Model (RandPAM), which is the 1st module of the 4-module fly photoreceptor model, to quantify the contribution of quantum-gain-nonlinearity in light adaptation. We show how quantum-gain-nonlinearity already results from photon sampling alone. In the extreme case, when two or more simultaneous photon-hits reduce to a single sublinear value, quantum-gain-nonlinearity is preset before the phototransduction reactions adapt the quantum bump waveform. However, the contribution of quantum-gain-nonlinearity in light adaptation depends upon the likelihood of multi-photon-hits, which is strictly determined by the number of microvilli and light intensity. Specifically, its contribution to light-adaptation is marginal (&#8804; 1%) in fly photoreceptors with many thousands of microvilli, because the probability of simultaneous multi-photon-hits on any one microvillus is low even during daylight conditions. However, in cells with fewer sampling units, the impact of quantum-gain-nonlinearity increases with brightening light. "}], "ArticleTitle": "Random Photon Absorption Model Elucidates How Early Gain Control in Fly Photoreceptors Arises from Quantal Sampling."}, "27445778": {"mesh": [], "AbstractText": [{"section": null, "text": "Exploring time-varying connectivity networks in neurodegenerative disorders is a recent field of research in functional MRI. Dementia with Lewy bodies (DLB) represents 20% of the neurodegenerative forms of dementia. Fluctuations of cognition and vigilance are the key symptoms of DLB. To date, no dynamic functional connectivity (DFC) investigations of this disorder have been performed. In this paper, we refer to the concept of connectivity state as a piecewise stationary configuration of functional connectivity between brain networks. From this concept, we propose a new method for group-level as well as for subject-level studies to compare and characterize connectivity state changes between a set of resting-state networks (RSNs). Dynamic Bayesian networks, statistical and graph theory-based models, enable one to learn dependencies between interacting state-based processes. Product hidden Markov models (PHMM), an instance of dynamic Bayesian networks, are introduced here to capture both statistical and temporal aspects of DFC of a set of RSNs. This analysis was based on sliding-window cross-correlations between seven RSNs extracted from a group independent component analysis performed on 20 healthy elderly subjects and 16 patients with DLB. Statistical models of DFC differed in patients compared to healthy subjects for the occipito-parieto-frontal network, the medial occipital network and the right fronto-parietal network. In addition, pairwise comparisons of DFC of RSNs revealed a decrease of dependency between these two visual networks (occipito-parieto-frontal and medial occipital networks) and the right fronto-parietal control network. The analysis of DFC state changes thus pointed out networks related to the cognitive functions that are known to be impaired in DLB: visual processing as well as attentional and executive functions. Besides this context, product HMM applied to RSNs cross-correlations offers a promising new approach to investigate structural and temporal aspects of brain DFC. "}], "ArticleTitle": "Identifying Dynamic Functional Connectivity Changes in Dementia with Lewy Bodies Based on Product Hidden Markov Models."}, "27378898": {"mesh": [], "AbstractText": [{"section": null, "text": "Nowadays, the experimental study of emotional learning is commonly based on classical conditioning paradigms and models, which have been thoroughly investigated in the last century. Unluckily, models based on classical conditioning are unable to explain or predict important psychophysiological phenomena, such as the failure of the extinction of emotional responses in certain circumstances (for instance, those observed in evaluative conditioning, in post-traumatic stress disorders and in panic attacks). In this manuscript, starting from the experimental results available from the literature, a computational model of implicit emotional learning based both on prediction errors computation and on statistical inference is developed. The model quantitatively predicts (a) the occurrence of evaluative conditioning, (b) the dynamics and the resistance-to-extinction of the traumatic emotional responses, (c) the mathematical relation between classical conditioning and unconditioned stimulus revaluation. Moreover, we discuss how the derived computational model can lead to the development of new animal models for resistant-to-extinction emotional reactions and novel methodologies of emotions modulation. "}], "ArticleTitle": "A System Computational Model of Implicit Emotional Learning."}, "27313528": {"mesh": [], "AbstractText": [{"section": null, "text": "As we look around a scene, we perceive it as continuous and stable even though each saccadic eye movement changes the visual input to the retinas. How the brain achieves this perceptual stabilization is unknown, but a major hypothesis is that it relies on presaccadic remapping, a process in which neurons shift their visual sensitivity to a new location in the scene just before each saccade. This hypothesis is difficult to test in vivo because complete, selective inactivation of remapping is currently intractable. We tested it in silico with a hierarchical, sheet-based neural network model of the visual and oculomotor system. The model generated saccadic commands to move a video camera abruptly. Visual input from the camera and internal copies of the saccadic movement commands, or corollary discharge, converged at a map-level simulation of the frontal eye field (FEF), a primate brain area known to receive such inputs. FEF output was combined with eye position signals to yield a suitable coordinate frame for guiding arm movements of a robot. Our operational definition of perceptual stability was \"useful stability,\" quantified as continuously accurate pointing to a visual object despite camera saccades. During training, the emergence of useful stability was correlated tightly with the emergence of presaccadic remapping in the FEF. Remapping depended on corollary discharge but its timing was synchronized to the updating of eye position. When coupled to predictive eye position signals, remapping served to stabilize the target representation for continuously accurate pointing. Graded inactivations of pathways in the model replicated, and helped to interpret, previous in vivo experiments. The results support the hypothesis that visual stability requires presaccadic remapping, provide explanations for the function and timing of remapping, and offer testable hypotheses for in vivo studies. We conclude that remapping allows for seamless coordinate frame transformations and quick actions despite visual afferent lags. With visual remapping in place for behavior, it may be exploited for perceptual continuity. "}], "ArticleTitle": "Neural Network Evidence for the Coupling of Presaccadic Visual Remapping to Predictive Eye Position Updating."}, "27303287": {"mesh": [], "AbstractText": [{"section": null, "text": "Production and comprehension of speech are closely interwoven. For example, the ability to detect an error in one's own speech, halt speech production, and finally correct the error can be explained by assuming an inner speech loop which continuously compares the word representations induced by production to those induced by perception at various cognitive levels (e.g., conceptual, word, or phonological levels). Because spontaneous speech errors are relatively rare, a picture naming and halt paradigm can be used to evoke them. In this paradigm, picture presentation (target word initiation) is followed by an auditory stop signal (distractor word) for halting speech production. The current study seeks to understand the neural mechanisms governing self-detection of speech errors by developing a biologically inspired neural model of the inner speech loop. The neural model is based on the Neural Engineering Framework (NEF) and consists of a network of about 500,000 spiking neurons. In the first experiment we induce simulated speech errors semantically and phonologically. In the second experiment, we simulate a picture naming and halt task. Target-distractor word pairs were balanced with respect to variation of phonological and semantic similarity. The results of the first experiment show that speech errors are successfully detected by a monitoring component in the inner speech loop. The results of the second experiment show that the model correctly reproduces human behavioral data on the picture naming and halt task. In particular, the halting rate in the production of target words was lower for phonologically similar words than for semantically similar or fully dissimilar distractor words. We thus conclude that the neural architecture proposed here to model the inner speech loop reflects important interactions in production and perception at phonological and semantic levels. "}], "ArticleTitle": "Modeling Interactions between Speech Production and Perception: Speech Error Detection at Semantic and Phonological Levels and the Inner Speech Loop."}, "33643017": {"mesh": [], "AbstractText": [{"section": null, "text": "It has been hypothesized that the brain optimizes its capacity for computation by self-organizing to a critical point. The dynamical state of criticality is achieved by striking a balance such that activity can effectively spread through the network without overwhelming it and is commonly identified in neuronal networks by observing the behavior of cascades of network activity termed \"neuronal avalanches.\" The dynamic activity that occurs in neuronal networks is closely intertwined with how the elements of the network are connected and how they influence each other's functional activity. In this review, we highlight how studying criticality with a broad perspective that integrates concepts from physics, experimental and theoretical neuroscience, and computer science can provide a greater understanding of the mechanisms that drive networks to criticality and how their disruption may manifest in different disorders. First, integrating graph theory into experimental studies on criticality, as is becoming more common in theoretical and modeling studies, would provide insight into the kinds of network structures that support criticality in networks of biological neurons. Furthermore, plasticity mechanisms play a crucial role in shaping these neural structures, both in terms of homeostatic maintenance and learning. Both network structures and plasticity have been studied fairly extensively in theoretical models, but much work remains to bridge the gap between theoretical and experimental findings. Finally, information theoretical approaches can tie in more concrete evidence of a network's computational capabilities. Approaching neural dynamics with all these facets in mind has the potential to provide a greater understanding of what goes wrong in neural disorders. Criticality analysis therefore holds potential to identify disruptions to healthy dynamics, granted that robust methods and approaches are considered."}], "ArticleTitle": "Criticality, Connectivity, and Neural Disorder: A Multifaceted Approach to Neural Computation."}, "28649196": {"mesh": [], "AbstractText": [{"section": null, "text": "Dynamic joint stiffness determines the relation between joint position and torque, and plays a vital role in the control of posture and movement. Dynamic joint stiffness can be quantified during quasi-stationary conditions using disturbance experiments, where small position perturbations are applied to the joint and the torque response is recorded. Dynamic joint stiffness is composed of intrinsic and reflex mechanisms that act and change together, so that nonlinear, mathematical models and specialized system identification techniques are necessary to estimate their relative contributions to overall joint stiffness. Quasi-stationary experiments have demonstrated that dynamic joint stiffness is heavily modulated by joint position and voluntary torque. Consequently, during movement, when joint position and torque change rapidly, dynamic joint stiffness will be Time-Varying (TV). This paper introduces a new method to quantify the TV intrinsic and reflex components of dynamic joint stiffness during movement. The algorithm combines ensemble and deterministic approaches for estimation of TV systems; and uses a TV, parallel-cascade, nonlinear system identification technique to separate overall dynamic joint stiffness into intrinsic and reflex components from position and torque records. Simulation studies of a stiffness model, whose parameters varied with time as is expected during walking, demonstrated that the new algorithm accurately tracked the changes in dynamic joint stiffness using as little as 40 gait cycles. The method was also used to estimate the intrinsic and reflex dynamic ankle stiffness from an experiment with a healthy subject during which ankle movements were imposed while the subject maintained a constant muscle contraction. The method identified TV stiffness model parameters that predicted the measured torque very well, accounting for more than 95% of its variance. Moreover, both intrinsic and reflex dynamic stiffness were heavily modulated through the movement in a manner that could not be predicted from quasi-stationary experiments. The new method provides the tool needed to explore the role of dynamic stiffness in the control of movement."}], "ArticleTitle": "Estimation of Time-Varying, Intrinsic and Reflex Dynamic Joint Stiffness during Movement. Application to the Ankle Joint."}, "28620292": {"mesh": [], "AbstractText": [{"section": null, "text": "Adolescence is a sensitive period for the development of romantic relationships. During this period the maturation of frontolimbic networks is particularly important for the capacity to regulate emotional experiences. In previous research, both functional magnetic resonance imaging (fMRI) and dense array electroencephalography (dEEG) measures have suggested that responses in limbic regions are enhanced in adolescents experiencing social rejection. In the present research, we examined social acceptance and rejection from romantic partners as they engaged in a Chatroom Interact Task. Dual 128-channel dEEG systems were used to record neural responses to acceptance and rejection from both adolescent romantic partners and unfamiliar peers (N = 75). We employed a two-step temporal principal component analysis (PCA) and spatial independent component analysis (ICA) approach to statistically identify the neural components related to social feedback. Results revealed that the early (288 ms) discrimination between acceptance and rejection reflected by the P3a component was significant for the romantic partner but not the unfamiliar peer. In contrast, the later (364 ms) P3b component discriminated between acceptance and rejection for both partners and peers. The two-step approach (PCA then ICA) was better able than either PCA or ICA alone in separating these components of the brain's electrical activity that reflected both temporal and spatial phases of the brain's processing of social feedback."}], "ArticleTitle": "Dynamic Responses in Brain Networks to Social Feedback: A Dual EEG Acquisition Study in Adolescent Couples."}, "28611619": {"mesh": [], "AbstractText": [{"section": null, "text": "While movement is essential to human wellbeing, we are still unable to reproduce the deftness and robustness of human movement in automatons or completely restore function to individuals with many types of motor impairment. To better understand how the human nervous system plans and controls movements, neuromechanists employ simple tasks such as upper extremity reaches and isometric force tasks. However, these simple tasks rarely consider impacts and may not capture aspects of motor control that arise from real-world complexity. Here we compared existing models of motor control with the results of a periodic targeted impact task extended from Bernstein's seminal work: hammering a nail into wood. We recorded impact forces and kinematics from 10 subjects hammering at different frequencies and with hammers with different physical properties (mass and face area). We found few statistical differences in most measures between different types of hammer, demonstrating human robustness to minor changes in dynamics. Because human motor control is thought to obey optimality principles, we also developed a feedforward optimal simulation with a neuromechanically inspired cost function that reproduces the experimental data. However, Fitts' Law, which relates movement time to distance traveled and target size, did not match our experimental data. We therefore propose a new model in which the distance moved is a logarithmic function of the time to move that yields better results (R2 &#8805; 0.99 compared to R2 &#8805; 0.88). These results support the argument that humans control movement in an optimal way, but suggest that Fitts' Law may not generalize to periodic impact tasks."}], "ArticleTitle": "Hammering Does Not Fit Fitts' Law."}, "28473765": {"mesh": [], "AbstractText": [{"section": null, "text": "Cortical activity exhibits distinct characteristics in different functional states. In awake behaving animals it shows less synchrony, while in rest or sleeping state cortical activity is most synchronous. Previous studies showed that switching between functional states can change the efficiency of flowing sensory information. Switching between functional states can be triggered by releasing neuromodulators which affect neurotransmitter release probability and depolarization of cortical neurons. In this work we focus on studying primary visual area V1, by using firing rate ring model with short-term synaptic depression (STD). We show that reconstruction of visual features from V1 activity depends on the functional state, with best precision achieved at the state with intermediate release probability. We suggest that this regime corresponds to the state of maximal visual attention."}], "ArticleTitle": "Feature Detection in Visual Cortex during Different Functional States."}, "28458634": {"mesh": [], "AbstractText": [{"section": null, "text": "Electro-cortical activity in patients with epilepsy may show abnormal rhythmic transients in response to stimulation. Even when using the same stimulation parameters in the same patient, wide variability in the duration of transient response has been reported. These transients have long been considered important for the mapping of the excitability levels in the epileptic brain but their dynamic mechanism is still not well understood. To investigate the occurrence of abnormal transients dynamically, we use a thalamo-cortical neural population model of epileptic spike-wave activity and study the interaction between slow and fast subsystems. In a reduced version of the thalamo-cortical model, slow wave oscillations arise from a fold of cycles (FoC) bifurcation. This marks the onset of a region of bistability between a high amplitude oscillatory rhythm and the background state. In vicinity of the bistability in parameter space, the model has excitable dynamics, showing prolonged rhythmic transients in response to suprathreshold pulse stimulation. We analyse the state space geometry of the bistable and excitable states, and find that the rhythmic transient arises when the impending FoC bifurcation deforms the state space and creates an area of locally reduced attraction to the fixed point. This area essentially allows trajectories to dwell there before escaping to the stable steady state, thus creating rhythmic transients. In the full thalamo-cortical model, we find a similar FoC bifurcation structure. Based on the analysis, we propose an explanation of why stimulation induced epileptiform activity may vary between trials, and predict how the variability could be related to ongoing oscillatory background activity. We compare our dynamic mechanism with other mechanisms (such as a slow parameter change) to generate excitable transients, and we discuss the proposed excitability mechanism in the context of stimulation responses in the epileptic cortex."}], "ArticleTitle": "Understanding Epileptiform After-Discharges as Rhythmic Oscillatory Transients."}, "28373839": {"mesh": [], "AbstractText": [{"section": null, "text": "Globular bushy cells (GBCs) located in the ventral cochlear nucleus are an essential part of the sound localization pathway in the mammalian auditory system. They receive inputs directly from the auditory nerve and are particularly sensitive to temporal cues due to their synaptic and membrane specializations. GBCs act as coincidence detectors for incoming spikes through large synapses-endbulbs of Held-which connect to their soma. Since endbulbs of Held are an integral part of the auditory information conveying and processing pathway, they were extensively studied. Virtually all in vitro studies showed large synaptic depression, but on the other hand a few in vivo studies showed relatively small depression. It is also still not well understood how synaptic properties functionally influence firing properties of GBCs. Here we show how different levels of synaptic depression shape firing properties of GBCs in in vivo-like conditions using computer simulations. We analyzed how an interplay of synaptic depression (0-70%) and the number of auditory nerve fiber inputs (10-70) contributes to the variability of the experimental data from previous studies. We predict that the majority of synapses of GBCs with high characteristic frequencies (CF > 500 Hz) have a rate dependent depression of less than 20%. GBCs with lower CF (<500 Hz) work also with strong depressing synapses (up to 50% or more). We also showed that synapses explicitly fitted to in vitro experiments with paired-pulse stimuli did not operate properly in in vivo-like conditions and required further extension to capture the differences between in vitro and in vivo experimental conditions. Overall, this study helps to understand how synaptic properties shape temporal processing in the auditory system. It also integrates, compares, and reconciles results of various experimental studies."}], "ArticleTitle": "High Entrainment Constrains Synaptic Depression Levels of an In vivo Globular Bushy Cell Model."}, "28270760": {"mesh": [], "AbstractText": [{"section": null, "text": "This paper proposed a new method to determine the neuronal tuning curves for maximum information efficiency by computing the optimum firing rate distribution. Firstly, we proposed a general definition for the information efficiency, which is relevant to mutual information and neuronal energy consumption. The energy consumption is composed of two parts: neuronal basic energy consumption and neuronal spike emission energy consumption. A parameter to model the relative importance of energy consumption is introduced in the definition of the information efficiency. Then, we designed a combination of exponential functions to describe the optimum firing rate distribution based on the analysis of the dependency of the mutual information and the energy consumption on the shape of the functions of the firing rate distributions. Furthermore, we developed a rapid algorithm to search the parameter values of the optimum firing rate distribution function. Finally, we found with the rapid algorithm that a combination of two different exponential functions with two free parameters can describe the optimum firing rate distribution accurately. We also found that if the energy consumption is relatively unimportant (important) compared to the mutual information or the neuronal basic energy consumption is relatively large (small), the curve of the optimum firing rate distribution will be relatively flat (steep), and the corresponding optimum tuning curve exhibits a form of sigmoid if the stimuli distribution is normal."}], "ArticleTitle": "Determine Neuronal Tuning Curves by Exploring Optimum Firing Rate Distribution for Information Efficiency."}, "28232797": {"mesh": [], "AbstractText": [{"section": null, "text": "Encoding models are used for predicting brain activity in response to sensory stimuli with the objective of elucidating how sensory information is represented in the brain. Encoding models typically comprise a nonlinear transformation of stimuli to features (feature model) and a linear convolution of features to responses (response model). While there has been extensive work on developing better feature models, the work on developing better response models has been rather limited. Here, we investigate the extent to which recurrent neural network models can use their internal memories for nonlinear processing of arbitrary feature sequences to predict feature-evoked response sequences as measured by functional magnetic resonance imaging. We show that the proposed recurrent neural network models can significantly outperform established response models by accurately estimating long-term dependencies that drive hemodynamic responses. The results open a new window into modeling the dynamics of brain activity in response to sensory stimuli."}], "ArticleTitle": "Modeling the Dynamics of Human Brain Activity with Recurrent Neural Networks."}, "28210218": {"mesh": [], "AbstractText": [{"section": null, "text": "Phasic neurons typically fire only for a fast-rising input, say at the onset of a step current, but not for steady or slow inputs, a property associated with type III excitability. Phasic neurons can show extraordinary temporal precision for phase locking and coincidence detection. Exemplars are found in the auditory brain stem where precise timing is used in sound localization. Phasicness at the cellular level arises from a dynamic, voltage-gated, negative feedback that can be recruited subthreshold, preventing the neuron from reaching spike threshold if the voltage does not rise fast enough. We consider two mechanisms for phasicness: a low threshold potassium current (subtractive mechanism) and a sodium current with subthreshold inactivation (divisive mechanism). We develop and analyze three reduced models with either divisive or subtractive mechanisms or both to gain insight into the dynamical mechanisms for the potentially high temporal precision of type III-excitable neurons. We compare their firing properties and performance for a range of stimuli. The models have characteristic non-monotonic input-output relations, firing rate vs. input intensity, for either stochastic current injection or Poisson-timed excitatory synaptic conductance trains. We assess performance according to precision of phase-locking and coincidence detection by the models' responses to repetitive packets of unitary excitatory synaptic inputs with more or less temporal coherence. We find that each mechanism contributes features but best performance is attained if both are present. The subtractive mechanism confers extraordinary precision for phase locking and coincidence detection but only within a restricted parameter range when the divisive mechanism of sodium inactivation is inoperative. The divisive mechanism guarantees robustness of phasic properties, without compromising excitability, although with somewhat less precision. Finally, we demonstrate that brief transient inhibition if properly timed can enhance the reliability of firing."}], "ArticleTitle": "Phasic Firing and Coincidence Detection by Subthreshold Negative Feedback: Divisive or Subtractive or, Better, Both."}, "28163679": {"mesh": [], "AbstractText": [{"section": null, "text": "Transcranial magneto-acoustical stimulation (TMAS) uses ultrasonic waves and a static magnetic field to generate electric current in nerve tissues for the purpose of modulating neuronal activities. It has the advantage of high spatial resolution and penetration depth. Neuronal firing rhythms carry and transmit nerve information in neural systems. In this study, we investigated the phase-locking characteristics of neuronal firing rhythms with TMAS based on the Hodgkin-Huxley neuron model. The simulation results indicate that the modulation frequency of ultrasound can affect the phase-locking behaviors. The results of this study may help us to explain the potential firing mechanism of TMAS."}], "ArticleTitle": "A Phase-Locking Analysis of Neuronal Firing Rhythms with Transcranial Magneto-Acoustical Stimulation Based on the Hodgkin-Huxley Neuron Model."}, "28119595": {"mesh": [], "AbstractText": [{"section": null, "text": "Recurrent neural networks (RNN) have traditionally been of great interest for their capacity to store memories. In past years, several works have been devoted to determine the maximum storage capacity of RNN, especially for the case of the Hopfield network, the most popular kind of RNN. Analyzing the thermodynamic limit of the statistical properties of the Hamiltonian corresponding to the Hopfield neural network, it has been shown in the literature that the retrieval errors diverge when the number of stored memory patterns (P) exceeds a fraction (&#8776; 14%) of the network size N. In this paper, we study the storage performance of a generalized Hopfield model, where the diagonal elements of the connection matrix are allowed to be different from zero. We investigate this model at finite N. We give an analytical expression for the number of retrieval errors and show that, by increasing the number of stored patterns over a certain threshold, the errors start to decrease and reach values below unit for P &#8811; N. We demonstrate that the strongest trade-off between efficiency and effectiveness relies on the number of patterns (P) that are stored in the network by appropriately fixing the connection weights. When P&#8811;N and the diagonal elements of the adjacency matrix are not forced to be zero, the optimal storage capacity is obtained with a number of stored memories much larger than previously reported. This theory paves the way to the design of RNN with high storage capacity and able to retrieve the desired pattern without distortions."}], "ArticleTitle": "On the Maximum Storage Capacity of the Hopfield Model."}, "28082891": {"mesh": [], "AbstractText": [{"section": null, "text": "Characteristic phase shifts between discharges of pyramidal cells and interneurons in oscillation have been widely observed in experiments, and they have been suggested to play important roles in neural computation. Previous studies mainly explored two independent mechanisms to generate neural oscillation, one is based on the interaction loop between pyramidal cells and interneurons, referred to as the E-I loop, and the other is based on the interaction loop between interneurons, referred to as the I-I loop. In the present study, we consider neural networks consisting of both the E-I and I-I loops, and the network oscillation can operate under either E-I loop dominating mode or I-I loop dominating mode, depending on the network structure, and neuronal connection patterns. We found that the phase shift between pyramidal cells and interneurons displays different characteristics in different oscillation modes, and its amplitude varies with the network parameters. We expect that this study helps us to understand the structural characteristics of neural circuits underlying various oscillation behaviors observed in experiments."}], "ArticleTitle": "On the Phase Relationship between Excitatory and Inhibitory Neurons in Oscillation."}, "28082890": {"mesh": [], "AbstractText": [{"section": null, "text": "Burst spike patterns are common in regions of the hippocampal formation such as the subiculum and medial entorhinal cortex (MEC). Neurons in these areas are immersed in extracellular electrical potential fluctuations often recorded as the local field potential (LFP). LFP rhythms within different frequency bands are linked to different behavioral states. For example, delta rhythms are often associated with slow-wave sleep, inactivity and anesthesia; whereas theta rhythms are prominent during awake exploratory behavior and REM sleep. Recent evidence suggests that bursting neurons in the hippocampal formation can encode LFP features. We explored this hypothesis using a two-compartment model of a bursting pyramidal neuron driven by time-varying input signals containing spectral peaks at either delta or theta rhythms. The model predicted a neural code in which bursts represented the instantaneous value, phase, slope and amplitude of the driving signal both in their timing and size (spike number). To verify whether this code is employed in vivo, we examined electrophysiological recordings from the subiculum of anesthetized rats and the MEC of a behaving rat containing prevalent delta or theta rhythms, respectively. In both areas, we found bursting cells that encoded information about the instantaneous voltage, phase, slope and/or amplitude of the dominant LFP rhythm with essentially the same neural code as the simulated neurons. A fraction of the cells encoded part of the information in burst size, in agreement with model predictions. These results provide in-vivo evidence that the output of bursting neurons in the mammalian brain is tuned to features of the LFP."}], "ArticleTitle": "Bursting Neurons in the Hippocampal Formation Encode Features of LFP Rhythms."}, "28066226": {"mesh": [], "AbstractText": [{"section": null, "text": "The common approach in morphological analysis of dendritic spines of mammalian neuronal cells is to categorize spines into subpopulations based on whether they are stubby, mushroom, thin, or filopodia shaped. The corresponding cellular models of synaptic plasticity, long-term potentiation, and long-term depression associate the synaptic strength with either spine enlargement or spine shrinkage. Although a variety of automatic spine segmentation and feature extraction methods were developed recently, no approaches allowing for an automatic and unbiased distinction between dendritic spine subpopulations and detailed computational models of spine behavior exist. We propose an automatic and statistically based method for the unsupervised construction of spine shape taxonomy based on arbitrary features. The taxonomy is then utilized in the newly introduced computational model of behavior, which relies on transitions between shapes. Models of different populations are compared using supplied bootstrap-based statistical tests. We compared two populations of spines at two time points. The first population was stimulated with long-term potentiation, and the other in the resting state was used as a control. The comparison of shape transition characteristics allowed us to identify the differences between population behaviors. Although some extreme changes were observed in the stimulated population, statistically significant differences were found only when whole models were compared. The source code of our software is freely available for non-commercial use."}, {"section": "CONTACT", "text": "d.plewczynski@cent.uw.edu.pl."}], "ArticleTitle": "Computational Approach to Dendritic Spine Taxonomy and Shape Transition Analysis."}, "28066224": {"mesh": [], "AbstractText": [{"section": null, "text": "The purpose of this study was to introduce an improved tool for automated classification of event-related potentials (ERPs) using spatiotemporally parcellated events incorporated into a functional brain network activation (BNA) analysis. The auditory oddball ERP paradigm was selected to demonstrate and evaluate the improved tool. Methods: The ERPs of each subject were decomposed into major dynamic spatiotemporal events. Then, a set of spatiotemporal events representing the group was generated by aligning and clustering the spatiotemporal events of all individual subjects. The temporal relationship between the common group events generated a network, which is the spatiotemporal reference BNA model. Scores were derived by comparing each subject's spatiotemporal events to the reference BNA model and were then entered into a support vector machine classifier to classify subjects into relevant subgroups. The reliability of the BNA scores (test-retest repeatability using intraclass correlation) and their utility as a classification tool were examined in the context of Target-Novel classification. Results: BNA intraclass correlation values of repeatability ranged between 0.51 and 0.82 for the known ERP components N100, P200, and P300. Classification accuracy was high when the trained data were validated on the same subjects for different visits (AUCs 0.93 and 0.95). The classification accuracy remained high for a test group recorded at a different clinical center with a different recording system (AUCs 0.81, 0.85 for 2 visits). Conclusion: The improved spatiotemporal BNA analysis demonstrates high classification accuracy. The BNA analysis method holds promise as a tool for diagnosis, follow-up and drug development associated with different neurological conditions."}], "ArticleTitle": "Brain Network Activation Analysis Utilizing Spatiotemporal Features for Event Related Potentials Classification."}, "28066222": {"mesh": [], "AbstractText": [{"section": null, "text": "Hypokinetic symptoms of Parkinson's disease are usually associated with excessively strong oscillations and synchrony in the beta frequency band. The origin of this synchronized oscillatory dynamics is being debated. Cortical circuits may be a critical source of excessive beta in Parkinson's disease. However, subthalamo-pallidal circuits were also suggested to be a substantial component in generation and/or maintenance of Parkinsonian beta activity. Here we study how the subthalamo-pallidal circuits interact with input signals in the beta frequency band, representing cortical input. We use conductance-based models of the subthalamo-pallidal network and two types of input signals: artificially-generated inputs and input signals obtained from recordings in Parkinsonian patients. The resulting model network dynamics is compared with the dynamics of the experimental recordings from patient's basal ganglia. Our results indicate that the subthalamo-pallidal model network exhibits multiple resonances in response to inputs in the beta band. For a relatively broad range of network parameters, there is always a certain input strength, which will induce patterns of synchrony similar to the experimentally observed ones. This ability of the subthalamo-pallidal network to exhibit realistic patterns of synchronous oscillatory activity under broad conditions may indicate that these basal ganglia circuits are directly involved in the expression of Parkinsonian synchronized beta oscillations. Thus, Parkinsonian synchronized beta oscillations may be promoted by the simultaneous action of both cortical (or some other) and subthalamo-pallidal network mechanisms. Hence, these mechanisms are not necessarily mutually exclusive."}], "ArticleTitle": "Synchronized Beta-Band Oscillations in a Model of the Globus Pallidus-Subthalamic Nucleus Network under External Input."}, "28066220": {"mesh": [], "AbstractText": [{"section": null, "text": "Brain computer interfaces allow users to preform various tasks using only the electrical activity of the brain. BCI applications often present the user a set of stimuli and record the corresponding electrical response. The BCI algorithm will then have to decode the acquired brain response and perform the desired task. In rapid serial visual presentation (RSVP) tasks, the subject is presented with a continuous stream of images containing rare target images among standard images, while the algorithm has to detect brain activity associated with target images. In this work, we suggest a multimodal neural network for RSVP tasks. The network operates on the brain response and on the initiating stimulus simultaneously, providing more information for the BCI application. We present two variants of the multimodal network, a supervised model, for the case when the targets are known in advanced, and a semi-supervised model for when the targets are unknown. We test the neural networks with a RSVP experiment on satellite imagery carried out with two subjects. The multimodal networks achieve a significant performance improvement in classification metrics. We visualize what the networks has learned and discuss the advantages of using neural network models for BCI applications."}], "ArticleTitle": "Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface."}, "28018204": {"mesh": [], "AbstractText": [{"section": null, "text": "To accurately perceive the world, people must efficiently combine internal beliefs and external sensory cues. We introduce a Bayesian framework that explains the role of internal balance cues and visual stimuli on perceived eye level (PEL)-a self-reported measure of elevation angle. This framework provides a single, coherent model explaining a set of experimentally observed PEL over a range of experimental conditions. Further, it provides a parsimonious explanation for the additive effect of low fidelity cues as well as the averaging effect of high fidelity cues, as also found in other Bayesian cue combination psychophysical studies. Our model accurately estimates the PEL and explains the form of previous equations used in describing PEL behavior. Most importantly, the proposed Bayesian framework for PEL is more powerful than previous behavioral modeling; it permits behavioral estimation in a wider range of cue combination and perceptual studies than models previously reported."}], "ArticleTitle": "Bayesian Analysis of Perceived Eye Level."}, "28018203": {"mesh": [], "AbstractText": [{"section": null, "text": "The mammalian brain is thought to use a version of Model-based Reinforcement Learning (MBRL) to guide \"goal-directed\" behavior, wherein animals consider goals and make plans to acquire desired outcomes. However, conventional MBRL algorithms do not fully explain animals' ability to rapidly adapt to environmental changes, or learn multiple complex tasks. They also require extensive computation, suggesting that goal-directed behavior is cognitively expensive. We propose here that key features of processing in the hippocampus support a flexible MBRL mechanism for spatial navigation that is computationally efficient and can adapt quickly to change. We investigate this idea by implementing a computational MBRL framework that incorporates features inspired by computational properties of the hippocampus: a hierarchical representation of space, \"forward sweeps\" through future spatial trajectories, and context-driven remapping of place cells. We find that a hierarchical abstraction of space greatly reduces the computational load (mental effort) required for adaptation to changing environmental conditions, and allows efficient scaling to large problems. It also allows abstract knowledge gained at high levels to guide adaptation to new obstacles. Moreover, a context-driven remapping mechanism allows learning and memory of multiple tasks. Simulating dorsal or ventral hippocampal lesions in our computational framework qualitatively reproduces behavioral deficits observed in rodents with analogous lesions. The framework may thus embody key features of how the brain organizes model-based RL to efficiently solve navigation and other difficult tasks."}], "ArticleTitle": "Computational Properties of the Hippocampus Increase the Efficiency of Goal-Directed Foraging through Hierarchical Reinforcement Learning."}, "27932970": {"mesh": [], "AbstractText": [{"section": null, "text": "Synaptic transmission is both history-dependent and stochastic, resulting in varying responses to presentations of the same presynaptic stimulus. This complicates attempts to infer synaptic parameters and has led to the proposal of a number of different strategies for their quantification. Recently Bayesian approaches have been applied to make more efficient use of the data collected in paired intracellular recordings. Methods have been developed that either provide a complete model of the distribution of amplitudes for isolated responses or approximate the amplitude distributions of a train of post-synaptic potentials, with correct short-term synaptic dynamics but neglecting correlations. In both cases the methods provided significantly improved inference of model parameters as compared to existing mean-variance fitting approaches. However, for synapses with high release probability, low vesicle number or relatively low restock rate and for data in which only one or few repeats of the same pattern are available, correlations between serial events can allow for the extraction of significantly more information from experiment: a more complete Bayesian approach would take this into account also. This has not been possible previously because of the technical difficulty in calculating the likelihood of amplitudes seen in correlated post-synaptic potential trains; however, recent theoretical advances have now rendered the likelihood calculation tractable for a broad class of synaptic dynamics models. Here we present a compact mathematical form for the likelihood in terms of a matrix product and demonstrate how marginals of the posterior provide information on covariance of parameter distributions. The associated computer code for Bayesian parameter inference for a variety of models of synaptic dynamics is provided in the Supplementary Material allowing for quantal and dynamical parameters to be readily inferred from experimental data sets."}], "ArticleTitle": "Bayesian Inference of Synaptic Quantal Parameters from Correlated Vesicle Release."}, "27899890": {"mesh": [], "AbstractText": [{"section": null, "text": "Experimental studies on the Lateral Geniculate Nucleus (LGN) of mammals and rodents show that the inhibitory interneurons (IN) receive around 47.1% of their afferents from the retinal spiking neurons, and constitute around 20-25% of the LGN cell population. However, there is a definite gap in knowledge about the role and impact of IN on thalamocortical dynamics in both experimental and model-based research. We use a neural mass computational model of the LGN with three neural populations viz. IN, thalamocortical relay (TCR), thalamic reticular nucleus (TRN), to study the causality of IN on LGN oscillations and state-transitions. The synaptic information transmission in the model is implemented with kinetic modeling, facilitating the linking of low-level cellular attributes with high-level population dynamics. The model is parameterized and tuned to simulate alpha (8-13 Hz) rhythm that is dominant in both Local Field Potential (LFP) of LGN and electroencephalogram (EEG) of visual cortex in an awake resting state with eyes closed. The results show that: First, the response of the TRN is suppressed in the presence of IN in the circuit; disconnecting the IN from the circuit effects a dramatic change in the model output, displaying high amplitude synchronous oscillations within the alpha band in both TCR and TRN. These observations conform to experimental reports implicating the IN as the primary inhibitory modulator of LGN dynamics in a cognitive state, and that reduced cognition is achieved by suppressing the TRN response. Second, the model validates steady state visually evoked potential response in humans corresponding to periodic input stimuli; however, when the IN is disconnected from the circuit, the output power spectra do not reflect the input frequency. This agrees with experimental reports underpinning the role of IN in efficient retino-geniculate information transmission. Third, a smooth transition from alpha to theta band is observed by progressive decrease of neurotransmitter concentrations in the synaptic clefts; however, the transition is abrupt with removal of the IN circuitry in the model. The results imply a role of IN toward maintaining homeostasis in the LGN by suppressing any instability that may arise due to anomalous synaptic attributes."}], "ArticleTitle": "Causal Role of Thalamic Interneurons in Brain State Transitions: A Study Using a Neural Mass Model Implementing Synaptic Kinetics."}, "27867353": {"mesh": [], "AbstractText": [{"section": null, "text": "Advanced statistical methods have enabled trial-by-trial inference of the underlying excitatory and inhibitory synaptic conductances (SCs) of membrane-potential recordings. Simultaneous inference of both excitatory and inhibitory SCs sheds light on the neural circuits underlying the neural activity and advances our understanding of neural information processing. Conventional Bayesian methods can infer excitatory and inhibitory SCs based on a single trial of observed membrane potential. However, if multiple recorded trials are available, this typically leads to suboptimal estimation because they neglect common statistics (of synaptic inputs (SIs)) across trials. Here, we establish a new expectation maximization (EM) algorithm that improves such single-trial Bayesian methods by exploiting multiple recorded trials to extract common SI statistics across the trials. In this paper, the proposed EM algorithm is embedded in parallel Kalman filters or particle filters for multiple recorded trials to integrate their outputs to iteratively update the common SI statistics. These statistics are then used to infer the excitatory and inhibitory SCs of individual trials. We demonstrate the superior performance of multiple-trial Kalman filtering (MtKF) and particle filtering (MtPF) relative to that of the corresponding single-trial methods. While relative estimation error of excitatory and inhibitory SCs is known to depend on the level of current injection into a cell, our numerical simulations using MtKF show that both excitatory and inhibitory SCs are reliably inferred using an optimal level of current injection. Finally, we validate the robustness and applicability of our technique through simulation studies, and we apply MtKF to in vivo data recorded from the rat barrel cortex."}], "ArticleTitle": "Simultaneous Bayesian Estimation of Excitatory and Inhibitory Synaptic Conductances by Exploiting Multiple Recorded Trials."}, "27818631": {"mesh": [], "AbstractText": [{"section": null, "text": "Flying insects, such as flies or bees, rely on consistent information regarding the depth structure of the environment when performing their flight maneuvers in cluttered natural environments. These behaviors include avoiding collisions, approaching targets or spatial navigation. Insects are thought to obtain depth information visually from the retinal image displacements (\"optic flow\") during translational ego-motion. Optic flow in the insect visual system is processed by a mechanism that can be modeled by correlation-type elementary motion detectors (EMDs). However, it is still an open question how spatial information can be extracted reliably from the responses of the highly contrast- and pattern-dependent EMD responses, especially if the vast range of light intensities encountered in natural environments is taken into account. This question will be addressed here by systematically modeling the peripheral visual system of flies, including various adaptive mechanisms. Different model variants of the peripheral visual system were stimulated with image sequences that mimic the panoramic visual input during translational ego-motion in various natural environments, and the resulting peripheral signals were fed into an array of EMDs. We characterized the influence of each peripheral computational unit on the representation of spatial information in the EMD responses. Our model simulations reveal that information about the overall light level needs to be eliminated from the EMD input as is accomplished under light-adapted conditions in the insect peripheral visual system. The response characteristics of large monopolar cells (LMCs) resemble that of a band-pass filter, which reduces the contrast dependency of EMDs strongly, effectively enhancing the representation of the nearness of objects and, especially, of their contours. We furthermore show that local brightness adaptation of photoreceptors allows for spatial vision under a wide range of dynamic light conditions."}], "ArticleTitle": "Peripheral Processing Facilitates Optic Flow-Based Depth Perception."}, "27803660": {"mesh": [], "AbstractText": [{"section": null, "text": "Synchrony and asynchrony are essential aspects of the functioning of interconnected neuronal cells and networks. New information on neuronal synchronization can be expected to aid in understanding these systems. Synchronization provides insight in the functional connectivity and the spatial distribution of the information processing in the networks. Synchronization is generally studied with time domain analysis of neuronal events, or using direct frequency spectrum analysis, e.g., in specific frequency bands. However, these methods have their pitfalls. Thus, we have previously proposed a method to analyze temporal changes in the complexity of the frequency of signals originating from different network regions. The method is based on the correlation of time varying spectral entropies (SEs). SE assesses the regularity, or complexity, of a time series by quantifying the uniformity of the frequency spectrum distribution. It has been previously employed, e.g., in electroencephalogram analysis. Here, we revisit our correlated spectral entropy method (CorSE), providing evidence of its justification, usability, and benefits. Here, CorSE is assessed with simulations and in vitro microelectrode array (MEA) data. CorSE is first demonstrated with a specifically tailored toy simulation to illustrate how it can identify synchronized populations. To provide a form of validation, the method was tested with simulated data from integrate-and-fire model based computational neuronal networks. To demonstrate the analysis of real data, CorSE was applied on in vitro MEA data measured from rat cortical cell cultures, and the results were compared with three known event based synchronization measures. Finally, we show the usability by tracking the development of networks in dissociated mouse cortical cell cultures. The results show that temporal correlations in frequency spectrum distributions reflect the network relations of neuronal populations. In the simulated data, CorSE unraveled the synchronizations. With the real in vitro MEA data, CorSE produced biologically plausible results. Since CorSE analyses continuous data, it is not affected by possibly poor spike or other event detection quality. We conclude that CorSE can reveal neuronal network synchronization based on in vitro MEA field potential measurements. CorSE is expected to be equally applicable also in the analysis of corresponding in vivo and ex vivo data analysis."}], "ArticleTitle": "Spectral Entropy Based Neuronal Network Synchronization Analysis Based on Microelectrode Array Measurements."}, "27790110": {"mesh": [], "AbstractText": [{"section": null, "text": "Glucose is the brain's principal source of ATP, but the extent to which cerebral glucose consumption (CMRglc) is coupled with its oxygen consumption (CMRO2) remains unclear. Measurements of the brain's oxygen-glucose index OGI = CMRO2/CMRglc suggest that its oxygen uptake largely suffices for oxidative phosphorylation. Nevertheless, during functional activation and in some disease states, brain tissue seemingly produces lactate although cerebral blood flow (CBF) delivers sufficient oxygen, so-called aerobic glycolysis. OGI measurements, in turn, are method-dependent in that estimates based on glucose analog uptake depend on the so-called lumped constant (LC) to arrive at CMRglc. Capillary transit time heterogeneity (CTH), which is believed to change during functional activation and in some disease states, affects the extraction efficacy of oxygen from blood. We developed a three-compartment model of glucose extraction to examine whether CTH also affects glucose extraction into brain tissue. We then combined this model with our previous model of oxygen extraction to examine whether differential glucose and oxygen extraction might favor non-oxidative glucose metabolism under certain conditions. Our model predicts that glucose uptake is largely unaffected by changes in its plasma concentration, while changes in CBF and CTH affect glucose and oxygen uptake to different extents. Accordingly, functional hyperemia facilitates glucose uptake more than oxygen uptake, favoring aerobic glycolysis during enhanced energy demands. Applying our model to glucose analogs, we observe that LC depends on physiological state, with a risk of overestimating relative increases in CMRglc during functional activation by as much as 50%."}], "ArticleTitle": "The Effects of Capillary Transit Time Heterogeneity (CTH) on the Cerebral Uptake of Glucose and Glucose Analogs: Application to FDG and Comparison to Oxygen Uptake."}, "27721751": {"mesh": [], "AbstractText": [{"section": null, "text": "As the sole output neurons in the retina, ganglion cells play significant roles in transforming visual information into spike trains, and then transmitting them to the higher visual centers. However, coding strategies that retinal ganglion cells (RGCs) adopt to accomplish these processes are not completely clear yet. To clarify these issues, we investigate the coding properties of three types of RGCs (repetitive spiking, tonic firing, and phasic firing) by two different measures (spike-rate and spike-latency). Model results show that for periodic stimuli, repetitive spiking RGC and tonic RGC exhibit similar spike-rate patterns. Their spike- rates decrease gradually with increased stimulus frequency, moreover, variation of stimulus amplitude would change the two RGCs' spike-rate patterns. For phasic RGC, it activates strongly at medium levels of frequency when the stimulus amplitude is low. While if high stimulus amplitude is applied, phasic RGC switches to respond strongly at low frequencies. These results suggest that stimulus amplitude is a prominent factor in regulating RGCs in encoding periodic signals. Similar conclusions can be drawn when analyzes spike-latency patterns of the three RGCs. More importantly, the above phenomena can be accurately reproduced by Hodgkin's three classes of neurons, indicating that RGCs can perform the typical three classes of firing dynamics, depending on the distinctions of ion channel densities. Consequently, model results from the three RGCs may be not specific, but can also applicable to neurons in other brain regions which exhibit part(s) or all of the Hodgkin's three excitabilities."}], "ArticleTitle": "Coding Properties of Three Intrinsically Distinct Retinal Ganglion Cells under Periodic Stimuli: A Computational Study."}, "27679569": {"mesh": [], "AbstractText": [{"section": null, "text": "Neural systems display rich short-term dynamics at various levels, e.g., spike-frequency adaptation (SFA) at the single-neuron level, and short-term facilitation (STF) and depression (STD) at the synapse level. These dynamical features typically cover a broad range of time scales and exhibit large diversity in different brain regions. It remains unclear what is the computational benefit for the brain to have such variability in short-term dynamics. In this study, we propose that the brain can exploit such dynamical features to implement multiple seemingly contradictory computations in a single neural circuit. To demonstrate this idea, we use continuous attractor neural network (CANN) as a working model and include STF, SFA and STD with increasing time constants in its dynamics. Three computational tasks are considered, which are persistent activity, adaptation, and anticipative tracking. These tasks require conflicting neural mechanisms, and hence cannot be implemented by a single dynamical feature or any combination with similar time constants. However, with properly coordinated STF, SFA and STD, we show that the network is able to implement the three computational tasks concurrently. We hope this study will shed light on the understanding of how the brain orchestrates its rich dynamics at various levels to realize diverse cognitive functions. "}], "ArticleTitle": "Neural Computations in a Dynamical System with Multiple Time Scales."}, "27601989": {"mesh": [], "AbstractText": [{"section": null, "text": "Joint Action is typically described as social interaction that requires coordination among two or more co-actors in order to achieve a common goal. In this article, we put forward a hypothesis for the existence of a neural-computational mechanism of affective valuation that may be critically exploited in Joint Action. Such a mechanism would serve to facilitate coordination between co-actors permitting a reduction of required information. Our hypothesized affective mechanism provides a value function based implementation of Associative Two-Process (ATP) theory that entails the classification of external stimuli according to outcome expectancies. This approach has been used to describe animal and human action that concerns differential outcome expectancies. Until now it has not been applied to social interaction. We describe our Affective ATP model as applied to social learning consistent with an \"extended common currency\" perspective in the social neuroscience literature. We contrast this to an alternative mechanism that provides an example implementation of the so-called social-specific value perspective. In brief, our Social-Affective ATP mechanism builds upon established formalisms for reinforcement learning (temporal difference learning models) nuanced to accommodate expectations (consistent with ATP theory) and extended to integrate non-social and social cues for use in Joint Action. "}], "ArticleTitle": "Minimalist Social-Affective Value for Use in Joint Action: A Neural-Computational Hypothesis."}, "27555816": {"mesh": [], "AbstractText": [{"section": null, "text": "Synapses may undergo variable changes during plasticity because of the variability of spike patterns such as temporal stochasticity and spatial randomness. Here, we call the variability of synaptic weight changes during plasticity to be efficacy variability. In this paper, we investigate how four aspects of spike pattern statistics (i.e., synchronous firing, burstiness/regularity, heterogeneity of rates and heterogeneity of cross-correlations) influence the efficacy variability under pair-wise additive spike-timing dependent plasticity (STDP) and synaptic homeostasis (the mean strength of plastic synapses into a neuron is bounded), by implementing spike shuffling methods onto spike patterns self-organized by a network of excitatory and inhibitory leaky integrate-and-fire (LIF) neurons. With the increase of the decay time scale of the inhibitory synaptic currents, the LIF network undergoes a transition from asynchronous state to weak synchronous state and then to synchronous bursting state. We first shuffle these spike patterns using a variety of methods, each designed to evidently change a specific pattern statistics; and then investigate the change of efficacy variability of the synapses under STDP and synaptic homeostasis, when the neurons in the network fire according to the spike patterns before and after being treated by a shuffling method. In this way, we can understand how the change of pattern statistics may cause the change of efficacy variability. Our results are consistent with those of our previous study which implements spike-generating models on converging motifs. We also find that burstiness/regularity is important to determine the efficacy variability under asynchronous states, while heterogeneity of cross-correlations is the main factor to cause efficacy variability when the network moves into synchronous bursting states (the states observed in epilepsy). "}], "ArticleTitle": "Spike Pattern Structure Influences Synaptic Efficacy Variability under STDP and Synaptic Homeostasis. II: Spike Shuffling Methods on LIF Networks."}, "27531978": {"mesh": [], "AbstractText": [{"section": null, "text": "Neurons across sensory systems and organisms often display complex patterns of action potentials in response to sensory input. One example of such a pattern is the tendency of neurons to fire packets of action potentials (i.e., a burst) followed by quiescence. While it is well known that multiple mechanisms can generate bursts of action potentials at both the single-neuron and the network level, the functional role of burst firing in sensory processing is not so well understood to date. Here we provide a comprehensive review of the known mechanisms and functions of burst firing in processing of electrosensory stimuli in gymnotiform weakly electric fish. We also present new evidence from existing data showing that bursts and isolated spikes provide distinct information about stimulus variance. It is likely that these functional roles will be generally applicable to other systems and species. "}], "ArticleTitle": "Burst Firing in the Electrosensory System of Gymnotiform Weakly Electric Fish: Mechanisms and Functional Roles."}, "27471460": {"mesh": [], "AbstractText": [{"section": null, "text": "Recently, biologically inspired models are gradually proposed to solve the problem in text analysis. Convolutional neural networks (CNN) are hierarchical artificial neural networks, which include a various of multilayer perceptrons. According to biological research, CNN can be improved by bringing in the attention modulation and memory processing of primate visual cortex. In this paper, we employ the above properties of primate visual cortex to improve CNN and propose a biological-mechanism-driven-feature-construction based answer recommendation method (BMFC-ARM), which is used to recommend the best answer for the corresponding given questions in community question answering. BMFC-ARM is an improved CNN with four channels respectively representing questions, answers, asker information and answerer information, and mainly contains two stages: biological mechanism driven feature construction (BMFC) and answer ranking. BMFC imitates the attention modulation property by introducing the asker information and answerer information of given questions and the similarity between them, and imitates the memory processing property through bringing in the user reputation information for answerers. Then the feature vector for answer ranking is constructed by fusing the asker-answerer similarities, answerer's reputation and the corresponding vectors of question, answer, asker, and answerer. Finally, the Softmax is used at the stage of answer ranking to get best answers by the feature vector. The experimental results of answer recommendation on the Stackexchange dataset show that BMFC-ARM exhibits better performance. "}], "ArticleTitle": "Visual Cortex Inspired CNN Model for Feature Construction in Text Analysis."}, "27468262": {"mesh": [], "AbstractText": [{"section": null, "text": "Connectionist models can be characterized within the more general framework of probabilistic graphical models, which allow to efficiently describe complex statistical distributions involving a large number of interacting variables. This integration allows building more realistic computational models of cognitive functions, which more faithfully reflect the underlying neural mechanisms at the same time providing a useful bridge to higher-level descriptions in terms of Bayesian computations. Here we discuss a powerful class of graphical models that can be implemented as stochastic, generative neural networks. These models overcome many limitations associated with classic connectionist models, for example by exploiting unsupervised learning in hierarchical architectures (deep networks) and by taking into account top-down, predictive processing supported by feedback loops. We review some recent cognitive models based on generative networks, and we point out promising research directions to investigate neuropsychological disorders within this approach. Though further efforts are required in order to fill the gap between structured Bayesian models and more realistic, biophysical models of neuronal dynamics, we argue that generative neural networks have the potential to bridge these levels of analysis, thereby improving our understanding of the neural bases of cognition and of pathologies caused by brain damage. "}], "ArticleTitle": "Probabilistic Models and Generative Neural Networks: Towards an Unified Framework for Modeling Normal and Impaired Neurocognitive Functions."}, "27462215": {"mesh": [], "AbstractText": [{"section": null, "text": "The brain must coordinate with redundant bodies to perform motion tasks. The aim of the present study is to propose a novel control model that predicts the characteristics of human joint coordination at a behavioral level. To evaluate the joint coordination, an uncontrolled manifold (UCM) analysis that focuses on the trial-to-trial variance of joints has been proposed. The UCM is a nonlinear manifold associated with redundant kinematics. In this study, we directly applied the notion of the UCM to our proposed control model called the \"UCM reference feedback control.\" To simplify the problem, the present study considered how the redundant joints were controlled to regulate a given target hand position. We considered a conventional method that pre-determined a unique target joint trajectory by inverse kinematics or any other optimization method. In contrast, our proposed control method generates a UCM as a control target at each time step. The target UCM is a subspace of joint angles whose variability does not affect the hand position. The joint combination in the target UCM is then selected so as to minimize the cost function, which consisted of the joint torque and torque change. To examine whether the proposed method could reproduce human-like joint coordination, we conducted simulation and measurement experiments. In the simulation experiments, a three-link arm with a shoulder, elbow, and wrist regulates a one-dimensional target of a hand through proposed method. In the measurement experiments, subjects performed a one-dimensional target-tracking task. The kinematics, dynamics, and joint coordination were quantitatively compared with the simulation data of the proposed method. As a result, the UCM reference feedback control could quantitatively reproduce the difference of the mean value for the end hand position between the initial postures, the peaks of the bell-shape tangential hand velocity, the sum of the squared torque, the mean value for the torque change, the variance components, and the index of synergy as well as the human subjects. We concluded that UCM reference feedback control can reproduce human-like joint coordination. The inference for motor control of the human central nervous system based on the proposed method was discussed. "}], "ArticleTitle": "Uncontrolled Manifold Reference Feedback Control of Multi-Joint Robot Arms."}, "27445780": {"mesh": [], "AbstractText": [{"section": null, "text": "Animals try to make sense of sensory information from multiple modalities by categorizing them into perceptions of individual or multiple external objects or internal concepts. For example, the brain constructs sensory, spatial representations of the locations of visual and auditory stimuli in the visual and auditory cortices based on retinal and cochlear stimulations. Currently, it is not known how the brain compares the temporal and spatial features of these sensory representations to decide whether they originate from the same or separate sources in space. Here, we propose a computational model of how the brain might solve such a task. We reduce the visual and auditory information to time-varying, finite-dimensional signals. We introduce controlled, leaky integrators as working memory that retains the sensory information for the limited time-course of task implementation. We propose our model within an evidence-based, decision-making framework, where the alternative plan units are saliency maps of space. A spatiotemporal similarity measure, computed directly from the unimodal signals, is suggested as the criterion to infer common or separate causes. We provide simulations that (1) validate our model against behavioral, experimental results in tasks where the participants were asked to report common or separate causes for cross-modal stimuli presented with arbitrary spatial and temporal disparities. (2) Predict the behavior in novel experiments where stimuli have different combinations of spatial, temporal, and reliability features. (3) Illustrate the dynamics of the proposed internal system. These results confirm our spatiotemporal similarity measure as a viable criterion for causal inference, and our decision-making framework as a viable mechanism for target selection, which may be used by the brain in cross-modal situations. Further, we suggest that a similar approach can be extended to other cognitive problems where working memory is a limiting factor, such as target selection among higher numbers of stimuli and selections among other modality combinations. "}], "ArticleTitle": "Causal Inference for Cross-Modal Action Selection: A Computational Study in a Decision Making Framework."}, "27445777": {"mesh": [], "AbstractText": [{"section": null, "text": "Previous work from our lab has demonstrated how the connectivity of brain circuits constrains the repertoire of activity patterns that those circuits can display. Specifically, we have shown that the principal components of spontaneous neural activity are uniquely determined by the underlying circuit connections, and that although the principal components do not uniquely resolve the circuit structure, they do reveal important features about it. Expanding upon this framework on a larger scale of neural dynamics, we have analyzed EEG data recorded with the standard 10-20 electrode system from 41 neurologically normal children and adolescents during stage 2, non-REM sleep. We show that the principal components of EEG spindles, or sigma waves (10-16 Hz), reveal non-propagating, standing waves in the form of spherical harmonics. We mathematically demonstrate that standing EEG waves exist when the spatial covariance and the Laplacian operator on the head's surface commute. This in turn implies that the covariance between two EEG channels decreases as the inverse of their relative distance; a relationship that we corroborate with empirical data. Using volume conduction theory, we then demonstrate that superficial current sources are more synchronized at larger distances, and determine the characteristic length of large-scale neural synchronization as 1.31 times the head radius, on average. Moreover, consistent with the hypothesis that EEG spindles are driven by thalamo-cortical rather than cortico-cortical loops, we also show that 8 additional patients with hypoplasia or complete agenesis of the corpus callosum, i.e., with deficient or no connectivity between cortical hemispheres, similarly exhibit standing EEG waves in the form of spherical harmonics. We conclude that spherical harmonics are a hallmark of spontaneous, large-scale synchronization of neural activity in the brain, which are associated with unconscious, light sleep. The analogy with spherical harmonics in quantum mechanics suggests that the variances (eigenvalues) of the principal components follow a Boltzmann distribution, or equivalently, that standing waves are in a sort of \"thermodynamic\" equilibrium during non-REM sleep. By extension, we speculate that consciousness emerges as the brain dynamics deviate from such equilibrium. "}], "ArticleTitle": "Spherical Harmonics Reveal Standing EEG Waves and Long-Range Neural Synchronization during Non-REM Sleep."}, "27378899": {"mesh": [], "AbstractText": [{"section": null, "text": "This paper describes an active inference scheme for visual searches and the perceptual synthesis entailed by scene construction. Active inference assumes that perception and action minimize variational free energy, where actions are selected to minimize the free energy expected in the future. This assumption generalizes risk-sensitive control and expected utility theory to include epistemic value; namely, the value (or salience) of information inherent in resolving uncertainty about the causes of ambiguous cues or outcomes. Here, we apply active inference to saccadic searches of a visual scene. We consider the (difficult) problem of categorizing a scene, based on the spatial relationship among visual objects where, crucially, visual cues are sampled myopically through a sequence of saccadic eye movements. This means that evidence for competing hypotheses about the scene has to be accumulated sequentially, calling upon both prediction (planning) and postdiction (memory). Our aim is to highlight some simple but fundamental aspects of the requisite functional anatomy; namely, the link between approximate Bayesian inference under mean field assumptions and functional segregation in the visual cortex. This link rests upon the (neurobiologically plausible) process theory that accompanies the normative formulation of active inference for Markov decision processes. In future work, we hope to use this scheme to model empirical saccadic searches and identify the prior beliefs that underwrite intersubject variability in the way people forage for information in visual scenes (e.g., in schizophrenia). "}], "ArticleTitle": "Scene Construction, Visual Foraging, and Active Inference."}, "33897395": {"mesh": [], "AbstractText": [{"section": null, "text": "Persistent cohomology is a powerful technique for discovering topological structure in data. Strategies for its use in neuroscience are still undergoing development. We comprehensively and rigorously assess its performance in simulated neural recordings of the brain's spatial representation system. Grid, head direction, and conjunctive cell populations each span low-dimensional topological structures embedded in high-dimensional neural activity space. We evaluate the ability for persistent cohomology to discover these structures for different dataset dimensions, variations in spatial tuning, and forms of noise. We quantify its ability to decode simulated animal trajectories contained within these topological structures. We also identify regimes under which mixtures of populations form product topologies that can be detected. Our results reveal how dataset parameters affect the success of topological discovery and suggest principles for applying persistent cohomology, as well as persistent homology, to experimental neural recordings."}], "ArticleTitle": "Evaluating State Space Discovery by Persistent Cohomology in the Spatial Representation System."}, "32848682": {"mesh": [], "AbstractText": [{"section": null, "text": "Glioblastoma is a WHO grade IV brain tumor, which leads to poor overall survival (OS) of patients. For precise surgical and treatment planning, OS prediction of glioblastoma (GBM) patients is highly desired by clinicians and oncologists. Radiomic research attempts at predicting disease prognosis, thus providing beneficial information for personalized treatment from a variety of imaging features extracted from multiple MR images. In this study, first-order, intensity-based volume and shape-based and textural radiomic features are extracted from fluid-attenuated inversion recovery (FLAIR) and T1ce MRI data. The region of interest is further decomposed with stationary wavelet transform with low-pass and high-pass filtering. Further, radiomic features are extracted on these decomposed images, which helped in acquiring the directional information. The efficiency of the proposed algorithm is evaluated on Brain Tumor Segmentation (BraTS) challenge training, validation, and test datasets. The proposed approach achieved 0.695, 0.571, and 0.558 on BraTS training, validation, and test datasets. The proposed approach secured the third position in BraTS 2018 challenge for the OS prediction task."}], "ArticleTitle": "Overall Survival Prediction in Glioblastoma With Radiomic Features Using Machine Learning."}, "27594832": {"mesh": [], "AbstractText": [{"section": null, "text": "Voluntary control of force is always marked by some degree of error and unsteadiness. Both neural and mechanical factors contribute to these fluctuations, but how they interact to produce them is poorly understood. In this study, we identify and characterize a previously undescribed neuromechanical interaction where the dynamics of voluntary force production suffice to generate involuntary tremor. Specifically, participants were asked to produce isometric force with the index finger and use visual feedback to track a sinusoidal target spanning 5-9% of each individual's maximal voluntary force level. Force fluctuations and EMG activity over the flexor digitorum superficialis (FDS) muscle were recorded and their frequency content was analyzed as a function of target phase. Force variability in either the 1-5 or 6-15 Hz frequency ranges tended to be largest at the peaks and valleys of the target sinusoid. In those same periods, FDS EMG activity was synchronized with force fluctuations. We then constructed a physiologically-realistic computer simulation in which a muscle-tendon complex was set inside of a feedback-driven control loop. Surprisingly, the model sufficed to produce phase-dependent modulation of tremor similar to that observed in humans. Further, the gain of afferent feedback from muscle spindles was critical for appropriately amplifying and shaping this tremor. We suggest that the experimentally-induced tremor may represent the response of a viscoelastic muscle-tendon system to dynamic drive, and therefore does not fall into known categories of tremor generation, such as tremorogenic descending drive, stretch-reflex loop oscillations, motor unit behavior, or mechanical resonance. Our findings motivate future efforts to understand tremor from a perspective that considers neuromechanical coupling within the context of closed-loop control. The strategy of combining experimental recordings with physiologically-sound simulations will enable thorough exploration of neural and mechanical contributions to force control in health and disease. "}], "ArticleTitle": "The Dynamics of Voluntary Force Production in Afferented Muscle Influence Involuntary Tremor."}, "27582702": {"mesh": [], "AbstractText": [{"section": null, "text": "Stroke, resulting in focal structural damage, induces changes in brain function at both local and global levels. Following stroke, cerebral networks present structural, and functional reorganization to compensate for the dysfunctioning provoked by the lesion itself and its remote effects. As some recent studies underlined the role of the contralesional hemisphere during recovery, we studied its role in the reorganization of brain function of stroke patients using resting state fMRI and graph theory. We explored this reorganization using the \"hub disruption index\" (&#954;), a global index sensitive to the reorganization of nodes within the graph. For a given graph metric, &#954; of a subject corresponds to the slope of the linear regression model between the mean local network measures of a reference group, and the difference between that reference and the subject under study. In order to translate the use of &#954; in clinical context, a prerequisite to achieve meaningful results is to investigate the reliability of this index. In a preliminary part, we studied the reliability of &#954; by computing the intraclass correlation coefficient in a cohort of 100 subjects from the Human Connectome Project. Then, we measured intra-hemispheric &#954; index in the contralesional hemisphere of 20 subacute stroke patients compared to 20 age-matched healthy controls. Finally, due to the small number of patients, we tested the robustness of our results repeating the experiment 1000 times by bootstrapping on the Human Connectome Project database. Statistical analysis showed a significant reduction of &#954; for the contralesional hemisphere of right stroke patients compared to healthy controls. Similar results were observed for the right contralesional hemisphere of left stroke patients. We showed that &#954;, is more reliable than global graph metrics and more sensitive to detect differences between groups of patients as compared to healthy controls. Using new graph metrics as &#954; allows us to show that stroke induces a network-wide pattern of reorganization in the contralesional hemisphere whatever the side of the lesion. Graph modeling combined with measure of reorganization at the level of large-scale networks can become a useful tool in clinic. "}], "ArticleTitle": "The \"Hub Disruption Index,\" a Reliable Index Sensitive to the Brain Networks Reorganization. A Study of the Contralesional Hemisphere in Stroke."}, "27499740": {"mesh": [], "AbstractText": [{"section": null, "text": "The role of dendritic spiking mechanisms in neural processing is so far poorly understood. To investigate the role of calcium spikes in the functional properties of the single neuron and recurrent networks, we investigated a three compartment neuron model of the layer 5 pyramidal neuron with calcium dynamics in the distal compartment. By performing single neuron simulations with noisy synaptic input and occasional large coincident input at either just the distal compartment or at both somatic and distal compartments, we show that the presence of calcium spikes confers a substantial advantage for coincidence detection in the former case and a lesser advantage in the latter. We further show that the experimentally observed critical frequency phenomenon, in which action potentials triggered by stimuli near the soma above a certain frequency trigger a calcium spike at distal dendrites, leading to further somatic depolarization, is not exhibited by a neuron receiving realistically noisy synaptic input, and so is unlikely to be a necessary component of coincidence detection. We next investigate the effect of calcium spikes in propagation of spiking activities in a feed-forward network (FFN) embedded in a balanced recurrent network. The excitatory neurons in the network are again connected to either just the distal, or both somatic and distal compartments. With purely distal connectivity, activity propagation is stable and distinguishable for a large range of recurrent synaptic strengths if the feed-forward connections are sufficiently strong, but propagation does not occur in the absence of calcium spikes. When connections are made to both the somatic and the distal compartments, activity propagation is achieved for neurons with active calcium dynamics at a much smaller number of neurons per pool, compared to a network of passive neurons, but quickly becomes unstable as the strength of recurrent synapses increases. Activity propagation at higher scaling factors can be stabilized by increasing network inhibition or introducing short term depression in the excitatory synapses, but the signal to noise ratio remains low. Our results demonstrate that the interaction of synchrony with dendritic spiking mechanisms can have profound consequences for the dynamics on the single neuron and network level. "}], "ArticleTitle": "Effects of Calcium Spikes in the Layer 5 Pyramidal Neuron on Coincidence Detection and Activity Propagation."}, "27489541": {"mesh": [], "AbstractText": [{"section": null, "text": "Lately, research on computational models of emotion had been getting much attention due to their potential for understanding the mechanisms of emotions and their promising broad range of applications that potentially bridge the gap between human and machine interactions. We propose a new method for emotion classification that relies on features extracted from those active brain areas that are most likely related to emotions. To this end, we carry out the selection of spatially compact regions of interest that are computed using the brain neural activity reconstructed from Electroencephalography data. Throughout this study, we consider three representative feature extraction methods widely applied to emotion detection tasks, including Power spectral density, Wavelet, and Hjorth parameters. Further feature selection is carried out using principal component analysis. For validation purpose, these features are used to feed a support vector machine classifier that is trained under the leave-one-out cross-validation strategy. Obtained results on real affective data show that incorporation of the proposed training method in combination with the enhanced spatial resolution provided by the source estimation allows improving the performed accuracy of discrimination in most of the considered emotions, namely: dominance, valence, and liking. "}], "ArticleTitle": "Emotion Discrimination Using Spatially Compact Regions of Interest Extracted from Imaging EEG Activity."}, "27486396": {"mesh": [], "AbstractText": [{"section": null, "text": "How visual information is encoded in spikes of retinal ganglion cells (RGCs) is essential in visual neuroscience. In the present study, we investigated the coding properties of mouse RGCs with dual-peak patterns with respect to visual stimulus intervals. We first analyzed the response properties, and observed that the latencies and spike counts of the two response peaks in the dual-peak pattern exhibited systematic changes with the preceding light-OFF interval. We then applied linear discriminant analysis (LDA) to assess the relative contributions of response characteristics of both peaks in information coding regarding the preceding stimulus interval. It was found that for each peak, the discrimination results were far better than chance level based on either latency or spike count, and were further improved by using the combination of the two parameters. Furthermore, the best discrimination results were obtained when latencies and spike counts of both peaks were considered in combination. In addition, the correct rate for stimulation discrimination was higher when RGC population activity was considered as compare to single neuron's activity, and the correct rate was increased with the group size. These results suggest that rate coding, temporal coding, and population coding are all involved in encoding the different stimulus-interval patterns, and the two response peaks in the dual-peak pattern carry complementary information about stimulus interval. "}], "ArticleTitle": "Coding Properties of Mouse Retinal Ganglion Cells with Dual-Peak Patterns with Respect to Stimulus Intervals."}, "27458366": {"mesh": [], "AbstractText": [{"section": null, "text": "Estimation of emotions is an essential aspect in developing intelligent systems intended for crowded environments. However, emotion estimation in crowds remains a challenging problem due to the complexity in which human emotions are manifested and the capability of a system to perceive them in such conditions. This paper proposes a hierarchical Bayesian model to learn in unsupervised manner the behavior of individuals and of the crowd as a single entity, and explore the relation between behavior and emotions to infer emotional states. Information about the motion patterns of individuals are described using a self-organizing map, and a hierarchical Bayesian network builds probabilistic models to identify behaviors and infer the emotional state of individuals and the crowd. This model is trained and tested using data produced from simulated scenarios that resemble real-life environments. The conducted experiments tested the efficiency of our method to learn, detect and associate behaviors with emotional states yielding accuracy levels of 74% for individuals and 81% for the crowd, similar in performance with existing methods for pedestrian behavior detection but with novel concepts regarding the analysis of crowds. "}], "ArticleTitle": "A Hierarchical Bayesian Model for Crowd Emotions."}, "27445781": {"mesh": [], "AbstractText": [{"section": null, "text": "Local Field Potentials (LFPs) are population signals generated by complex spatiotemporal interaction of current sources and dipoles. Mathematical computations of LFPs allow the study of circuit functions and dysfunctions via simulations. This paper introduces LFPsim, a NEURON-based tool for computing population LFP activity and single neuron extracellular potentials. LFPsim was developed to be used on existing cable compartmental neuron and network models. Point source, line source, and RC based filter approximations can be used to compute extracellular activity. As a demonstration of efficient implementation, we showcase LFPs from mathematical models of electrotonically compact cerebellum granule neurons and morphologically complex neurons of the neocortical column. LFPsim reproduced neocortical LFP at 8, 32, and 56 Hz via current injection, in vitro post-synaptic N2a, N2b waves and in vivo T-C waves in cerebellum granular layer. LFPsim also includes a simulation of multi-electrode array of LFPs in network populations to aid computational inference between biophysical activity in neural networks and corresponding multi-unit activity resulting in extracellular and evoked LFP signals. "}], "ArticleTitle": "Computational Modeling of Single Neuron Extracellular Electric Potentials and Network Local Field Potentials using LFPsim."}, "27378900": {"mesh": [], "AbstractText": [{"section": null, "text": "Pitch is a perceptual correlate of periodicity. Sounds with distinct spectra can elicit the same pitch. Despite the importance of pitch perception, understanding the cellular mechanism of pitch perception is still a major challenge and a mechanistic model of pitch is lacking. A multi-stage neuronal network model is developed for pitch frequency estimation using biophysically-based, high-resolution coincidence detector neurons. The neuronal units respond only to highly coincident input among convergent auditory nerve fibers across frequency channels. Their selectivity for only very fast rising slopes of convergent input enables these slope-detectors to distinguish the most prominent coincidences in multi-peaked input time courses. Pitch can then be estimated from the first-order interspike intervals of the slope-detectors. The regular firing pattern of the slope-detector neurons are similar for sounds sharing the same pitch despite the distinct timbres. The decoded pitch strengths also correlate well with the salience of pitch perception as reported by human listeners. Therefore, our model can serve as a neural representation for pitch. Our model performs successfully in estimating the pitch of missing fundamental complexes and reproducing the pitch variation with respect to the frequency shift of inharmonic complexes. It also accounts for the phase sensitivity of pitch perception in the cases of Schroeder phase, alternating phase and random phase relationships. Moreover, our model can also be applied to stochastic sound stimuli, iterated-ripple-noise, and account for their multiple pitch perceptions. "}], "ArticleTitle": "A Neuronal Network Model for Pitch Selectivity and Representation."}, "27375470": {"mesh": [], "AbstractText": [{"section": null, "text": "Deep brain stimulation (DBS) leads with radially distributed electrodes have potential to improve clinical outcomes through more selective targeting of pathways and networks within the brain. However, increasing the number of electrodes on clinical DBS leads by replacing conventional cylindrical shell electrodes with radially distributed electrodes raises practical design and stimulation programming challenges. We used computational modeling to investigate: (1) how the number of radial electrodes impact the ability to steer, shift, and sculpt a region of neural activation (RoA), and (2) which RoA features are best used in combination with machine learning classifiers to predict programming settings to target a particular area near the lead. Stimulation configurations were modeled using 27 lead designs with one to nine radially distributed electrodes. The computational modeling framework consisted of a three-dimensional finite element tissue conductance model in combination with a multi-compartment biophysical axon model. For each lead design, two-dimensional threshold-dependent RoAs were calculated from the computational modeling results. The models showed more radial electrodes enabled finer resolution RoA steering; however, stimulation amplitude, and therefore spatial extent of the RoA, was limited by charge injection and charge storage capacity constraints due to the small electrode surface area for leads with more than four radially distributed electrodes. RoA shifting resolution was improved by the addition of radial electrodes when using uniform multi-cathode stimulation, but non-uniform multi-cathode stimulation produced equivalent or better resolution shifting without increasing the number of radial electrodes. Robust machine learning classification of 15 monopolar stimulation configurations was achieved using as few as three geometric features describing a RoA. The results of this study indicate that, for a clinical-scale DBS lead, more than four radial electrodes minimally improved in the ability to steer, shift, and sculpt axonal activation around a DBS lead and a simple feature set consisting of the RoA center of mass and orientation enabled robust machine learning classification. These results provide important design constraints for future development of high-density DBS arrays. "}], "ArticleTitle": "Model-Based Comparison of Deep Brain Stimulation Array Functionality with Varying Number of Radial Electrodes and Machine Learning Feature Sets."}, "27313527": {"mesh": [], "AbstractText": [{"section": null, "text": "It is widely accepted that the hippocampal place cells' spiking activity produces a cognitive map of space. However, many details of this representation's physiological mechanism remain unknown. For example, it is believed that the place cells exhibiting frequent coactivity form functionally interconnected groups-place cell assemblies-that drive readout neurons in the downstream networks. However, the sheer number of coactive combinations is extremely large, which implies that only a small fraction of them actually gives rise to cell assemblies. The physiological processes responsible for selecting the winning combinations are highly complex and are usually modeled via detailed synaptic and structural plasticity mechanisms. Here we propose an alternative approach that allows modeling the cell assembly network directly, based on a small number of phenomenological selection rules. We then demonstrate that the selected population of place cell assemblies correctly encodes the topology of the environment in biologically plausible time, and may serve as a schematic model of the hippocampal network. "}], "ArticleTitle": "A Topological Model of the Hippocampal Cell Assembly Network."}, "34025380": {"mesh": [], "AbstractText": [{"section": null, "text": "Neurological disorders dramatically impact patients of any age population, their families, and societies. Pediatrics are among vulnerable age populations who differently experience the devastating consequences of neurological conditions, such as attention-deficit hyperactivity disorders (ADHD), autism spectrum disorders (ASD), cerebral palsy, concussion, and epilepsy. System-level understanding of these neurological disorders, particularly from the brain networks' dynamic perspective, has led to the significant trend of recent scientific investigations. While a dramatic maturation in the network science application domain is evident, leading to a better understanding of neurological disorders, such rapid utilization for studying pediatric neurological disorders falls behind that of the adult population. Aside from the specific technological needs and constraints in studying neurological disorders in children, the concept of development introduces uncertainty and further complexity topping the existing neurologically driven processes caused by disorders. To unravel these complexities, indebted to the availability of high-dimensional data and computing capabilities, approaches based on machine learning have rapidly emerged a new trend to understand pathways better, accurately diagnose, and better manage the disorders. Deep learning has recently gained an ever-increasing role in the era of health and medical investigations. Thanks to its relatively more minor dependency on feature exploration and engineering, deep learning may overcome the challenges mentioned earlier in studying neurological disorders in children. The current scoping review aims to explore challenges concerning pediatric brain development studies under the constraints of neurological disorders and offer an insight into the potential role of deep learning methodology on such a task with varying and uncertain nature. Along with pinpointing recent advancements, possible research directions are highlighted where deep learning approaches can assist in computationally targeting neurological disorder-related processes and translating them into windows of opportunities for interventions in diagnosis, treatment, and management of neurological disorders in children."}], "ArticleTitle": "Can Deep Learning Hit a Moving Target? A Scoping Review of Its Role to Study Neurological Disorders in Children."}, "33897397": {"mesh": [], "AbstractText": [{"section": null, "text": "Over the last few decades, electroencephalogram (EEG) has become one of the most vital tools used by physicians to diagnose several neurological disorders of the human brain and, in particular, to detect seizures. Because of its peculiar nature, the consequent impact of epileptic seizures on the quality of life of patients made the precise diagnosis of epilepsy extremely essential. Therefore, this article proposes a novel deep-learning approach for detecting seizures in pediatric patients based on the classification of raw multichannel EEG signal recordings that are minimally pre-processed. The new approach takes advantage of the automatic feature learning capabilities of a two-dimensional deep convolution autoencoder (2D-DCAE) linked to a neural network-based classifier to form a unified system that is trained in a supervised way to achieve the best classification accuracy between the ictal and interictal brain state signals. For testing and evaluating our approach, two models were designed and assessed using three different EEG data segment lengths and a 10-fold cross-validation scheme. Based on five evaluation metrics, the best performing model was a supervised deep convolutional autoencoder (SDCAE) model that uses a bidirectional long short-term memory (Bi-LSTM) - based classifier, and EEG segment length of 4 s. Using the public dataset collected from the Children's Hospital Boston (CHB) and the Massachusetts Institute of Technology (MIT), this model has obtained 98.79 &#177; 0.53% accuracy, 98.72 &#177; 0.77% sensitivity, 98.86 &#177; 0.53% specificity, 98.86 &#177; 0.53% precision, and an F1-score of 98.79 &#177; 0.53%, respectively. Based on these results, our new approach was able to present one of the most effective seizure detection methods compared to other existing state-of-the-art methods applied to the same dataset."}], "ArticleTitle": "A Deep Learning Approach for Automatic Seizure Detection in Children With Epilepsy."}, "30617922": {"mesh": [], "AbstractText": [{"section": null, "text": "The ongoing acquisition of large and multifaceted data sets in neuroscience requires new mathematical tools for quantitatively grounding these experimental findings. Since 2015, the International Conference on Mathematical Neuroscience (ICMNS) has provided a forum for researchers to discuss current mathematical innovations emerging in neuroscience. This special issue assembles current research and tutorials that were presented at the 2017 ICMNS held in Boulder, Colorado from May 30 to June 2. Topics discussed at the meeting include correlation analysis of network activity, information theory for plastic synapses, combinatorics for attractor neural networks, and novel data assimilation methods for neuroscience-all of which are represented in this special issue."}], "ArticleTitle": "Special Issue from the 2017 International Conference on Mathematical Neuroscience."}, "32886221": {"mesh": [], "AbstractText": [{"section": null, "text": "Morris-Lecar model is arguably the simplest dynamical model that retains both the slow-fast geometry of excitable phase portraits and the physiological interpretation of a conductance-based model. We augment this model with one slow inward current to capture the additional property of bistability between a resting state and a spiking limit cycle for a range of input current. The resulting dynamical system is a core structure for many dynamical phenomena such as slow spiking and bursting. We show how the proposed model combines physiological interpretation and mathematical tractability and we discuss the benefits of the proposed approach with respect to alternative models in the literature."}], "ArticleTitle": "The geometry of rest-spike bistability."}, "32405723": {"mesh": [], "AbstractText": [{"section": null, "text": "Networks of neurons in the cerebral cortex exhibit a balance between excitation (positive input current) and inhibition (negative input current). Balanced network theory provides a parsimonious mathematical model of this excitatory-inhibitory balance using randomly connected networks of model neurons in which balance is realized as a stable fixed point of network dynamics in the limit of large network size. Balanced network theory reproduces many salient features of cortical network dynamics such as asynchronous-irregular spiking activity. Early studies of balanced networks did not account for the spatial topology of cortical networks. Later works introduced spatial connectivity structure, but were restricted to networks with translationally invariant connectivity structure in which connection probability depends on distance alone and boundaries are assumed to be periodic. Spatial connectivity structure in cortical network does not always satisfy these assumptions. We use the mathematical theory of integral equations to extend the mean-field theory of balanced networks to account for more general dependence of connection probability on the spatial location of pre- and postsynaptic neurons. We compare our mathematical derivations to simulations of large networks of recurrently connected spiking neuron models."}], "ArticleTitle": "Spatially extended balanced networks without translationally invariant connectivity."}, "33259016": {"mesh": [], "AbstractText": [{"section": null, "text": "We provide theoretical conditions guaranteeing that a self-organizing map efficiently develops representations of the input space. The study relies on a neural field model of spatiotemporal activity in area 3b of the primary somatosensory cortex. We rely on Lyapunov's theory for neural fields to derive theoretical conditions for stability. We verify the theoretical conditions by numerical experiments. The analysis highlights the key role played by the balance between excitation and inhibition of lateral synaptic coupling and the strength of synaptic gains in the formation and maintenance of self-organizing maps."}], "ArticleTitle": "Stability analysis of a neural field self-organizing map."}, "32002707": {"mesh": [], "AbstractText": [{"section": null, "text": "The modeling of neural fields in the visual cortex involves geometrical structures which describe in mathematical formalism the functional architecture of this cortical area. The case of contour detection and orientation tuning has been extensively studied and has become a paradigm for the mathematical analysis of image processing by the brain. Ten years ago an attempt was made to extend these models by replacing orientation (an angle) with a second-order tensor built from the gradient of the image intensity, and it was named the structure tensor. This assumption does not follow from biological observations (experimental evidence is still lacking) but from the idea that the effectiveness of texture processing with the structure tensor in computer vision may well be exploited by the brain itself. The drawback is that in this case the geometry is not Euclidean but hyperbolic instead, which complicates the analysis substantially. The purpose of this review is to present the methodology that was developed in a series of papers to investigate this quite unusual problem, specifically from the point of view of tuning and pattern formation. These methods, which rely on bifurcation theory with symmetry in the hyperbolic context, might be of interest for the modeling of other features such as color vision or other brain functions."}], "ArticleTitle": "The hyperbolic model for edge and texture detection in the primary visual cortex."}, "33175257": {"mesh": [], "AbstractText": [{"section": null, "text": "How much visual information about the retinal images can be extracted from the different layers of the visual pathway? This question depends on the complexity of the visual input, the set of transforms applied to this multivariate input, and the noise of the sensors in the considered layer. Separate subsystems (e.g. opponent channels, spatial filters, nonlinearities of the texture sensors) have been suggested to be organized for optimal information transmission. However, the efficiency of these different layers has not been measured when they operate together on colorimetrically calibrated natural images and using multivariate information-theoretic units over the joint spatio-chromatic array of responses.In this work, we present a statistical tool to address this question in an appropriate (multivariate) way. Specifically, we propose an empirical estimate of the information transmitted by the system based on a recent Gaussianization technique. The total correlation measured using the proposed estimator is consistent with predictions based on the analytical Jacobian of a standard spatio-chromatic model of the retina-cortex pathway. If the noise at certain representation is proportional to the dynamic range of the response, and one assumes sensors of equivalent noise level, then transmitted information shows the following trends: (1)&#160;progressively deeper representations are better in terms of the amount of captured information, (2)&#160;the transmitted information up to the cortical representation follows the probability of natural scenes over the chromatic and achromatic dimensions of the stimulus space, (3)&#160;the contribution of spatial transforms to capture visual information is substantially greater than the contribution of chromatic transforms, and (4)&#160;nonlinearities of the responses contribute substantially to the transmitted information but less than the linear transforms."}], "ArticleTitle": "Spatio-chromatic information available from different neural layers via Gaussianization."}, "32915327": {"mesh": [], "AbstractText": [{"section": null, "text": "Neural populations with strong excitatory recurrent connections can support bistable states in their mean firing rates. Multiple fixed points in a network of such bistable units can be used to model memory retrieval and pattern separation. The stability of fixed points may change on a slower timescale than that of the dynamics due to short-term synaptic depression, leading to transitions between quasi-stable point attractor states in a sequence that depends on the history of stimuli. To better understand these behaviors, we study a minimal model, which characterizes multiple fixed points and transitions between them in response to stimuli with diverse time- and amplitude-dependencies. The interplay between the fast dynamics of firing rate and synaptic responses and the slower timescale of synaptic depression makes the neural activity sensitive to the amplitude and duration of square-pulse stimuli in a nontrivial, history-dependent manner. Weak cross-couplings further deform the basins of attraction for different fixed points into intricate shapes. We find that while short-term synaptic depression can reduce the total number of stable fixed points in a network, it tends to strongly increase the number of fixed points visited upon repetitions of fixed stimuli. Our analysis provides a natural explanation for the system's rich responses to stimuli of different durations and amplitudes while demonstrating the encoding capability of bistable neural populations for dynamical features of incoming stimuli."}], "ArticleTitle": "Attractor-state itinerancy in neural circuits with synaptic depression."}, "33201339": {"mesh": [], "AbstractText": [{"section": null, "text": "Neural oscillations, including rhythms in the beta1 band (12-20&#160;Hz), are important in various cognitive functions. Often neural networks receive rhythmic input at frequencies different from their natural frequency, but very little is known about how such input affects the network's behavior. We use a simplified, yet biophysical, model of a beta1 rhythm that occurs in the parietal cortex, in order to study its response to oscillatory inputs. We demonstrate that a cell has the ability to respond at the same time to two periodic stimuli of unrelated frequencies, firing in phase with one, but with a mean firing rate equal to that of the other. We show that this is a very general phenomenon, independent of the model used. We next show numerically that the behavior of a different cell, which is modeled as a high-dimensional dynamical system, can be described in a surprisingly simple way, owing to a reset that occurs in the state space when the cell fires. The interaction of the two cells leads to novel combinations of properties for neural dynamics, such as mode-locking to an input without phase-locking to it."}], "ArticleTitle": "Interactions of multiple rhythms in a biophysical network of neurons."}, "32462281": {"mesh": [], "AbstractText": [{"section": null, "text": "Many biological and neural systems can be seen as networks of interacting periodic processes. Importantly, their functionality, i.e., whether these networks can perform their function or not, depends on the emerging collective dynamics of the network. Synchrony of oscillations is one of the most prominent examples of such collective behavior and has been associated both with function and dysfunction. Understanding how network structure and interactions, as well as the microscopic properties of individual units, shape the emerging collective dynamics is critical to find factors that lead to malfunction. However, many biological systems such as the brain consist of a large number of dynamical units. Hence, their analysis has either relied on simplified heuristic models on a coarse scale, or the analysis comes at a huge computational cost. Here we review recently introduced approaches, known as the Ott-Antonsen and Watanabe-Strogatz reductions, allowing one to simplify the analysis by bridging small and large scales. Thus, reduced model equations are obtained that exactly describe the collective dynamics for each subpopulation in the oscillator network via few collective variables only. The resulting equations are next-generation models: Rather than being heuristic, they exactly link microscopic and macroscopic descriptions and therefore accurately capture microscopic properties of the underlying system. At the same time, they are sufficiently simple to analyze without great computational effort. In the last decade, these reduction methods have become instrumental in understanding how network structure and interactions shape the collective dynamics and the emergence of synchrony. We review this progress based on concrete examples and outline possible limitations. Finally, we discuss how linking the reduced models with experimental data can guide the way towards the development of new treatment approaches, for example, for neurological disease."}], "ArticleTitle": "Understanding the dynamics of biological and neural oscillator networks through exact mean-field reductions: a review."}, "30094571": {"mesh": [], "AbstractText": [{"section": null, "text": "This tutorial illustrates the use of data assimilation algorithms to estimate unobserved variables and unknown parameters of conductance-based neuronal models. Modern data assimilation (DA) techniques are widely used in climate science and weather prediction, but have only recently begun to be applied in neuroscience. The two main classes of DA techniques are sequential methods and variational methods. We provide computer code implementing basic versions of a method from each class, the Unscented Kalman Filter and 4D-Var, and demonstrate how to use these algorithms to infer several parameters of the Morris-Lecar model from a single voltage trace. Depending on parameters, the Morris-Lecar model exhibits qualitatively different types of neuronal excitability due to changes in the underlying bifurcation structure. We show that when presented with voltage traces from each of the various excitability regimes, the DA methods can identify parameter sets that produce the correct bifurcation structure even with initial parameter guesses that correspond to a different excitability regime. This demonstrates the ability of DA techniques to perform nonlinear state and parameter estimation and introduces the geometric structure of inferred models as a novel qualitative measure of estimation success. We conclude by discussing extensions of these DA algorithms that have appeared in the neuroscience literature."}], "ArticleTitle": "Data Assimilation Methods for Neuronal State and Parameter Estimation."}, "33095343": {"mesh": [], "AbstractText": [{"section": null, "text": "In this paper, we address the problem of the use of a human visual system (HVS) model to improve watermark invisibility. We propose a new color watermarking algorithm based on the minimization of the perception of color differences. This algorithm is based on a psychovisual model of the dynamics of cone photoreceptors. We used this model to determine the discrimination power of the human for a particular color and thus the best strategy to modify color pixels. Results were obtained on a color version of the lattice quantization index modulation (LQIM) method and showed improvements on psychovisual invisibility and robustness against several image distortions."}], "ArticleTitle": "A new blind color watermarking based on a psychovisual model."}, "30136005": {"mesh": [], "AbstractText": [{"section": null, "text": "We review recent work on the theory and applications of stochastic hybrid systems in cellular neuroscience. A stochastic hybrid system or piecewise deterministic Markov process involves the coupling between a piecewise deterministic differential equation and a time-homogeneous Markov chain on some discrete space. The latter typically represents some random switching process. We begin by summarizing the basic theory of stochastic hybrid systems, including various approximation schemes in the fast switching (weak noise) limit. In subsequent sections, we consider various applications of stochastic hybrid systems, including stochastic ion channels and membrane voltage fluctuations, stochastic gap junctions and diffusion in randomly switching environments, and intracellular transport in axons and dendrites. Finally, we describe recent work on phase reduction methods for stochastic hybrid limit cycle oscillators."}], "ArticleTitle": "Stochastic Hybrid Systems in Cellular Neuroscience."}, "31270706": {"mesh": [], "AbstractText": [{"section": null, "text": "The canonical computational model for the cognitive process underlying two-alternative forced-choice decision making is the so-called drift-diffusion model (DDM). In this model, a decision variable keeps track of the integrated difference in sensory evidence for two competing alternatives. Here I extend the notion of a drift-diffusion process to multiple alternatives. The competition between n alternatives takes place in a linear subspace of [Formula: see text] dimensions; that is, there are [Formula: see text] decision variables, which are coupled through correlated noise sources. I derive the multiple-alternative DDM starting from a system of coupled, linear firing rate equations. I also show that a Bayesian sequential probability ratio test for multiple alternatives is, in fact, equivalent to these same linear DDMs, but with time-varying thresholds. If the original neuronal system is nonlinear, one can once again derive a model describing a lower-dimensional diffusion process. The dynamics of the nonlinear DDM can be recast as the motion of a particle on a potential, the general form of which is given analytically for an arbitrary number of alternatives."}], "ArticleTitle": "Drift-diffusion models for multiple-alternative forced-choice decision making."}, "32253526": {"mesh": [], "AbstractText": [{"section": null, "text": "Coarse-graining microscopic models of biological neural networks to obtain mesoscopic models of neural activities is an essential step towards multi-scale models of the brain. Here, we extend a recent theory for mesoscopic population dynamics with static synapses to the case of dynamic synapses exhibiting short-term plasticity (STP). The extended theory offers an approximate mean-field dynamics for the synaptic input currents arising from populations of spiking neurons and synapses undergoing Tsodyks-Markram STP. The approximate mean-field dynamics accounts for both finite number of synapses and correlation between the two synaptic variables of the model (utilization and available resources) and its numerical implementation is simple. Comparisons with Monte Carlo simulations of the microscopic model show that in both feedforward and recurrent networks, the mesoscopic mean-field model accurately reproduces the first- and second-order statistics of the total synaptic input into a postsynaptic neuron and accounts for stochastic switches between Up and Down states and for population spikes. The extended mesoscopic population theory of spiking neural networks with STP may be useful for a systematic reduction of detailed biophysical models of cortical microcircuits to numerically efficient and mathematically tractable mean-field models."}], "ArticleTitle": "Mesoscopic population equations for spiking neural networks with synaptic short-term plasticity."}, "32542516": {"mesh": [], "AbstractText": [{"section": null, "text": "Binocular rivalry occurs when the two eyes are presented with incompatible stimuli and perception alternates between these two stimuli. This phenomenon has been investigated in two types of experiments: (1) Traditional experiments where the stimulus is fixed, (2) eye-swap experiments in which the stimulus periodically swaps between eyes many times per second (Logothetis et al. in Nature 380(6575):621-624, 1996). In spite of the rapid swapping between eyes, perception can be stable for many seconds with specific stimulus parameter configurations. Wilson introduced a two-stage, hierarchical model to explain both types of experiments (Wilson in Proc. Natl. Acad. Sci. 100(24):14499-14503, 2003). Wilson's model and other rivalry models have been only studied with bifurcation analysis for fixed inputs and different types of dynamical behavior that can occur with periodically forcing inputs have not been investigated. Here we report (1) a more complete description of the complex dynamics in the unforced Wilson model, (2) a bifurcation analysis with periodic forcing. Previously, bifurcation analysis of the Wilson model with fixed inputs has revealed three main types of dynamical behaviors: Winner-takes-all (WTA), Rivalry oscillations (RIV), Simultaneous activity (SIM). Our results have revealed richer dynamics including mixed-mode oscillations (MMOs) and a period-doubling cascade, which corresponds to low-amplitude WTA (LAWTA) oscillations. On the other hand, studying rivalry models with numerical continuation shows that periodic forcing with high frequency (e.g. 18&#160;Hz, known as flicker) modulates the three main types of behaviors that occur with fixed inputs with forcing frequency (WTA-Mod, RIV-Mod, SIM-Mod). However, dynamical behavior will be different with low frequency periodic forcing (around 1.5&#160;Hz, so-called swap). In addition to WTA-Mod and SIM-Mod, cycle skipping, multi-cycle skipping and chaotic dynamics are found. This research provides a framework for either assessing binocular rivalry models to check consistency with empirical results, or for better understanding neural dynamics and mechanisms necessary to implement a minimal binocular rivalry model."}], "ArticleTitle": "Methods to assess binocular rivalry with periodic stimuli."}, "32399688": {"mesh": [], "AbstractText": [{"section": null, "text": "This is the first half of a two-part paper dealing with the geometry of color perception. Here we analyze in detail the seminal 1974 work by H.L. Resnikoff, who showed that there are only two possible geometric structures and Riemannian metrics on the perceived color space [Formula: see text] compatible with the set of Schr&#246;dinger's axioms completed with the hypothesis of homogeneity. We recast Resnikoff's model into a more modern colorimetric setting, provide a much simpler proof of the main result of the original paper, and motivate the need of psychophysical experiments to confute or confirm the linearity of background transformations, which act transitively on [Formula: see text]. Finally, we show that the Riemannian metrics singled out by Resnikoff through an axiom on invariance under background transformations are not compatible with the crispening effect, thus motivating the need of further research about perceptual color metrics."}], "ArticleTitle": "Geometry of color perception. Part 1: structures and metrics of a homogeneous color space."}, "31147800": {"mesh": [], "AbstractText": [{"section": null, "text": "To establish and exploit novel biomarkers of demyelinating diseases requires a mechanistic understanding of axonal propagation. Here, we present a novel computational framework called the stochastic spike-diffuse-spike (SSDS) model for assessing the effects of demyelination on axonal transmission. It models transmission through nodal and internodal compartments with two types of operations: a stochastic integrate-and-fire operation captures nodal excitability and a linear filtering operation describes internodal propagation. The effects of demyelinated segments on the probability of transmission, transmission delay and spike time jitter are explored. We argue that demyelination-induced impedance mismatch prevents propagation mostly when the action potential leaves a demyelinated region, not when it enters a demyelinated region. In addition, we model sodium channel remodeling as a homeostatic control of nodal excitability. We find that the effects of mild demyelination on transmission probability and delay can be largely counterbalanced by an increase in excitability at the nodes surrounding the demyelination. The spike timing jitter, however, reflects the level of demyelination whether excitability is fixed or is allowed to change in compensation. This jitter can accumulate over long axons and leads to a broadening of the compound action potential, linking microscopic defects to a mesoscopic observable. Our findings articulate why action potential jitter and compound action potential dispersion can serve as potential markers of weak and sporadic demyelination."}], "ArticleTitle": "Linking demyelination to compound action potential dispersion with a spike-diffuse-spike approach."}, "29340803": {"mesh": [], "AbstractText": [{"section": null, "text": "The Hopfield recurrent neural network is a classical auto-associative model of memory, in which collections of symmetrically coupled McCulloch-Pitts binary neurons interact to perform emergent computation. Although previous researchers have explored the potential of this network to solve combinatorial optimization problems or store reoccurring activity patterns as attractors of its deterministic dynamics, a basic open problem is to design a family of Hopfield networks with a number of noise-tolerant memories that grows exponentially with neural population size. Here, we discover such networks by minimizing probability flow, a recently proposed objective for estimating parameters in discrete maximum entropy models. By descending the gradient of the convex probability flow, our networks adapt synaptic weights to achieve robust exponential storage, even when presented with vanishingly small numbers of training patterns. In addition to providing a new set of low-density error-correcting codes that achieve Shannon's noisy channel bound, these networks also efficiently solve a variant of the hidden clique problem in computer science, opening new avenues for real-world applications of computational models originating from biology."}], "ArticleTitle": "Robust Exponential Memory in Hopfield Networks."}, "31993756": {"mesh": [], "AbstractText": [{"section": null, "text": "Neurons are biological cells with uniquely complex dendritic morphologies that are not present in other cell types. Electrical signals in a neuron with branching dendrites can be studied by cable theory which provides a general mathematical modelling framework of spatio-temporal voltage dynamics. Typically such models need to be solved numerically unless the cell membrane is modelled either by passive or quasi-active dynamics, in which cases analytical solutions can be reduced to calculation of the Green's function describing the fundamental input-output relationship in a given morphology. Such analytically tractable models often assume individual dendritic segments to be cylinders. However, it is known that dendritic segments in many types of neurons taper, i.e. their radii decline from proximal to distal ends. Here we consider a generalised form of cable theory which takes into account both branching and tapering structures of dendritic trees. We demonstrate that analytical solutions can be found in compact algebraic forms in an arbitrary branching neuron with a class of tapering dendrites studied earlier in the context of single neuronal cables by Poznanski (Bull. Math. Biol. 53(3):457-467, 1991). We apply this extended framework to a number of simplified neuronal models and contrast their output dynamics in the presence of tapering versus cylindrical segments."}], "ArticleTitle": "Exact solutions to cable equations in branching neurons with tapering dendrites."}, "32902776": {"mesh": [], "AbstractText": [{"section": null, "text": "Inspired by the pioneer work of H.L. Resnikoff, which is described in full detail in the first part of this two-part paper, we give a quantum description of the space [Formula: see text] of perceived colors. We show that [Formula: see text] is the effect space of a rebit, a real quantum qubit, whose state space is isometric to Klein's hyperbolic disk. This chromatic state space of perceived colors can be represented as a Bloch disk of real dimension 2 that coincides with Hering's disk given by the color opponency mechanism. Attributes of perceived colors, hue and saturation, are defined in terms of Von Neumann entropy."}], "ArticleTitle": "Geometry of color perception. Part 2: perceived colors from real quantum states and Hering's rebit."}, "31152270": {"mesh": [], "AbstractText": [{"section": "BACKGROUND", "text": "Trigeminal neuralgia (TN) is a severe neuropathic pain, which has an electric shock-like characteristic. There are some common treatments for this pain such as medicine, microvascular decompression or radio frequency. In this regard, transcranial direct current stimulation (tDCS) is another therapeutic method to reduce pain, which has been recently attracting the therapists' attention. The positive effect of tDCS on TN was shown in many previous studies. However, the mechanism of the tDCS effect has remained unclear."}, {"section": "OBJECTIVE", "text": "This study aims to model the neuronal behavior of the main known regions of the brain participating in TN pathways to study the effect of transcranial direct current stimulation."}, {"section": "METHOD", "text": "The proposed model consists of several blocks: (1)&#160;trigeminal nerve, (2)&#160;trigeminal ganglion, (3)&#160;PAG (periaqueductal gray in the brainstem), (4)&#160;thalamus, (5)&#160;motor cortex (M1) and (6)&#160;somatosensory cortex (S1). Each of these components is represented by a modified Hodgkin-Huxley (HH) model. The modification of the HH model was done based on some neurological facts of pain sodium channels. The input of the model involves any stimuli to the 'trigeminal nerve,' which cause the pain, and the output is the activity of the somatosensory cortex. An external current, which is considered as an electrical current, was applied to the motor cortex block of the model."}, {"section": "RESULT", "text": "The results showed that by decreasing the conductivity of the slow sodium channels (pain channels) and applying tDCS over the M1, the activity of the somatosensory cortex would be reduced. This reduction can cause pain relief."}, {"section": "CONCLUSION", "text": "The proposed model provided some possible suggestions about the relationship between the effects of tDCS and associated components in TN, and also the relationship between the pain measurement index, somatosensory cortex activity, and the strength of tDCS."}], "ArticleTitle": "A modified Hodgkin-Huxley model to show the effect of motor cortex stimulation on the trigeminal neuralgia network."}, "31350644": {"mesh": [], "AbstractText": [{"section": null, "text": "The emergent activity of biological systems can often be represented as low-dimensional, Langevin-type stochastic differential equations. In certain systems, however, large and abrupt events occur and violate the assumptions of this approach. We address this situation here by providing a novel method that reconstructs a jump-diffusion stochastic process based solely on the statistics of the original data. Our method assumes that these data are stationary, that diffusive noise is additive, and that jumps are Poisson. We use threshold-crossing of the increments to detect jumps in the time series. This is followed by an iterative scheme that compensates for the presence of diffusive fluctuations that are falsely detected as jumps. Our approach is based on probabilistic calculations associated with these fluctuations and on the use of the Fokker-Planck and the differential Chapman-Kolmogorov equations. After some validation cases, we apply this method to recordings of membrane noise in pyramidal neurons of the electrosensory lateral line lobe of weakly electric fish. These recordings display large, jump-like depolarization events that occur at random times, the biophysics of which is unknown. We find that some pyramidal cells increase their jump rate and noise intensity as the membrane potential approaches spike threshold, while their drift function and jump amplitude distribution remain unchanged. As our method is fully data-driven, it provides a valuable means to further investigate the functional role of these jump-like events without relying on unconstrained biophysical models."}], "ArticleTitle": "Data-driven inference for stationary jump-diffusion processes with application to membrane voltage fluctuations in pyramidal neurons."}, "29399710": {"mesh": [], "AbstractText": [{"section": null, "text": "Understanding the neural field activity for realistic living systems is a challenging task in contemporary neuroscience. Neural fields have been studied and developed theoretically and numerically with considerable success over the past four decades. However, to make effective use of such models, we need to identify their constituents in practical systems. This includes the determination of model parameters and in particular the reconstruction of the underlying effective connectivity in biological tissues.In this work, we provide an integral equation approach to the reconstruction of the neural connectivity in the case where the neural activity is governed by a delay neural field equation. As preparation, we study the solution of the direct problem based on the Banach fixed-point theorem. Then we reformulate the inverse problem into a family of integral equations of the first kind. This equation will be vector valued when several neural activity trajectories are taken as input for the inverse problem. We employ spectral regularization techniques for its stable solution. A sensitivity analysis of the regularized kernel reconstruction with respect to the input signal u is carried out, investigating the Fr&#233;chet differentiability of the kernel with respect to the signal. Finally, we use numerical examples to show the feasibility of the approach for kernel reconstruction, including numerical sensitivity tests, which show that the integral equation approach is a very stable and promising approach for practical computational neuroscience."}], "ArticleTitle": "Kernel Reconstruction for Delayed Neural Field Equations."}, "32232686": {"mesh": [], "AbstractText": [{"section": null, "text": "Essential tremor manifests predominantly as a tremor of the upper limbs. One therapy option is high-frequency deep brain stimulation, which continuously delivers electrical stimulation to the ventral intermediate nucleus of the thalamus at about 130&#160;Hz. Constant stimulation can lead to side effects, it is therefore desirable to find ways to stimulate less while maintaining clinical efficacy. One strategy, phase-locked deep brain stimulation, consists of stimulating according to the phase of the tremor. To advance methods to optimise deep brain stimulation while providing insights into tremor circuits, we ask the question: can the effects of phase-locked stimulation be accounted for by a canonical Wilson-Cowan model? We first analyse patient data, and identify in half of the datasets significant dependence of the effects of stimulation on the phase at which stimulation is provided. The full nonlinear Wilson-Cowan model is fitted to datasets identified as statistically significant, and we show that in each case the model can fit to the dynamics of patient tremor as well as to the phase response curve. The vast majority of top fits are stable foci. The model provides satisfactory prediction of how patient tremor will react to phase-locked stimulation by predicting patient amplitude response curves although they were not explicitly fitted. We also approximate response curves of the significant datasets by providing analytical results for the linearisation of a stable focus model, a simplification of the Wilson-Cowan model in the stable focus regime. We report that the nonlinear Wilson-Cowan model is able to describe response to stimulation more precisely than the linearisation."}], "ArticleTitle": "Phase-dependence of response curves to deep brain stimulation and their relationship: from essential tremor patient data to a Wilson-Cowan model."}, "32728818": {"mesh": [], "AbstractText": [{"section": null, "text": "In this paper, we present a novel model of the primary visual cortex (V1) based on orientation, frequency, and phase selective behavior of V1 simple cells. We start from the first-level mechanisms of visual perception, receptive profiles. The model interprets V1 as a fiber bundle over the two-dimensional retinal plane by introducing orientation, frequency, and phase as intrinsic variables. Each receptive profile on the fiber is mathematically interpreted as rotated, frequency modulated, and phase shifted Gabor function. We start from the Gabor function and show that it induces in a natural way the model geometry and the associated horizontal connectivity modeling of the neural connectivity patterns in V1. We provide an image enhancement algorithm employing the model framework. The algorithm is capable of exploiting not only orientation but also frequency and phase information existing intrinsically in a two-dimensional input image. We provide the experimental results corresponding to the enhancement algorithm."}], "ArticleTitle": "A sub-Riemannian model of the visual cortex with frequency and phase."}, "32809093": {"mesh": [], "AbstractText": [{"section": null, "text": "Coding for visual stimuli in the ventral stream is known to be invariant to object identity preserving nuisance transformations. Indeed, much recent theoretical and experimental work suggests that the main challenge for the visual cortex is to build up such nuisance invariant representations. Recently, artificial convolutional networks have succeeded in both learning such invariant properties and, surprisingly, predicting cortical responses in macaque and mouse visual cortex with unprecedented accuracy. However, some of the key ingredients that enable such success-supervised learning and the backpropagation algorithm-are neurally implausible. This makes it difficult to relate advances in understanding convolutional networks to the brain. In contrast, many of the existing neurally plausible theories of invariant representations in the brain involve unsupervised learning, and have been strongly tied to specific plasticity rules. To close this gap, we study an instantiation of simple-complex cell model and show, for a broad class of unsupervised learning rules (including Hebbian learning), that we can learn object representations that are invariant to nuisance transformations belonging to a finite orthogonal group. These findings may have implications for developing neurally plausible theories and models of how the visual cortex or artificial neural networks build selectivity for discriminating objects and invariance to real-world nuisance transformations."}], "ArticleTitle": "Neurally plausible mechanisms for learning selective and invariant representations."}, "26728012": {"mesh": [], "AbstractText": [{"section": null, "text": "In 1972-1973 Wilson and Cowan introduced a mathematical model of the population dynamics of synaptically coupled excitatory and inhibitory neurons in the neocortex. The model dealt only with the mean numbers of activated and quiescent excitatory and inhibitory neurons, and said nothing about fluctuations and correlations of such activity. However, in 1997 Ohira and Cowan, and then in 2007-2009 Buice and Cowan introduced Markov models of such activity that included fluctuation and correlation effects. Here we show how both models can be used to provide a quantitative account of the population dynamics of neocortical activity.We first describe how the Markov models account for many recent measurements of the resting or spontaneous activity of the neocortex. In particular we show that the power spectrum of large-scale neocortical activity has a Brownian motion baseline, and that the statistical structure of the random bursts of spiking activity found near the resting state indicates that such a state can be represented as a percolation process on a random graph, called directed percolation.Other data indicate that resting cortex exhibits pair correlations between neighboring populations of cells, the amplitudes of which decay slowly with distance, whereas stimulated cortex exhibits pair correlations which decay rapidly with distance. Here we show how the Markov model can account for the behavior of the pair correlations.Finally we show how the 1972-1973 Wilson-Cowan equations can account for recent data which indicates that there are at least two distinct modes of cortical responses to stimuli. In mode 1 a low intensity stimulus triggers a wave that propagates at a velocity of about 0.3 m/s, with an amplitude that decays exponentially. In mode 2 a high intensity stimulus triggers a larger response that remains local and does not propagate to neighboring regions. "}], "ArticleTitle": "Wilson-Cowan Equations for Neocortical Dynamics."}, "32052209": {"mesh": [], "AbstractText": [{"section": null, "text": "The fruit fly's natural visual environment is often characterized by light intensities ranging across several orders of magnitude and by rapidly varying contrast across space and time. Fruit fly photoreceptors robustly transduce and, in conjunction with amacrine cells, process visual scenes and provide the resulting signal to downstream targets. Here, we model the first step of visual processing in the photoreceptor-amacrine cell layer. We propose a novel divisive normalization processor (DNP) for modeling the computation taking place in the photoreceptor-amacrine cell layer. The DNP explicitly models the photoreceptor feedforward and temporal feedback processing paths and the spatio-temporal feedback path of the amacrine cells. We then formally characterize the contrast gain control of the DNP and provide sparse identification algorithms that can efficiently identify each the feedforward and feedback DNP components. The algorithms presented here are the first demonstration of tractable and robust identification of the components of a divisive normalization processor. The sparse identification algorithms can be readily employed in experimental settings, and their effectiveness is demonstrated with several examples."}], "ArticleTitle": "Sparse identification of contrast gain control in the fruit fly photoreceptor and amacrine cell layer."}, "29767380": {"mesh": [], "AbstractText": [{"section": null, "text": "The theory of attractor neural networks has been influential in our understanding of the neural processes underlying spatial, declarative, and episodic memory. Many theoretical studies focus on the inherent properties of an attractor, such as its structure and capacity. Relatively little is known about how an attractor neural network responds to external inputs, which often carry conflicting information about a stimulus. In this paper we analyze the behavior of an attractor neural network driven by two conflicting external inputs. Our focus is on analyzing the emergent properties of the megamap model, a quasi-continuous attractor network in which place cells are flexibly recombined to represent a large spatial environment. In this model, the system shows a sharp transition from the winner-take-all mode, which is characteristic of standard continuous attractor neural networks, to a combinatorial mode in which the equilibrium activity pattern combines embedded attractor states in response to conflicting external inputs. We derive a numerical test for determining the operational mode of the system a priori. We then derive a linear transformation from the full megamap model with thousands of neurons to a reduced 2-unit model that has similar qualitative behavior. Our analysis of the reduced model and explicit expressions relating the parameters of the reduced model to the megamap elucidate the conditions under which the combinatorial mode emerges and the dynamics in each mode given the relative strength of the attractor network and the relative strength of the two conflicting inputs. Although we focus on a particular attractor network model, we describe a set of conditions under which our analysis can be applied to more general attractor neural networks."}], "ArticleTitle": "Analysis of an Attractor Neural Network's Response to Conflicting External Inputs."}, "31728676": {"mesh": [], "AbstractText": [{"section": null, "text": "Recovering brain connectivity from tract tracing data is an important computational problem in the neurosciences. Mesoscopic connectome reconstruction was previously formulated as a structured matrix regression problem (Harris et al. in Neural Information Processing Systems, 2016), but existing techniques do not scale to the whole-brain setting. The corresponding matrix equation is challenging to solve due to large scale, ill-conditioning, and a general form that lacks a convergent splitting. We propose a greedy low-rank algorithm for the connectome reconstruction problem in very high dimensions. The algorithm approximates the solution by a sequence of rank-one updates which exploit the sparse and positive definite problem structure. This algorithm was described previously (Kressner and Sirkovi&#263; in Numer Lin Alg Appl 22(3):564-583, 2015) but never implemented for this connectome problem, leading to a number of challenges. We have had to design judicious stopping criteria and employ efficient solvers for the three main sub-problems of the algorithm, including an efficient GPU implementation that alleviates the main bottleneck for large datasets. The performance of the method is evaluated on three examples: an artificial \"toy\" dataset and two whole-cortex instances using data from the Allen Mouse Brain Connectivity Atlas. We find that the method is significantly faster than previous methods and that moderate ranks offer a good approximation. This speedup allows for the estimation of increasingly large-scale connectomes across taxa as these data become available from tracing experiments. The data and code are available online."}], "ArticleTitle": "Greedy low-rank algorithm for spatial connectome regression."}, "29845383": {"mesh": [], "AbstractText": [{"section": null, "text": "Neurons in a micro-circuit connected by chemical synapses can have their connectivity affected by the prior activity of the cells. The number of synapses available for releasing neurotransmitter can be decreased by repetitive activation through depletion of readily releasable neurotransmitter (NT), or increased through facilitation, where the probability of release of NT is increased by prior activation. These competing effects can create a complicated and subtle range of time-dependent connectivity. Here we investigate the probabilistic properties of facilitation and depression (FD) for a presynaptic neuron that is receiving a Poisson spike train of input. We use a model of FD that is parameterized with experimental data from a hippocampal basket cell and pyramidal cell connection, for fixed frequency input spikes at frequencies in the range of theta (3-8 Hz) and gamma (20-100 Hz) oscillations. Hence our results will apply to micro-circuits in the hippocampus that are responsible for the interaction of theta and gamma rhythms associated with learning and memory. A control situation is compared with one in which a pharmaceutical neuromodulator (muscarine) is employed. We apply standard information-theoretic measures such as entropy and mutual information, and find a closed form approximate expression for the probability distribution of release probability. We also use techniques that measure the dependence of the response on the exact history of stimulation the synapse has received, which uncovers some unexpected differences between control and muscarine-added cases."}], "ArticleTitle": "Effect of Neuromodulation of Short-term Plasticity on Information Processing in Hippocampal Interneuron Synapses."}, "33484358": {"mesh": [], "AbstractText": [{"section": null, "text": "Memory and forgetting constitute two sides of the same coin, and although the first has been extensively investigated, the latter is often overlooked. A possible approach to better understand forgetting is to develop phenomenological models that implement its putative mechanisms in the most elementary way possible, and then experimentally test the theoretical predictions of these models. One such mechanism proposed in previous studies is retrograde interference, stating that a memory can be erased due to subsequently acquired memories. In the current contribution, we hypothesize that retrograde erasure is controlled by the relevant \"importance\" measures such that more important memories eliminate less important ones acquired earlier. We show that some versions of the resulting mathematical model are broadly compatible with the previously reported power-law forgetting time course and match well the results of our recognition experiments with long, randomly assembled streams of words."}], "ArticleTitle": "Retroactive interference model of forgetting."}, "28220467": {"mesh": [], "AbstractText": [{"section": null, "text": "The Bienenstock-Cooper-Munro (BCM) learning rule provides a simple setup for synaptic modification that combines a Hebbian product rule with a homeostatic mechanism that keeps the weights bounded. The homeostatic part of the learning rule depends on the time average of the post-synaptic activity and provides a sliding threshold that distinguishes between increasing or decreasing weights. There are, thus, two essential time scales in the BCM rule: a homeostatic time scale, and a synaptic modification time scale. When the dynamics of the stimulus is rapid enough, it is possible to reduce the BCM rule to a simple averaged set of differential equations. In previous analyses of this model, the time scale of the sliding threshold is usually faster than that of the synaptic modification. In this paper, we study the dynamical properties of these averaged equations when the homeostatic time scale is close to the synaptic modification time scale. We show that instabilities arise leading to oscillations and in some cases chaos and other complex dynamics. We consider three cases: one neuron with two weights and two stimuli, one neuron with two weights and three stimuli, and finally a weakly interacting network of neurons."}], "ArticleTitle": "Emergent Dynamical Properties of the BCM Learning Rule."}, "29404814": {"mesh": [], "AbstractText": [{"section": null, "text": "We consider finite and infinite all-to-all coupled networks of identical theta neurons. Two types of synaptic interactions are investigated: instantaneous and delayed (via first-order synaptic processing). Extensive use is made of the Watanabe/Strogatz (WS) ansatz for reducing the dimension of networks of identical sinusoidally-coupled oscillators. As well as the degeneracy associated with the constants of motion of the WS ansatz, we also find continuous families of solutions for instantaneously coupled neurons, resulting from the reversibility of the reduced model and the form of the synaptic input. We also investigate a number of similar related models. We conclude that the dynamics of networks of all-to-all coupled identical neurons can be surprisingly complicated."}], "ArticleTitle": "The Dynamics of Networks of Identical Theta Neurons."}, "28647913": {"mesh": [], "AbstractText": [{"section": null, "text": "Low frequency firing is modeled by Type 1 neurons with a SNIC, but, because of the vertical slope of the square-root-like f-I curve, low f only occurs over a narrow range of I. When an adaptive current is added, however, the f-I curve is linearized, and low f occurs robustly over a large I range. Ermentrout (Neural Comput. 10(7):1721-1729, 1998) showed that this feature of adaptation paradoxically arises from the SNIC that is responsible for the vertical slope. We show, using a simplified Hindmarsh-Rose neuron with negative feedback acting directly on the adaptation current, that whereas a SNIC contributes to linearization, in practice linearization over a large interval may require strong adaptation strength. We also find that a type 2 neuron with threshold generated by a Hopf bifurcation can also show linearization if adaptation strength is strong. Thus, a SNIC is not necessary. More fundamental than a SNIC is stretching the steep region near threshold, which stems from sufficiently strong adaptation, though a SNIC contributes if present. In a more realistic conductance-based model, Morris-Lecar, with negative feedback acting on the adaptation conductance, an additional assumption that the driving force of the adaptation current is independent of I is needed. If this holds, strong adaptive conductance is both necessary and sufficient for linearization of f-I curves of type 2 f-I curves."}], "ArticleTitle": "How Adaptation Makes Low Firing Rates Robust."}, "26739133": {"mesh": [], "AbstractText": [{"section": null, "text": "The tools of weakly coupled phase oscillator theory have had a profound impact on the neuroscience community, providing insight into a variety of network behaviours ranging from central pattern generation to synchronisation, as well as predicting novel network states such as chimeras. However, there are many instances where this theory is expected to break down, say in the presence of strong coupling, or must be carefully interpreted, as in the presence of stochastic forcing. There are also surprises in the dynamical complexity of the attractors that can robustly appear-for example, heteroclinic network attractors. In this review we present a set of mathematical tools that are suitable for addressing the dynamics of oscillatory neural networks, broadening from a standard phase oscillator perspective to provide a practical framework for further successful applications of mathematics to understanding network dynamics in neuroscience. "}], "ArticleTitle": "Mathematical Frameworks for Oscillatory Network Dynamics in Neuroscience."}, "33394219": {"mesh": [], "AbstractText": [{"section": null, "text": "The reconstruction mechanisms built by the human auditory system during sound reconstruction are still a matter of debate. The purpose of this study is to propose a mathematical model of sound reconstruction based on the functional architecture of the auditory cortex (A1). The model is inspired by the geometrical modelling of vision, which has undergone a great development in the last ten years. There are, however, fundamental dissimilarities, due to the different role played by time and the different group of symmetries. The algorithm transforms the degraded sound in an 'image' in the time-frequency domain via a short-time Fourier transform. Such an image is then lifted to the Heisenberg group and is reconstructed via a Wilson-Cowan integro-differential equation. Preliminary numerical experiments are provided, showing the good reconstruction properties of the algorithm on synthetic sounds concentrated around two frequencies."}], "ArticleTitle": "A bio-inspired geometric model for sound reconstruction."}, "29675585": {"mesh": [], "AbstractText": [{"section": null, "text": "Many physiological phenomena have the property that some variables evolve much faster than others. For example, neuron models typically involve observable differences in time scales. The Hodgkin-Huxley model is well known for explaining the ionic mechanism that generates the action potential in the squid giant axon. Rubin and Wechselberger (Biol. Cybern. 97:5-32, 2007) nondimensionalized this model and obtained a singularly perturbed system with two fast, two slow variables, and an explicit time-scale ratio &#949;. The dynamics of this system are complex and feature periodic orbits with a series of action potentials separated by small-amplitude oscillations (SAOs); also referred to as mixed-mode oscillations (MMOs). The slow dynamics of this system are organized by two-dimensional locally invariant manifolds called slow manifolds which can be either attracting or of saddle type.In this paper, we introduce a general approach for computing two-dimensional saddle slow manifolds and their stable and unstable fast manifolds. We also develop a technique for detecting and continuing associated canard orbits, which arise from the interaction between attracting and saddle slow manifolds, and provide a mechanism for the organization of SAOs in [Formula: see text]. We first test our approach with an extended four-dimensional normal form of a folded node. Our results demonstrate that our computations give reliable approximations of slow manifolds and canard orbits of this model. Our computational approach is then utilized to investigate the role of saddle slow manifolds and associated canard orbits of the full Hodgkin-Huxley model in organizing MMOs and determining the firing rates of action potentials. For &#949; sufficiently large, canard orbits are arranged in pairs of twin canard orbits with the same number of SAOs. We illustrate how twin canard orbits partition the attracting slow manifold into a number of ribbons that play the role of sectors of rotations. The upshot is that we are able to unravel the geometry of slow manifolds and associated canard orbits without the need to reduce the model."}], "ArticleTitle": "Saddle Slow Manifolds and Canard Orbits in [Formula: see text] and Application to the Full Hodgkin-Huxley Model."}, "27043152": {"mesh": [], "AbstractText": [{"section": null, "text": "Jack Cowan's remarkable career has spanned, and molded, the development of neuroscience as a quantitative and mathematical discipline combining deep theoretical contributions, rigorous mathematical work and groundbreaking biological insights. The Banff International Research Station hosted a workshop in his honor, on Stochastic Network Models of Neocortex, July 17-24, 2014. This accompanying Festschrift celebrates Cowan's contributions by assembling current research in stochastic phenomena in neural networks. It combines historical perspectives with new results including applications to epilepsy, path-integral methods, stochastic synchronization, higher-order correlation analysis, and pattern formation in visual cortex. "}], "ArticleTitle": "Stochastic Network Models in Neuroscience: A Festschrift for Jack Cowan. Introduction to the Special Issue."}, "27091694": {"mesh": [], "AbstractText": [{"section": null, "text": "Sensory input to the lamprey central pattern generator (CPG) for locomotion is known to have a significant role in modulating lamprey swimming. Lamprey CPGs are known to have the ability to entrain to a bending stimulus, that is, in the presence of a rhythmic signal, the CPG will change its frequency to match the stimulus frequency. Bending experiments in which the lamprey spinal cord has been removed and mechanically bent back and forth at a single point have been used to determine the range of frequencies that can entrain the CPG rhythm. First, we model the lamprey locomotor CPG as a chain of neural oscillators with three classes of neurons and sinusoidal forcing representing edge cell input. We derive a phase model using the connections described in the neural model. This results in a simpler model yet maintains some properties of the neural model. For both the neural model and the derived phase model, entrainment ranges are computed for forcing at different points along the chain while varying both intersegmental coupling strength and the coupling strength between the forcer and chain. Entrainment ranges for chains with nonuniform intersegmental coupling asymmetry are larger when forcing is applied to the middle of the chain than when it is applied to either end, a result that is qualitatively similar to the experimental results. In the limit of weak coupling in the chain, the entrainment results of the neural model approach the entrainment results for the derived phase model. Both biological experiments and the robustness of non-monotonic entrainment ranges as a function of the forcing position across different classes of CPG models with nonuniform asymmetric coupling suggest that a specific property of the intersegmental coupling of the CPG is key to entrainment. "}], "ArticleTitle": "Entrainment Ranges for Chains of Forced Neural and Phase Oscillators."}, "33296032": {"mesh": [], "AbstractText": [{"section": null, "text": "A neural field models the large scale behaviour of large groups of neurons. We extend previous results for these models by including a diffusion term into the neural field, which models direct, electrical connections. We extend known and prove new sun-star calculus results for delay equations to be able to include diffusion and explicitly characterise the essential spectrum. For a certain class of connectivity functions in the neural field model, we are able to compute its spectral properties and the first Lyapunov coefficient of a Hopf bifurcation. By examining a numerical example, we find that the addition of diffusion suppresses non-synchronised steady-states while favouring synchronised oscillatory modes."}], "ArticleTitle": "Neural field models with transmission delays and diffusion."}, "31073652": {"mesh": [], "AbstractText": [{"section": null, "text": "Understanding nervous system function requires careful study of transient (non-equilibrium) neural response to rapidly changing, noisy input from the outside world. Such neural response results from dynamic interactions among multiple, heterogeneous brain regions. Realistic modeling of these large networks requires enormous computational resources, especially when high-dimensional parameter spaces are considered. By assuming quasi-steady-state activity, one can neglect the complex temporal dynamics; however, in many cases the quasi-steady-state assumption fails. Here, we develop a new reduction method for a general heterogeneous firing-rate model receiving background correlated noisy inputs that accurately handles highly non-equilibrium statistics and interactions of heterogeneous cells. Our method involves solving an efficient set of nonlinear ODEs, rather than time-consuming Monte Carlo simulations or high-dimensional PDEs, and it captures the entire set of first and second order statistics while allowing significant heterogeneity in all model parameters."}], "ArticleTitle": "Efficient calculation of heterogeneous non-equilibrium statistics in coupled firing-rate models."}, "32936367": {"mesh": [], "AbstractText": [{"section": null, "text": "White matter pathways form a complex network of myelinated axons that regulate signal transmission in the nervous system and play a key role in behaviour and cognition. Recent evidence reveals that white matter networks are adaptive and that myelin remodels itself in an activity-dependent way, during both developmental stages and later on through behaviour and learning. As a result, axonal conduction delays continuously adjust in order to regulate the timing of neural signals propagating between different brain areas. This delay plasticity mechanism has yet to be integrated in computational neural models, where conduction delays are oftentimes constant or simply ignored. As a first approach to adaptive white matter remodeling, we modified the canonical Kuramoto model by enabling all connections with adaptive, phase-dependent delays. We analyzed the equilibria and stability of this system, and applied our results to two-oscillator and large-dimensional networks. Our joint mathematical and numerical analysis demonstrates that plastic delays act as a stabilizing mechanism promoting the network's ability to maintain synchronous activity. Our work also shows that global synchronization is more resilient to perturbations and injury towards network architecture. Our results provide key insights about the analysis and potential significance of activity-dependent myelination in large-scale brain synchrony."}], "ArticleTitle": "Synchronization and resilience in the Kuramoto white matter network model with adaptive state-dependent delays."}, "30006849": {"mesh": [], "AbstractText": [{"section": null, "text": "Neuronal calcium signals propagating by simple diffusion and reaction with mobile and stationary buffers are limited to cellular microdomains. The distance intracellular calcium signals can travel may be significantly increased by means of calcium-induced calcium release from internal calcium stores, notably the endoplasmic reticulum. The organelle, which can be thought of as a cell-within-a-cell, is able to sequester large amounts of cytosolic calcium ions via SERCA pumps and selectively release them into the cytosol through ryanodine receptor channels leading to the formation of calcium waves. In this study, we set out to investigate the basic properties of such dendritic calcium waves and how they depend on the three parameters dendrite radius, ER radius and ryanodine receptor density in the endoplasmic membrane. We demonstrate that there are stable and abortive regimes for calcium waves, depending on the above morphological and physiological parameters. In stable regimes, calcium waves can travel across long dendritic distances, similar to electrical action potentials. We further observe that abortive regimes exist, which could be relevant for spike-timing dependent plasticity, as travel distances and wave velocities vary with changing intracellular architecture. For some of these regimes, analytic functions could be derived that fit the simulation data. In parameter spaces, that are non-trivially influenced by the three-dimensional calcium concentration profile, we were not able to derive such a functional description, demonstrating the mathematical requirement to model and simulate biochemical signaling in three-dimensional space."}], "ArticleTitle": "What Is Required for Neuronal Calcium Waves? A Numerical Parameter Study."}, "28097513": {"mesh": [], "AbstractText": [{"section": null, "text": "Homeostatic processes that provide negative feedback to regulate neuronal firing rates are essential for normal brain function. Indeed, multiple parameters of individual neurons, including the scale of afferent synapse strengths and the densities of specific ion channels, have been observed to change on homeostatic time scales to oppose the effects of chronic changes in synaptic input. This raises the question of whether these processes are controlled by a single slow feedback variable or multiple slow variables. A single homeostatic process providing negative feedback to a neuron's firing rate naturally maintains a stable homeostatic equilibrium with a characteristic mean firing rate; but the conditions under which multiple slow feedbacks produce a stable homeostatic equilibrium have not yet been explored. Here we study a highly general model of homeostatic firing rate control in which two slow variables provide negative feedback to drive a firing rate toward two different target rates. Using dynamical systems techniques, we show that such a control system can be used to stably maintain a neuron's characteristic firing rate mean and variance in the face of perturbations, and we derive conditions under which this happens. We also derive expressions that clarify the relationship between the homeostatic firing rate targets and the resulting stable firing rate mean and variance. We provide specific examples of neuronal systems that can be effectively regulated by dual homeostasis. One of these examples is a recurrent excitatory network, which a dual feedback system can robustly tune to serve as an integrator."}], "ArticleTitle": "Stable Control of Firing Rate Mean and Variance by Dual Homeostatic Mechanisms."}, "29349664": {"mesh": [], "AbstractText": [{"section": null, "text": "We investigate the sparse functional identification of complex cells and the decoding of spatio-temporal visual stimuli encoded by an ensemble of complex cells. The reconstruction algorithm is formulated as a rank minimization problem that significantly reduces the number of sampling measurements (spikes) required for decoding. We also establish the duality between sparse decoding and functional identification and provide algorithms for identification of low-rank dendritic stimulus processors. The duality enables us to efficiently evaluate our functional identification algorithms by reconstructing novel stimuli in the input space. Finally, we demonstrate that our identification algorithms substantially outperform the generalized quadratic model, the nonlinear input model, and the widely used spike-triggered covariance algorithm."}], "ArticleTitle": "Sparse Functional Identification of Complex Cells from Spike Times and the Decoding of Visual Stimuli."}, "29019105": {"mesh": [], "AbstractText": [{"section": null, "text": "We examine a family of random firing-rate neural networks in which we enforce the neurobiological constraint of Dale's Law-each neuron makes either excitatory or inhibitory connections onto its post-synaptic targets. We find that this constrained system may be described as a perturbation from a system with nontrivial symmetries. We analyze the symmetric system using the tools of equivariant bifurcation theory and demonstrate that the symmetry-implied structures remain evident in the perturbed system. In comparison, spectral characteristics of the network coupling matrix are relatively uninformative about the behavior of the constrained system."}], "ArticleTitle": "Symmetries Constrain Dynamics in a Family of Balanced Neural Networks."}, "28842863": {"mesh": [], "AbstractText": [{"section": null, "text": "Layer II stellate cells in the medial enthorinal cortex (MEC) express hyperpolarisation-activated cyclic-nucleotide-gated (HCN) channels that allow for rebound spiking via an [Formula: see text] current in response to hyperpolarising synaptic input. A&#160;computational modelling study by Hasselmo (Philos. Trans. R. Soc. Lond. B, Biol. Sci. 369:20120523, 2013) showed that an inhibitory network of such cells can support periodic travelling waves with a period that is controlled by the dynamics of the [Formula: see text] current. Hasselmo has suggested that these waves can underlie the generation of grid cells, and that the known difference in [Formula: see text] resonance frequency along the dorsal to ventral axis can explain the observed size and spacing between grid cell firing fields. Here we develop a biophysical spiking model within a framework that allows for analytical tractability. We combine the simplicity of integrate-and-fire neurons with a piecewise linear caricature of the gating dynamics for HCN channels to develop a spiking neural field model of MEC. Using techniques primarily drawn from the field of nonsmooth dynamical systems we show how to construct periodic travelling waves, and in particular the dispersion curve that determines how wave speed varies as a function of period. This exhibits a wide range of long wavelength solutions, reinforcing the idea that rebound spiking is a candidate mechanism for generating grid cell firing patterns. Importantly we develop a wave stability analysis to show how the maximum allowed period is controlled by the dynamical properties of the [Formula: see text] current. Our theoretical work is validated by numerical simulations of the spiking model in both one and two dimensions."}], "ArticleTitle": "An Analysis of Waves Underlying Grid Cell Firing in the Medial Enthorinal Cortex."}, "30022326": {"mesh": [], "AbstractText": [{"section": null, "text": "We present the study of a minimal microcircuit controlling locomotion in two-day-old Xenopus tadpoles. During swimming, neurons in the spinal central pattern generator (CPG) generate anti-phase oscillations between left and right half-centres. Experimental recordings show that the same CPG neurons can also generate transient bouts of long-lasting in-phase oscillations between left-right centres. These synchronous episodes are rarely recorded and have no identified behavioural purpose. However, metamorphosing tadpoles require both anti-phase and in-phase oscillations for swimming locomotion. Previous models have shown the ability to generate biologically realistic patterns of synchrony and swimming oscillations in tadpoles, but a mathematical description of how these oscillations appear is still missing. We define a simplified model that incorporates the key operating principles of tadpole locomotion. The model generates the various outputs seen in experimental recordings, including swimming and synchrony. To study the model, we perform detailed one- and two-parameter bifurcation analysis. This reveals the critical boundaries that separate different dynamical regimes and demonstrates the existence of parameter regions of bi-stable swimming and synchrony. We show that swimming is stable in a significantly larger range of parameters, and can be initiated more robustly, than synchrony. Our results can explain the appearance of long-lasting synchrony bouts seen in experiments at the start of a swimming episode."}], "ArticleTitle": "Bifurcations of Limit Cycles in a Reduced Model of the Xenopus Tadpole Central Pattern Generator."}, "30519798": {"mesh": [], "AbstractText": [{"section": null, "text": "Theta (4-8&#160;Hz) and gamma (30-80&#160;Hz) rhythms in the brain are commonly associated with memory and learning (Kahana in J&#160;Neurosci 26:1669-1672, 2006; Quilichini et&#160;al. in J&#160;Neurosci 30:11128-11142, 2010). The precision of co-firing between neurons and incoming inputs is critical in these cognitive functions. We consider an inhibitory neuron model with M-current under forcing from gamma pulses and a sinusoidal current of theta frequency. The M-current has a long time constant (&#8764;90&#160;ms) and it has been shown to generate resonance at theta frequencies (Hutcheon and Yarom in Trends Neurosci 23:216-222, 2000; Hu et&#160;al. in J&#160;Physiol 545:783-805, 2002). We have found that this slow M-current contributes to the precise co-firing between the network and fast gamma pulses in the presence of a slow sinusoidal forcing. The M-current expands the phase-locking frequency range of the network, counteracts the slow theta forcing, and admits bistability in some parameter range. The effects of the M-current balancing the theta forcing are reduced if the sinusoidal current is faster than the theta frequency band. We characterize the dynamical mechanisms underlying the role of the M-current in enabling a network to be entrained to gamma frequency inputs using averaging methods, geometric singular perturbation theory, and bifurcation analysis."}], "ArticleTitle": "M-Current Expands the Range of Gamma Frequency Inputs to Which a Neuronal Target Entrains."}, "32314104": {"mesh": [], "AbstractText": [{"section": null, "text": "Following publication of the original article (Naud and Longtin in J Math Neurosci 9:3, 2019), the authors noticed a mistake in the first paragraph within \"Altered propagation\"."}], "ArticleTitle": "Correction to: Linking demyelination to compound action potential dispersion with a spike-diffuse-spike approach."}, "31440910": {"mesh": [], "AbstractText": [{"section": null, "text": "Following publication of the original article [1], the authors noticed a mistake in the first paragraph within \"Altered propagation\"."}], "ArticleTitle": "Correction to: Linking demyelination to compound action potential dispersion with a spike-diffuse-spike approach."}, "28744735": {"mesh": [], "AbstractText": [{"section": null, "text": "Bursting is a phenomenon found in a variety of physical and biological systems. For example, in neuroscience, bursting is believed to play a key role in the way information is transferred in the nervous system. In this work, we propose a model that, appropriately tuned, can display several types of bursting behaviors. The model contains two subsystems acting at different time scales. For the fast subsystem we use the planar unfolding of a high codimension singularity. In its bifurcation diagram, we locate paths that underlie the right sequence of bifurcations necessary for bursting. The slow subsystem steers the fast one back and forth along these paths leading to bursting behavior. The model is able to produce almost all the classes of bursting predicted for systems with a planar fast subsystem. Transitions between classes can be obtained through an ultra-slow modulation of the model's parameters. A detailed exploration of the parameter space allows predicting possible transitions. This provides a single framework to understand the coexistence of diverse bursting patterns in physical and biological systems or in models."}], "ArticleTitle": "Fast-Slow Bursters in the Unfolding of a High Codimension Singularity and the Ultra-slow Transitions of Classes."}, "26936267": {"mesh": [], "AbstractText": [{"section": null, "text": "The original neural field model of Wilson and Cowan is often interpreted as the averaged behaviour of a network of switch like neural elements with a distribution of switch thresholds, giving rise to the classic sigmoidal population firing-rate function so prevalent in large scale neuronal modelling. In this paper we explore the effects of such threshold noise without recourse to averaging and show that spatial correlations can have a strong effect on the behaviour of waves and patterns in continuum models. Moreover, for a prescribed spatial covariance function we explore the differences in behaviour that can emerge when the underlying stationary distribution is changed from Gaussian to non-Gaussian. For travelling front solutions, in a system with exponentially decaying spatial interactions, we make use of an interface approach to calculate the instantaneous wave speed analytically as a series expansion in the noise strength. From this we find that, for weak noise, the spatially averaged speed depends only on the choice of covariance function and not on the shape of the stationary distribution. For a system with a Mexican-hat spatial connectivity we further find that noise can induce localised bump solutions, and using an interface stability argument show that there can be multiple stable solution branches. "}], "ArticleTitle": "Neural Field Models with Threshold Noise."}, "33420903": {"mesh": [], "AbstractText": [{"section": null, "text": "We analyse the potential effects of lateral connectivity (amacrine cells and gap junctions) on motion anticipation in the retina. Our main result is that lateral connectivity can-under conditions analysed in the paper-trigger a wave of activity enhancing the anticipation mechanism provided by local gain control (Berry et al. in Nature 398(6725):334-338, 1999; Chen et al. in J. Neurosci. 33(1):120-132, 2013). We illustrate these predictions by two examples studied in the experimental literature: differential motion sensitive cells (Baccus and Meister in Neuron 36(5):909-919, 2002) and direction sensitive cells where direction sensitivity is inherited from asymmetry in gap junctions connectivity (Trenholm et al. in Nat. Neurosci. 16:154-156, 2013). We finally present reconstructions of retinal responses to 2D visual inputs to assess the ability of our model to anticipate motion in the case of three different 2D stimuli."}], "ArticleTitle": "On the potential role of lateral connectivity in retinal anticipation."}, "28707194": {"mesh": [], "AbstractText": [{"section": null, "text": "Point neuron models with a Heaviside firing rate function can be ill-posed. That is, the initial-condition-to-solution map might become discontinuous in finite time. If a Lipschitz continuous but steep firing rate function is employed, then standard ODE theory implies that such models are well-posed and can thus, approximately, be solved with finite precision arithmetic. We investigate whether the solution of this well-posed model converges to a solution of the ill-posed limit problem as the steepness parameter of the firing rate function tends to infinity. Our argument employs the Arzel&#224;-Ascoli theorem and also yields the existence of a solution of the limit problem. However, we only obtain convergence of a subsequence of the regularized solutions. This is consistent with the fact that models with a Heaviside firing rate function can have several solutions, as we show. Our analysis assumes that the vector-valued limit function v, provided by the Arzel&#224;-Ascoli theorem, is threshold simple: That is, the set containing the times when one or more of the component functions of v equal the threshold value for firing, has zero Lebesgue measure. If this assumption does not hold, we argue that the regularized solutions may not converge to a solution of the limit problem with a Heaviside firing function."}], "ArticleTitle": "Regularization of Ill-Posed Point Neuron Models."}, "29230566": {"mesh": [], "AbstractText": [{"section": null, "text": "We present a simple rate-reduced neuron model that captures a wide range of complex, biologically plausible, and physiologically relevant spiking behavior. This includes spike-frequency adaptation, postinhibitory rebound, phasic spiking and accommodation, first-spike latency, and inhibition-induced spiking. Furthermore, the model can mimic different neuronal filter properties. It can be used to extend existing neural field models, adding more biological realism and yielding a richer dynamical structure. The model is based on a slight variation of the Rulkov map."}], "ArticleTitle": "A Rate-Reduced Neuron Model for Complex Spiking Behavior."}, "28004309": {"mesh": [], "AbstractText": [{"section": null, "text": "Recent experimental evidence on the clustering of glutamate and GABA transporters on astrocytic processes surrounding synaptic terminals pose the question of the functional relevance of the astrocytes in the regulation of neural activity. In this perspective, we introduce a new computational model that embeds recent findings on neuron-astrocyte coupling at the mesoscopic scale intra- and inter-layer local neural circuits. The model consists of a mass model for the neural compartment and an astrocyte compartment which controls dynamics of extracellular glutamate and GABA concentrations. By arguments based on bifurcation theory, we use the model to study the impact of deficiency of astrocytic glutamate and GABA uptakes on neural activity. While deficient astrocytic GABA uptake naturally results in increased neuronal inhibition, which in turn results in a decreased neuronal firing, deficient glutamate uptake by astrocytes may either decrease or increase neuronal firing either transiently or permanently. Given the relevance of neuronal hyperexcitability (or lack thereof) in the brain pathophysiology, we provide biophysical conditions for the onset identifying different physiologically relevant regimes of operation for astrocytic uptake transporters."}], "ArticleTitle": "A Theoretical Study on the Role of Astrocytic Activity in Neuronal Hyperexcitability by a Novel Neuron-Glia Mass Model."}, "28685484": {"mesh": [], "AbstractText": [{"section": null, "text": "Neural field equations are used to describe the spatio-temporal evolution of the activity in a network of synaptically coupled populations of neurons in the continuum limit. Their heuristic derivation involves two approximation steps. Under the assumption that each population in the network is large, the activity is described in terms of a population average. The discrete network is then approximated by a continuum. In this article we make the two approximation steps explicit. Extending a model by Bressloff and Newby, we describe the evolution of the activity in a discrete network of finite populations by a Markov chain. In order to determine finite-size effects-deviations from the mean-field limit due to the finite size of the populations in the network-we analyze the fluctuations of this Markov chain and set up an approximating system of diffusion processes. We show that a well-posed stochastic neural field equation with a noise term accounting for finite-size effects on traveling wave solutions is obtained as the strong continuum limit."}], "ArticleTitle": "Finite-Size Effects on Traveling Wave Solutions to Neural Field Equations."}, "33394133": {"mesh": [], "AbstractText": [{"section": null, "text": "The brain is intrinsically organized into large-scale networks that constantly re-organize on multiple timescales, even when the brain is at rest. The timing of these dynamics is crucial for sensation, perception, cognition, and ultimately consciousness, but the underlying dynamics governing the constant reorganization and switching between networks are not yet well understood. Electroencephalogram (EEG) microstates are brief periods of stable scalp topography that have been identified as the electrophysiological correlate of functional magnetic resonance imaging defined resting-state networks. Spatiotemporal microstate sequences maintain high temporal resolution and have been shown to be scale-free with long-range temporal correlations. Previous attempts to model EEG microstate sequences have failed to capture this crucial property and so cannot fully capture the dynamics; this paper answers the call for more sophisticated modeling approaches. We present a dynamical model that exhibits a noisy network attractor between nodes that represent the microstates. Using an excitable network between four nodes, we can reproduce the transition probabilities between microstates but not the heavy tailed residence time distributions. We present two extensions to this model: first, an additional hidden node at each state; second, an additional layer that controls the switching frequency in the original network. Introducing either extension to the network gives the flexibility to capture these heavy tails. We compare the model generated sequences to microstate sequences from EEG data collected from healthy subjects at rest. For the first extension, we show that the hidden nodes 'trap' the trajectories allowing the control of residence times at each node. For the second extension, we show that two nodes in the controlling layer are sufficient to model the long residence times. Finally, we show that in addition to capturing the residence time distributions and transition probabilities of the sequences, these two models capture additional properties of the sequences including having interspersed long and short residence times and long range temporal correlations in line with the data as measured by the Hurst exponent."}], "ArticleTitle": "Noisy network attractor models for transitions between EEG microstates."}, "29075933": {"mesh": [], "AbstractText": [{"section": null, "text": "Continuum neural field equations model the large-scale spatio-temporal dynamics of interacting neurons on a cortical surface. They have been extensively studied, both analytically and numerically, on bounded as well as unbounded domains. Neural field models do not require the specification of boundary conditions. Relatively little attention has been paid to the imposition of neural activity on the boundary, or to its role in inducing patterned states. Here we redress this imbalance by studying neural field models of Amari type (posed on one- and two-dimensional bounded domains) with Dirichlet boundary conditions. The Amari model has a Heaviside nonlinearity that allows for a description of localised solutions of the neural field with an interface dynamics. We show how to generalise this reduced but exact description by deriving a normal velocity rule for an interface that encapsulates boundary effects. The linear stability analysis of localised states in the interface dynamics is used to understand how spatially extended patterns may develop in the absence and presence of boundary conditions. Theoretical results for pattern formation are shown to be in excellent agreement with simulations of the full neural field model. Furthermore, a numerical scheme for the interface dynamics is introduced and used to probe the way in which a Dirichlet boundary condition can limit the growth of labyrinthine structures."}], "ArticleTitle": "The Dynamics of Neural Fields on Bounded Domains: An Interface Approach for Dirichlet Boundary Conditions."}, "28791604": {"mesh": [], "AbstractText": [{"section": null, "text": "Neural mass models provide a useful framework for modelling mesoscopic neural dynamics and in this article we consider the Jansen and Rit neural mass model (JR-NMM). We formulate a stochastic version of it which arises by incorporating random input and has the structure of a damped stochastic Hamiltonian system with nonlinear displacement. We then investigate path properties and moment bounds of the model. Moreover, we study the asymptotic behaviour of the model and provide long-time stability results by establishing the geometric ergodicity of the system, which means that the system-independently of the initial values-always converges to an invariant measure. In the last part, we simulate the stochastic JR-NMM by an efficient numerical scheme based on a splitting approach which preserves the qualitative behaviour of the solution."}], "ArticleTitle": "A Stochastic Version of the Jansen and Rit Neural Mass Model: Analysis and Numerics."}, "27059027": {"mesh": [], "AbstractText": [{"section": null, "text": "We investigate the dynamics of unidirectional semi-infinite chains of type-I oscillators that are periodically forced at their root node, as an archetype of wave generation in neural networks. In previous studies, numerical simulations based on uniform forcing have revealed that trajectories approach a traveling wave in the far-downstream, large time limit. While this phenomenon seems typical, it is hardly anticipated because the system does not exhibit any of the crucial properties employed in available proofs of existence of traveling waves in lattice dynamical systems. Here, we give a full mathematical proof of generation under uniform forcing in a simple piecewise affine setting for which the dynamics can be solved explicitly. In particular, our analysis proves existence, global stability, and robustness with respect to perturbations of the forcing, of families of waves with arbitrary period/wave number in some range, for every value of the parameters in the system. "}], "ArticleTitle": "Wave Generation in Unidirectional Chains of Idealized Neural Oscillators."}, "31385150": {"mesh": [], "AbstractText": [{"section": null, "text": "We study the dynamics arising when two identical oscillators are coupled near a Hopf bifurcation where we assume a parameter &#1013; uncouples the system at [Formula: see text]. Using a normal form for [Formula: see text] identical systems undergoing Hopf bifurcation, we explore the dynamical properties. Matching the normal form coefficients to a coupled Wilson-Cowan oscillator network gives an understanding of different types of behaviour that arise in a model of perceptual bistability. Notably, we find bistability between in-phase and anti-phase solutions that demonstrates the feasibility for synchronisation to act as the mechanism by which periodic inputs can be segregated (rather than via strong inhibitory coupling, as in the existing models). Using numerical continuation we confirm our theoretical analysis for small coupling strength and explore the bifurcation diagrams for large coupling strength, where the normal form approximation breaks down."}], "ArticleTitle": "The uncoupling limit of identical Hopf bifurcations with an application to perceptual bistability."}, "33796951": {"mesh": [], "AbstractText": [{"section": null, "text": "The neurons of the deep cerebellar nuclei (DCNn) represent the main functional link between the cerebellar cortex and the rest of the central nervous system. Therefore, understanding the electrophysiological properties of DCNn is of fundamental importance to understand the overall functioning of the cerebellum. Experimental data suggest that DCNn can reversibly switch between two states: the firing of spikes (F state) and a stable depolarized state (SD state). We introduce a new biophysical model of the DCNn membrane electro-responsiveness to investigate how the interplay between the documented conductances identified in DCNn give rise to these states. In the model, the F state emerges as an isola of limit cycles, i.e. a closed loop of periodic solutions disconnected from the branch of SD fixed points. This bifurcation structure endows the model with the ability to reproduce the [Formula: see text] transition triggered by hyperpolarizing current pulses. The model also reproduces the [Formula: see text] transition induced by blocking Ca currents and ascribes this transition to the blocking of the high-threshold Ca current. The model suggests that intracellular current injections can trigger fully reversible [Formula: see text] transitions. Investigation of low-dimension reduced models suggests that the voltage-dependent Na current is prominent for these dynamical features. Finally, simulations of the model suggest that physiological synaptic inputs may trigger [Formula: see text] transitions. These transitions could explain the puzzling observation of positively correlated activities of connected Purkinje cells and DCNn despite the former inhibit the latter."}], "ArticleTitle": "A model of on/off transitions in neurons of the deep cerebellar nuclei: deciphering the underlying ionic mechanisms."}, "27613652": {"mesh": [], "AbstractText": [{"section": null, "text": "Presented here is a model of neural tissue in a conductive medium stimulated by externally injected currents. The tissue is described as a conductively isotropic bidomain, i.e. comprised of intra and extracellular regions that occupy the same space, as well as the membrane that divides them, and the injection currents are described as a pair of source and sink points. The problem is solved in three spatial dimensions and defined in spherical coordinates [Formula: see text]. The system of coupled partial differential equations is solved by recasting the problem to be in terms of the membrane and a monodomain, interpreted as a weighted average of the intra and extracellular domains. The membrane and monodomain are defined by the scalar Helmholtz and Laplace equations, respectively, which are both separable in spherical coordinates. Product solutions are thus assumed and given through certain transcendental functions. From these electrical potentials, analytic expressions for current density are derived and from those fields the magnetic flux density is calculated. Numerical examples are considered wherein the interstitial conductivity is varied, as well as the limiting case of the problem simplifying to two dimensions due to azimuthal independence. Finally, future modeling work is discussed. "}], "ArticleTitle": "Analytic Modeling of Neural Tissue: I. A Spherical Bidomain."}, "28589465": {"mesh": [], "AbstractText": [{"section": null, "text": "Neural networks generate a variety of rhythmic activity patterns, often involving different timescales. One example arises in the respiratory network in the pre-B&#246;tzinger complex of the mammalian brainstem, which can generate the eupneic rhythm associated with normal respiration as well as recurrent low-frequency, large-amplitude bursts associated with sighing. Two competing hypotheses have been proposed to explain sigh generation: the recruitment of a neuronal population distinct from the eupneic rhythm-generating subpopulation or the reconfiguration of activity within a single population. Here, we consider two recent computational models, one of which represents each of the hypotheses. We use methods of dynamical systems theory, such as fast-slow decomposition, averaging, and bifurcation analysis, to understand the multiple-timescale mechanisms underlying sigh generation in each model. In the course of our analysis, we discover that a third timescale is required to generate sighs in both models. Furthermore, we identify the similarities of the underlying mechanisms in the two models and the aspects in which they differ."}], "ArticleTitle": "Timescales and Mechanisms of Sigh-Like Bursting and Spiking in Models of Rhythmic Respiratory Neurons."}, "33939042": {"mesh": [], "AbstractText": [{"section": null, "text": "In the auditory streaming paradigm, alternating sequences of pure tones can be perceived as a single galloping rhythm (integration) or as two sequences with separated low and high tones (segregation). Although studied for decades, the neural mechanisms underlining this perceptual grouping of sound remains a mystery. With the aim of identifying a plausible minimal neural circuit that captures this phenomenon, we propose a firing rate model with two periodically forced neural populations coupled by fast direct excitation and slow delayed inhibition. By analyzing the model in a non-smooth, slow-fast regime we analytically prove the existence of a rich repertoire of dynamical states and of their parameter dependent transitions. We impose plausible parameter restrictions and link all states with perceptual interpretations. Regions of stimulus parameters occupied by states linked with each percept match those found in behavioural experiments. Our model suggests that slow inhibition masks the perception of subsequent tones during segregation (forward masking), whereas fast excitation enables integration for large pitch differences between the two tones."}], "ArticleTitle": "Auditory streaming emerges from fast excitation and slow delayed inhibition."}, "33587210": {"mesh": [], "AbstractText": [{"section": null, "text": "In this work, we consider a general conductance-based neuron model with the inclusion of the acetycholine sensitive, M-current. We study bifurcations in the parameter space consisting of the applied current [Formula: see text], the maximal conductance of the M-current [Formula: see text] and the conductance of the leak current [Formula: see text]. We give precise conditions for the model that ensure the existence of a Bogdanov-Takens (BT) point and show that such a point can occur by varying [Formula: see text] and [Formula: see text]. We discuss the case when the BT point becomes a Bogdanov-Takens-cusp (BTC) point and show that such a point can occur in the three-dimensional parameter space. The results of the bifurcation analysis are applied to different neuronal models and are verified and supplemented by numerical bifurcation diagrams generated using the package MATCONT. We conclude that there is a transition in the neuronal excitability type organised by the BT point and the neuron switches from Class-I to Class-II as conductance of the M-current increases."}], "ArticleTitle": "M-current induced Bogdanov-Takens bifurcation and switching of neuron excitability class."}, "29872932": {"mesh": [], "AbstractText": [{"section": null, "text": "The structure of spiking activity in cortical networks has important implications for how the brain ultimately codes sensory signals. However, our understanding of how network and intrinsic cellular mechanisms affect spiking is still incomplete. In particular, whether cell pairs in a neural network show a positive (or no) relationship between pairwise spike count correlation and average firing rate is generally unknown. This relationship is important because it has been observed experimentally in some sensory systems, and it can enhance information in a common population code. Here we extend our prior work in developing mathematical tools to succinctly characterize the correlation and firing rate relationship in heterogeneous coupled networks. We find that very modest changes in how heterogeneous networks occupy parameter space can dramatically alter the correlation-firing rate relationship."}], "ArticleTitle": "Investigating the Correlation-Firing Rate Relationship in Heterogeneous Recurrent Networks."}, "27215548": {"mesh": [], "AbstractText": [{"section": null, "text": "A fundamental question concerning the way the visual world is represented in our brain is how a cortical cell responds when its classical receptive field contains a plurality of stimuli. Two opposing models have been proposed. In the response-averaging model, the neuron responds with a weighted average of all individual stimuli. By contrast, in the probability-mixing model, the cell responds to a plurality of stimuli as if only one of the stimuli were present. Here we apply the probability-mixing and the response-averaging model to leaky integrate-and-fire neurons, to describe neuronal behavior based on observed spike trains. We first estimate the parameters of either model using numerical methods, and then test which model is most likely to have generated the observed data. Results show that the parameters can be successfully estimated and the two models are distinguishable using model selection. "}], "ArticleTitle": "Responses of Leaky Integrate-and-Fire Neurons to a Plurality of Stimuli in Their Receptive Fields."}, "27129667": {"mesh": [], "AbstractText": [{"section": null, "text": "We show that point-neuron models with a Heaviside firing rate function can be ill posed. More specifically, the initial-condition-to-solution map might become discontinuous in finite time. Consequently, if finite precision arithmetic is used, then it is virtually impossible to guarantee the accurate numerical solution of such models. If a smooth firing rate function is employed, then standard ODE theory implies that point-neuron models are well posed. Nevertheless, in the steep firing rate regime, the problem may become close to ill posed, and the error amplification, in finite time, can be very large. This observation is illuminated by numerical experiments. We conclude that, if a steep firing rate function is employed, then minor round-off errors can have a devastating effect on simulations, unless proper error-control schemes are used. "}], "ArticleTitle": "Ill-Posed Point Neuron Models."}, "33606089": {"mesh": [], "AbstractText": [{"section": null, "text": "Decoding approaches provide a useful means of estimating the information contained in neuronal circuits. In this work, we analyze the expected classification error of a decoder based on Fisher linear discriminant analysis. We provide expressions that relate decoding error to the specific parameters of a population model that performs linear integration of sensory input. Results show conditions that lead to beneficial and detrimental effects of noise correlation on decoding. Further, the proposed framework sheds light on the contribution of neuronal noise, highlighting cases where, counter-intuitively, increased noise may lead to improved decoding performance. Finally, we examined the impact of dynamical parameters, including neuronal leak and integration time constant, on decoding. Overall, this work presents a fruitful approach to the study of decoding using a comprehensive theoretical framework that merges dynamical parameters with estimates of readout error."}], "ArticleTitle": "Estimating Fisher discriminant error in a linear integrator model of neural population activity."}, "27626960": {"mesh": [], "AbstractText": [{"section": null, "text": "Linear-nonlinear (LN) models and their extensions have proven successful in describing transformations from stimuli to spiking responses of neurons in early stages of sensory hierarchies. Neural responses at later stages are highly nonlinear and have generally been better characterized in terms of their decoding performance on prespecified tasks. Here we develop a biologically plausible decoding model for classification tasks, that we refer to as neural quadratic discriminant analysis (nQDA). Specifically, we reformulate an optimal quadratic classifier as an LN-LN computation, analogous to \"subunit\" encoding models that have been used to describe responses in retina and primary visual cortex. We propose a physiological mechanism by which the parameters of the nQDA classifier could be optimized, using a supervised variant of a Hebbian learning rule. As an example of its applicability, we show that nQDA provides a better account than many comparable alternatives for the transformation between neural representations in two high-level brain areas recorded as monkeys performed a visual delayed-match-to-sample task."}], "ArticleTitle": "Neural Quadratic Discriminant Analysis: Nonlinear Decoding with V1-Like Computation."}, "32946704": {"mesh": ["Animals", "Humans", "Markov Chains", "Models, Neurological", "Models, Theoretical", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "This letter considers a class of biologically plausible cost functions for neural networks, where the same cost function is minimized by both neural activity and plasticity. We show that such cost functions can be cast as a variational bound on model evidence under an implicit generative model. Using generative models based on partially observed Markov decision processes (POMDP), we show that neural activity and plasticity perform Bayesian inference and learning, respectively, by maximizing model evidence. Using mathematical and numerical analyses, we establish the formal equivalence between neural network cost functions and variational free energy under some prior beliefs about latent states that generate inputs. These prior beliefs are determined by particular constants (e.g., thresholds) that define the cost function. This means that the Bayes optimal encoding of latent or hidden states is achieved when the network's implicit priors match the process that generates its inputs. This equivalence is potentially important because it suggests that any hyperparameter of a neural network can itself be optimized-by minimization with respect to variational free energy. Furthermore, it enables one to characterize a neural network formally, in terms of its prior beliefs."}], "ArticleTitle": "Reverse-Engineering Neural Networks to Characterize Their Cost Functions."}, "31393825": {"mesh": ["Brain", "Humans", "Memory Consolidation", "Models, Neurological", "Models, Theoretical", "Nerve Net", "Neural Networks, Computer", "Synapses"], "AbstractText": [{"section": null, "text": "Cortical oscillations are central to information transfer in neural systems. Significant evidence supports the idea that coincident spike input can allow the neural threshold to be overcome and spikes to be propagated downstream in a circuit. Thus, an observation of oscillations in neural circuits would be an indication that repeated synchronous spiking may be enabling information transfer. However, for memory transfer, in which synaptic weights must be being transferred from one neural circuit (region) to another, what is the mechanism? Here, we present a synaptic transfer mechanism whose structure provides some understanding of the phenomena that have been implicated in memory transfer, including nested oscillations at various frequencies. The circuit is based on the principle of pulse-gated, graded information transfer between neural populations."}], "ArticleTitle": "A Mechanism for Synaptic Copy Between Neural Circuits."}, "32186997": {"mesh": ["Biomechanical Phenomena", "Biophysical Phenomena", "Data Accuracy", "Feedback", "Humans", "Motor Cortex", "Movement", "Neurons", "Nonlinear Dynamics"], "AbstractText": [{"section": null, "text": "The ability to move fast and accurately track moving objects is fundamentally constrained by the biophysics of neurons and dynamics of the muscles involved. Yet the corresponding trade-offs between these factors and tracking motor commands have not been rigorously quantified. We use feedback control principles to quantify performance limitations of the sensorimotor control system (SCS) to track fast periodic movements. We show that (1) linear models of the SCS fail to predict known undesirable phenomena, including skipped cycles, overshoot and undershoot, produced when tracking signals in the \"fast regime,\" while nonlinear pulsatile control models can predict such undesirable phenomena, and (2) tools from nonlinear control theory allow us to characterize fundamental limitations in this fast regime. Using a validated and tractable nonlinear model of the SCS, we derive an analytical upper bound on frequencies that the SCS model can reliably track before producing such undesirable phenomena as a function of the neurons' biophysical constraints and muscle dynamics. The performance limitations derived here have important implications in sensorimotor control. For example, if the primary motor cortex is compromised due to disease or damage, the theory suggests ways to manipulate muscle dynamics by adding the necessary compensatory forces using an assistive neuroprosthetic device to restore motor performance and, more important, fast and agile movements. Just how one should compensate can be informed by our SCS model and the theory developed here."}], "ArticleTitle": "Performance Limitations in Sensorimotor Control: Trade-Offs Between Neural Computation and Accuracy in Tracking Fast Movements."}, "30883276": {"mesh": ["Algorithms", "Animals", "Association", "Brain", "Computer Simulation", "Memory", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "It is still unknown how associative biological memories operate. Hopfield networks are popular models of associative memory, but they suffer from spurious memories and low efficiency. Here, we present a new model of an associative memory that overcomes these deficiencies. We call this model sparse associative memory (SAM) because it is based on sparse projections from neural patterns to pattern-specific neurons. These sparse projections have been shown to be sufficient to uniquely encode a neural pattern. Based on this principle, we investigate theoretically and in simulation our SAM model, which turns out to have high memory efficiency and a vanishingly small probability of spurious memories. This model may serve as a basic building block of brain functions involving associative memory."}], "ArticleTitle": "Sparse Associative Memory."}, "30645177": {"mesh": [], "AbstractText": [{"section": null, "text": "We study numerically the memory that forgets, introduced in 1986 by Parisi by bounding the synaptic strength, with a mechanism that avoids confusion; allows remembering the pattern learned more recently; and has a physiologically very well-defined meaning. We analyze a number of features of this learning for a finite number of neurons and finite number of patterns. We discuss how the system behaves in the large but finite <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>N</mml:mi></mml:math> limit. We analyze the basin of attraction of the patterns that have been learned, and we show that it is exponentially small in the age of the pattern."}], "ArticleTitle": "Forgetting Memories and Their Attractiveness."}, "32795231": {"mesh": [], "AbstractText": [{"section": null, "text": "Principal component analysis (PCA) is a widely used method for data processing, such as for dimension reduction and visualization. Standard PCA is known to be sensitive to outliers, and various robust PCA methods have been proposed. It has been shown that the robustness of many statistical methods can be improved using mode estimation instead of mean estimation, because mode estimation is not significantly affected by the presence of outliers. Thus, this study proposes a modal principal component analysis (MPCA), which is a robust PCA method based on mode estimation. The proposed method finds the minor component by estimating the mode of the projected data points. As a theoretical contribution, probabilistic convergence property, influence function, finite-sample breakdown point, and its lower bound for the proposed MPCA are derived. The experimental results show that the proposed method has advantages over conventional methods."}], "ArticleTitle": "Modal Principal Component Analysis."}, "33080161": {"mesh": ["Animals", "Humans", "Neural Networks, Computer", "Neurons"], "AbstractText": [{"section": null, "text": "Pruning is an effective way to slim and speed up convolutional neural networks. Generally previous work directly pruned neural networks in the original feature space without considering the correlation of neurons. We argue that such a way of pruning still keeps some redundancy in the pruned networks. In this letter, we proposed to prune in the intermediate space in which the correlation of neurons is eliminated. To achieve this goal, the input and output of a convolutional layer are first mapped to an intermediate space by orthogonal transformation. Then neurons are evaluated and pruned in the intermediate space. Extensive experiments have shown that our redundancy-aware pruning method surpasses state-of-the-art pruning methods on both efficiency and accuracy. Notably, using our redundancy-aware pruning method, ResNet models with three times the speed-up could achieve competitive performance with fewer floating point operations per second even compared to DenseNet."}], "ArticleTitle": "Redundancy-Aware Pruning of Convolutional Neural Networks."}, "31335292": {"mesh": ["Bayes Theorem", "Cognition", "Electrodes, Implanted", "Electroencephalography", "Gyrus Cinguli", "Humans", "Models, Neurological", "Psychomotor Performance", "Reaction Time", "Stochastic Processes"], "AbstractText": [{"section": null, "text": "Cognitive processes, such as learning and cognitive flexibility, are both difficult to measure and to sample continuously using objective tools because cognitive processes arise from distributed, high-dimensional neural activity. For both research and clinical applications, that dimensionality must be reduced. To reduce dimensionality and measure underlying cognitive processes, we propose a modeling framework in which a cognitive process is defined as a low-dimensional dynamical latent variable-called a cognitive state, which links high-dimensional neural recordings and multidimensional behavioral readouts. This framework allows us to decompose the hard problem of modeling the relationship between neural and behavioral data into separable encoding-decoding approaches. We first use a state-space modeling framework, the behavioral decoder, to articulate the relationship between an objective behavioral readout (e.g., response times) and cognitive state. The second step, the neural encoder, involves using a generalized linear model (GLM) to identify the relationship between the cognitive state and neural signals, such as local field potential (LFP). We then use the neural encoder model and a Bayesian filter to estimate cognitive state using neural data (LFP power) to generate the neural decoder. We provide goodness-of-fit analysis and model selection criteria in support of the encoding-decoding result. We apply this framework to estimate an underlying cognitive state from neural data in human participants (N=8) performing a cognitive conflict task. We successfully estimated the cognitive state within the 95% confidence intervals of that estimated using behavior readout for an average of 90% of task trials across participants. In contrast to previous encoder-decoder models, our proposed modeling framework incorporates LFP spectral power to encode and decode a cognitive state. The framework allowed us to capture the temporal evolution of the underlying cognitive processes, which could be key to the development of closed-loop experiments and treatments."}], "ArticleTitle": "Decoding Hidden Cognitive States From Behavior and Physiology Using a Bayesian Approach."}, "29652587": {"mesh": [], "AbstractText": [{"section": null, "text": "A vast majority of computation in the brain is performed by spiking neural networks. Despite the ubiquity of such spiking, we currently lack an understanding of how biological spiking neural circuits learn and compute in vivo, as well as how we can instantiate such capabilities in artificial spiking circuits in silico. Here we revisit the problem of supervised learning in temporally coding multilayer spiking neural networks. First, by using a surrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based three-factor learning rule capable of training multilayer networks of deterministic integrate-and-fire neurons to perform nonlinear computations on spatiotemporal spike patterns. Second, inspired by recent results on feedback alignment, we compare the performance of our learning rule under different credit assignment strategies for propagating output errors to hidden units. Specifically, we test uniform, symmetric, and random feedback, finding that simpler tasks can be solved with any type of feedback, while more complex tasks require symmetric feedback. In summary, our results open the door to obtaining a better scientific understanding of learning and computation in spiking neural networks by advancing our ability to train them to solve nonlinear problems involving transformations between different spatiotemporal spike time patterns."}], "ArticleTitle": "SuperSpike: Supervised Learning in Multilayer Spiking Neural Networks."}, "31525312": {"mesh": ["Computer Simulation", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Any function can be constructed using a hierarchy of simpler functions through compositions. Such a hierarchy can be characterized by a binary rooted tree. Each node of this tree is associated with a function that takes as inputs two numbers from its children and produces one output. Since thinking about functions in terms of computation graphs is becoming popular, we may want to know which functions can be implemented on a given tree. Here, we describe a set of necessary constraints in the form of a system of nonlinear partial differential equations that must be satisfied. Moreover, we prove that these conditions are sufficient in contexts of analytic and bit-valued functions. In the latter case, we explicitly enumerate discrete functions and observe that there are relatively few. Our point of view allows us to compare different neural network architectures in regard to their function spaces. Our work connects the structure of computation graphs with the functions they can implement and has potential applications to neuroscience and computer science."}], "ArticleTitle": "On Functions Computed on Trees."}, "32687771": {"mesh": ["Action Potentials", "Animals", "Cerebral Cortex", "Mice", "Models, Neurological", "Neural Networks, Computer", "Neurons"], "AbstractText": [{"section": null, "text": "The Poisson variability in cortical neural responses has been typically modeled using spike averaging techniques, such as trial averaging and rate coding, since such methods can produce reliable correlates of behavior. However, mechanisms that rely on counting spikes could be slow and inefficient and thus might not be useful in the brain for computations at timescales in the 10 millisecond range. This issue has motivated a search for alternative spike codes that take advantage of spike timing and has resulted in many studies that use synchronized neural networks for communication. Here we focus on recent studies that suggest that the gamma frequency may provide a reference that allows local spike phase representations that could result in much faster information transmission. We have developed a unified model (gamma spike multiplexing) that takes advantage of a single cycle of a cell's somatic gamma frequency to modulate the generation of its action potentials. An important consequence of this coding mechanism is that it allows multiple independent neural processes to run in parallel, thereby greatly increasing the processing capability of the cortex. System-level simulations and preliminary analysis of mouse cortical cell data are presented as support for the proposed theoretical model."}], "ArticleTitle": "Parallel Neural Multiprocessing with Gamma Frequency Latencies."}, "30979352": {"mesh": ["Humans", "Machine Learning", "Models, Neurological", "Neural Networks, Computer", "Neurons"], "AbstractText": [{"section": null, "text": "Quantifying mutual information between inputs and outputs of a large neural circuit is an important open problem in both machine learning and neuroscience. However, evaluation of the mutual information is known to be generally intractable for large systems due to the exponential growth in the number of terms that need to be evaluated. Here we show how information contained in the responses of large neural populations can be effectively computed provided the input-output functions of individual neurons can be measured and approximated by a logistic function applied to a potentially nonlinear function of the stimulus. Neural responses in this model can remain sensitive to multiple stimulus components. We show that the mutual information in this model can be effectively approximated as a sum of lower-dimensional conditional mutual information terms. The approximations become exact in the limit of large neural populations and for certain conditions on the distribution of receptive fields across the neural population. We empirically find that these approximations continue to work well even when the conditions on the receptive field distributions are not fulfilled. The computing cost for the proposed methods grows linearly in the dimension of the input and compares favorably with other approximations."}], "ArticleTitle": "Quantifying Information Conveyed by Large Neuronal Populations."}, "32946705": {"mesh": ["Animals", "Brain", "Brain Mapping", "Cognition", "Humans", "Models, Neurological", "Space Perception", "Spatial Navigation"], "AbstractText": [{"section": null, "text": "In this study, we integrated neural encoding and decoding into a unified framework for spatial information processing in the brain. Specifically, the neural representations of self-location in the hippocampus (HPC) and entorhinal cortex (EC) play crucial roles in spatial navigation. Intriguingly, these neural representations in these neighboring brain areas show stark differences. Whereas the place cells in the HPC fire as a unimodal function of spatial location, the grid cells in the EC show periodic tuning curves with different periods for different subpopulations (called modules). By combining an encoding model for this modular neural representation and a realistic decoding model based on belief propagation, we investigated the manner in which self-location is encoded by neurons in the EC and then decoded by downstream neurons in the HPC. Through the results of numerical simulations, we first show the positive synergy effects of the modular structure in the EC. The modular structure introduces more coupling between heterogeneous modules with different periodicities, which provides increased error-correcting capabilities. This is also demonstrated through a comparison of the beliefs produced for decoding two- and four-module codes. Whereas the former resulted in a complete decoding failure, the latter correctly recovered the self-location even from the same inputs. Further analysis of belief propagation during decoding revealed complex dynamics in information updates due to interactions among multiple modules having diverse scales. Therefore, the proposed unified framework allows one to investigate the overall flow of spatial information, closing the loop of encoding and decoding self-location in the brain."}], "ArticleTitle": "Toward a Unified Framework for Cognitive Maps."}, "32687773": {"mesh": ["Algorithms", "Computer Simulation", "Neural Networks, Computer", "Neurons"], "AbstractText": [{"section": null, "text": "For most multistate Hopfield neural networks, the stability conditions in asynchronous mode are known, whereas those in synchronous mode are not. If they were to converge in synchronous mode, recall would be accelerated by parallel processing. Complex-valued Hopfield neural networks (CHNNs) with a projection rule do not converge in synchronous mode. In this work, we provide stability conditions for hyperbolic Hopfield neural networks (HHNNs) in synchronous mode instead of CHNNs. HHNNs provide better noise tolerance than CHNNs. In addition, the stability conditions are applied to the projection rule, and HHNNs with a projection rule converge in synchronous mode. By computer simulations, we find that the projection rule for HHNNs in synchronous mode maintains a high noise tolerance."}], "ArticleTitle": "Hyperbolic-Valued Hopfield Neural Networks in Synchronous Mode."}, "30462582": {"mesh": [], "AbstractText": [{"section": null, "text": "We introduce a novel data-driven approach to discover and decode features in the neural code coming from large population neural recordings with minimal assumptions, using cohomological feature extraction. We apply our approach to neural recordings of mice moving freely in a box, where we find a circular feature. We then observe that the decoded value corresponds well to the head direction of the mouse. Thus, we capture head direction cells and decode the head direction from the neural population activity without having to process the mouse's behavior. Interestingly, the decoded values convey more information about the neural activity than the tracked head direction does, with differences that have some spatial organization. Finally, we note that the residual population activity, after the head direction has been accounted for, retains some low-dimensional structure that is correlated with the speed of the mouse."}], "ArticleTitle": "Decoding of Neural Data Using Cohomological Feature Extraction."}, "33253029": {"mesh": ["Action Potentials", "Adaptation, Physiological", "Animals", "Humans", "Models, Neurological", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "The intrinsic electrophysiological properties of single neurons can be described by a broad spectrum of models, from realistic Hodgkin-Huxley-type models with numerous detailed mechanisms to the phenomenological models. The adaptive exponential integrate-and-fire (AdEx) model has emerged as a convenient middle-ground model. With a low computational cost but keeping biophysical interpretation of the parameters, it has been extensively used for simulations of large neural networks. However, because of its current-based adaptation, it can generate unrealistic behaviors. We show the limitations of the AdEx model, and to avoid them, we introduce the conductance-based adaptive exponential integrate-and-fire model (CAdEx). We give an analysis of the dynamics of the CAdEx model and show the variety of firing patterns it can produce. We propose the CAdEx model as a richer alternative to perform network simulations with simplified models reproducing neuronal intrinsic properties."}], "ArticleTitle": "Conductance-Based Adaptive Exponential Integrate-and-Fire Model."}, "30883277": {"mesh": ["Action Potentials", "Adaptation, Physiological", "Animals", "Information Theory", "Learning", "Models, Neurological", "Neural Networks, Computer", "Neurons", "Synapses"], "AbstractText": [{"section": null, "text": "A key aspect of the neural coding problem is understanding how representations of afferent stimuli are built through the dynamics of learning and adaptation within neural networks. The infomax paradigm is built on the premise that such learning attempts to maximize the mutual information between input stimuli and neural activities. In this letter, we tackle the problem of such information-based neural coding with an eye toward two conceptual hurdles. Specifically, we examine and then show how this form of coding can be achieved with online input processing. Our framework thus obviates the biological incompatibility of optimization methods that rely on global network awareness and batch processing of sensory signals. Central to our result is the use of variational bounds as a surrogate objective function, an established technique that has not previously been shown to yield online policies. We obtain learning dynamics for both linear-continuous and discrete spiking neural encoding models under the umbrella of linear gaussian decoders. This result is enabled by approximating certain information quantities in terms of neuronal activity via pairwise feedback mechanisms. Furthermore, we tackle the problem of how such learning dynamics can be realized with strict energetic constraints. We show that endowing networks with auxiliary variables that evolve on a slower timescale can allow for the realization of saddle-point optimization within the neural dynamics, leading to neural codes with favorable properties in terms of both information and energy."}], "ArticleTitle": "Multiple Timescale Online Learning Rules for Information Maximization with Energetic Constraints."}, "32433898": {"mesh": ["Action Potentials", "Animals", "Deep Learning", "Humans", "Neurons"], "AbstractText": [{"section": null, "text": "The multispike tempotron (MST) is a powersul, single spiking neuron model that can solve complex supervised classification tasks. It is also internally complex, computationally expensive to evaluate, and unsuitable for neuromorphic hardware. Here we aim to understand whether it is possible to simplify the MST model while retaining its ability to learn and process information. To this end, we introduce a family of generalized neuron models (GNMs) that are a special case of the spike response model and much simpler and cheaper to simulate than the MST. We find that over a wide range of parameters, the GNM can learn at least as well as the MST does. We identify the temporal autocorrelation of the membrane potential as the most important ingredient of the GNM that enables it to classify multiple spatiotemporal patterns. We also interpret the GNM as a chemical system, thus conceptually bridging computation by neural networks with molecular information processing. We conclude the letter by proposing alternative training approaches for the GNM, including error trace learning and error backpropagation."}], "ArticleTitle": "Minimal Spiking Neuron for Solving Multilabel Classification Tasks."}, "31113303": {"mesh": [], "AbstractText": [{"section": null, "text": "Interest in quantum computing has increased significantly. Tensor network theory has become increasingly popular and widely used to simulate strongly entangled correlated systems. Matrix product state (MPS) is a well-designed class of tensor network states that plays an important role in processing quantum information. In this letter, we show that MPS, as a one-dimensional array of tensors, can be used to classify classical and quantum data. We have performed binary classification of the classical machine learning data set Iris encoded in a quantum state. We have also investigated its performance by considering different parameters on the ibmqx4 quantum computer and proved that MPS circuits can be used to attain better accuracy. Furthermore the learning ability of an MPS quantum classifier is tested to classify evapotranspiration (ET o ) for the Patiala meteorological station located in northern Punjab (India), using three years of a historical data set (Agri). We have used different performance metrics of classification to measure its capability. Finally, the results are plotted and the degree of correspondence among values of each sample is shown."}], "ArticleTitle": "Matrix Product State-Based Quantum Classifier."}, "31951796": {"mesh": [], "AbstractText": [{"section": null, "text": "Learning from triplet comparison data has been extensively studied in the context of metric learning, where we want to learn a distance metric between two instances, and ordinal embedding, where we want to learn an embedding in a Euclidean space of the given instances that preserve the comparison order as much as possible. Unlike fully labeled data, triplet comparison data can be collected in a more accurate and human-friendly way. Although learning from triplet comparison data has been considered in many applications, an important fundamental question of whether we can learn a classifier only from triplet comparison data without all the labels has remained unanswered. In this letter, we give a positive answer to this important question by proposing an unbiased estimator for the classification risk under the empirical risk minimization framework. Since the proposed method is based on the empirical risk minimization framework, it inherently has the advantage that any surrogate loss function and any model, including neural networks, can be easily applied. Furthermore, we theoretically establish an estimation error bound for the proposed empirical risk minimizer. Finally, we provide experimental results to show that our method empirically works well and outperforms various baseline methods."}], "ArticleTitle": "Classification from Triplet Comparison Data."}, "31113301": {"mesh": ["Algorithms", "Data Analysis", "Humans", "Memory, Long-Term", "Memory, Short-Term", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Recurrent neural networks (RNNs) have been widely adopted in research areas concerned with sequential data, such as text, audio, and video. However, RNNs consisting of sigma cells or tanh cells are unable to learn the relevant information of input data when the input gap is large. By introducing gate functions into the cell structure, the long short-term memory (LSTM) could handle the problem of long-term dependencies well. Since its introduction, almost all the exciting results based on RNNs have been achieved by the LSTM. The LSTM has become the focus of deep learning. We review the LSTM cell and its variants to explore the learning capacity of the LSTM cell. Furthermore, the LSTM networks are divided into two broad categories: LSTM-dominated networks and integrated LSTM networks. In addition, their various applications are discussed. Finally, future research directions are presented for LSTM networks."}], "ArticleTitle": "A Review of Recurrent Neural Networks: LSTM Cells and Network Architectures."}, "33080167": {"mesh": ["Algorithms", "Humans", "Neural Networks, Computer", "Pattern Recognition, Automated"], "AbstractText": [{"section": null, "text": "We investigate a latent variable model for multinomial classification inspired by recent capsule architectures for visual object recognition (Sabour, Frosst, & Hinton, 2017). Capsule architectures use vectors of hidden unit activities to encode the pose of visual objects in an image, and they use the lengths of these vectors to encode the probabilities that objects are present. Probabilities from different capsules can also be propagated through deep multilayer networks to model the part-whole relationships of more complex objects. Notwithstanding the promise of these networks, there still remains much to understand about capsules as primitive computing elements in their own right. In this letter, we study the problem of capsule regression-a higher-dimensional analog of logistic, probit, and softmax regression in which class probabilities are derived from vectors of competing magnitude. To start, we propose a simple capsule architecture for multinomial classification: the architecture has one capsule per class, and each capsule uses a weight matrix to compute the vector of hidden unit activities for patterns it seeks to recognize. Next, we show how to model these hidden unit activities as latent variables, and we use a squashing nonlinearity to convert their magnitudes as vectors into normalized probabilities for multinomial classification. When different capsules compete to recognize the same pattern, the squashing nonlinearity induces nongaussian terms in the posterior distribution over their latent variables. Nevertheless, we show that exact inference remains tractable and use an expectation-maximization procedure to derive least-squares updates for each capsule's weight matrix. We also present experimental results to demonstrate how these ideas work in practice."}], "ArticleTitle": "An EM Algorithm for Capsule Regression."}, "32433902": {"mesh": ["Animals", "Brain", "Exercise Test", "Linear Models", "Memory", "Neural Networks, Computer", "Neurons", "Rats"], "AbstractText": [{"section": null, "text": "Sequential neural activity has been observed in many parts of the brain and has been proposed as a neural mechanism for memory. The natural world expresses temporal relationships at a wide range of scales. Because we cannot know the relevant scales a priori, it is desirable that memory, and thus the generated sequences, is scale invariant. Although recurrent neural network models have been proposed as a mechanism for generating sequences, the requirements for scale-invariant sequences are not known. This letter reports the constraints that enable a linear recurrent neural network model to generate scale-invariant sequential activity. A straightforward eigendecomposition analysis results in two independent conditions that are required for scale invariance for connectivity matrices with real, distinct eigenvalues. First, the eigenvalues of the network must be geometrically spaced. Second, the eigenvectors must be related to one another via translation. These constraints are easily generalizable for matrices that have complex and distinct eigenvalues. Analogous albeit less compact constraints hold for matrices with degenerate eigenvalues. These constraints, along with considerations on initial conditions, provide a general recipe to build linear recurrent neural networks that support scale-invariant sequential activity."}], "ArticleTitle": "Generation of Scale-Invariant Sequential Activity in Linear Recurrent Networks."}, "31260388": {"mesh": [], "AbstractText": [{"section": null, "text": "We investigate the complexity of logistic regression models, which is defined by counting the number of indistinguishable distributions that the model can represent (Balasubramanian, 1997). We find that the complexity of logistic models with binary inputs depends not only on the number of parameters but also on the distribution of inputs in a nontrivial way that standard treatments of complexity do not address. In particular, we observe that correlations among inputs induce effective dependencies among parameters, thus constraining the model and, consequently, reducing its complexity. We derive simple relations for the upper and lower bounds of the complexity. Furthermore, we show analytically that defining the model parameters on a finite support rather than the entire axis decreases the complexity in a manner that critically depends on the size of the domain. Based on our findings, we propose a novel model selection criterion that takes into account the entropy of the input distribution. We test our proposal on the problem of selecting the input variables of a logistic regression model in a Bayesian model selection framework. In our numerical tests, we find that while the reconstruction errors of standard model selection approaches (AIC, BIC, &#8467;1 regularization) strongly depend on the sparsity of the ground truth, the reconstruction error of our method is always close to the minimum in all conditions of sparsity, data size, and strength of input correlations. Finally, we observe that when considering categorical instead of binary inputs, in a simple and mathematically tractable case, the contribution of the alphabet size to the complexity is very small compared to that of parameter space dimension. We further explore the issue by analyzing the data set of the \"13 keys to the White House,\" a method for forecasting the outcomes of US presidential elections."}], "ArticleTitle": "On the Complexity of Logistic Regression Models."}, "31703175": {"mesh": [], "AbstractText": [{"section": null, "text": "Catastrophic forgetting and capacity saturation are the central challenges of any parametric lifelong learning system. In this work, we study these challenges in the context of sequential supervised learning with an emphasis on recurrent neural networks. To evaluate the models in the lifelong learning setting, we propose a curriculum-based, simple, and intuitive benchmark where the models are trained on tasks with increasing levels of difficulty. To measure the impact of catastrophic forgetting, the model is tested on all the previous tasks as it completes any task. As a step toward developing true lifelong learning systems, we unify gradient episodic memory (a catastrophic forgetting alleviation approach) and Net2Net (a capacity expansion approach). Both models are proposed in the context of feedforward networks, and we evaluate the feasibility of using them for recurrent networks. Evaluation on the proposed benchmark shows that the unified model is more suitable than the constituent models for lifelong learning setting."}], "ArticleTitle": "Toward Training Recurrent Neural Networks for Lifelong Learning."}, "30645178": {"mesh": [], "AbstractText": [{"section": null, "text": "Recently, graph-based unsupervised feature selection algorithms (GUFS) have been shown to efficiently handle prevalent high-dimensional unlabeled data. One common drawback associated with existing graph-based approaches is that they tend to be time-consuming and in need of large storage, especially when faced with the increasing size of data. Research has started using anchors to accelerate graph-based learning model for feature selection, while the hard linear constraint between the data matrix and the lower-dimensional representation is usually overstrict in many applications. In this letter, we propose a flexible linearization model with anchor graph and <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msub><mml:mi>&#8467;</mml:mi><mml:mn>21</mml:mn></mml:msub></mml:math> -norm regularization, which can deal with large-scale data sets and improve the performance of the existing anchor-based method. In addition, the anchor-based graph Laplacian is constructed to characterize the manifold embedding structure by means of a parameter-free adaptive neighbor assignment strategy. An efficient iterative algorithm is developed to address the optimization problem, and we also prove the convergence of the algorithm. Experiments on several public data sets demonstrate the effectiveness and efficiency of the method we propose."}], "ArticleTitle": "Scalable and Flexible Unsupervised Feature Selection."}, "32069177": {"mesh": [], "AbstractText": [{"section": null, "text": "The coding of line orientation in the visual system has been investigated extensively. During the prolonged viewing of a stimulus, the perceived orientation continuously changes (normalization effect). Also, the orientation of the adapting stimulus and the background stimuli influence the perceived orientation of the subsequently displayed stimulus: tilt after-effect (TAE) or tilt illusion (TI). The neural mechanisms of these effects are not fully understood. The proposed model includes many local analyzers, each consisting of two sets of neurons. The first set has two independent cardinal detectors (CDs), whose responses depend on stimulus orientation. The second set has many orientation detectors (OD) tuned to different orientations of the stimulus. The ODs sum up the responses of the two CDs with respective weightings and output a preferred orientation depending on the ratio of CD responses. It is suggested that during prolonged viewing, the responses of the CDs decrease: the greater the excitation of the detector, the more rapid the decrease in its response. Thereby, the ratio of CD responses changes during the adaptation, causing the normalization effect and the TAE. The CDs of the different local analyzers laterally inhibit each other and cause the TI. We show that the properties of this model are consistent with both psychophysical and neurophysiological findings related to the properties of orientation perception, and we investigate how these mechanisms can affect the orientation's sensitivity."}], "ArticleTitle": "Neural Model of Coding Stimulus Orientation and Adaptation."}, "31835006": {"mesh": ["Algorithms", "Animals", "Biometry", "Haplorhini", "Humans", "Neurons", "Pattern Recognition, Visual", "Photic Stimulation", "Recognition, Psychology"], "AbstractText": [{"section": null, "text": "Neurons selective for faces exist in humans and monkeys. However, characteristics of face cell receptive fields are poorly understood. In this theoretical study, we explore the effects of complexity, defined as algorithmic information (Kolmogorov complexity) and logical depth, on possible ways that face cells may be organized. We use tensor decompositions to decompose faces into a set of components, called tensorfaces, and their associated weights, which can be interpreted as model face cells and their firing rates. These tensorfaces form a high-dimensional representation space in which each tensorface forms an axis of the space. A distinctive feature of the decomposition algorithm is the ability to specify tensorface complexity. We found that low-complexity tensorfaces have blob-like appearances crudely approximating faces, while high-complexity tensorfaces appear clearly face-like. Low-complexity tensorfaces require a larger population to reach a criterion face reconstruction error than medium- or high-complexity tensorfaces, and thus are inefficient by that criterion. Low-complexity tensorfaces, however, generalize better when representing statistically novel faces, which are faces falling beyond the distribution of face description parameters found in the tensorface training set. The degree to which face representations are parts based or global forms a continuum as a function of tensorface complexity, with low and medium tensorfaces being more parts based. Given the computational load imposed in creating high-complexity face cells (in the form of algorithmic information and logical depth) and in the absence of a compelling advantage to using high-complexity cells, we suggest face representations consist of a mixture of low- and medium-complexity face cells."}], "ArticleTitle": "Face Representations via Tensorfaces of Various Complexities."}, "33080157": {"mesh": ["Algorithms", "Machine Learning", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Recent advances in weakly supervised classification allow us to train a classifier from only positive and unlabeled (PU) data. However, existing PU classification methods typically require an accurate estimate of the class-prior probability, a critical bottleneck particularly for high-dimensional data. This problem has been commonly addressed by applying principal component analysis in advance, but such unsupervised dimension reduction can collapse the underlying class structure. In this letter, we propose a novel representation learning method from PU data based on the information-maximization principle. Our method does not require class-prior estimation and thus can be used as a preprocessing method for PU classification. Through experiments, we demonstrate that our method, combined with deep neural networks, highly improves the accuracy of PU class-prior estimation, leading to state-of-the-art PU classification performance."}], "ArticleTitle": "Information-Theoretic Representation Learning for Positive-Unlabeled Classification."}, "32687772": {"mesh": [], "AbstractText": [{"section": null, "text": "In the real world, a limited number of labeled finely grained images per class can hardly represent the class distribution effectively. Due to the more subtle visual differences in fine-grained images than simple images with obvious objects, that is, there exist smaller interclass and larger intraclass variations. To solve these issues, we propose an end-to-end attention-based model for fine-grained few-shot image classification (AFG) with the recent episode training strategy. It is composed mainly of a feature learning module, an image reconstruction module, and a label distribution module. The feature learning module mainly devises a 3D-Attention mechanism, which considers both the spatial positions and different channel attentions of the image features, in order to learn more discriminative local features to better represent the class distribution. The image reconstruction module calculates the mappings between local features and the original images. It is constrained by a designed loss function as auxiliary supervised information, so that the learning of each local feature does not need extra annotations. The label distribution module is used to predict the label distribution of a given unlabeled sample, and we use the local features to represent the image features for classification. By conducting comprehensive experiments on Mini-ImageNet and three fine-grained data sets, we demonstrate that the proposed model achieves superior performance over the competitors."}], "ArticleTitle": "Fine-Grained 3D-Attention Prototypes for Few-Shot Learning."}, "31335294": {"mesh": ["Action Potentials", "Animals", "Nerve Net", "Neural Networks, Computer", "Neurons", "Somatosensory Cortex", "Synapses"], "AbstractText": [{"section": null, "text": "Behavior is controlled by complex neural networks in which neurons process thousands of inputs. However, even short spike trains evoked in a single cortical neuron were demonstrated to be sufficient to influence behavior in vivo. Specifically, irregular sequences of interspike intervals (ISIs) had a more reliable influence on behavior despite their resemblance to stochastic activity. Similarly, irregular tactile stimulation led to higher rates of behavioral responses. In this study, we identify the mechanisms enabling this sensitivity to stimulus irregularity (SSI) on the neuronal and network levels using simulated spiking neural networks. Matching in vivo experiments, we find that irregular stimulation elicits more detectable network events (bursts) than regular stimulation. Dissecting the stimuli, we identify short ISIs-occurring more frequently in irregular stimulations-as the main drivers of SSI rather than complex irregularity per se. In addition, we find that short-term plasticity modulates SSI. We subsequently eliminate the different mechanisms in turn to assess their role in generating SSI. Removing inhibitory interneurons, we find that SSI is retained, suggesting that SSI is not dependent on inhibition. Removing recurrency, we find that SSI is retained due to the ability of individual neurons to integrate activity over short timescales (\"cell memory\"). Removing single-neuron dynamics, we find that SSI is retained based on the short-term retention of activity within the recurrent network structure (\"network memory\"). Finally, using a further simplified probabilistic model, we find that local network structure is not required for SSI. Hence, SSI is identified as a general property that we hypothesize to be ubiquitous in neural networks with different structures and biophysical properties. Irregular sequences contain shorter ISIs, which are the main drivers underlying SSI. The experimentally observed SSI should thus generalize to other systems, suggesting a functional role for irregular activity in cortex."}], "ArticleTitle": "Sensitivity to Stimulus Irregularity Is Inherent in Neural Networks."}, "32795234": {"mesh": ["Discrimination Learning", "Forecasting", "Humans", "Neural Networks, Computer", "Pattern Recognition, Automated"], "AbstractText": [{"section": null, "text": "Predictive coding (PC) networks are a biologically interesting class of neural networks. Their layered hierarchy mimics the reciprocal connectivity pattern observed in the mammalian cortex, and they can be trained using local learning rules that approximate backpropagation (Bogacz, 2017). However, despite having feedback connections that enable information to flow down the network hierarchy, discriminative PC networks are not typically generative. Clamping the output class and running the network to equilibrium yields an input sample that usually does not resemble the training input. This letter studies this phenomenon and proposes a simple solution that promotes the generation of input samples that resemble the training inputs. Simple decay, a technique already in wide use in neural networks, pushes the PC network toward a unique minimum two-norm solution, and that unique solution provably (for linear networks) matches the training inputs. The method also vastly improves the samples generated for nonlinear networks, as we demonstrate on MNIST."}], "ArticleTitle": "A Predictive-Coding Network That Is Both Discriminative and Generative."}, "30764739": {"mesh": ["Animals", "Anticipation, Psychological", "Brain", "Computer Simulation", "Decision Making", "Humans", "Machine Learning", "Memory", "Models, Neurological", "Models, Psychological", "Reinforcement, Psychology", "Time", "Time Perception"], "AbstractText": [{"section": null, "text": "Natural learners must compute an estimate of future outcomes that follow from a stimulus in continuous time. Widely used reinforcement learning algorithms discretize continuous time and estimate either transition functions from one step to the next (model-based algorithms) or a scalar value of exponentially discounted future reward using the Bellman equation (model-free algorithms). An important drawback of model-based algorithms is that computational cost grows linearly with the amount of time to be simulated. An important drawback of model-free algorithms is the need to select a timescale required for exponential discounting. We present a computational mechanism, developed based on work in psychology and neuroscience, for computing a scale-invariant timeline of future outcomes. This mechanism efficiently computes an estimate of inputs as a function of future time on a logarithmically compressed scale and can be used to generate a scale-invariant power-law-discounted estimate of expected future reward. The representation of future time retains information about what will happen when. The entire timeline can be constructed in a single parallel operation that generates concrete behavioral and neural predictions. This computational mechanism could be incorporated into future reinforcement learning algorithms."}], "ArticleTitle": "Estimating Scale-Invariant Future in Continuous Time."}, "31614101": {"mesh": [], "AbstractText": [{"section": null, "text": "Distance metric learning has been widely used to obtain the optimal distance function based on the given training data. We focus on a triplet-based loss function, which imposes a penalty such that a pair of instances in the same class is closer than a pair in different classes. However, the number of possible triplets can be quite large even for a small data set, and this considerably increases the computational cost for metric optimization. In this letter, we propose safe triplet screening that identifies triplets that can be safely removed from the optimization problem without losing the optimality. In comparison with existing safe screening studies, triplet screening is particularly significant because of the huge number of possible triplets and the semidefinite constraint in the optimization problem. We demonstrate and verify the effectiveness of our screening rules by using several benchmark data sets."}], "ArticleTitle": "Safe Triplet Screening for Distance Metric Learning."}, "32521217": {"mesh": [], "AbstractText": [{"section": null, "text": "We propose a new formulation of multiple-instance learning (MIL), in which a unit of data consists of a set of instances called a bag. The goal is to find a good classifier of bags based on the similarity with a \"shapelet\" (or pattern), where the similarity of a bag with a shapelet is the maximum similarity of instances in the bag. In previous work, some of the training instances have been chosen as shapelets with no theoretical justification. In our formulation, we use all possible, and thus infinitely many, shapelets, resulting in a richer class of classifiers. We show that the formulation is tractable, that is, it can be reduced through linear programming boosting (LPBoost) to difference of convex (DC) programs of finite (actually polynomial) size. Our theoretical result also gives justification to the heuristics of some previous work. The time complexity of the proposed algorithm highly depends on the size of the set of all instances in the training sample. To apply to the data containing a large number of instances, we also propose a heuristic option of the algorithm without the loss of the theoretical guarantee. Our empirical study demonstrates that our algorithm uniformly works for shapelet learning tasks on time-series classification and various MIL tasks with comparable accuracy to the existing methods. Moreover, we show that the proposed heuristics allow us to achieve the result in reasonable computational time."}], "ArticleTitle": "Theory and Algorithms for Shapelet-Based Multiple-Instance Learning."}, "31703171": {"mesh": [], "AbstractText": [{"section": null, "text": "Gated working memory is defined as the capacity of holding arbitrary information at any time in order to be used at a later time. Based on electrophysiological recordings, several computational models have tackled the problem using dedicated and explicit mechanisms. We propose instead to consider an implicit mechanism based on a random recurrent neural network. We introduce a robust yet simple reservoir model of gated working memory with instantaneous updates. The model is able to store an arbitrary real value at random time over an extended period of time. The dynamics of the model is a line attractor that learns to exploit reentry and a nonlinearity during the training phase using only a few representative values. A deeper study of the model shows that there is actually a large range of hyperparameters for which the results hold (e.g., number of neurons, sparsity, global weight scaling) such that any large enough population, mixing excitatory and inhibitory neurons, can quickly learn to realize such gated working memory. In a nutshell, with a minimal set of hypotheses, we show that we can have a robust model of working memory. This suggests this property could be an implicit property of any random population, that can be acquired through learning. Furthermore, considering working memory to be a physically open but functionally closed system, we give account on some counterintuitive electrophysiological recordings."}], "ArticleTitle": "A Robust Model of Gated Working Memory."}, "31260392": {"mesh": ["Animals", "Macaca", "Models, Neurological", "Neural Networks, Computer", "Neural Pathways", "Neurons", "Visual Cortex"], "AbstractText": [{"section": null, "text": "Deep convolutional neural networks (CNNs) have certain structural, mechanistic, representational, and functional parallels with primate visual cortex and also many differences. However, perhaps some of the differences can be reconciled. This study develops a cortex-like CNN architecture, via (1) a loss function that quantifies the consistency of a CNN architecture with neural data from tract tracing, cell reconstruction, and electrophysiology studies; (2) a hyperparameter-optimization approach for reducing this loss, and (3) heuristics for organizing units into convolutional-layer grids. The optimized hyperparameters are consistent with neural data. The cortex-like architecture differs from typical CNN architectures. In particular, it has longer skip connections, larger kernels and strides, and qualitatively different connection sparsity. Importantly, layers of the cortex-like network have one-to-one correspondences with cortical neuron populations. This should allow unambiguous comparison of model and brain representations in the future and, consequently, more precise measurement of progress toward more biologically realistic deep networks."}], "ArticleTitle": "Approximating the Architecture of Visual Cortex in a Convolutional Network."}, "30021083": {"mesh": [], "AbstractText": [{"section": null, "text": "Neural-inspired spike-based computing machines often claim to achieve considerable advantages in terms of energy and time efficiency by using spikes for computation and communication. However, fundamental questions about spike-based computation remain unanswered. For instance, how much advantage do spike-based approaches have over conventional methods, and under what circumstances does spike-based computing provide a comparative advantage? Simply implementing existing algorithms using spikes as the medium of computation and communication is not guaranteed to yield an advantage. Here, we demonstrate that spike-based communication and computation within algorithms can increase throughput, and they can decrease energy cost in some cases. We present several spiking algorithms, including sorting a set of numbers in ascending/descending order, as well as finding the maximum or minimum or median of a set of numbers. We also provide an example application: a spiking median-filtering approach for image processing providing a low-energy, parallel implementation. The algorithms and analyses presented here demonstrate that spiking algorithms can provide performance advantages and offer efficient computation of fundamental operations useful in more complex algorithms."}], "ArticleTitle": "Computing with Spikes: The Advantage of Fine-Grained Timing."}, "32343648": {"mesh": [], "AbstractText": [{"section": null, "text": "Sparse regularization such as &#8467;1 regularization is a quite powerful and widely used strategy for high-dimensional learning problems. The effectiveness of sparse regularization has been supported practically and theoretically by several studies. However, one of the biggest issues in sparse regularization is that its performance is quite sensitive to correlations between features. Ordinary &#8467;1 regularization selects variables correlated with each other under weak regularizations, which results in deterioration of not only its estimation error but also interpretability. In this letter, we propose a new regularization method, independently interpretable lasso (IILasso), for generalized linear models. Our proposed regularizer suppresses selecting correlated variables, so that each active variable affects the response independently in the model. Hence, we can interpret regression coefficients intuitively, and the performance is also improved by avoiding overfitting. We analyze the theoretical property of the IILasso and show that the proposed method is advantageous for its sign recovery and achieves almost minimax optimal convergence rate. Synthetic and real data analyses also indicate the effectiveness of the IILasso."}], "ArticleTitle": "Independently Interpretable Lasso for Generalized Linear Models."}, "33080159": {"mesh": ["Animals", "Attention", "Humans", "Learning", "Memory, Short-Term", "Models, Neurological", "Neural Networks, Computer", "Reinforcement, Psychology", "Sensory Gating"], "AbstractText": [{"section": null, "text": "Working memory is essential: it serves to guide intelligent behavior of humans and nonhuman primates when task-relevant stimuli are no longer present to the senses. Moreover, complex tasks often require that multiple working memory representations can be flexibly and independently maintained, prioritized, and updated according to changing task demands. Thus far, neural network models of working memory have been unable to offer an integrative account of how such control mechanisms can be acquired in a biologically plausible manner. Here, we present WorkMATe, a neural network architecture that models cognitive control over working memory content and learns the appropriate control operations needed to solve complex working memory tasks. Key components of the model include a gated memory circuit that is controlled by internal actions, encoding sensory information through untrained connections, and a neural circuit that matches sensory inputs to memory content. The network is trained by means of a biologically plausible reinforcement learning rule that relies on attentional feedback and reward prediction errors to guide synaptic updates. We demonstrate that the model successfully acquires policies to solve classical working memory tasks, such as delayed recognition and delayed pro-saccade/anti-saccade tasks. In addition, the model solves much more complex tasks, including the hierarchical 12-AX task or the ABAB ordered recognition task, both of which demand an agent to independently store and updated multiple items separately in memory. Furthermore, the control strategies that the model acquires for these tasks subsequently generalize to new task contexts with novel stimuli, thus bringing symbolic production rule qualities to a neural network architecture. As such, WorkMATe provides a new solution for the neural implementation of flexible memory control."}], "ArticleTitle": "Flexible Working Memory Through Selective Gating and Attentional Tagging."}, "29342399": {"mesh": ["Algorithms", "Computer Simulation", "Humans", "Information Theory", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "While Shannon's mutual information has widespread applications in many disciplines, for practical applications it is often difficult to calculate its value accurately for high-dimensional variables because of the curse of dimensionality. This article focuses on effective approximation methods for evaluating mutual information in the context of neural population coding. For large but finite neural populations, we derive several information-theoretic asymptotic bounds and approximation formulas that remain valid in high-dimensional spaces. We prove that optimizing the population density distribution based on these approximation formulas is a convex optimization problem that allows efficient numerical solutions. Numerical simulation results confirmed that our asymptotic formulas were highly accurate for approximating mutual information for large neural populations. In special cases, the approximation formulas are exactly equal to the true mutual information. We also discuss techniques of variable transformation and dimensionality reduction to facilitate computation of the approximations."}], "ArticleTitle": "Information-Theoretic Bounds and Approximations in Neural Population Coding."}, "31614108": {"mesh": ["Animals", "Computer Simulation", "Entorhinal Cortex", "Grid Cells", "Nerve Net", "Neural Networks, Computer", "Space Perception"], "AbstractText": [{"section": null, "text": "The way grid cells represent space in the rodent brain has been a striking discovery, with theoretical implications still unclear. Unlike hippocampal place cells, which are known to encode multiple, environment-dependent spatial maps, grid cells have been widely believed to encode space through a single low-dimensional manifold, in which coactivity relations between different neurons are preserved when the environment is changed. Does it have to be so? Here, we compute, using two alternative mathematical models, the storage capacity of a population of grid-like units, embedded in a continuous attractor neural network, for multiple spatial maps. We show that distinct representations of multiple environments can coexist, as existing models for grid cells have the potential to express several sets of hexagonal grid patterns, challenging the view of a universal grid map. This suggests that a population of grid cells can encode multiple noncongruent metric relationships, a feature that could in principle allow a grid-like code to represent environments with a variety of different geometries and possibly conceptual and cognitive spaces, which may be expected to entail such context-dependent metric relationships."}], "ArticleTitle": "Can Grid Cell Ensembles Represent Multiple Spaces?"}, "31393824": {"mesh": ["Brain", "Entropy", "Female", "Humans", "Learning", "Male", "Models, Neurological", "Motor Skills", "Young Adult"], "AbstractText": [{"section": null, "text": "Even highly trained behaviors demonstrate variability, which is correlated with performance on current and future tasks. An objective of motor learning that is general enough to explain these phenomena has not been precisely formulated. In this six-week longitudinal learning study, participants practiced a set of motor sequences each day, and neuroimaging data were collected on days 1, 14, 28, and 42 to capture the neural correlates of the learning process. In our analysis, we first modeled the underlying neural and behavioral dynamics during learning. Our results demonstrate that the densities of whole-brain response, task-active regional response, and behavioral performance evolve according to a Fokker-Planck equation during the acquisition of a motor skill. We show that this implies that the brain concurrently optimizes the entropy of a joint density over neural response and behavior (as measured by sampling over multiple trials and subjects) and the expected performance under this density; we call this formulation of learning minimum free energy learning (MFEL). This model provides an explanation as to how behavioral variability can be tuned while simultaneously improving performance during learning. We then develop a novel variant of inverse reinforcement learning to retrieve the cost function optimized by the brain during the learning process, as well as the parameter used to tune variability. We show that this population-level analysis can be used to derive a learning objective that each subject optimizes during his or her study. In this way, MFEL effectively acts as a unifying principle, allowing users to precisely formulate learning objectives and infer their structure."}], "ArticleTitle": "A Minimum Free Energy Model of Motor Learning."}, "29652582": {"mesh": [], "AbstractText": [{"section": null, "text": "Minimax similarity stresses the connectedness of points via mediating elements rather than favoring high mutual similarity. The grouping principle yields superior clustering results when mining arbitrarily-shaped clusters in data. However, it is not robust against noises and outliers in the data. There are two main problems with the grouping principle: first, a single object that is far away from all other objects defines a separate cluster, and second, two connected clusters would be regarded as two parts of one cluster. In order to solve such problems, we propose robust minimum spanning tree (MST)-based clustering algorithm in this letter. First, we separate the connected objects by applying a density-based coarsening phase, resulting in a low-rank matrix in which the element denotes the supernode by combining a set of nodes. Then a greedy method is presented to partition those supernodes through working on the low-rank matrix. Instead of removing the longest edges from MST, our algorithm groups the data set based on the minimax similarity. Finally, the assignment of all data points can be achieved through their corresponding supernodes. Experimental results on many synthetic and real-world data sets show that our algorithm consistently outperforms compared clustering algorithms."}], "ArticleTitle": "Robust MST-Based Clustering Algorithm."}, "30764740": {"mesh": ["Algorithms", "Bayes Theorem", "Computer Simulation", "Computers", "Models, Theoretical", "Nonlinear Dynamics"], "AbstractText": [{"section": null, "text": "The need to reason about uncertainty in large, complex, and multimodal data sets has become increasingly common across modern scientific environments. The ability to transform samples from one distribution <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>P</mml:mi></mml:math> to another distribution <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>Q</mml:mi></mml:math> enables the solution to many problems in machine learning (e.g., Bayesian inference, generative modeling) and has been actively pursued from theoretical, computational, and application perspectives across the fields of information theory, computer science, and biology. Performing such transformations in general still leads to computational difficulties, especially in high dimensions. Here, we consider the problem of computing such \"measure transport maps\" with efficient and parallelizable methods. Under the mild assumptions that <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>P</mml:mi></mml:math> need not be known but can be sampled from and that the density of <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>Q</mml:mi></mml:math> is known up to a proportionality constant, and that <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>Q</mml:mi></mml:math> is log-concave, we provide in this work a convex optimization problem pertaining to relative entropy minimization. We show how an empirical minimization formulation and polynomial chaos map parameterization can allow for learning a transport map between <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>P</mml:mi></mml:math> and <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>Q</mml:mi></mml:math> with distributed and scalable methods. We also leverage findings from nonequilibrium thermodynamics to represent the transport map as a composition of simpler maps, each of which is learned sequentially with a transport cost regularized version of the aforementioned problem formulation. We provide examples of our framework within the context of Bayesian inference for the Boston housing data set and generative modeling for handwritten digit images from the MNIST data set."}], "ArticleTitle": "A Distributed Framework for the Construction of Transport Maps."}, "32946711": {"mesh": ["Computer Simulation", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "A complex-valued Hopfield neural network (CHNN) with a multistate activation function is a multistate model of neural associative memory. The weight parameters need a lot of memory resources. Twin-multistate activation functions were introduced to quaternion- and bicomplex-valued Hopfield neural networks. Since their architectures are much more complicated than that of CHNN, the architecture should be simplified. In this work, the number of weight parameters is reduced by bicomplex projection rule for CHNNs, which is given by the decomposition of bicomplex-valued Hopfield neural networks. Computer simulations support that the noise tolerance of CHNN with a bicomplex projection rule is equal to or even better than that of quaternion- and bicomplex-valued Hopfield neural networks. By computer simulations, we find that the projection rule for hyperbolic-valued Hopfield neural networks in synchronous mode maintains a high noise tolerance."}], "ArticleTitle": "Bicomplex Projection Rule for Complex-Valued Hopfield Neural Networks."}, "32521213": {"mesh": ["Adult", "Algorithms", "Brain", "Electroencephalography", "Female", "Humans", "Male", "Mental Fatigue", "Reaction Time", "Signal Processing, Computer-Assisted"], "AbstractText": [{"section": null, "text": "A driver's cognitive state of mental fatigue significantly affects his or her driving performance and more important, public safety. Previous studies have leveraged reaction time (RT) as the metric for mental fatigue and aim at estimating the exact value of RT using electroencephalogram (EEG) signals within a regression model. However, due to the easily corrupted and also nonsmooth properties of RTs during data collection, methods focusing on predicting the exact value of a noisy measurement, RT generally suffer from poor generalization performance. Considering that human RT is the reflection of brain dynamics preference (BDP) rather than a single regression output of EEG signals, we propose a novel channel-reliability-aware ranking (CArank) model for the multichannel ranking problem. CArank learns from BDPs using EEG data robustly and aims at preserving the ordering corresponding to RTs. In particular, we introduce a transition matrix to characterize the reliability of each channel used in the EEG data, which helps in learning with BDPs only from informative EEG channels. To handle large-scale EEG signals, we propose a stochastic-generalized expectation maximum (SGEM) algorithm to update CArank in an online fashion. Comprehensive empirical analysis on EEG signals from 40 participants shows that our CArank achieves substantial improvements in reliability while simultaneously detecting noisy or less informative EEG channels."}], "ArticleTitle": "Stochastic Multichannel Ranking with Brain Dynamics Preferences."}, "30764742": {"mesh": ["Computer Simulation", "Humans", "Language", "Learning", "Logic", "Memory", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "We present a novel recurrent neural network (RNN)-based model that combines the remembering ability of unitary evolution RNNs with the ability of gated RNNs to effectively forget redundant or irrelevant information in its memory. We achieve this by extending restricted orthogonal evolution RNNs with a gating mechanism similar to gated recurrent unit RNNs with a reset gate and an update gate. Our model is able to outperform long short-term memory, gated recurrent units, and vanilla unitary or orthogonal RNNs on several long-term-dependency benchmark tasks. We empirically show that both orthogonal and unitary RNNs lack the ability to forget. This ability plays an important role in RNNs. We provide competitive results along with an analysis of our model on many natural sequential tasks, including question answering, speech spectrum prediction, character-level language modeling, and synthetic tasks that involve long-term dependencies such as algorithmic, denoising, and copying tasks."}], "ArticleTitle": "Gated Orthogonal Recurrent Units: On Learning to Forget."}, "28957019": {"mesh": ["Algorithms", "Brain", "Brain-Computer Interfaces", "Humans", "Linear Models", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Decoding in the context of brain-machine interface is a prediction problem, with the aim of retrieving the most accurate kinematic predictions attainable from the available neural signals. While selecting models that reduce the prediction error is done to various degrees, decoding has not received the attention that the fields of statistics and machine learning have lavished on the prediction problem in the past two decades. Here, we take a more systematic approach to the decoding prediction problem and search for risk-optimized reverse regression, optimal linear estimation (OLE), and Kalman filter models within a large model space composed of several nonlinear transformations of neural spike counts at multiple temporal lags. The reverse regression decoding framework is a standard prediction problem, where penalized methods such as ridge regression or Lasso are routinely used to find minimum risk models. We argue that minimum risk reverse regression is always more efficient than OLE and also happens to be 44% more efficient than a standard Kalman filter in a particular application of offline reconstruction of arm reaches of a rhesus macaque monkey. Yet model selection for tuning curves-based decoding models such as OLE and Kalman filtering is not a standard statistical prediction problem, and no efficient method exists to identify minimum risk models. We apply several methods to build low-risk models and show that in our application, a Kalman filter that includes multiple carefully chosen observation equations per neural unit is 67% more efficient than a standard Kalman filter, but with the drawback that finding such a model is computationally very costly."}], "ArticleTitle": "Neural Decoding: A Predictive Viewpoint."}, "29381445": {"mesh": [], "AbstractText": [{"section": null, "text": "By controlling the state of neuronal populations, neuromodulators ultimately affect behavior. A key neuromodulation mechanism is the alteration of neuronal excitability via the modulation of ion channel expression. This type of neuromodulation is normally studied with conductance-based models, but those models are computationally challenging for large-scale network simulations needed in population studies. This article studies the modulation properties of the multiquadratic integrate-and-fire model, a generalization of the classical quadratic integrate-and-fire model. The model is shown to combine the computational economy of integrate-and-fire modeling and the physiological interpretability of conductance-based modeling. It is therefore a good candidate for affordable computational studies of neuromodulation in large networks."}], "ArticleTitle": "Robust Modulation of Integrate-and-Fire Models."}, "31614103": {"mesh": [], "AbstractText": [{"section": null, "text": "Winner-take-all (WTA) refers to the neural operation that selects a (typically small) group of neurons from a large neuron pool. It is conjectured to underlie many of the brain's fundamental computational abilities. However, not much is known about the robustness of a spike-based WTA network to the inherent randomness of the input spike trains. In this work, we consider a spike-based k-WTA model wherein n randomly generated input spike trains compete with each other based on their underlying firing rates and k winners are supposed to be selected. We slot the time evenly with each time slot of length 1 ms and model the n input spike trains as n independent Bernoulli processes. We analytically characterize the minimum waiting time needed so that a target minimax decision accuracy (success probability) can be reached. We first derive an information-theoretic lower bound on the waiting time. We show that to guarantee a (minimax) decision error &#8804;&#948; (where &#948;&#8712;(0,1)), the waiting time of any WTA circuit is at least [Formula: see text]where R&#8838;(0,1) is a finite set of rates and TR is a difficulty parameter of a WTA task with respect to set R for independent input spike trains. Additionally, TR is independent of &#948;, n, and k. We then design a simple WTA circuit whose waiting time is [Formula: see text]provided that the local memory of each output neuron is sufficiently long. It turns out that for any fixed &#948;, this decision time is order-optimal (i.e., it matches the above lower bound up to a multiplicative constant factor) in terms of its scaling in n, k, and TR."}], "ArticleTitle": "Spike-Based Winner-Take-All Computation: Fundamental Limits and Order-Optimal Circuits."}, "31703174": {"mesh": [], "AbstractText": [{"section": null, "text": "A spiking neural network (SNN) is a type of biological plausibility model that performs information processing based on spikes. Training a deep SNN effectively is challenging due to the nondifferention of spike signals. Recent advances have shown that high-performance SNNs can be obtained by converting convolutional neural networks (CNNs). However, the large-scale SNNs are poorly served by conventional architectures due to the dynamic nature of spiking neurons. In this letter, we propose a hardware architecture to enable efficient implementation of SNNs. All layers in the network are mapped on one chip so that the computation of different time steps can be done in parallel to reduce latency. We propose new spiking max-pooling method to reduce computation complexity. In addition, we apply approaches based on shift register and coarsely grained parallels to accelerate convolution operation. We also investigate the effect of different encoding methods on SNN accuracy. Finally, we validate the hardware architecture on the Xilinx Zynq ZCU102. The experimental results on the MNIST data set show that it can achieve an accuracy of 98.94% with eight-bit quantized weights. Furthermore, it achieves 164 frames per second (FPS) under 150 MHz clock frequency and obtains 41&#215; speed-up compared to CPU implementation and 22 times lower power than GPU implementation."}], "ArticleTitle": "An FPGA Implementation of Deep Spiking Neural Networks for Low-Power and Fast Classification."}, "31703173": {"mesh": [], "AbstractText": [{"section": null, "text": "It is well known in machine learning that models trained on a training set generated by a probability distribution function perform far worse on test sets generated by a different probability distribution function. In the limit, it is feasible that a continuum of probability distribution functions might have generated the observed test set data; a desirable property of a learned model in that case is its ability to describe most of the probability distribution functions from the continuum equally well. This requirement naturally leads to sampling methods from the continuum of probability distribution functions that lead to the construction of optimal training sets. We study the sequential prediction of Ornstein-Uhlenbeck processes that form a parametric family. We find empirically that a simple deep network trained on optimally constructed training sets using the methods described in this letter can be robust to changes in the test set distribution."}], "ArticleTitle": "Optimal Sampling of Parametric Families: Implications for Machine Learning."}, "30883280": {"mesh": [], "AbstractText": [{"section": null, "text": "Decision making is a complex task, and its underlying mechanisms that regulate behavior, such as the implementation of the coupling between physiological states and neural networks, are hard to decipher. To gain more insight into neural computations underlying ongoing binary decision-making tasks, we consider a neural circuit that guides the feeding behavior of a hypothetical animal making dietary choices. We adopt an inhibition motif from neural network theory and propose a dynamical system characterized by nonlinear feedback, which links mechanism (the implementation of the neural circuit and its coupling to the animal's nutritional state) and function (improving behavioral performance). A central inhibitory unit influences evidence-integrating excitatory units, which in our terms correspond to motivations competing for selection. We determine the parameter regime where the animal exhibits improved decision-making behavior and explain different behavioral outcomes by making the link between accessible states of the nonlinear neural circuit model and decision-making performance. We find that for given deficits in nutritional items, the variation of inhibition strength and ratio of excitation and inhibition strengths in the decision circuit allows the animal to enter an oscillatory phase that describes its internal motivational state. Our findings indicate that this oscillatory phase may improve the overall performance of the animal in an ongoing foraging task and underpin the importance of an integrated functional and mechanistic study of animal activity selection."}], "ArticleTitle": "Inhibition and Excitation Shape Activity Selection: Effect of Oscillations in a Decision-Making Circuit."}, "31335290": {"mesh": [], "AbstractText": [{"section": null, "text": "This letter proposes a multichannel source separation technique, the multichannel variational autoencoder (MVAE) method, which uses a conditional VAE (CVAE) to model and estimate the power spectrograms of the sources in a mixture. By training the CVAE using the spectrograms of training examples with source-class labels, we can use the trained decoder distribution as a universal generative model capable of generating spectrograms conditioned on a specified class index. By treating the latent space variables and the class index as the unknown parameters of this generative model, we can develop a convergence-guaranteed algorithm for supervised determined source separation that consists of iteratively estimating the power spectrograms of the underlying sources, as well as the separation matrices. In experimental evaluations, our MVAE produced better separation performance than a baseline method."}], "ArticleTitle": "Supervised Determined Source Separation with Multichannel Variational Autoencoder."}, "30645180": {"mesh": ["Computer Simulation", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "This letter deals with neural networks as dynamical systems governed by finite difference equations. It shows that the introduction of <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>k</mml:mi></mml:math> -many skip connections into network architectures, such as residual networks and additive dense networks, defines <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>k</mml:mi></mml:math> th order dynamical equations on the layer-wise transformations. Closed-form solutions for the state-space representations of general <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>k</mml:mi></mml:math> th order additive dense networks, where the concatenation operation is replaced by addition, as well as <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>k</mml:mi></mml:math> th order smooth networks, are found. The developed provision endows deep neural networks with an algebraic structure. Furthermore, it is shown that imposing <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>k</mml:mi></mml:math> th order smoothness on network architectures with <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>d</mml:mi></mml:math> -many nodes per layer increases the state-space dimension by a multiple of <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>k</mml:mi></mml:math> , and so the effective embedding dimension of the data manifold by the neural network is <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:math> -many dimensions. It follows that network architectures of these types reduce the number of parameters needed to maintain the same embedding dimension by a factor of <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math> when compared to an equivalent first-order, residual network. Numerical simulations and experiments on CIFAR10, SVHN, and MNIST have been conducted to help understand the developed theory and efficacy of the proposed concepts."}], "ArticleTitle": "State-Space Representations of Deep Neural Networks."}, "32186998": {"mesh": ["Algorithms", "Artificial Intelligence", "Deep Learning", "Neural Networks, Computer", "Surveys and Questionnaires"], "AbstractText": [{"section": null, "text": "With the wide deployments of heterogeneous networks, huge amounts of data with characteristics of high volume, high variety, high velocity, and high veracity are generated. These data, referred to multimodal big data, contain abundant intermodality and cross-modality information and pose vast challenges on traditional data fusion methods. In this review, we present some pioneering deep learning models to fuse these multimodal big data. With the increasing exploration of the multimodal big data, there are still some challenges to be addressed. Thus, this review presents a survey on deep learning for multimodal data fusion to provide readers, regardless of their original community, with the fundamentals of multimodal deep learning fusion method and to motivate new multimodal data fusion techniques of deep learning. Specifically, representative architectures that are widely used are summarized as fundamental to the understanding of multimodal deep learning. Then the current pioneering multimodal data fusion deep learning models are summarized. Finally, some challenges and future topics of multimodal data fusion deep learning models are described."}], "ArticleTitle": "A Survey on Deep Learning for Multimodal Data Fusion."}, "32946708": {"mesh": [], "AbstractText": [{"section": null, "text": "A reflex is a simple closed-loop control approach that tries to minimize an error but fails to do so because it will always react too late. An adaptive algorithm can use this error to learn a forward model with the help of predictive cues. For example, a driver learns to improve steering by looking ahead to avoid steering in the last minute. In order to process complex cues such as the road ahead, deep learning is a natural choice. However, this is usually achieved only indirectly by employing deep reinforcement learning having a discrete state space. Here, we show how this can be directly achieved by embedding deep learning into a closed-loop system and preserving its continuous processing. We show in z-space specifically how error backpropagation can be achieved and in general how gradient-based approaches can be analyzed in such closed-loop scenarios. The performance of this learning paradigm is demonstrated using a line follower in simulation and on a real robot that shows very fast and continuous learning."}], "ArticleTitle": "Closed-Loop Deep Learning: Generating Forward Models With Backpropagation."}, "32521214": {"mesh": ["Humans", "Neural Networks, Computer", "Pattern Recognition, Automated"], "AbstractText": [{"section": null, "text": "Sparsity is a desirable property in many nonnegative matrix factorization (NMF) applications. Although some level of sparseness of NMF solutions can be achieved by using regularization, the resulting sparsity depends highly on the regularization parameter to be valued in an ad hoc way. In this letter we formulate sparse NMF as a mixed-integer optimization problem with sparsity as binary constraints. A discrete-time projection neural network is developed for solving the formulated problem. Sufficient conditions for its stability and convergence are analytically characterized by using Lyapunov's method. Experimental results on sparse feature extraction are discussed to substantiate the superiority of this approach to extracting highly sparse features."}], "ArticleTitle": "A Discrete-Time Neurodynamic Approach to Sparsity-Constrained Nonnegative Matrix Factorization."}, "31260393": {"mesh": ["Adult", "Computer Simulation", "Diabetes Mellitus", "Female", "Humans", "Logistic Models", "Machine Learning", "Neoplasms", "Post-Cardiac Arrest Syndrome", "Renal Insufficiency, Chronic", "Statistics, Nonparametric"], "AbstractText": [{"section": null, "text": "In this letter, we propose a variable selection method for general nonparametric kernel-based estimation. The proposed method consists of two-stage estimation: (1) construct a consistent estimator of the target function, and (2) approximate the estimator using a few variables by &#8467;1-type penalized estimation. We see that the proposed method can be applied to various kernel nonparametric estimation such as kernel ridge regression, kernel-based density, and density-ratio estimation. We prove that the proposed method has the property of variable selection consistency when the power series kernel is used. Here, the power series kernel is a certain class of kernels containing polynomial and exponential kernels. This result is regarded as an extension of the variable selection consistency for the nonnegative garrote (NNG), a special case of the adaptive Lasso, to the kernel-based estimators. Several experiments, including simulation studies and real data applications, show the effectiveness of the proposed method."}], "ArticleTitle": "Variable Selection for Nonparametric Learning with Power Series Kernels."}, "30576611": {"mesh": [], "AbstractText": [{"section": null, "text": "Recurrent backpropagation and equilibrium propagation are supervised learning algorithms for fixed-point recurrent neural networks, which differ in their second phase. In the first phase, both algorithms converge to a fixed point that corresponds to the configuration where the prediction is made. In the second phase, equilibrium propagation relaxes to another nearby fixed point corresponding to smaller prediction error, whereas recurrent backpropagation uses a side network to compute error derivatives iteratively. In this work, we establish a close connection between these two algorithms. We show that at every moment in the second phase, the temporal derivatives of the neural activities in equilibrium propagation are equal to the error derivatives computed iteratively by recurrent backpropagation in the side network. This work shows that it is not required to have a side network for the computation of error derivatives and supports the hypothesis that in biological neural networks, temporal derivatives of neural activities may code for error signals."}], "ArticleTitle": "Equivalence of Equilibrium Propagation and Recurrent Backpropagation."}, "31835002": {"mesh": [], "AbstractText": [{"section": null, "text": "Recently, a set of tensor norms known as coupled norms has been proposed as a convex solution to coupled tensor completion. Coupled norms have been designed by combining low-rank inducing tensor norms with the matrix trace norm. Though coupled norms have shown good performances, they have two major limitations: they do not have a method to control the regularization of coupled modes and uncoupled modes, and they are not optimal for couplings among higher-order tensors. In this letter, we propose a method that scales the regularization of coupled components against uncoupled components to properly induce the low-rankness on the coupled mode. We also propose coupled norms for higher-order tensors by combining the square norm to coupled norms. Using the excess risk-bound analysis, we demonstrate that our proposed methods lead to lower risk bounds compared to existing coupled norms. We demonstrate the robustness of our methods through simulation and real-data experiments."}], "ArticleTitle": "Scaled Coupled Norms and Coupled Higher-Order Tensor Completion."}, "32946716": {"mesh": [], "AbstractText": [{"section": null, "text": "Hierarchical sparse coding (HSC) is a powerful model to efficiently represent multidimensional, structured data such as images. The simplest solution to solve this computationally hard problem is to decompose it into independent layer-wise subproblems. However, neuroscientific evidence would suggest interconnecting these subproblems as in predictive coding (PC) theory, which adds top-down connections between consecutive layers. In this study, we introduce a new model, 2-layer sparse predictive coding (2L-SPC), to assess the impact of this interlayer feedback connection. In particular, the 2L-SPC is compared with a hierarchical Lasso (Hi-La) network made out of a sequence of independent Lasso layers. The 2L-SPC and a 2-layer Hi-La networks are trained on four different databases and with different sparsity parameters on each layer. First, we show that the overall prediction error generated by 2L-SPC is lower thanks to the feedback mechanism as it transfers prediction error between layers. Second, we demonstrate that the inference stage of the 2L-SPC is faster to converge and generates a refined representation in the second layer compared to the Hi-La model. Third, we show that the 2L-SPC top-down connection accelerates the learning process of the HSC problem. Finally, the analysis of the emerging dictionaries shows that the 2L-SPC features are more generic and present a larger spatial extension."}], "ArticleTitle": "Effect of Top-Down Connections in Hierarchical Sparse Coding."}, "30764745": {"mesh": [], "AbstractText": [{"section": null, "text": "Alpha integration methods have been used for integrating stochastic models and fusion in the context of detection (binary classification). Our work proposes separated score integration (SSI), a new method based on alpha integration to perform soft fusion of scores in multiclass classification problems, one of the most common problems in automatic classification. Theoretical derivation is presented to optimize the parameters of this method to achieve the least mean squared error (LMSE) or the minimum probability of error (MPE). The proposed alpha integration method was tested on several sets of simulated and real data. The first set of experiments used synthetic data to replicate a problem of automatic detection and classification of three types of ultrasonic pulses buried in noise (four-class classification). The second set of experiments analyzed two databases (one publicly available and one private) of real polysomnographic records from subjects with sleep disorders. These records were automatically staged in wake, rapid eye movement (REM) sleep, and non-REM sleep (three-class classification). Finally, the third set of experiments was performed on a publicly available database of single-channel real electroencephalographic data that included epileptic patients and healthy controls in five conditions (five-class classification). In all cases, alpha integration performed better than the considered single classifiers and classical fusion techniques."}], "ArticleTitle": "Multiclass Alpha Integration of Scores from Multiple Classifiers."}, "30462584": {"mesh": [], "AbstractText": [{"section": null, "text": "This study, which examines a calculation method on the basis of a dual neural network for solving multiple definite integrals, addresses the problems of inefficiency, inaccuracy, and difficulty in finding solutions. First, the method offers a dual neural network method to construct a primitive function of the integral problem; it can approximate the primitive function of any given integrand with any precision. On this basis, a neural network calculation method that can solve multiple definite integrals whose upper and lower bounds are arbitrarily given is obtained with repeated applications of the dual neural network to construction of the primitive function. Example simulations indicate that compared with traditional methods, the proposed algorithm is more efficient and precise in obtaining solutions for multiple integrals with unknown integrand, except for the finite input-output data points. The advantages of the proposed method include the following: (1) integral multiplicity shows no influence and restriction on the employment of the method; (2) only a finite set of known sample points is required without the need to know the exact analytical expression of the integrand; and (3) high calculation accuracy is obtained for multiple definite integrals whose integrand is expressed by sample data points."}], "ArticleTitle": "Dual Neural Network Method for Solving Multiple Definite Integrals."}, "29894653": {"mesh": ["Action Potentials", "Algorithms", "Animals", "Cluster Analysis", "Electrophysiology", "Extracellular Space", "Motor Cortex", "Neurons", "Pattern Recognition, Automated", "Rats", "Signal Processing, Computer-Assisted", "Stochastic Processes", "User-Computer Interface"], "AbstractText": [{"section": null, "text": "Electrophysiology is entering the era of big data. Multiple probes, each with hundreds to thousands of individual electrodes, are now capable of simultaneously recording from many brain regions. The major challenge confronting these new technologies is transforming the raw data into physiologically meaningful signals, that is, single unit spikes. Sorting the spike events of individual neurons from a spatiotemporally dense sampling of the extracellular electric field is a problem that has attracted much attention (Rey, Pedreira, & Quian Quiroga, 2015 ; Rossant et&#160;al., 2016 ) but is still far from solved. Current methods still rely on human input and thus become unfeasible as the size of the data sets grows exponentially. Here we introduce the [Formula: see text]-student stochastic neighbor embedding (t-SNE) dimensionality reduction method (Van der Maaten & Hinton, 2008 ) as a visualization tool in the spike sorting process. t-SNE embeds the [Formula: see text]-dimensional extracellular spikes ([Formula: see text] = number of features by which each spike is decomposed) into a low- (usually two-) dimensional space. We show that such embeddings, even starting from different feature spaces, form obvious clusters of spikes that can be easily visualized and manually delineated with a high degree of precision. We propose that these clusters represent single units and test this assertion by applying our algorithm on labeled data sets from both hybrid (Rossant et&#160;al., 2016 ) and paired juxtacellular/extracellular recordings (Neto et&#160;al., 2016 ). We have released a graphical user interface (GUI) written in Python as a tool for the manual clustering of the t-SNE embedded spikes and as a tool for an informed overview and fast manual curation of results from different clustering algorithms. Furthermore, the generated visualizations offer evidence in favor of the use of probes with higher density and smaller electrodes. They also graphically demonstrate the diverse nature of the sorting problem when spikes are recorded with different methods and arise from regions with different background spiking statistics."}], "ArticleTitle": "t-SNE Visualization of Large-Scale Neural Recordings."}, "31614099": {"mesh": ["Action Potentials", "Computer Simulation", "Models, Neurological", "Neural Networks, Computer", "Neuronal Plasticity", "Neurons", "Reinforcement, Psychology", "Synapses", "Synaptic Transmission"], "AbstractText": [{"section": null, "text": "Though succeeding in solving various learning tasks, most existing reinforcement learning (RL) models have failed to take into account the complexity of synaptic plasticity in the neural system. Models implementing reinforcement learning with spiking neurons involve only a single plasticity mechanism. Here, we propose a neural realistic reinforcement learning model that coordinates the plasticities of two types of synapses: stochastic and deterministic. The plasticity of the stochastic synapse is achieved by the hedonistic rule through modulating the release probability of synaptic neurotransmitter, while the plasticity of the deterministic synapse is achieved by a variant of a reward-modulated spike-timing-dependent plasticity rule through modulating the synaptic strengths. We evaluate the proposed learning model on two benchmark tasks: learning a logic gate function and the 19-state random walk problem. Experimental results show that the coordination of diverse synaptic plasticities can make the RL model learn in a rapid and stable form."}], "ArticleTitle": "Reinforcement Learning in Spiking Neural Networks with Stochastic and Deterministic Synapses."}, "29894649": {"mesh": ["Animals", "Computer Simulation", "Discrimination, Psychological", "Generalization, Psychological", "Humans", "Models, Neurological", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Transposition is a tendency for organisms to generalize relationships between stimuli in situations where training does not objectively reward relationships over absolute, static associations. Transposition has most commonly been explained as either conceptual understanding of relationships (K&#246;hler, 1938) as nonconceptual effects of neural memory gradients (as in Spence's stimulus discrimination theory, 1937 ). Most behavioral evidence can be explained by the gradient account, but a key finding unexplained by gradients is intermediate transposition, where a central (of three) stimulus, \"relationally correct response,\" is generalized from training to test. Here, we introduce a dynamic neural field (DNF) model that captures intermediate transposition effects while using neural mechanisms closely resembling those of Spence's original proposal. The DNF model operates on dynamic rather than linear neural relationships, but it still functions by way of gradient interactions, and it does not invoke relational conceptual understanding in order to explain transposition behaviors. In addition to intermediate transposition, the DNF model also replicates the predictions of stimulus discrimination theory with respect to basic two-stimulus transposition. Effects of wider test item spacing were additionally captured. Overall, the DNF model captures a wider range of effects in transposition than stimulus discrimination theory, uses more fully specified neural mechanics, and integrates transposition into a wider modeling effort across cognitive tasks and phenomena. At the same time, the model features a similar low-level focus and emphasis on gradient interactions as Spence's, serving as a conceptual continuation and updating of Spence's work in the field of transposition."}], "ArticleTitle": "A Dynamic Neural Gradient Model of Two-Item and Intermediate Transposition."}, "30883282": {"mesh": ["Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "We present a new binding operation, vector-derived transformation binding (VTB), for use in vector symbolic architectures (VSA). The performance of VTB is compared to circular convolution, used in holographic reduced representations (HRRs), in terms of list and stack encoding capacity. A special focus is given to the possibility of a neural implementation by the means of the Neural Engineering Framework (NEF). While the scaling of required neural resources is slightly worse for VTB, it is found to be on par with circular convolution for list encoding and better for encoding of stacks. Furthermore, VTB influences the vector length less, which also benefits a neural implementation. Consequently, we argue that VTB is an improvement over HRRs for neurally implemented VSAs."}], "ArticleTitle": "Vector-Derived Transformation Binding: An Improved Binding Operation for Deep Symbol-Like Processing in Neural Networks."}, "28562221": {"mesh": [], "AbstractText": [{"section": null, "text": "Optimal control theory and machine learning techniques are combined to formulate and solve in closed form an optimal control formulation of online learning from supervised examples with regularization of the updates. The connections with the classical linear quadratic gaussian (LQG) optimal control problem, of which the proposed learning paradigm is a nontrivial variation as it involves random matrices, are investigated. The obtained optimal solutions are compared with the Kalman filter estimate of the parameter vector to be learned. It is shown that the proposed algorithm is less sensitive to outliers with respect to the Kalman estimate (thanks to the presence of the regularization term), thus providing smoother estimates with respect to time. The basic formulation of the proposed online learning framework refers to a discrete-time setting with a finite learning horizon and a linear model. Various extensions are investigated, including the infinite learning horizon and, via the so-called kernel trick, the case of nonlinear models."}], "ArticleTitle": "LQG Online Learning."}, "28957027": {"mesh": [], "AbstractText": [{"section": null, "text": "Sparse coding algorithms with continuous latent variables have been the subject of a large number of studies. However, discrete latent spaces for sparse coding have been largely ignored. In this work, we study sparse coding with latents described by discrete instead of continuous prior distributions. We consider the general case in which the latents (while being sparse) can take on any value of a finite set of possible values and in which we learn the prior probability of any value from data. This approach can be applied to any data generated by discrete causes, and it can be applied as an approximation of continuous causes. As the prior probabilities are learned, the approach then allows for estimating the prior shape without assuming specific functional forms. To efficiently train the parameters of our probabilistic generative model, we apply a truncated expectation-maximization approach (expectation truncation) that we modify to work with a general discrete prior. We evaluate the performance of the algorithm by applying it to a variety of tasks: (1)&#160;we use artificial data to verify that the algorithm can recover the generating parameters from a random initialization, (2)&#160;use image patches of natural images and discuss the role of the prior for the extraction of image components, (3)&#160;use extracellular recordings of neurons to present a novel method of analysis for spiking neurons that includes an intuitive discretization strategy, and (4) apply the algorithm on the task of encoding audio waveforms of human speech. The diverse set of numerical experiments presented in this letter suggests that discrete sparse coding algorithms can scale efficiently to work with realistic data sets and provide novel statistical quantities to describe the structure of the data."}], "ArticleTitle": "Discrete Sparse Coding."}, "28957023": {"mesh": [], "AbstractText": [{"section": null, "text": "The hypothesis that the phasic dopamine response reports a reward prediction error has become deeply entrenched. However, dopamine neurons exhibit several notable deviations from this hypothesis. A coherent explanation for these deviations can be obtained by analyzing the dopamine response in terms of Bayesian reinforcement learning. The key idea is that prediction errors are modulated by probabilistic beliefs about the relationship between cues and outcomes, updated through Bayesian inference. This account can explain dopamine responses to inferred value in sensory preconditioning, the effects of cue preexposure (latent inhibition), and adaptive coding of prediction errors when rewards vary across orders of magnitude. We further postulate that orbitofrontal cortex transforms the stimulus representation through recurrent dynamics, such that a simple error-driven learning rule operating on the transformed representation can implement the Bayesian reinforcement learning update."}], "ArticleTitle": "Dopamine, Inference, and Uncertainty."}, "30216144": {"mesh": ["Environment", "Forecasting", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Analysis and forecasting of sequential data, key problems in various domains of engineering and science, have attracted the attention of many researchers from different communities. When predicting the future probability of events using time series, recurrent neural networks (RNNs) are an effective tool that have the learning ability of feedforward neural networks and expand their expression ability using dynamic equations. Moreover, RNNs are able to model several computational structures. Researchers have developed various RNNs with different architectures and topologies. To summarize the work of RNNs in forecasting and provide guidelines for modeling and novel applications in future studies, this review focuses on applications of RNNs for time series forecasting in environmental factor forecasting. We present the structure, processing flow, and advantages of RNNs and analyze the applications of various RNNs in time series forecasting. In addition, we discuss limitations and challenges of applications based on RNNs and future research directions. Finally, we summarize applications of RNNs in forecasting."}], "ArticleTitle": "Applications of Recurrent Neural Networks in Environmental Factor Forecasting: A Review."}, "29566354": {"mesh": [], "AbstractText": [{"section": null, "text": "Chromatically perceptive observers are endowed with a sense of similarity between colors. For example, two shades of green that are only slightly discriminable are perceived as similar, whereas other pairs of colors, for example, blue and yellow, typically elicit markedly different sensations. The notion of similarity need not be shared by different observers. Dichromat and trichromat subjects perceive colors differently, and two dichromats (or two trichromats, for that matter) may judge chromatic differences inconsistently. Moreover, there is ample evidence that different animal species sense colors diversely. To capture the subjective metric of color perception, here we construct a notion of distance in color space based on the physiology of the retina, and is thereby individually tailored for different observers. By applying the Fisher metric to an analytical model of color representation, we construct a notion of distance that reproduces behavioral experiments of classical discrimination tasks. We then derive a coordinate transformation that defines a new chromatic space in which the Euclidean distance between any two colors is equal to the perceptual distance, as seen by one individual subject, endowed with an arbitrary number of color-sensitive photoreceptors, each with arbitrary absorption probability curves and appearing in arbitrary proportions."}], "ArticleTitle": "Novel Perceptually Uniform Chromatic Space."}, "32433901": {"mesh": [], "AbstractText": [{"section": null, "text": "Simultaneous recordings from the cortex have revealed that neural activity is highly variable and that some variability is shared across neurons in a population. Further experimental work has demonstrated that the shared component of a neuronal population's variability is typically comparable to or larger than its private component. Meanwhile, an abundance of theoretical work has assessed the impact that shared variability has on a population code. For example, shared input noise is understood to have a detrimental impact on a neural population's coding fidelity. However, other contributions to variability, such as common noise, can also play a role in shaping correlated variability. We present a network of linear-nonlinear neurons in which we introduce a common noise input to model-for instance, variability resulting from upstream action potentials that are irrelevant to the task at hand. We show that by applying a heterogeneous set of synaptic weights to the neural inputs carrying the common noise, the network can improve its coding ability as measured by both Fisher information and Shannon mutual information, even in cases where this results in amplification of the common noise. With a broad and heterogeneous distribution of synaptic weights, a population of neurons can remove the harmful effects imposed by afferents that are uninformative about a stimulus. We demonstrate that some nonlinear networks benefit from weight diversification up to a certain population size, above which the drawbacks from amplified noise dominate over the benefits of diversification. We further characterize these benefits in terms of the relative strength of shared and private variability sources. Finally, we studied the asymptotic behavior of the mutual information and Fisher information analytically in our various networks as a function of population size. We find some surprising qualitative changes in the asymptotic behavior as we make seemingly minor changes in the synaptic weight distributions."}], "ArticleTitle": "Heterogeneous Synaptic Weighting Improves Neural Coding in the Presence of Common Noise."}, "31393826": {"mesh": ["Animals", "Models, Neurological", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Artificial neural networks, trained to perform cognitive tasks, have recently been used as models for neural recordings from animals performing these tasks. While some progress has been made in performing such comparisons, the evolution of network dynamics throughout learning remains unexplored. This is paralleled by an experimental focus on recording from trained animals, with few studies following neural activity throughout training. In this work, we address this gap in the realm of artificial networks by analyzing networks that are trained to perform memory and pattern generation tasks. The functional aspect of these tasks corresponds to dynamical objects in the fully trained network-a line attractor or a set of limit cycles for the two respective tasks. We use these dynamical objects as anchors to study the effect of learning on their emergence. We find that the sequential nature of learning-one trial at a time-has major consequences for the learning trajectory and its final outcome. Specifically, we show that least mean squares (LMS), a simple gradient descent suggested as a biologically plausible version of the FORCE algorithm, is constantly obstructed by forgetting, which is manifested as the destruction of dynamical objects from previous trials. The degree of interference is determined by the correlation between different trials. We show which specific ingredients of FORCE avoid this phenomenon. Overall, this difference results in convergence that is orders of magnitude slower for LMS. Learning implies accumulating information across multiple trials to form the overall concept of the task. Our results show that interference between trials can greatly affect learning in a learning-rule-dependent manner. These insights can help design experimental protocols that minimize such interference, and possibly infer underlying learning rules by observing behavior and neural activity throughout learning."}], "ArticleTitle": "One Step Back, Two Steps Forward: Interference and Learning in Recurrent Neural Networks."}, "33253034": {"mesh": [], "AbstractText": [{"section": null, "text": "Spike trains with negative interspike interval (ISI) correlations, in which long/short ISIs are more likely followed by short/long ISIs, are common in many neurons. They can be described by stochastic models with a spike-triggered adaptation variable. We analyze a phenomenon in these models where such statistically dependent ISI sequences arise in tandem with quasi-statistically independent and identically distributed (quasi-IID) adaptation variable sequences. The sequences of adaptation states and resulting ISIs are linked by a nonlinear decorrelating transformation. We establish general conditions on a family of stochastic spiking models that guarantee this quasi-IID property and establish bounds on the resulting baseline ISI correlations. Inputs that elicit weak firing rate changes in samples with many spikes are known to be more detectible when negative ISI correlations are present because they reduce spike count variance; this defines a variance-reduced firing rate coding benchmark. We performed a Fisher information analysis on these adapting models exhibiting ISI correlations to show that a spike pattern code based on the quasi-IID property achieves the upper bound of detection performance, surpassing rate codes with the same mean rate-including the variance-reduced rate code benchmark-by 20% to 30%. The information loss in rate codes arises because the benefits of reduced spike count variance cannot compensate for the lower firing rate gain due to adaptation. Since adaptation states have similar dynamics to synaptic responses, the quasi-IID decorrelation transformation of the spike train is plausibly implemented by downstream neurons through matched postsynaptic kinetics. This provides an explanation for observed coding performance in sensory systems that cannot be accounted for by rate coding, for example, at the detection threshold where rate changes can be insignificant."}], "ArticleTitle": "Enhanced Signal Detection by Adaptive Decorrelation of Interspike Intervals."}, "32795235": {"mesh": [], "AbstractText": [{"section": null, "text": "Fox and Lu introduced a Langevin framework for discrete-time stochastic models of randomly gated ion channels such as the Hodgkin-Huxley (HH) system. They derived a Fokker-Planck equation with state-dependent diffusion tensor D and suggested a Langevin formulation with noise coefficient matrix S such that SS&#8868;=D. Subsequently, several authors introduced a variety of Langevin equations for the HH system. In this article, we present a natural 14-dimensional dynamics for the HH system in which each directed edge in the ion channel state transition graph acts as an independent noise source, leading to a 14 &#215; 28 noise coefficient matrix S. We show that (1) the corresponding 14D system of ordinary differential equations is consistent with the classical 4D representation of the HH system; (2) the 14D representation leads to a noise coefficient matrix S that can be obtained cheaply on each time step, without requiring a matrix decomposition; (3) sample trajectories of the 14D representation are pathwise equivalent to trajectories of Fox and Lu's system, as well as trajectories of several existing Langevin models; (4) our 14D representation (and those equivalent to it) gives the most accurate interspike interval distribution, not only with respect to moments but under both the L1 and L&#8734; metric-space norms; and (5) the 14D representation gives an approximation to exact Markov chain simulations that are as fast and as efficient as all equivalent models. Our approach goes beyond existing models, in that it supports a stochastic shielding decomposition that dramatically simplifies S with minimal loss of accuracy under both voltage- and current-clamp conditions."}], "ArticleTitle": "Fast and Accurate Langevin Simulations of Stochastic Hodgkin-Huxley Dynamics."}, "29566348": {"mesh": ["Algorithms", "Brain", "Brain-Computer Interfaces", "Computer Simulation", "Formative Feedback", "Humans", "Learning", "Neurons", "Nonlinear Dynamics", "Time Factors"], "AbstractText": [{"section": null, "text": "Brain-computer interfaces are in the process of moving from the laboratory to the clinic. These devices act by reading neural activity and using it to directly control a device, such as a cursor on a computer screen. An open question in the field is how to map neural activity to device movement in order to achieve the most proficient control. This question is complicated by the fact that learning, especially the long-term skill learning that accompanies weeks of practice, can allow subjects to improve performance over time. Typical approaches to this problem attempt to maximize the biomimetic properties of the device in order to limit the need for extensive training. However, it is unclear if this approach would ultimately be superior to performance that might be achieved with a nonbiomimetic device once the subject has engaged in extended practice and learned how to use it. Here we approach this problem using ideas from optimal control theory. Under the assumption that the brain acts as an optimal controller, we present a formal definition of the usability of a device and show that the optimal postlearning mapping can be written as the solution of a constrained optimization problem. We then derive the optimal mappings for particular cases common to most brain-computer interfaces. Our results suggest that the common approach of creating biomimetic interfaces may not be optimal when learning is taken into account. More broadly, our method provides a blueprint for optimal device design in general control-theoretic contexts."}], "ArticleTitle": "Optimizing the Usability of Brain-Computer Interfaces."}, "29652580": {"mesh": ["Action Potentials", "Animals", "Hippocampus", "Long-Term Potentiation", "Memory", "Models, Neurological", "Models, Statistical", "Neural Networks, Computer", "Neurons", "Rats", "Sleep", "Spatial Learning", "Wakefulness"], "AbstractText": [{"section": null, "text": "It has been suggested that reactivation of previously acquired experiences or stored information in declarative memories in the hippocampus and neocortex contributes to memory consolidation and learning. Understanding memory consolidation depends crucially on the development of robust statistical methods for assessing memory reactivation. To date, several statistical methods have seen established for assessing memory reactivation based on bursts of ensemble neural spike activity during offline states. Using population-decoding methods, we propose a new statistical metric, the weighted distance correlation, to assess hippocampal memory reactivation (i.e., spatial memory replay) during quiet wakefulness and slow-wave sleep. The new metric can be combined with an unsupervised population decoding analysis, which is invariant to latent state labeling and allows us to detect statistical dependency beyond linearity in memory traces. We validate the new metric using two rat hippocampal recordings in spatial navigation tasks. Our proposed analysis framework may have a broader impact on assessing memory reactivations in other brain regions under different behavioral tasks."}], "ArticleTitle": "Methods for Assessment of Memory Reactivation."}, "32687769": {"mesh": [], "AbstractText": [{"section": null, "text": "We study the problem of stochastic multiple-arm identification, where an agent sequentially explores a size-k subset of arms (also known as a super arm) from given n arms and tries to identify the best super arm. Most work so far has considered the semi-bandit setting, where the agent can observe the reward of each pulled arm or assumed each arm can be queried at each round. However, in real-world applications, it is costly or sometimes impossible to observe a reward of individual arms. In this study, we tackle the full-bandit setting, where only a noisy observation of the total sum of a super arm is given at each pull. Although our problem can be regarded as an instance of the best arm identification in linear bandits, a naive approach based on linear bandits is computationally infeasible since the number of super arms K is exponential. To cope with this problem, we first design a polynomial-time approximation algorithm for a 0-1 quadratic programming problem arising in confidence ellipsoid maximization. Based on our approximation algorithm, we propose a bandit algorithm whose computation time is O(log K), thereby achieving an exponential speedup over linear bandit algorithms. We provide a sample complexity upper bound that is still worst-case optimal. Finally, we conduct experiments on large-scale data sets with more than 1010 super arms, demonstrating the superiority of our algorithms in terms of both the computation time and the sample complexity."}], "ArticleTitle": "Polynomial-Time Algorithms for Multiple-Arm Identification with Full-Bandit Feedback."}, "29381443": {"mesh": [], "AbstractText": [{"section": null, "text": "Humans are able to robustly maintain desired motion and posture under dynamically changing circumstances, including novel conditions. To accomplish this, the brain needs to optimize the synergistic control between muscles against external dynamic factors. However, previous related studies have usually simplified the control of multiple muscles using two opposing muscles, which are minimum actuators to simulate linear feedback control. As a result, they have been unable to analyze how muscle synergy contributes to motion control robustness in a biological system. To address this issue, we considered a new muscle synergy concept used to optimize the synergy between muscle units against external dynamic conditions, including novel conditions. We propose that two main muscle control policies synergistically control muscle units to maintain the desired motion against external dynamic conditions. Our assumption is based on biological evidence regarding the control of multiple muscles via the corticospinal tract. One of the policies is the group control policy (GCP), which is used to control muscle group units classified based on functional similarities in joint control. This policy is used to effectively resist external dynamic circumstances, such as disturbances. The individual control policy (ICP) assists the GCP in precisely controlling motion by controlling individual muscle units. To validate this hypothesis, we simulated the reinforcement of the synergistic actions of the two control policies during the reinforcement learning of feedback motion control. Using this learning paradigm, the two control policies were synergistically combined to result in robust feedback control under novel transient and sustained disturbances that did not involve learning. Further, by comparing our data to experimental data generated by human subjects under the same conditions as those of the simulation, we showed that the proposed synergy concept may be used to analyze muscle synergy-driven motion control robustness in humans."}], "ArticleTitle": "Muscle Synergy-Driven Robust Motion Control."}, "30883279": {"mesh": [], "AbstractText": [{"section": null, "text": "Stochastic kernel-based dimensionality-reduction approaches have become popular in the past decade. The central component of many of these methods is a symmetric kernel that quantifies the vicinity between pairs of data points and a kernel-induced Markov chain on the data. Typically, the Markov chain is fully specified by the kernel through row normalization. However, in many cases, it is desirable to impose user-specified stationary-state and dynamical constraints on the Markov chain. Unfortunately, no systematic framework exists to impose such user-defined constraints. Here, based on our previous work on inference of Markov models, we introduce a path entropy maximization based approach to derive the transition probabilities of Markov chains using a kernel and additional user-specified constraints. We illustrate the usefulness of these Markov chains with examples."}], "ArticleTitle": "Introducing User-Prescribed Constraints in Markov Chains for Nonlinear Dimensionality Reduction."}, "30148706": {"mesh": [], "AbstractText": [{"section": null, "text": "We propose a set of convex low-rank inducing norms for coupled matrices and tensors (hereafter referred to as coupled tensors), in which information is shared between the matrices and tensors through common modes. More specifically, we first propose a mixture of the overlapped trace norm and the latent norms with the matrix trace norm, and then, propose a completion model regularized using these norms to impute coupled tensors. A key advantage of the proposed norms is that they are convex and can be used to find a globally optimal solution, whereas existing methods for coupled learning are nonconvex. We also analyze the excess risk bounds of the completion model regularized using our proposed norms and show that they can exploit the low-rankness of coupled tensors, leading to better bounds compared to those obtained using uncoupled norms. Through synthetic and real-data experiments, we show that the proposed completion model compares favorably with existing ones."}], "ArticleTitle": "Convex Coupled Matrix and Tensor Completion."}, "30764743": {"mesh": ["Action Potentials", "Algorithms", "Animals", "Humans", "Models, Neurological", "Odorants", "Olfactory Pathways", "Olfactory Perception", "Olfactory Receptor Neurons", "Receptors, Odorant", "Smell"], "AbstractText": [{"section": null, "text": "In the olfactory system, odor percepts retain their identity despite substantial variations in concentration, timing, and background. We study a novel strategy for encoding intensity-invariant stimulus identity that is based on representing relative rather than absolute values of stimulus features. For example, in what is known as the primacy coding model, odorant identities are represented by the conditions that some odorant receptors are activated more strongly than others. Because, in this scheme, odorant identity depends only on the relative amplitudes of olfactory receptor responses, identity is invariant to changes in both intensity and monotonic nonlinear transformations of its neuronal responses. Here we show that sparse vectors representing odorant mixtures can be recovered in a compressed sensing framework via elastic net loss minimization. In the primacy model, this minimization is performed under the constraint that some receptors respond to a given odorant more strongly than others. Using duality transformation, we show that this constrained optimization problem can be solved by a neural network whose Lyapunov function represents the dual Lagrangian and whose neural responses represent the Lagrange coefficients of primacy and other constraints. The connectivity in such a dual network resembles known features of connectivity in olfactory circuits. We thus propose that networks in the piriform cortex implement dual computations to compute odorant identity with the sparse activities of individual neurons representing Lagrange coefficients. More generally, we propose that sparse neuronal firing rates may represent Lagrange multipliers, which we call the dual brain hypothesis. We show such a formulation is well suited to solve problems with multiple interacting relative constraints."}], "ArticleTitle": "Deconstructing Odorant Identity via Primacy in Dual Networks."}, "30462583": {"mesh": ["Humans", "Models, Neurological", "Nerve Net", "Neural Networks, Computer", "Neurons", "Nonlinear Dynamics"], "AbstractText": [{"section": null, "text": "Threshold-linear networks (TLNs) are models of neural networks that consist of simple, perceptron-like neurons and exhibit nonlinear dynamics determined by the network's connectivity. The fixed points of a TLN, including both stable and unstable equilibria, play a critical role in shaping its emergent dynamics. In this work, we provide two novel characterizations for the set of fixed points of a competitive TLN: the first is in terms of a simple sign condition, while the second relies on the concept of domination. We apply these results to a special family of TLNs, called combinatorial threshold-linear networks (CTLNs), whose connectivity matrices are defined from directed graphs. This leads us to prove a series of graph rules that enable one to determine fixed points of a CTLN by analyzing the underlying graph. In addition, we study larger networks composed of smaller building block subnetworks and prove several theorems relating the fixed points of the full network to those of its components. Our results provide the foundation for a kind of graphical calculus to infer features of the dynamics from a network's connectivity."}], "ArticleTitle": "Fixed Points of Competitive Threshold-Linear Networks."}, "27557100": {"mesh": ["Energy Metabolism", "Humans", "Neural Networks, Computer", "Semiconductors", "Support Vector Machine"], "AbstractText": [{"section": null, "text": "Neuromorphic engineering combines the architectural and computational principles of systems neuroscience with semiconductor electronics, with the aim of building efficient and compact devices that mimic the synaptic and neural machinery of the brain. The energy consumptions promised by neuromorphic engineering are extremely low, comparable to those of the nervous system. Until now, however, the neuromorphic approach has been restricted to relatively simple circuits and specialized functions, thereby obfuscating a direct comparison of their energy consumption to that used by conventional von Neumann digital machines solving real-world tasks. Here we show that a recent technology developed by IBM can be leveraged to realize neuromorphic circuits that operate as classifiers of complex real-world stimuli. Specifically, we provide a set of general prescriptions to enable the practical implementation of neural architectures that compete with state-of-the-art classifiers. We also show that the energy consumption of these architectures, realized on the IBM chip, is typically two or more orders of magnitude lower than that of conventional digital machines implementing classifiers with comparable performance. Moreover, the spike-based dynamics display a trade-off between integration time and accuracy, which naturally translates into algorithms that can be flexibly deployed for either fast and approximate classifications, or more accurate classifications at the mere expense of longer running times and higher energy costs. This work finally proves that the neuromorphic approach can be efficiently used in real-world applications and has significant advantages over conventional digital devices when energy consumption is considered."}], "ArticleTitle": "Energy-Efficient Neuromorphic Classifiers."}, "28410050": {"mesh": ["Algorithms", "Animals", "Cluster Analysis", "Data Compression", "Entropy", "Humans"], "AbstractText": [{"section": null, "text": "Lossy compression and clustering fundamentally involve a decision about which features are relevant and which are not. The information bottleneck method (IB) by Tishby, Pereira, and Bialek ( 1999 ) formalized this notion as an information-theoretic optimization problem and proposed an optimal trade-off between throwing away as many bits as possible and selectively keeping those that are most important. In the IB, compression is measured by mutual information. Here, we introduce an alternative formulation that replaces mutual information with entropy, which we call the deterministic information bottleneck (DIB) and argue better captures this notion of compression. As suggested by its name, the solution to the DIB problem turns out to be a deterministic encoder, or hard clustering, as opposed to the stochastic encoder, or soft clustering, that is optimal under the IB. We compare the IB and DIB on synthetic data, showing that the IB and DIB perform similarly in terms of the IB cost function, but that the DIB significantly outperforms the IB in terms of the DIB cost function. We also empirically find that the DIB offers a considerable gain in computational efficiency over the IB, over a range of convergence parameters. Our derivation of the DIB also suggests a method for continuously interpolating between the soft clustering of the IB and the hard clustering of the DIB."}], "ArticleTitle": "The Deterministic Information Bottleneck."}, "32795229": {"mesh": ["Action Potentials", "Cognition", "Humans", "Neural Prostheses", "Nonlinear Dynamics", "Spatial Analysis"], "AbstractText": [{"section": null, "text": "Modeling spike train transformation among brain regions helps in designing a cognitive neural prosthesis that restores lost cognitive functions. Various methods analyze the nonlinear dynamic spike train transformation between two cortical areas with low computational eficiency. The application of a real-time neural prosthesis requires computational eficiency, performance stability, and better interpretation of the neural firing patterns that modulate target spike generation. We propose the binless kernel machine in the point-process framework to describe nonlinear dynamic spike train transformations. Our approach embeds the binless kernel to eficiently capture the feedforward dynamics of spike trains and maps the input spike timings into reproducing kernel Hilbert space (RKHS). An inhomogeneous Bernoulli process is designed to combine with a kernel logistic regression that operates on the binless kernel to generate an output spike train as a point process. Weights of the proposed model are estimated by maximizing the log likelihood of output spike trains in RKHS, which allows a global-optimal solution. To reduce computational complexity, we design a streaming-based clustering algorithm to extract typical and important spike train features. The cluster centers and their weights enable the visualization of the important input spike train patterns that motivate or inhibit output neuron firing. We test the proposed model on both synthetic data and real spike train data recorded from the dorsal premotor cortex and the primary motor cortex of a monkey performing a center-out task. Performances are evaluated by discrete-time rescaling Kolmogorov-Smirnov tests. Our model outperforms the existing methods with higher stability regardless of weight initialization and demonstrates higher eficiency in analyzing neural patterns from spike timing with less historical input (50%). Meanwhile, the typical spike train patterns selected according to weights are validated to encode output spike from the spike train of single-input neuron and the interaction of two input neurons."}], "ArticleTitle": "Binless Kernel Machine: Modeling Spike Train Transformation for Cognitive Neural Prostheses."}, "31951794": {"mesh": ["Adaptation, Physiological", "Feedback, Physiological", "Humans", "Models, Biological", "Movement", "Psychomotor Performance"], "AbstractText": [{"section": null, "text": "Sensorimotor tasks that humans perform are often affected by different sources of uncertainty. Nevertheless, the central nervous system (CNS) can gracefully coordinate our movements. Most learning frameworks rely on the internal model principle, which requires a precise internal representation in the CNS to predict the outcomes of our motor commands. However, learning a perfect internal model in a complex environment over a short period of time is a nontrivial problem. Indeed, achieving proficient motor skills may require years of training for some difficult tasks. Internal models alone may not be adequate to explain the motor adaptation behavior during the early phase of learning. Recent studies investigating the active regulation of motor variability, the presence of suboptimal inference, and model-free learning have challenged some of the traditional viewpoints on the sensorimotor learning mechanism. As a result, it may be necessary to develop a computational framework that can account for these new phenomena. Here, we develop a novel theory of motor learning, based on model-free adaptive optimal control, which can bypass some of the difficulties in existing theories. This new theory is based on our recently developed adaptive dynamic programming (ADP) and robust ADP (RADP) methods and is especially useful for accounting for motor learning behavior when an internal model is inaccurate or unavailable. Our preliminary computational results are in line with experimental observations reported in the literature and can account for some phenomena that are inexplicable using existing models."}], "ArticleTitle": "Model-Free Robust Optimal Feedback Mechanisms of Biological Motor Control."}, "30314426": {"mesh": ["Algorithms", "Cluster Analysis"], "AbstractText": [{"section": null, "text": "The information bottleneck (IB) approach to clustering takes a joint distribution <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math> and maps the data <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>X</mml:mi></mml:math> to cluster labels <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>T</mml:mi></mml:math> , which retain maximal information about <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>Y</mml:mi></mml:math> (Tishby, Pereira, & Bialek, 1999 ). This objective results in an algorithm that clusters data points based on the similarity of their conditional distributions <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math> . This is in contrast to classic geometric clustering algorithms such as <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>k</mml:mi></mml:math> -means and gaussian mixture models (GMMs), which take a set of observed data points <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math> and cluster them based on their geometric (typically Euclidean) distance from one another. Here, we show how to use the deterministic information bottleneck (DIB) (Strouse & Schwab, 2017 ), a variant of IB, to perform geometric clustering by choosing cluster labels that preserve information about data point location on a smoothed data set. We also introduce a novel intuitive method to choose the number of clusters via kinks in the information curve. We apply this approach to a variety of simple clustering problems, showing that DIB with our model selection procedure recovers the generative cluster labels. We also show that, in particular limits of our model parameters, clustering with DIB and IB is equivalent to <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>k</mml:mi></mml:math> -means and EM fitting of a GMM with hard and soft assignments, respectively. Thus, clustering with (D)IB generalizes and provides an information-theoretic perspective on these classic algorithms."}], "ArticleTitle": "The Information Bottleneck and Geometric Clustering."}, "33080158": {"mesh": ["Action Potentials", "Dendrites", "Humans", "Models, Neurological", "Neural Networks, Computer", "Nonlinear Dynamics"], "AbstractText": [{"section": null, "text": "Nonlinear interactions in the dendritic tree play a key role in neural computation. Nevertheless, modeling frameworks aimed at the construction of large-scale, functional spiking neural networks, such as the Neural Engineering Framework, tend to assume a linear superposition of postsynaptic currents. In this letter, we present a series of extensions to the Neural Engineering Framework that facilitate the construction of networks incorporating Dale's principle and nonlinear conductance-based synapses. We apply these extensions to a two-compartment LIF neuron that can be seen as a simple model of passive dendritic computation. We show that it is possible to incorporate neuron models with input-dependent nonlinearities into the Neural Engineering Framework without compromising high-level function and that nonlinear postsynaptic currents can be systematically exploited to compute a wide variety of multivariate, band-limited functions, including the Euclidean norm, controlled shunting, and nonnegative multiplication. By avoiding an additional source of spike noise, the function approximation accuracy of a single layer of two-compartment LIF neurons is on a par with or even surpasses that of two-layer spiking neural networks up to a certain target function bandwidth."}], "ArticleTitle": "Passive Nonlinear Dendritic Interactions as a Computational Resource in Spiking Neural Networks."}, "32795232": {"mesh": [], "AbstractText": [{"section": null, "text": "Multiview alignment, achieving one-to-one correspondence of multiview inputs, is critical in many real-world multiview applications, especially for cross-view data analysis problems. An increasing amount of work has studied this alignment problem with canonical correlation analysis (CCA). However, existing CCA models are prone to misalign the multiple views due to either the neglect of uncertainty or the inconsistent encoding of the multiple views. To tackle these two issues, this letter studies multiview alignment from a Bayesian perspective. Delving into the impairments of inconsistent encodings, we propose to recover correspondence of the multiview inputs by matching the marginalization of the joint distribution of multiview random variables under different forms of factorization. To realize our design, we present adversarial CCA (ACCA), which achieves consistent latent encodings by matching the marginalized latent encodings through the adversarial training paradigm. Our analysis, based on conditional mutual information, reveals that ACCA is flexible for handling implicit distributions. Extensive experiments on correlation analysis and cross-view generation under noisy input settings demonstrate the superiority of our model."}], "ArticleTitle": "Multiview Alignment and Generation in CCA via Consistent Latent Encoding."}, "30883281": {"mesh": [], "AbstractText": [{"section": null, "text": "We propose a new divergence on the manifold of probability distributions, building on the entropic regularization of optimal transportation problems. As Cuturi ( 2013 ) showed, regularizing the optimal transport problem with an entropic term is known to bring several computational benefits. However, because of that regularization, the resulting approximation of the optimal transport cost does not define a proper distance or divergence between probability distributions. We recently tried to introduce a family of divergences connecting the Wasserstein distance and the Kullback-Leibler divergence from an information geometry point of view (see Amari, Karakida, & Oizumi, 2018 ). However, that proposal was not able to retain key intuitive aspects of the Wasserstein geometry, such as translation invariance, which plays a key role when used in the more general problem of computing optimal transport barycenters. The divergence we propose in this work is able to retain such properties and admits an intuitive interpretation."}], "ArticleTitle": "Information Geometry for Regularized Optimal Transport and Barycenters of Patterns."}, "31113300": {"mesh": ["Computer Simulation", "Humans", "Models, Neurological", "Nerve Net", "Neural Networks, Computer", "Neuronal Plasticity", "Neurons", "Reinforcement, Psychology", "Reward"], "AbstractText": [{"section": null, "text": "Reservoir computing is a biologically inspired class of learning algorithms in which the intrinsic dynamics of a recurrent neural network are mined to produce target time series. Most existing reservoir computing algorithms rely on fully supervised learning rules, which require access to an exact copy of the target response, greatly reducing the utility of the system. Reinforcement learning rules have been developed for reservoir computing, but we find that they fail to converge on complex motor tasks. Current theories of biological motor learning pose that early learning is controlled by dopamine-modulated plasticity in the basal ganglia that trains parallel cortical pathways through unsupervised plasticity as a motor task becomes well learned. We developed a novel learning algorithm for reservoir computing that models the interaction between reinforcement and unsupervised learning observed in experiments. This novel learning algorithm converges on simulated motor tasks on which previous reservoir computing algorithms fail and reproduces experimental findings that relate Parkinson's disease and its treatments to motor learning. Hence, incorporating biological theories of motor learning improves the effectiveness and biological relevance of reservoir computing models."}], "ArticleTitle": "A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity."}, "30764738": {"mesh": ["Algorithms", "Computer Simulation", "Eye", "Feedback, Sensory", "Hand", "Humans", "Models, Neurological", "Motor Activity", "Movement"], "AbstractText": [{"section": null, "text": "Compensating for sensorimotor noise and for temporal delays has been identified as a major function of the nervous system. Although these aspects have often been described separately in the frameworks of optimal cue combination or motor prediction during movement planning, control-theoretic models suggest that these two operations are performed simultaneously, and mounting evidence supports that motor commands are based on sensory predictions rather than sensory states. In this letter, we study the benefit of state estimation for predictive sensorimotor control. More precisely, we combine explicit compensation for sensorimotor delays and optimal estimation derived in the context of Kalman filtering. We show, based on simulations of human-inspired eye and arm movements, that filtering sensory predictions improves the stability margin of the system against prediction errors due to low-dimensional predictions or to errors in the delay estimate. These simulations also highlight that prediction errors qualitatively account for a broad variety of movement disorders typically associated with cerebellar dysfunctions. We suggest that adaptive filtering in cerebellum, instead of often-assumed feedforward predictions, may achieve simple compensation for sensorimotor delays and support stable closed-loop control of movements."}], "ArticleTitle": "Filtering Compensation for Delays and Prediction Errors during Sensorimotor Control."}, "32521212": {"mesh": [], "AbstractText": [{"section": null, "text": "Understanding how rich dynamics emerge in neural populations requires models exhibiting a wide range of behaviors while remaining interpretable in terms of connectivity and single-neuron dynamics. However, it has been challenging to fit such mechanistic spiking networks at the single-neuron scale to empirical population data. To close this gap, we propose to fit such data at a mesoscale, using a mechanistic but low-dimensional and, hence, statistically tractable model. The mesoscopic representation is obtained by approximating a population of neurons as multiple homogeneous pools of neurons and modeling the dynamics of the aggregate population activity within each pool. We derive the likelihood of both single-neuron and connectivity parameters given this activity, which can then be used to optimize parameters by gradient ascent on the log likelihood or perform Bayesian inference using Markov chain Monte Carlo (MCMC) sampling. We illustrate this approach using a model of generalized integrate-and-fire neurons for which mesoscopic dynamics have been previously derived and show that both single-neuron and connectivity parameters can be recovered from simulated data. In particular, our inference method extracts posterior correlations between model parameters, which define parameter subsets able to reproduce the data. We compute the Bayesian posterior for combinations of parameters using MCMC sampling and investigate how the approximations inherent in a mesoscopic population model affect the accuracy of the inferred single-neuron parameters."}], "ArticleTitle": "Inference of a Mesoscopic Population Model from Population Spike Trains."}, "31260389": {"mesh": ["Algorithms", "Animals", "Entorhinal Cortex", "Hippocampus", "Models, Neurological", "Place Cells", "Spatial Learning", "Synaptic Transmission"], "AbstractText": [{"section": null, "text": "Place cells in the hippocampus (HC) are active when an animal visits a certain location (referred to as a place field) within an environment. Grid cells in the medial entorhinal cortex (MEC) respond at multiple locations, with firing fields that form a periodic and hexagonal tiling of the environment. The joint activity of grid and place cell populations, as a function of location, forms a neural code for space. In this article, we develop an understanding of the relationships between coding theoretically relevant properties of the combined activity of these populations and how these properties limit the robustness of this representation to noise-induced interference. These relationships are revisited by measuring the performances of biologically realizable algorithms implemented by networks of place and grid cell populations, as well as constraint neurons, which perform denoising operations. Contributions of this work include the investigation of coding theoretic limitations of the mammalian neural code for location and how communication between grid and place cell networks may improve the accuracy of each population's representation. Simulations demonstrate that denoising mechanisms analyzed here can significantly improve the fidelity of this neural representation of space. Furthermore, patterns observed in connectivity of each population of simulated cells predict that anti-Hebbian learning drives decreases in inter-HC-MEC connectivity along the dorsoventral axis."}], "ArticleTitle": "On the Organization of Grid and Place Cells: Neural Denoising via Subspace Learning."}, "30979353": {"mesh": ["Algorithms", "Machine Learning", "Models, Theoretical", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Recurrent neural networks have been extensively studied in the context of neuroscience and machine learning due to their ability to implement complex computations. While substantial progress in designing effective learning algorithms has been achieved, a full understanding of trained recurrent networks is still lacking. Specifically, the mechanisms that allow computations to emerge from the underlying recurrent dynamics are largely unknown. Here we focus on a simple yet underexplored computational setup: a feedback architecture trained to associate a stationary output to a stationary input. As a starting point, we derive an approximate analytical description of global dynamics in trained networks, which assumes uncorrelated connectivity weights in the feedback and in the random bulk. The resulting mean-field theory suggests that the task admits several classes of solutions, which imply different stability properties. Different classes are characterized in terms of the geometrical arrangement of the readout with respect to the input vectors, defined in the high-dimensional space spanned by the network population. We find that such an approximate theoretical approach can be used to understand how standard training techniques implement the input-output task in finite-size feedback networks. In particular, our simplified description captures the local and the global stability properties of the target solution, and thus predicts training performance."}], "ArticleTitle": "A Geometrical Analysis of Global Stability in Trained Feedback Networks."}, "31835003": {"mesh": ["Animals", "Entorhinal Cortex", "Grid Cells", "Hippocampus", "Learning", "Models, Neurological", "Neurons", "Space Perception"], "AbstractText": [{"section": null, "text": "Although hippocampal grid cells are thought to be crucial for spatial navigation, their computational purpose remains disputed. Recently, they were proposed to represent spatial transitions and convey this knowledge downstream to place cells. However, a single scale of transitions is insufficient to plan long goal-directed sequences in behaviorally acceptable time. Here, a scale-space data structure is suggested to optimally accelerate retrievals from transition systems, called transition scale-space (TSS). Remaining exclusively on an algorithmic level, the scale increment is proved to be ideally 2 for biologically plausible receptive fields. It is then argued that temporal buffering is necessary to learn the scale-space online. Next, two modes for retrieval of sequences from the TSS are presented: top down and bottom up. The two modes are evaluated in symbolic simulations (i.e., without biologically plausible spiking neurons). Additionally, a TSS is used for short-cut discovery in a simulated Morris water maze. Finally, the results are discussed in depth with respect to biological plausibility, and several testable predictions are derived. Moreover, relations to other grid cell models, multiresolution path planning, and scale-space theory are highlighted. Summarized, reward-free transition encoding is shown here, in a theoretical model, to be compatible with the observed discretization along the dorso-ventral axis of the medial entorhinal cortex. Because the theoretical model generalizes beyond navigation, the TSS is suggested to be a general-purpose cortical data structure for fast retrieval of sequences and relational knowledge. Source code for all simulations presented in this paper can be found at https://github.com/rochus/transitionscalespace."}], "ArticleTitle": "Transition Scale-Spaces: A Computational Theory for the Discretized Entorhinal Cortex."}, "31703176": {"mesh": [], "AbstractText": [{"section": null, "text": "Neural associative memories (NAM) are perceptron-like single-layer networks with fast synaptic learning typically storing discrete associations between pairs of neural activity patterns. Gripon and Berrou (2011) investigated NAM employing block coding, a particular sparse coding method, and reported a significant increase in storage capacity. Here we verify and extend their results for both heteroassociative and recurrent autoassociative networks. For this we provide a new analysis of iterative retrieval in finite autoassociative and heteroassociative networks that allows estimating storage capacity for random and block patterns. Furthermore, we have implemented various retrieval algorithms for block coding and compared them in simulations to our theoretical results and previous simulation data. In good agreement of theory and experiments, we find that finite networks employing block coding can store significantly more memory patterns. However, due to the reduced information per block pattern, it is not possible to significantly increase stored information per synapse. Asymptotically, the information retrieval capacity converges to the known limits C=ln2&#8776;0.69 and C=(ln2)/4&#8776;0.17 also for block coding. We have also implemented very large recurrent networks up to n=2&#183;106 neurons, showing that maximal capacity C&#8776;0.2 bit per synapse occurs for finite networks having a size n&#8776;105 similar to cortical macrocolumns."}], "ArticleTitle": "Iterative Retrieval and Block Coding in Autoassociative and Heteroassociative Memory."}, "30979354": {"mesh": ["Cerebral Cortex", "Computer Simulation", "Humans", "Nerve Net", "Neuronal Plasticity"], "AbstractText": [{"section": null, "text": "Recently, Markram et al. (2015) presented a model of the rat somatosensory microcircuit (Markram model). Their model is high in anatomical and physiological detail, and its simulation requires supercomputers. The lack of neuroinformatics and computing power is an obstacle for using a similar approach to build models of other cortical areas or larger cortical systems. Simplified neuron models offer an attractive alternative to high-fidelity Hodgkin-Huxley-type neuron models, but their validity in modeling cortical circuits is unclear. We simplified the Markram model to a network of exponential integrate-and-fire (EIF) neurons that runs on a single CPU core in reasonable time. We analyzed the electrophysiology and the morphology of the Markram model neurons with eFel and NeuroM tools, provided by the Blue Brain Project. We then constructed neurons with few compartments and averaged parameters from the reference model. We used the CxSystem simulation framework to explore the role of short-term plasticity and GABA <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msub><mml:mrow/><mml:mi>B</mml:mi></mml:msub></mml:math> and NMDA synaptic conductances in replicating oscillatory phenomena in the Markram model. We show that having a slow inhibitory synaptic conductance (GABA <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:msub><mml:mrow/><mml:mi>B</mml:mi></mml:msub><mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math> allows replication of oscillatory behavior in the high-calcium state. Furthermore, we show that qualitatively similar dynamics are seen even with a reduced number of cell types (from 55 to 17 types). This reduction halved the computation time. Our results suggest that qualitative dynamics of cortical microcircuits can be studied using limited neuroinformatics and computing resources supporting parameter exploration and simulation of cortical systems. The simplification procedure can easily be adapted to studying other microcircuits for which sparse electrophysiological and morphological data are available."}], "ArticleTitle": "Controlling Complexity of Cerebral Cortex Simulations-II: Streamlined Microcircuits."}, "29894654": {"mesh": ["Humans", "Image Processing, Computer-Assisted", "Machine Learning", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "As the optical lenses for cameras always have limited depth of field, the captured images with the same scene are not all in focus. Multifocus image fusion is an efficient technology that can synthesize an all-in-focus image using several partially focused images. Previous methods have accomplished the fusion task in spatial or transform domains. However, fusion rules are always a problem in most methods. In this letter, from the aspect of focus region detection, we propose a novel multifocus image fusion method based on a fully convolutional network (FCN) learned from synthesized multifocus images. The primary novelty of this method is that the pixel-wise focus regions are detected through a learning FCN, and the entire image, not just the image patches, are exploited to train the FCN. First, we synthesize 4500 pairs of multifocus images by repeatedly using a gaussian filter for each image from PASCAL VOC 2012, to train the FCN. After that, a pair of source images is fed into the trained FCN, and two score maps indicating the focus property are generated. Next, an inversed score map is averaged with another score map to produce an aggregative score map, which take full advantage of focus probabilities in two score maps. We implement the fully connected conditional random field (CRF) on the aggregative score map to accomplish and refine a binary decision map for the fusion task. Finally, we exploit the weighted strategy based on the refined decision map to produce the fused image. To demonstrate the performance of the proposed method, we compare its fused results with several start-of-the-art methods not only on a gray data set but also on a color data set. Experimental results show that the proposed method can achieve superior fusion performance in both human visual quality and objective assessment."}], "ArticleTitle": "Fully Convolutional Network-Based Multifocus Image Fusion."}, "31113299": {"mesh": ["Artificial Intelligence", "Cerebral Cortex", "Cognition", "Humans", "Neural Pathways", "Thalamus"], "AbstractText": [{"section": null, "text": "The thalamus has traditionally been considered as only a relay source of cortical inputs, with hierarchically organized cortical circuits serially transforming thalamic signals to cognitively relevant representations. Given the absence of local excitatory connections within the thalamus, the notion of thalamic relay seemed like a reasonable description over the past several decades. Recent advances in experimental approaches and theory provide a broader perspective on the role of the thalamus in cognitively relevant cortical computations and suggest that only a subset of thalamic circuit motifs fits the relay description. Here, we discuss this perspective and highlight the potential role for the thalamus, and specifically the mediodorsal (MD) nucleus, in the dynamic selection of cortical representations through a combination of intrinsic thalamic computations and output signals that change cortical network functional parameters. We suggest that through the contextual modulation of cortical computation, the thalamus and cortex jointly optimize the information and cost trade-off in an emergent fashion. We emphasize that coordinated experimental and theoretical efforts will provide a path to understanding the role of the thalamus in cognition, along with an understanding to augment cognitive capacity in health and disease."}], "ArticleTitle": "A Computational Perspective of the Role of the Thalamus in Cognition."}, "31835005": {"mesh": ["Action Potentials", "Brain", "Computer Simulation", "Humans", "Models, Neurological", "Neural Networks, Computer", "Neuronal Plasticity", "Neurons", "Synapses"], "AbstractText": [{"section": null, "text": "Neural mass models offer a way of studying the development and behavior of large-scale brain networks through computer simulations. Such simulations are currently mainly research tools, but as they improve, they could soon play a role in understanding, predicting, and optimizing patient treatments, particularly in relation to effects and outcomes of brain injury. To bring us closer to this goal, we took an existing state-of-the-art neural mass model capable of simulating connection growth through simulated plasticity processes. We identified and addressed some of the model's limitations by implementing biologically plausible mechanisms. The main limitation of the original model was its instability, which we addressed by incorporating a representation of the mechanism of synaptic scaling and examining the effects of optimizing parameters in the model. We show that the updated model retains all the merits of the original model, while being more stable and capable of generating networks that are in several aspects similar to those found in real brains."}], "ArticleTitle": "Synaptic Scaling Improves the Stability of Neural Mass Models Capable of Simulating Brain Plasticity."}, "32795230": {"mesh": [], "AbstractText": [{"section": null, "text": "We study active learning (AL) based on gaussian processes (GPs) for efficiently enumerating all of the local minimum solutions of a black-box function. This problem is challenging because local solutions are characterized by their zero gradient and positive-definite Hessian properties, but those derivatives cannot be directly observed. We propose a new AL method in which the input points are sequentially selected such that the confidence intervals of the GP derivatives are effectively updated for enumerating local minimum solutions. We theoretically analyze the proposed method and demonstrate its usefulness through numerical experiments."}], "ArticleTitle": "Active Learning for Enumerating Local Minima Based on Gaussian Process Derivatives."}, "30576613": {"mesh": [], "AbstractText": [{"section": null, "text": "The principles of neural encoding and computations are inherently collective and usually involve large populations of interacting neurons with highly correlated activities. While theories of neural function have long recognized the importance of collective effects in populations of neurons, only in the past two decades has it become possible to record from many cells simultaneously using advanced experimental techniques with single-spike resolution and to relate these correlations to function and behavior. This review focuses on the modeling and inference approaches that have been recently developed to describe the correlated spiking activity of populations of neurons. We cover a variety of models describing correlations between pairs of neurons, as well as between larger groups, synchronous or delayed in time, with or without the explicit influence of the stimulus, and including or not latent variables. We discuss the advantages and drawbacks or each method, as well as the computational challenges related to their application to recordings of ever larger populations."}], "ArticleTitle": "Modeling the Correlated Activity of Neural Populations: A Review."}, "30576618": {"mesh": ["Action Potentials", "Animals", "Humans", "Models, Neurological", "Photic Stimulation", "Retina", "Retinal Ganglion Cells"], "AbstractText": [{"section": null, "text": "Within a given brain region, individual neurons exhibit a wide variety of different feature selectivities. Here, we investigated the impact of this extensive functional diversity on the population neural code. Our approach was to build optimal decoders to discriminate among stimuli using the spiking output of a real, measured neural population and compare its performance against a matched, homogeneous neural population with the same number of cells and spikes. Analyzing large populations of retinal ganglion cells, we found that the real, heterogeneous population can yield a discrimination error lower than the homogeneous population by several orders of magnitude and consequently can encode much more visual information. This effect increases with population size and with graded degrees of heterogeneity. We complemented these results with an analysis of coding based on the Chernoff distance, as well as derivations of inequalities on coding in certain limits, from which we can conclude that the beneficial effect of heterogeneity occurs over a broad set of conditions. Together, our results indicate that the presence of functional diversity in neural populations can enhance their coding fidelity appreciably. A noteworthy outcome of our study is that this effect can be extremely strong and should be taken into account when investigating design principles for neural circuits."}], "ArticleTitle": "Functional Diversity in the Retina Improves the Population Code."}, "30645181": {"mesh": ["Biomimetics", "Humans", "Models, Biological", "Musculoskeletal Physiological Phenomena", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Redundancy is a fundamental characteristic of many biological processes such as those in the genetic, visual, muscular, and nervous systems, yet its driven mechanism has not been fully comprehended. Until recently, the only understanding of redundancy is as a mean to attain fault tolerance, which is reflected in the design of many man-made systems. On the contrary, our previous work on redundant sensing (RS) has demonstrated an example where redundancy can be engineered solely for enhancing accuracy and precision. The design was inspired by the binocular structure of human vision, which we believe may share a similar operation. In this letter, we present a unified theory describing how such utilization of redundancy is feasible through two complementary mechanisms: representational redundancy (RPR) and entangled redundancy (ETR). We also point out two additional examples where our new understanding of redundancy can be applied to justify a system's superior performance. One is the human musculoskeletal system (HMS), a biological instance, and the other is the deep residual neural network (ResNet), an artificial counterpart. We envision that our theory would provide a framework for the future development of bio-inspired redundant artificial systems, as well as assist studies of the fundamental mechanisms governing various biological processes."}], "ArticleTitle": "Advancing System Performance with Redundancy: From Biological to Artificial Designs."}, "32069174": {"mesh": [], "AbstractText": [{"section": null, "text": "We investigate an approach based on DC (Difference of Convex functions) programming and DCA (DC Algorithm) for online learning techniques. The prediction problem of an online learner can be formulated as a DC program for which online DCA is applied. We propose the two so-called complete/approximate versions of online DCA scheme and prove their logarithmic/sublinear regrets. Six online DCA-based algorithms are developed for online binary linear classification. Numerical experiments on a variety of benchmark classification data sets show the efficiency of our proposed algorithms in comparison with the state-of-the-art online classification algorithms."}], "ArticleTitle": "Online Learning Based on Online DCA and Application to Online Classification."}, "32687770": {"mesh": ["Action Potentials", "Computer Simulation", "Humans", "Nerve Net", "Neural Networks, Computer", "Neurons", "Synaptic Transmission"], "AbstractText": [{"section": null, "text": "Bursting plays an important role in neural communication. At the population level, macroscopic bursting has been identified in populations of neurons that do not express intrinsic bursting mechanisms. For the analysis of phase transitions between bursting and non-bursting states, mean-field descriptions of macroscopic bursting behavior are a valuable tool. In this article, we derive mean-field descriptions of populations of spiking neurons and examine whether states of collective bursting behavior can arise from short-term adaptation mechanisms. Specifically, we consider synaptic depression and spike-frequency adaptation in networks of quadratic integrate-and-fire neurons. Analyzing the mean-field model via bifurcation analysis, we find that bursting behavior emerges for both types of short-term adaptation. This bursting behavior can coexist with steady-state behavior, providing a bistable regime that allows for transient switches between synchronized and nonsynchronized states of population dynamics. For all of these findings, we demonstrate a close correspondence between the spiking neural network and the mean-field model. Although the mean-field model has been derived under the assumptions of an infinite population size and all-to-all coupling inside the population, we show that this correspondence holds even for small, sparsely coupled networks. In summary, we provide mechanistic descriptions of phase transitions between bursting and steady-state population dynamics, which play important roles in both healthy neural communication and neurological disorders."}], "ArticleTitle": "A Mean-Field Description of Bursting Dynamics in Spiking Neural Networks with Short-Term Adaptation."}, "30576616": {"mesh": [], "AbstractText": [{"section": null, "text": "We propose a general framework to accelerate significantly the algorithms for nonnegative matrix factorization (NMF). This framework is inspired from the extrapolation scheme used to accelerate gradient methods in convex optimization and from the method of parallel tangents. However, the use of extrapolation in the context of the exact coordinate descent algorithms tackling the nonconvex NMF problems is novel. We illustrate the performance of this approach on two state-of-the-art NMF algorithms: accelerated hierarchical alternating least squares and alternating nonnegative least squares, using synthetic, image, and document data sets."}], "ArticleTitle": "Accelerating Nonnegative Matrix Factorization Algorithms Using Extrapolation."}, "30216140": {"mesh": ["Adult", "Brain-Computer Interfaces", "Humans", "Male", "Neural Networks, Computer", "Quadriplegia", "User-Computer Interface"], "AbstractText": [{"section": null, "text": "Intracortical brain computer interfaces can enable individuals with paralysis to control external devices through voluntarily modulated brain activity. Decoding quality has been previously shown to degrade with signal nonstationarities-specifically, the changes in the statistics of the data between training and testing data sets. This includes changes to the neural tuning profiles and baseline shifts in firing rates of recorded neurons, as well as nonphysiological noise. While progress has been made toward providing long-term user control via decoder recalibration, relatively little work has been dedicated to making the decoding algorithm more resilient to signal nonstationarities. Here, we describe how principled kernel selection with gaussian process regression can be used within a Bayesian filtering framework to mitigate the effects of commonly encountered nonstationarities. Given a supervised training set of (neural features, intention to move in a direction)-pairs, we use gaussian process regression to predict the intention given the neural data. We apply kernel embedding for each neural feature with the standard radial basis function. The multiple kernels are then summed together across each neural dimension, which allows the kernel to effectively ignore large differences that occur only in a single feature. The summed kernel is used for real-time predictions of the posterior mean and variance under a gaussian process framework. The predictions are then filtered using the discriminative Kalman filter to produce an estimate of the neural intention given the history of neural data. We refer to the multiple kernel approach combined with the discriminative Kalman filter as the MK-DKF. We found that the MK-DKF decoder was more resilient to nonstationarities frequently encountered in-real world settings yet provided similar performance to the currently used Kalman decoder. These results demonstrate a method by which neural decoding can be made more resistant to nonstationarities."}], "ArticleTitle": "Robust Closed-Loop Control of a Cursor in a Person with Tetraplegia using Gaussian Process Regression."}, "29652591": {"mesh": ["Computer Simulation", "Neural Networks, Computer", "Nonlinear Dynamics", "Reinforcement, Psychology", "Robotics"], "AbstractText": [{"section": null, "text": "We propose a neural network model for reinforcement learning to control a robotic manipulator with unknown parameters and dead zones. The model is composed of three networks. The state of the robotic manipulator is predicted by the state network of the model, the action policy is learned by the action network, and the performance index of the action policy is estimated by a critic network. The three networks work together to optimize the performance index based on the reinforcement learning control scheme. The convergence of the learning methods is analyzed. Application of the proposed model on a simulated two-link robotic manipulator demonstrates the effectiveness and the stability of the model."}], "ArticleTitle": "A Reinforcement Learning Neural Network for Robotic Manipulator Control."}, "31113302": {"mesh": ["Algorithms", "Bayes Theorem", "Computer Simulation", "Learning", "Perception", "Psychomotor Performance", "Sensation"], "AbstractText": [{"section": null, "text": "This letter shows by digital simulation that a simple rule applied to one-dimensional self-organized maps for integrating sensory perceptions from two identical sources yielding position information as integers, corrupted by independent noise sources, yields almost statistically optimal results for position estimation as determined by maximum likelihood estimation. There is no learning of the corrupting noise sources nor is any information about the statistics of the noise sources available to the integrating process. The simple rule employed yields a measure of the quality of the estimated position of the source. The letter also shows that if the Bayesian estimates, which are rational numbers, are rounded in order to comply with the stipulation that integers be identified, the Bayesian estimation will have a larger variance than the proposed integration."}], "ArticleTitle": "A Case of Near-Optimal Sensory Integration Based on Kohonen Self-Organizing Maps."}, "32687768": {"mesh": [], "AbstractText": [{"section": null, "text": "Sparse signal representations have gained much interest recently in both signal processing and statistical communities. Compared to orthogonal matching pursuit (OMP) and basis pursuit, which solve the L0 and L1 constrained sparse least-squares problems, respectively, least angle regression (LARS) is a computationally efficient method to solve both problems for all critical values of the regularization parameter &#955;. However, all of these methods are not suitable for solving large multidimensional sparse least-squares problems, as they would require extensive computational power and memory. An earlier generalization of OMP, known as Kronecker-OMP, was developed to solve the L0 problem for large multidimensional sparse least-squares problems. However, its memory usage and computation time increase quickly with the number of problem dimensions and iterations. In this letter, we develop a generalization of LARS, tensor least angle regression (T-LARS) that could efficiently solve either large L0 or large L1 constrained multidimensional, sparse, least-squares problems (underdetermined or overdetermined) for all critical values of the regularization parameter &#955; and with lower computational complexity and memory usage than Kronecker-OMP. To demonstrate the validity and performance of our T-LARS algorithm, we used it to successfully obtain different sparse representations of two relatively large 3D brain images, using fixed and learned separable overcomplete dictionaries, by solving both L0 and L1 constrained sparse least-squares problems. Our numerical experiments demonstrate that our T-LARS algorithm is significantly faster (46 to 70 times) than Kronecker-OMP in obtaining K-sparse solutions for multilinear leastsquares problems. However, the K-sparse solutions obtained using Kronecker-OMP always have a slightly lower residual error (1.55% to 2.25%) than ones obtained by T-LARS. Therefore, T-LARS could be an important tool for numerous multidimensional biomedical signal processing applications."}], "ArticleTitle": "Tensor Least Angle Regression for Sparse Representations of Multidimensional Signals."}, "29381446": {"mesh": ["Action Potentials", "Animals", "Association Learning", "Computer Simulation", "Fear", "Humans", "Likelihood Functions", "Markov Chains", "Models, Neurological", "Neurons", "Nonlinear Dynamics", "Time Factors"], "AbstractText": [{"section": null, "text": "A fundamental problem in neuroscience is to characterize the dynamics of spiking from the neurons in a circuit that is involved in learning about a stimulus or a contingency. A key limitation of current methods to analyze neural spiking data is the need to collapse neural activity over time or trials, which may cause the loss of information pertinent to understanding the function of a neuron or circuit. We introduce a new method that can determine not only the trial-to-trial dynamics that accompany the learning of a contingency by a neuron, but also the latency of this learning with respect to the onset of a conditioned stimulus. The backbone of the method is a separable two-dimensional (2D) random field (RF) model of neural spike rasters, in which the joint conditional intensity function of a neuron over time and trials depends on two latent Markovian state sequences that evolve separately but in parallel. Classical tools to estimate state-space models cannot be applied readily to our 2D separable RF model. We develop efficient statistical and computational tools to estimate the parameters of the separable 2D RF model. We apply these to data collected from neurons in the prefrontal cortex in an experiment designed to characterize the neural underpinnings of the associative learning of fear in mice. Overall, the separable 2D RF model provides a detailed, interpretable characterization of the dynamics of neural spiking that accompany the learning of a contingency."}], "ArticleTitle": "Estimating a Separably Markov Random Field from Binary Observations."}, "28777724": {"mesh": [], "AbstractText": [{"section": null, "text": "This article offers a formal account of curiosity and insight in terms of active (Bayesian) inference. It deals with the dual problem of inferring states of the world and learning its statistical structure. In contrast to current trends in machine learning (e.g., deep learning), we focus on how people attain insight and understanding using just a handful of observations, which are solicited through curious behavior. We use simulations of abstract rule learning and approximate Bayesian inference to show that minimizing (expected) variational free energy leads to active sampling of novel contingencies. This epistemic behavior closes explanatory gaps in generative models of the world, thereby reducing uncertainty and satisfying curiosity. We then move from epistemic learning to model selection or structure learning to show how abductive processes emerge when agents test plausible hypotheses about symmetries (i.e., invariances or rules) in their generative models. The ensuing Bayesian model reduction evinces mechanisms associated with sleep and has all the hallmarks of \"aha\" moments. This formulation moves toward a computational account of consciousness in the pre-Cartesian sense of sharable knowledge (i.e., con: \"together\"; scire: \"to know\")."}], "ArticleTitle": "Active Inference, Curiosity and Insight."}, "29381440": {"mesh": [], "AbstractText": [{"section": null, "text": "We extend the neural Turing machine (NTM) model into a dynamic neural Turing machine (D-NTM) by introducing trainable address vectors. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies, including both linear and nonlinear ones. We implement the D-NTM with both continuous and discrete read and write mechanisms. We investigate the mechanisms and effects of learning to read and write into a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU controller. We provide extensive analysis of our model and compare different variations of neural Turing machines on this task. We show that our model outperforms long short-term memory and NTM variants. We provide further experimental results on the sequential [Formula: see text]MNIST, Stanford Natural Language Inference, associative recall, and copy tasks."}], "ArticleTitle": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes."}, "31525314": {"mesh": ["Animals", "Humans", "Models, Neurological", "Neural Networks, Computer", "Visual Cortex"], "AbstractText": [{"section": null, "text": "Deep convolutional neural networks (CNNs) are becoming increasingly popular models to predict neural responses in visual cortex. However, contextual effects, which are prevalent in neural processing and in perception, are not explicitly handled by current CNNs, including those used for neural prediction. In primary visual cortex, neural responses are modulated by stimuli spatially surrounding the classical receptive field in rich ways. These effects have been modeled with divisive normalization approaches, including flexible models, where spatial normalization is recruited only to the degree that responses from center and surround locations are deemed statistically dependent. We propose a flexible normalization model applied to midlevel representations of deep CNNs as a tractable way to study contextual normalization mechanisms in midlevel cortical areas. This approach captures nontrivial spatial dependencies among midlevel features in CNNs, such as those present in textures and other visual stimuli, that arise from tiling high-order features geometrically. We expect that the proposed approach can make predictions about when spatial normalization might be recruited in midlevel cortical areas. We also expect this approach to be useful as part of the CNN tool kit, therefore going beyond more restrictive fixed forms of normalization."}], "ArticleTitle": "Integrating Flexible Normalization into Midlevel Representations of Deep Convolutional Neural Networks."}, "30216142": {"mesh": ["Entropy", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "The success of CNNs is accompanied by deep models and heavy storage costs. For compressing CNNs, we propose an efficient and robust pruning approach, cross-entropy pruning (CEP). Given a trained CNN model, connections were divided into groups in a group-wise way according to their corresponding output neurons. All connections with their cross-entropy errors below a grouping threshold were then removed. A sparse model was obtained and the number of parameters in the baseline model significantly reduced. This letter also presents a highest cross-entropy pruning (HCEP) method that keeps a small portion of weights with the highest CEP. This method further improves the accuracy of CEP. To validate CEP, we conducted the experiments on low redundant networks that are hard to compress. For the MNIST data set, CEP achieves an 0.08% accuracy drop required by LeNet-5 benchmark with only 16% of original parameters. Our proposed CEP also reduces approximately 75% of the storage cost of AlexNet on the ILSVRC 2012 data set, increasing the top-1 errorby only 0.4% and top-5 error by only 0.2%. Compared with three existing methods on LeNet-5, our proposed CEP and HCEP perform significantly better than the existing methods in terms of the accuracy and stability. Some computer vision tasks on CNNs such as object detection and style transfer can be computed in a high-performance way using our CEP and HCEP strategies."}], "ArticleTitle": "Cross-Entropy Pruning for Compressing Convolutional Neural Networks."}, "31525311": {"mesh": ["Animals", "Humans", "Models, Neurological", "Neural Networks, Computer", "Neurons", "Synapses", "Synaptic Transmission"], "AbstractText": [{"section": null, "text": "In computational neural network models, neurons are usually allowed to excite some and inhibit other neurons, depending on the weight of their synaptic connections. The traditional way to transform such networks into networks that obey Dale's law (i.e., a neuron can either excite or inhibit) is to accompany each excitatory neuron with an inhibitory one through which inhibitory signals are mediated. However, this requires an equal number of excitatory and inhibitory neurons, whereas a realistic number of inhibitory neurons is much smaller. In this letter, we propose a model of nonlinear interaction of inhibitory synapses on dendritic compartments of excitatory neurons that allows the excitatory neurons to mediate inhibitory signals through a subset of the inhibitory population. With this construction, the number of required inhibitory neurons can be reduced tremendously."}], "ArticleTitle": "Mutual Inhibition with Few Inhibitory Cells via Nonlinear Inhibitory Synaptic Interaction."}, "33400899": {"mesh": ["Action Potentials", "Biophysics", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "This article proposes a methodology to extract a low-dimensional integrate-and-fire model from an arbitrarily detailed single-compartment biophysical model. The method aims at relating the modulation of maximal conductance parameters in the biophysical model to the modulation of parameters in the proposed integrate-and-fire model. The approach is illustrated on two well-documented examples of cellular neuromodulation: the transition between type I and type II excitability and the transition between spiking and bursting."}], "ArticleTitle": "From Biophysical to Integrate-and-Fire Modeling."}, "32946714": {"mesh": ["Animals", "Brain", "Brain Mapping", "Humans", "Image Processing, Computer-Assisted", "Magnetic Resonance Imaging", "Models, Neurological", "Nerve Net", "Neural Networks, Computer", "Neural Pathways"], "AbstractText": [{"section": null, "text": "Measuring functional connectivity from fMRI recordings is important in understanding processing in cortical networks. However, because the brain's connection pattern is complex, currently used methods are prone to producing false functional connections. We introduce differential covariance analysis, a new method that uses derivatives of the signal for estimating functional connectivity. We generated neural activities from dynamical causal modeling and a neural network of Hodgkin-Huxley neurons and then converted them to hemodynamic signals using the forward balloon model. The simulated fMRI signals, together with the ground-truth connectivity pattern, were used to benchmark our method with other commonly used methods. Differential covariance achieved better results in complex network simulations. This new method opens an alternative way to estimate functional connectivity."}], "ArticleTitle": "Differential Covariance: A New Method to Estimate Functional Connectivity in fMRI."}, "32946706": {"mesh": [], "AbstractText": [{"section": null, "text": "This letter proves that a ReLU network can approximate any continuous function with arbitrary precision by means of piecewise linear or constant approximations. For univariate function f(x), we use the composite of ReLUs to produce a line segment; all of the subnetworks of line segments comprise a ReLU network, which is a piecewise linear approximation to f(x). For multivariate function f(x), ReLU networks are constructed to approximate a piecewise linear function derived from triangulation methods approximating f(x). A neural unit called TRLU is designed by a ReLU network; the piecewise constant approximation, such as Haar wavelets, is implemented by rectifying the linear output of a ReLU network via TRLUs. New interpretations of deep layers, as well as some other results, are also presented."}], "ArticleTitle": "ReLU Networks Are Universal Approximators via Piecewise Linear or Constant Functions."}, "30576614": {"mesh": [], "AbstractText": [{"section": null, "text": "It is difficult to estimate the mutual information between spike trains because established methods require more data than are usually available. Kozachenko-Leonenko estimators promise to solve this problem but include a smoothing parameter that must be set. We propose here that the smoothing parameter can be selected by maximizing the estimated unbiased mutual information. This is tested on fictive data and shown to work very well."}], "ArticleTitle": "Calculating the Mutual Information between Two Spike Trains."}, "30979349": {"mesh": ["Algorithms", "Bayes Theorem", "Humans", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Bayesian networks have been widely used in many scientific fields for describing the conditional independence relationships for a large set of random variables. This letter proposes a novel algorithm, the so-called p-learning algorithm, for learning moral graphs for high-dimensional Bayesian networks. The moral graph is a Markov network representation of the Bayesian network and also the key to construction of the Bayesian network for constraint-based algorithms. The consistency of the p-learning algorithm is justified under the small-n, large-p scenario. The numerical results indicate that the p-learning algorithm significantly outperforms the existing ones, such as the PC, grow-shrink, incremental association, semi-interleaved hiton, hill-climbing, and max-min hill-climbing. Under the sparsity assumption, the p-learning algorithm has a computational complexity of O(p2) even in the worst case, while the existing algorithms have a computational complexity of O(p3) in the worst case."}], "ArticleTitle": "Learning Moral Graphs in Construction of High-Dimensional Bayesian Networks for Mixed Data."}, "28599118": {"mesh": ["Afferent Pathways", "Algorithms", "Calibration", "Computer Simulation", "Entropy", "Humans", "Models, Biological", "Models, Theoretical", "Motor Skills", "Neurons", "Sensation", "Sensorimotor Cortex"], "AbstractText": [{"section": null, "text": "Today digital sources supply a historically unprecedented component of human sensorimotor data, the consumption of which is correlated with poorly understood maladies such as Internet addiction disorder and Internet gaming disorder. Because both natural and digital sensorimotor data share common mathematical descriptions, one can quantify our informational sensorimotor needs using the signal processing metrics of entropy, noise, dimensionality, continuity, latency, and bandwidth. Such metrics describe in neutral terms the informational diet human brains require to self-calibrate, allowing individuals to maintain trusting relationships. With these metrics, we define the trust humans experience using the mathematical language of computational models, that is, as a primitive statistical algorithm processing finely grained sensorimotor data from neuromechanical interaction. This definition of neuromechanical trust implies that artificial sensorimotor inputs and interactions that attract low-level attention through frequent discontinuities and enhanced coherence will decalibrate a brain's representation of its world over the long term by violating the implicit statistical contract for which self-calibration evolved. Our hypersimplified mathematical understanding of human sensorimotor processing as multiscale, continuous-time vibratory interaction allows equally broad-brush descriptions of failure modes and solutions.&#160;For example, we model addiction in general as the result of homeostatic regulation gone awry in novel environments (sign reversal) and digital dependency as a sub-case in which the decalibration caused by digital sensorimotor data spurs yet more consumption of them. We predict that institutions can use these sensorimotor metrics to quantify media richness to improve employee well-being; that dyads and family-size groups will bond and heal best through low-latency, high-resolution multisensory interaction such as shared meals and reciprocated touch; and that individuals can improve sensory and sociosensory resolution through deliberate sensory reintegration practices. We conclude that we humans are the victims of our own success, our hands so skilled they fill the world with captivating things, our eyes so innocent they follow eagerly."}], "ArticleTitle": "Sensory Metrics of Neuromechanical Trust."}, "30314423": {"mesh": ["Animals", "Bayes Theorem", "Brain", "Decision Making", "Humans", "Models, Neurological", "Self Concept"], "AbstractText": [{"section": null, "text": "The Bayesian model of confidence posits that confidence reflects the observer's posterior probability that the decision is correct. Hangya, Sanders, and Kepecs (2016) have proposed that researchers can test the Bayesian model by deriving qualitative signatures of Bayesian confidence (i.e., patterns that one would expect to see if an observer were Bayesian) and looking for those signatures in human or animal data. We examine two proposed signatures, showing that their derivations contain hidden assumptions that limit their applicability and that they are neither necessary nor sufficient conditions for Bayesian confidence. One signature is an average confidence of 0.75 on trials with neutral evidence. This signature holds only when class-conditioned stimulus distributions do not overlap and when internal noise is very low. Another signature is that as stimulus magnitude increases, confidence increases on correct trials but decreases on incorrect trials. This divergence signature holds only when stimulus distributions do not overlap or when noise is high. Navajas et al. (2017) have proposed an alternative form of this signature; we find no indication that this alternative form is expected under Bayesian confidence. Our observations give us pause about the usefulness of the qualitative signatures of Bayesian confidence. To determine the nature of the computations underlying confidence reports, there may be no shortcut to quantitative model comparison."}], "ArticleTitle": "Limitations of Proposed Signatures of Bayesian Confidence."}, "29894659": {"mesh": ["Action Potentials", "Animals", "Brain", "Computers", "Humans", "Models, Neurological", "Nerve Net", "Neurons", "Nonlinear Dynamics"], "AbstractText": [{"section": null, "text": "The extreme complexity of the brain has attracted the attention of neuroscientists and other researchers for a long time. More recently, the neuromorphic hardware has matured to provide a new powerful tool to study neuronal dynamics. Here, we study neuronal dynamics using different settings on a neuromorphic chip built with flexible parameters of neuron models. Our unique setting in the network of leaky integrate-and-fire (LIF) neurons is to introduce a weak noise environment. We observed three different types of collective neuronal activities, or phases, separated by sharp boundaries, or phase transitions. From this, we construct a rudimentary phase diagram of neuronal dynamics and demonstrate that a noise-induced chaotic phase (N-phase), which is dominated by neuronal avalanche activity (intermittent aperiodic neuron firing), emerges in the presence of noise and its width grows with the noise intensity. The dynamics can be manipulated in this N-phase. Our results and comparison with clinical data is consistent with the literature and our previous work showing that healthy brain must reside in the N-phase. We argue that the brain phase diagram with further refinement may be used for the diagnosis and treatment of mental disease and also suggest that the dynamics may be manipulated to serve as a means of new information processing (e.g., for optimization). Neuromorphic chips, similar to the one we used but with a variety of neuron models, may be used to further enhance the understanding of human brain function and accelerate the development of neuroscience research."}], "ArticleTitle": "A Basic Phase Diagram of Neuronal Dynamics."}, "31113305": {"mesh": ["Action Potentials", "Computer Simulation", "Hippocampus", "Humans", "Models, Neurological", "Nerve Net", "Neurons", "Nonlinear Dynamics"], "AbstractText": [{"section": null, "text": "This letter proposes a novel method, multi-input, multi-output neuronal mode network (MIMO-NMN), for modeling encoding dynamics and functional connectivity in neural ensembles such as the hippocampus. Compared with conventional approaches such as the Volterra-Wiener model, linear-nonlinear-cascade (LNC) model, and generalized linear model (GLM), the NMN has several advantages in terms of estimation accuracy, model interpretation, and functional connectivity analysis. We point out the limitations of current neural spike modeling methods, especially the estimation biases caused by the imbalanced class problem when the number of zeros is significantly larger than ones in the spike data. We use synthetic data to test the performance of NMN with a comparison of the traditional methods, and the results indicate the NMN approach could reduce the imbalanced class problem and achieve better predictions. Subsequently, we apply the MIMO-NMN method to analyze data from the human hippocampus. The results indicate that the MIMO-NMN method is a promising approach to modeling neural dynamics and analyzing functional connectivity of multi-neuronal data."}], "ArticleTitle": "Multi-Input, Multi-Output Neuronal Mode Network Approach to Modeling the Encoding Dynamics and Functional Connectivity of Neural Systems."}, "30314422": {"mesh": [], "AbstractText": [{"section": null, "text": "We study a multi-instance (MI) learning dimensionality-reduction algorithm through sparsity and orthogonality, which is especially useful for high-dimensional MI data sets. We develop a novel algorithm to handle both sparsity and orthogonality constraints that existing methods do not handle well simultaneously. Our main idea is to formulate an optimization problem where the sparse term appears in the objective function and the orthogonality term is formed as a constraint. The resulting optimization problem can be solved by using approximate augmented Lagrangian iterations as the outer loop and inertial proximal alternating linearized minimization (iPALM) iterations as the inner loop. The main advantage of this method is that both sparsity and orthogonality can be satisfied in the proposed algorithm. We show the global convergence of the proposed iterative algorithm. We also demonstrate that the proposed algorithm can achieve high sparsity and orthogonality requirements, which are very important for dimensionality reduction. Experimental results on both synthetic and real data sets show that the proposed algorithm can obtain learning performance comparable to that of other tested MI learning algorithms."}], "ArticleTitle": "Multi-Instance Dimensionality Reduction via Sparsity and Orthogonality."}, "30979350": {"mesh": ["Humans", "Models, Neurological", "Motion Perception", "Photic Stimulation", "Probability", "Retina", "Vision, Ocular", "Visual Cortex"], "AbstractText": [{"section": null, "text": "In this work, we propose a two-layered descriptive model for motion processing from retina to the cortex, with an event-based input from the asynchronous time-based image sensor (ATIS) camera. Spatial and spatiotemporal filtering of visual scenes by motion energy detectors has been implemented in two steps in a simple layer of a lateral geniculate nucleus model and a set of three-dimensional Gabor kernels, eventually forming a probabilistic population response. The high temporal resolution of independent and asynchronous local sensory pixels from the ATIS provides a realistic stimulation to study biological motion processing, as well as developing bio-inspired motion processors for computer vision applications. Our study combines two significant theories in neuroscience: event-based stimulation and probabilistic sensory representation. We have modeled how this might be done at the vision level, as well as suggesting this framework as a generic computational principle among different sensory modalities."}], "ArticleTitle": "Asynchronous Event-Based Motion Processing: From Visual Events to Probabilistic Sensory Representation."}, "32433903": {"mesh": [], "AbstractText": [{"section": null, "text": "Data samples collected for training machine learning models are typically assumed to be independent and identically distributed (i.i.d.). Recent research has demonstrated that this assumption can be problematic as it simplifies the manifold of structured data. This has motivated different research areas such as data poisoning, model improvement, and explanation of machine learning models. In this work, we study the influence of a sample on determining the intrinsic topological features of its underlying manifold. We propose the Shapley homology framework, which provides a quantitative metric for the influence of a sample of the homology of a simplicial complex. Our proposed framework consists of two main parts: homology analysis, where we compute the Betti number of the target topological space, and Shapley value calculation, where we decompose the topological features of a complex built from data points to individual points. By interpreting the influence as a probability measure, we further define an entropy that reflects the complexity of the data manifold. Furthermore, we provide a preliminary discussion of the connection of the Shapley homology to the Vapnik-Chervonenkis dimension. Empirical studies show that when the zero-dimensional Shapley homology is used on neighboring graphs, samples with higher influence scores have a greater impact on the accuracy of neural networks that determine graph connectivity and on several regular grammars whose higher entropy values imply greater difficulty in being learned."}], "ArticleTitle": "Shapley Homology: Topological Analysis of Sample Influence for Neural Networks."}, "32946710": {"mesh": ["Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Spiking neural networks (SNNs) with the event-driven manner of transmitting spikes consume ultra-low power on neuromorphic chips. However, training deep SNNs is still challenging compared to convolutional neural networks (CNNs). The SNN training algorithms have not achieved the same performance as CNNs. In this letter, we aim to understand the intrinsic limitations of SNN training to design better algorithms. First, the pros and cons of typical SNN training algorithms are analyzed. Then it is found that the spatiotemporal backpropagation algorithm (STBP) has potential in training deep SNNs due to its simplicity and fast convergence. Later, the main bottlenecks of the STBP algorithm are analyzed, and three conditions for training deep SNNs with the STBP algorithm are derived. By analyzing the connection between CNNs and SNNs, we propose a weight initialization algorithm to satisfy the three conditions. Moreover, we propose an error minimization method and a modified loss function to further improve the training performance. Experimental results show that the proposed method achieves 91.53% accuracy on the CIFAR10 data set with 1% accuracy increase over the STBP algorithm and decreases the training epochs on the MNIST data set to 15 epochs (over 13 times speed-up compared to the STBP algorithm). The proposed method also decreases classification latency by over 25 times compared to the CNN-SNN conversion algorithms. In addition, the proposed method works robustly for very deep SNNs, while the STBP algorithm fails in a 19-layer SNN."}], "ArticleTitle": "Analyzing and Accelerating the Bottlenecks of Training Deep SNNs With Backpropagation."}, "30314428": {"mesh": ["Animals", "Brain", "Humans", "Linear Models", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Generalized linear models (GLMs) have a wide range of applications in systems neuroscience describing the encoding of stimulus and behavioral variables, as well as the dynamics of single neurons. However, in any given experiment, many variables that have an impact on neural activity are not observed or not modeled. Here we demonstrate, in both theory and practice, how these omitted variables can result in biased parameter estimates for the effects that are included. In three case studies, we estimate tuning functions for common experiments in motor cortex, hippocampus, and visual cortex. We find that including traditionally omitted variables changes estimates of the original parameters and that modulation originally attributed to one variable is reduced after new variables are included. In GLMs describing single-neuron dynamics, we then demonstrate how postspike history effects can also be biased by omitted variables. Here we find that omitted variable bias can lead to mistaken conclusions about the stability of single-neuron firing. Omitted variable bias can appear in any model with confounders-where omitted variables modulate neural activity and the effects of the omitted variables covary with the included effects. Understanding how and to what extent omitted variable bias affects parameter estimates is likely to be important for interpreting the parameters and predictions of many neural encoding models."}], "ArticleTitle": "Omitted Variable Bias in GLMs of Neural Spiking Activity."}, "28410053": {"mesh": ["Association Learning", "Brain", "Computer Simulation", "Humans", "Memory", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Clique-based neural associative memories introduced by Gripon and Berrou (GB), have been shown to have good performance, and in our previous work we improved the learning capacity and retrieval rate by local coding and precoding in the presence of partial erasures. We now take a step forward and consider nested-clique graph structures for the network. The GB model stores patterns as small cliques, and we here replace these by nested cliques. Simulation results show that the nested-clique structure enhances the clique-based model."}], "ArticleTitle": "Nested-Clique Network Model of Neural Associative Memory."}, "31393828": {"mesh": [], "AbstractText": [{"section": null, "text": "Tasking machine learning to predict segments of a time series requires estimating the parameters of a ML model with input/output pairs from the time series. We borrow two techniques used in statistical data assimilation in order to accomplish this task: time-delay embedding to prepare our input data and precision annealing as a training method. The precision annealing approach identifies the global minimum of the action (-log[P]). In this way, we are able to identify the number of training pairs required to produce good generalizations (predictions) for the time series. We proceed from a scalar time series s(tn);tn=t0+n&#916;t and, using methods of nonlinear time series analysis, show how to produce a DE>1-dimensional time-delay embedding space in which the time series has no false neighbors as does the observed s(tn) time series. In that DE-dimensional space, we explore the use of feedforward multilayer perceptrons as network models operating on DE-dimensional input and producing DE-dimensional outputs."}], "ArticleTitle": "Machine Learning of Time Series Using Time-Delay Embedding and Precision Annealing."}, "28095192": {"mesh": ["Action Potentials", "Algorithms", "Animals", "Computer Simulation", "Humans", "Models, Neurological", "Neurons", "Time Factors"], "AbstractText": [{"section": null, "text": "Jitter-type spike resampling methods are routinely applied in neurophysiology for detecting temporal structure in spike trains (point processes). Several variations have been proposed. The concern has been raised, based on numerical experiments involving Poisson spike processes, that such procedures can be conservative. We study the issue and find it can be resolved by reemphasizing the distinction between spike-centered (basic) jitter and interval jitter. Focusing on spiking processes with no temporal structure, interval jitter generates an exact hypothesis test, guaranteeing valid conclusions. In contrast, such a guarantee is not available for spike-centered jitter. We construct explicit examples in which spike-centered jitter hallucinates temporal structure, in the sense of exaggerated false-positive rates. Finally, we illustrate numerically that Poisson approximations to jitter computations, while computationally efficient, can also result in inaccurate hypothesis tests. We highlight the value of classical statistical frameworks for guiding the design and interpretation of spike resampling methods."}], "ArticleTitle": "Spike-Centered Jitter Can Mistake Temporal Structure."}, "31951797": {"mesh": ["Adolescent", "Adult", "Aged", "Autistic Disorder", "Child", "Child, Preschool", "Female", "Humans", "Male", "Middle Aged", "Reproducibility of Results", "Sensitivity and Specificity", "Young Adult"], "AbstractText": [{"section": null, "text": "The research-grade Autism Diagnostic Observational Schedule (ADOS) is a broadly used instrument that informs and steers much of the science of autism. Despite its broad use, little is known about the empirical variability inherently present in the scores of the ADOS scale or their appropriateness to define change and its rate, to repeatedly use this test to characterize neurodevelopmental trajectories. Here we examine the empirical distributions of research-grade ADOS scores from 1324 records in a cross-section of the population comprising participants with autism between five and 65 years of age. We find that these empirical distributions violate the theoretical requirements of normality and homogeneous variance, essential for independence between bias and sensitivity. Further, we assess a subset of 52 typical controls versus those with autism and find a lack of proper elements to characterize neurodevelopmental trajectories in a coping nervous system changing at nonuniform, nonlinear rates. Repeating the assessments over four visits in a subset of the participants with autism for whom verbal criteria retained the same appropriate ADOS modules over the time span of the four visits reveals that switching the clinician changes the cutoff scores and consequently influences the diagnosis, despite maintaining fidelity in the same test's modules, room conditions, and tasks' fluidity per visit. Given the changes in probability distribution shape and dispersion of these ADOS scores, the lack of appropriate metric spaces to define similarity measures to characterize change and the impact that these elements have on sensitivity-bias codependencies and on longitudinal tracking of autism, we invite a discussion on readjusting the use of this test for scientific purposes."}], "ArticleTitle": "Hidden Aspects of the Research ADOS Are Bound to Affect Autism Science."}, "32946712": {"mesh": [], "AbstractText": [{"section": null, "text": "Marked point process models have recently been used to capture the coding properties of neural populations from multiunit electrophysiological recordings without spike sorting. These clusterless models have been shown in some instances to better describe the firing properties of neural populations than collections of receptive field models for sorted neurons and to lead to better decoding results. To assess their quality, we previously proposed a goodness-of-fit technique for marked point process models based on time rescaling, which for a correct model produces a set of uniform samples over a random region of space. However, assessing uniformity over such a region can be challenging, especially in high dimensions. Here, we propose a set of new transformations in both time and the space of spike waveform features, which generate events that are uniformly distributed in the new mark and time spaces. These transformations are scalable to multidimensional mark spaces and provide uniformly distributed samples in hypercubes, which are well suited for uniformity tests. We discuss the properties of these transformations and demonstrate aspects of model fit captured by each transformation. We also compare multiple uniformity tests to determine their power to identify lack-of-fit in the rescaled data. We demonstrate an application of these transformations and uniformity tests in a simulation study. Proofs for each transformation are provided in the appendix."}], "ArticleTitle": "Assessing Goodness-of-Fit in Marked Point Process Models of Neural Population Coding via Time and Rate Rescaling."}, "31335291": {"mesh": ["Action Potentials", "Algorithms", "Humans", "Machine Learning", "Models, Neurological", "Neural Networks, Computer", "Speech Recognition Software"], "AbstractText": [{"section": null, "text": "There is extensive evidence that biological neural networks encode information in the precise timing of the spikes generated and transmitted by neurons, which offers several advantages over rate-based codes. Here we adopt a vector space formulation of spike train sequences and introduce a new liquid state machine (LSM) network architecture and a new forward orthogonal regression algorithm to learn an input-output signal mapping or to decode the brain activity. The proposed algorithm uses precise spike timing to select the presynaptic neurons relevant to each learning task. We show that using precise spike timing to train the LSM and selecting the readout presynaptic neurons leads to a significant increase in performance on binary classification tasks, in decoding neural activity from multielectrode array recordings, as well as in a speech recognition task, compared with what is achieved using the standard architecture and training methods."}], "ArticleTitle": "Learning with Precise Spike Times: A New Decoding Algorithm for Liquid State Machines."}, "30216141": {"mesh": ["Animals", "Brain", "Humans", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Throughout the nervous system, information is commonly coded in activity distributed over populations of neurons. In idealized situations where a single, continuous stimulus is encoded in a homogeneous population code, the value of the encoded stimulus can be read out without bias. However, in many situations, multiple stimuli are simultaneously present; for example, multiple motion patterns might overlap. Here we find that when multiple stimuli that overlap in their neural representation are simultaneously encoded in the population, biases in the read-out emerge. Although the bias disappears in the absence of noise, the bias is remarkably persistent at low noise levels. The bias can be reduced by competitive encoding schemes or by employing complex decoders. To study the origin of the bias, we develop a novel general framework based on gaussian processes that allows an accurate calculation of the estimate distributions of maximum likelihood decoders, and reveals that the distribution of estimates is bimodal for overlapping stimuli. The results have implications for neural coding and behavioral experiments on, for instance, overlapping motion patterns."}], "ArticleTitle": "Unconscious Biases in Neural Populations Coding Multiple Stimuli."}, "31614102": {"mesh": [], "AbstractText": [{"section": null, "text": "We describe the construction and theoretical analysis of a framework derived from canonical neurophysiological principles that model the competing dynamics of incident signals into nodes along directed edges in a network. The framework describes the dynamics between the offset in the latencies of propagating signals, which reflect the geometry of the edges and conduction velocities, and the internal refractory dynamics and processing times of the downstream node receiving the signals. This framework naturally extends to the construction of a perceptron model that takes into account such dynamic geometric considerations. We first describe the model in detail, culminating with the model of a geometric dynamic perceptron. We then derive upper and lower bounds for a notion of optimal efficient signaling between vertex pairs based on the structure of the framework. Efficient signaling in the context of the framework we develop here means that there needs to be a temporal match between the arrival time of the signals relative to how quickly nodes can internally process signals. These bounds reflect numerical constraints on the compensation of the timing of signaling events of upstream nodes attempting to activate downstream nodes they connect into that preserve this notion of efficiency. When a mismatch between signal arrival times and the internal states of activated nodes occurs, it can cause a breakdown in the signaling dynamics of the network. In contrast to essentially all of the current state of the art in machine learning, this work provides a theoretical foundation for machine learning and intelligence architectures based on the timing of node activations and their abilities to respond rather than necessary changes in synaptic weights. At the same time, the theoretical ideas we developed are guiding the discovery of experimentally testable new structure-function principles in the biological brain."}], "ArticleTitle": "The Effect of Signaling Latencies and Node Refractory States on the Dynamics of Networks."}, "33253028": {"mesh": [], "AbstractText": [{"section": null, "text": "The positive-negative axis of emotional valence has long been recognized as fundamental to adaptive behavior, but its origin and underlying function have largely eluded formal theorizing and computational modeling. Using deep active inference, a hierarchical inference scheme that rests on inverting a model of how sensory data are generated, we develop a principled Bayesian model of emotional valence. This formulation asserts that agents infer their valence state based on the expected precision of their action model-an internal estimate of overall model fitness (\"subjective fitness\"). This index of subjective fitness can be estimated within any environment and exploits the domain generality of second-order beliefs (beliefs about beliefs). We show how maintaining internal valence representations allows the ensuing affective agent to optimize confidence in action selection preemptively. Valence representations can in turn be optimized by leveraging the (Bayes-optimal) updating term for subjective fitness, which we label affective charge (AC). AC tracks changes in fitness estimates and lends a sign to otherwise unsigned divergences between predictions and outcomes. We simulate the resulting affective inference by subjecting an in silico affective agent to a T-maze paradigm requiring context learning, followed by context reversal. This formulation of affective inference offers a principled account of the link between affect, (mental) action, and implicit metacognition. It characterizes how a deep biological system can infer its affective state and reduce uncertainty about such inferences through internal action (i.e., top-down modulation of priors that underwrite confidence). Thus, we demonstrate the potential of active inference to provide a formal and computationally tractable account of affect. Our demonstration of the face validity and potential utility of this formulation represents the first step within a larger research program. Next, this model can be leveraged to test the hypothesized role of valence by fitting the model to behavioral and neuronal responses."}], "ArticleTitle": "Deeply Felt Affect: The Emergence of Valence in Deep Active Inference."}, "32946709": {"mesh": ["Animals", "Cerebellum", "Computer Simulation", "Humans", "Models, Neurological", "Time Perception"], "AbstractText": [{"section": null, "text": "The cerebellum is known to have an important role in sensing and execution of precise time intervals, but the mechanism by which arbitrary time intervals can be recognized and replicated with high precision is unknown. We propose a computational model in which precise time intervals can be identified from the pattern of individual spike activity in a population of parallel fibers in the cerebellar cortex. The model depends on the presence of repeatable sequences of spikes in response to conditioned stimulus input. We emulate granule cells using a population of Izhikevich neuron approximations driven by random but repeatable mossy fiber input. We emulate long-term depression (LTD) and long-term potentiation (LTP) synaptic plasticity at the parallel fiber to Purkinje cell synapse. We simulate a delay conditioning paradigm with a conditioned stimulus (CS) presented to the mossy fibers and an unconditioned stimulus (US) some time later issued to the Purkinje cells as a teaching signal. We show that Purkinje cells rapidly adapt to decrease firing probability following onset of the CS only at the interval for which the US had occurred. We suggest that detection of replicable spike patterns provides an accurate and easily learned timing structure that could be an important mechanism for behaviors that require identification and production of precise time intervals."}], "ArticleTitle": "A Cerebellar Computational Mechanism for Delay Conditioning at Precise Time Intervals."}, "30764741": {"mesh": ["Action Potentials", "Adaptation, Physiological", "Animals", "Computer Simulation", "Models, Neurological", "Neural Inhibition", "Neurons", "Periodicity"], "AbstractText": [{"section": null, "text": "Accurate population models are needed to build very large-scale neural models, but their derivation is difficult for realistic networks of neurons, in particular when nonlinear properties are involved, such as conductance-based interactions and spike-frequency adaptation. Here, we consider such models based on networks of adaptive exponential integrate-and-fire excitatory and inhibitory neurons. Using a master equation formalism, we derive a mean-field model of such networks and compare it to the full network dynamics. The mean-field model is capable of correctly predicting the average spontaneous activity levels in asynchronous irregular regimes similar to in vivo activity. It also captures the transient temporal response of the network to complex external inputs. Finally, the mean-field model is also able to quantitatively describe regimes where high- and low-activity states alternate (up-down state dynamics), leading to slow oscillations. We conclude that such mean-field models are biologically realistic in the sense that they can capture both spontaneous and evoked activity, and they naturally appear as candidates to build very large-scale models involving multiple brain areas."}], "ArticleTitle": "Biologically Realistic Mean-Field Models of Conductance-Based Networks of Spiking Neurons with Adaptation."}, "31614100": {"mesh": ["Animals", "Auditory Perception", "Bayes Theorem", "Emotional Intelligence", "Finches", "Learning", "Models, Neurological", "Social Perception"], "AbstractText": [{"section": null, "text": "To exhibit social intelligence, animals have to recognize whom they are communicating with. One way to make this inference is to select among internal generative models of each conspecific who may be encountered. However, these models also have to be learned via some form of Bayesian belief updating. This induces an interesting problem: When receiving sensory input generated by a particular conspecific, how does an animal know which internal model to update? We consider a theoretical and neurobiologically plausible solution that enables inference and learning of the processes that generate sensory inputs (e.g., listening and understanding) and reproduction of those inputs (e.g., talking or singing), under multiple generative models. This is based on recent advances in theoretical neurobiology-namely, active inference and post hoc (online) Bayesian model selection. In brief, this scheme fits sensory inputs under each generative model. Model parameters are then updated in proportion to the probability that each model could have generated the input (i.e., model evidence). The proposed scheme is demonstrated using a series of (real zebra finch) birdsongs, where each song is generated by several different birds. The scheme is implemented using physiologically plausible models of birdsong production. We show that generalized Bayesian filtering, combined with model selection, leads to successful learning across generative models, each possessing different parameters. These results highlight the utility of having multiple internal models when making inferences in social environments with multiple sources of sensory information."}], "ArticleTitle": "Bayesian Filtering with Multiple Internal Models: Toward a Theory of Social Intelligence."}, "33400900": {"mesh": [], "AbstractText": [{"section": null, "text": "The expected free energy (EFE) is a central quantity in the theory of active inference. It is the quantity that all active inference agents are mandated to minimize through action, and its decomposition into extrinsic and intrinsic value terms is key to the balance of exploration and exploitation that active inference agents evince. Despite its importance, the mathematical origins of this quantity and its relation to the variational free energy (VFE) remain unclear. In this letter, we investigate the origins of the EFE in detail and show that it is not simply \"the free energy in the future.\" We present a functional that we argue is the natural extension of the VFE but actively discourages exploratory behavior, thus demonstrating that exploration does not directly follow from free energy minimization into the future. We then develop a novel objective, the free energy of the expected future (FEEF), which possesses both the epistemic component of the EFE and an intuitive mathematical grounding as the divergence between predicted and desired futures."}], "ArticleTitle": "Whence the Expected Free Energy?"}, "31335289": {"mesh": ["Animals", "Forecasting", "Macaca mulatta", "Male", "Markov Chains", "Neurons", "Photic Stimulation", "Probability", "Psychomotor Performance"], "AbstractText": [{"section": null, "text": "Beyond average firing rate, other measurable signals of neuronal activity are fundamental to an understanding of behavior. Recently, hidden Markov models (HMMs) have been applied to neural recordings and have described how neuronal ensembles process information by going through sequences of different states. Such collective dynamics are impossible to capture by just looking at the average firing rate. To estimate how well HMMs can decode information contained in single trials, we compared HMMs with a recently developed classification method based on the peristimulus time histogram (PSTH). The accuracy of the two methods was tested by using the activity of prefrontal neurons recorded while two monkeys were engaged in a strategy task. In this task, the monkeys had to select one of three spatial targets based on an instruction cue and on their previous choice. We show that by using the single trial's neural activity in a period preceding action execution, both models were able to classify the monkeys' choice with an accuracy higher than by chance. Moreover, the HMM was significantly more accurate than the PSTH-based method, even in cases in which the HMM performance was low, although always above chance. Furthermore, the accuracy of both methods was related to the number of neurons exhibiting spatial selectivity within an experimental session. Overall, our study shows that neural activity is better described when not only the mean activity of individual neurons is considered and that therefore, the study of other signals rather than only the average firing rate is fundamental to an understanding of the dynamics of neuronal ensembles."}], "ArticleTitle": "Hidden Markov Models Predict the Future Choice Better Than a PSTH-Based Method."}, "27136970": {"mesh": ["Brain", "Electroencephalography", "Humans", "Models, Neurological", "Random Allocation"], "AbstractText": [{"section": null, "text": "In a pioneering classic, Warren McCulloch and Walter Pitts proposed a model of the central nervous system. Motivated by EEG recordings of normal brain activity, Chv&#225;tal and Goldsmith asked whether these dynamical systems can be engineered to produce trajectories that are irregular, disorderly, and apparently unpredictable. We show that they cannot build weak pseudorandom functions."}], "ArticleTitle": "McCulloch-Pitts Brains and Pseudorandom Functions."}, "30148708": {"mesh": ["Animals", "Models, Neurological", "Nonlinear Dynamics", "Rats", "Retinal Ganglion Cells"], "AbstractText": [{"section": null, "text": "Neural noise sets a limit to information transmission in sensory systems. In several areas, the spiking response (to a repeated stimulus) has shown a higher degree of regularity than predicted by a Poisson process. However, a simple model to explain this low variability is still lacking. Here we introduce a new model, with a correction to Poisson statistics, that can accurately predict the regularity of neural spike trains in response to a repeated stimulus. The model has only two parameters but can reproduce the observed variability in retinal recordings in various conditions. We show analytically why this approximation can work. In a model of the spike-emitting process where a refractory period is assumed, we derive that our simple correction can well approximate the spike train statistics over a broad range of firing rates. Our model can be easily plugged to stimulus processing models, like a linear-nonlinear model or its generalizations, to replace the Poisson spike train hypothesis that is commonly assumed. It estimates the amount of information transmitted much more accurately than Poisson models in retinal recordings. Thanks to its simplicity, this model has the potential to explain low variability in other areas."}], "ArticleTitle": "A Simple Model for Low Variability in Neural Spike Trains."}, "30148702": {"mesh": [], "AbstractText": [{"section": null, "text": "We consider the problem of classifying data manifolds where each manifold represents invariances that are parameterized by continuous degrees of freedom. Conventional data augmentation methods rely on sampling large numbers of training examples from these manifolds. Instead, we propose an iterative algorithm, [Formula: see text], based on a cutting plane approach that efficiently solves a quadratic semi-infinite programming problem to find the maximum margin solution. We provide a proof of convergence as well as a polynomial bound on the number of iterations required for a desired tolerance in the objective function. The efficiency and performance of [Formula: see text] are demonstrated in high-dimensional simulations and on image manifolds generated from the ImageNet data set. Our results indicate that [Formula: see text] is able to rapidly learn good classifiers and shows superior generalization performance compared with conventional maximum margin methods using data augmentation methods."}], "ArticleTitle": "Learning Data Manifolds with a Cutting Plane Method."}, "32343646": {"mesh": ["Action Potentials", "Animals", "Calcium Signaling", "Female", "Hippocampus", "Male", "Mice", "Mice, Inbred C57BL", "Optical Imaging", "Supervised Machine Learning", "Unsupervised Machine Learning"], "AbstractText": [{"section": null, "text": "Large-scale fluorescence calcium imaging methods have become widely adopted for studies of long-term hippocampal and cortical neuronal dynamics. Pyramidal neurons of the rodent hippocampus show spatial tuning in freely foraging or head-fixed navigation tasks. Development of efficient neural decoding methods for reconstructing the animal's position in real or virtual environments can provide a fast readout of spatial representations in closed-loop neuroscience experiments. Here, we develop an efficient strategy to extract features from fluorescence calcium imaging traces and further decode the animal's position. We validate our spike inference-free decoding methods in multiple in vivo calcium imaging recordings of the mouse hippocampus based on both supervised and unsupervised decoding analyses. We systematically investigate the decoding performance of our proposed methods with respect to the number of neurons, imaging frame rate, and signal-to-noise ratio. Our proposed supervised decoding analysis is ultrafast and robust, and thereby appealing for real-time position decoding applications based on calcium imaging."}], "ArticleTitle": "Efficient Position Decoding Methods Based on Fluorescence Calcium Imaging in the Mouse Hippocampus."}, "29652585": {"mesh": ["Algorithms", "Computer Simulation", "Humans", "Memory, Short-Term", "Models, Neurological", "Neural Networks, Computer", "Neurons"], "AbstractText": [{"section": null, "text": "To accommodate structured approaches of neural computation, we propose a class of recurrent neural networks for indexing and storing sequences of symbols or analog data vectors. These networks with randomized input weights and orthogonal recurrent weights implement coding principles previously described in vector symbolic architectures (VSA) and leverage properties of reservoir computing. In general, the storage in reservoir computing is lossy, and crosstalk noise limits the retrieval accuracy and information capacity. A novel theory to optimize memory performance in such networks is presented and compared with simulation experiments. The theory describes linear readout of analog data and readout with winner-take-all error correction of symbolic data as proposed in VSA models. We find that diverse VSA models from the literature have universal performance properties, which are superior to what previous analyses predicted. Further, we propose novel VSA models with the statistically optimal Wiener filter in the readout that exhibit much higher information capacity, in particular for storing analog data. The theory we present also applies to memory buffers, networks with gradual forgetting, which can operate on infinite data streams without memory overflow. Interestingly, we find that different forgetting mechanisms, such as attenuating recurrent weights or neural nonlinearities, produce very similar behavior if the forgetting time constants are matched. Such models exhibit extensive capacity when their forgetting time constant is optimized for given noise conditions and network size. These results enable the design of new types of VSA models for the online processing of data streams."}], "ArticleTitle": "A Theory of Sequence Indexing and Working Memory in Recurrent Neural Networks."}, "29064787": {"mesh": [], "AbstractText": [{"section": null, "text": "Accurate causal inference among time series helps to better understand the interactive scheme behind the temporal variables. For time series analysis, an unavoidable issue is the existence of time lag among different temporal variables. That is, past evidence would take some time to cause a future effect instead of an immediate response. To model this process, existing approaches commonly adopt a prefixed time window to define the lag. However, in many real-world applications, this parameter may vary among different time series, and it is hard to be predefined with a fixed value. In this letter, we propose to learn the causal relations as well as the lag among different time series simultaneously from data. Specifically, we develop a probabilistic decomposed slab-and-spike (DSS) model to perform the inference by applying a pair of decomposed spike-and-slab variables for the model coefficients, where the first variable is used to estimate the causal relationship and the second one captures the lag information among different temporal variables. For parameter inference, we propose an efficient expectation propagation (EP) algorithm to solve the DSS model. Experimental results conducted on both synthetic and real-world problems demonstrate the effectiveness of the proposed method. The revealed time lag can be well validated by the domain knowledge within the real-world applications."}], "ArticleTitle": "Temporal Causal Inference with Time Lag."}, "32187000": {"mesh": ["Algorithms", "Bayes Theorem", "Brain-Computer Interfaces", "Humans", "Learning", "Models, Biological", "Nonlinear Dynamics"], "AbstractText": [{"section": null, "text": "The Kalman filter provides a simple and efficient algorithm to compute the posterior distribution for state-space models where both the latent state and measurement models are linear and gaussian. Extensions to the Kalman filter, including the extended and unscented Kalman filters, incorporate linearizations for models where the observation model p(observation|state) is nonlinear. We argue that in many cases, a model for p(state|observation) proves both easier to learn and more accurate for latent state estimation. Approximating p(state|observation) as gaussian leads to a new filtering algorithm, the discriminative Kalman filter (DKF), which can perform well even when p(observation|state) is highly nonlinear and/or nongaussian. The approximation, motivated by the Bernstein-von Mises theorem, improves as the dimensionality of the observations increases. The DKF has computational complexity similar to the Kalman filter, allowing it in some cases to perform much faster than particle filters with similar precision, while better accounting for nonlinear and nongaussian observation models than Kalman-based extensions. When the observation model must be learned from training data prior to filtering, off-the-shelf nonlinear and nonparametric regression techniques can provide a gaussian model for p(observation|state) that cleanly integrates with the DKF. As part of the BrainGate2 clinical trial, we successfully implemented gaussian process regression with the DKF framework in a brain-computer interface to provide real-time, closed-loop cursor control to a person with a complete spinal cord injury. In this letter, we explore the theory underlying the DKF, exhibit some illustrative examples, and outline potential extensions."}], "ArticleTitle": "The Discriminative Kalman Filter for Bayesian Filtering with Nonlinear and Nongaussian Observation Models."}, "32946707": {"mesh": [], "AbstractText": [{"section": null, "text": "The brain may be considered as a synchronized dynamic network with several coherent dynamical units. However, concerns remain whether synchronizability is a stable state in the brain networks. If so, which index can best reveal the synchronizability in brain networks? To answer these questions, we tested the application of the spectral graph theory and the Shannon entropy as alternative approaches in neuroimaging. We specifically tested the alpha rhythm in the resting-state eye closed (rsEC) and the resting-state eye open (rsEO) conditions, a well-studied classical example of synchrony in neuroimaging EEG. Since the synchronizability of alpha rhythm is more stable during the rsEC than the rsEO, we hypothesized that our suggested spectral graph theory indices (as reliable measures to interpret the synchronizability of brain signals) should exhibit higher values in the rsEC than the rsEO condition. We performed two separate analyses of two different datasets (as elementary and confirmatory studies). Based on the results of both studies and in agreement with our hypothesis, the spectral graph indices revealed higher stability of synchronizability in the rsEC condition. The k-mean analysis indicated that the spectral graph indices can distinguish the rsEC and rsEO conditions by considering the synchronizability of brain networks. We also computed correlations among the spectral indices, the Shannon entropy, and the topological indices of brain networks, as well as random networks. Correlation analysis indicated that although the spectral and the topological properties of random networks are completely independent, these features are significantly correlated with each other in brain networks. Furthermore, we found that complexity in the investigated brain networks is inversely related to the stability of synchronizability. In conclusion, we revealed that the spectral graph theory approach can be reliably applied to study the stability of synchronizability of state-related brain networks."}], "ArticleTitle": "Synchrony and Complexity in State-Related EEG Networks: An Application of Spectral Graph Theory."}, "31951798": {"mesh": ["Acoustic Stimulation", "Auditory Perception", "Deep Learning", "Humans", "Neural Networks, Computer", "Photic Stimulation", "Speech", "Visual Perception"], "AbstractText": [{"section": null, "text": "Sensory processing is increasingly conceived in a predictive framework in which neurons would constantly process the error signal resulting from the comparison of expected and observed stimuli. Surprisingly, few data exist on the accuracy of predictions that can be computed in real sensory scenes. Here, we focus on the sensory processing of auditory and audiovisual speech. We propose a set of computational models based on artificial neural networks (mixing deep feedforward and convolutional networks), which are trained to predict future audio observations from present and past audio or audiovisual observations (i.e., including lip movements). Those predictions exploit purely local phonetic regularities with no explicit call to higher linguistic levels. Experiments are conducted on the multispeaker LibriSpeech audio speech database (around 100 hours) and on the NTCD-TIMIT audiovisual speech database (around 7 hours). They appear to be efficient in a short temporal range (25-50 ms), predicting 50% to 75% of the variance of the incoming stimulus, which could result in potentially saving up to three-quarters of the processing power. Then they quickly decrease and almost vanish after 250 ms. Adding information on the lips slightly improves predictions, with a 5% to 10% increase in explained variance. Interestingly the visual gain vanishes more slowly, and the gain is maximum for a delay of 75 ms between image and predicted sound."}], "ArticleTitle": "Evaluating the Potential Gain of Auditory and Audiovisual Speech-Predictive Coding Using Deep Learning."}, "28333591": {"mesh": [], "AbstractText": [{"section": null, "text": "In a constantly changing world, animals must account for environmental volatility when making decisions. To appropriately discount older, irrelevant information, they need to learn the rate at which the environment changes. We develop an ideal observer model capable of inferring the present state of the environment along with its rate of change. Key to this computation is an update of the posterior probability of all possible change point counts. This computation can be challenging, as the number of possibilities grows rapidly with time. However, we show how the computations can be simplified in the continuum limit by a moment closure approximation. The resulting low-dimensional system can be used to infer the environmental state and change rate with accuracy comparable to the ideal observer. The approximate computations can be performed by a neural network model via a rate-correlation-based plasticity rule. We thus show how optimal observers accumulate evidence in changing environments and map this computation to reduced models that perform inference using plausible neural mechanisms."}], "ArticleTitle": "Evidence Accumulation and Change Rate Inference in Dynamic Environments."}, "33253033": {"mesh": [], "AbstractText": [{"section": null, "text": "Formation of stimulus equivalence classes has been recently modeled through equivalence projective simulation (EPS), a modified version of a projective simulation (PS) learning agent. PS is endowed with an episodic memory that resembles the internal representation in the brain and the concept of cognitive maps. PS flexibility and interpretability enable the EPS model and, consequently the model we explore in this letter, to simulate a broad range of behaviors in matching-to-sample experiments. The episodic memory, the basis for agent decision making, is formed during the training phase. Derived relations in the EPS model that are not trained directly but can be established via the network's connections are computed on demand during the test phase trials by likelihood reasoning. In this letter, we investigate the formation of derived relations in the EPS model using network enhancement (NE), an iterative diffusion process, that yields an offline approach to the agent decision making at the testing phase. The NE process is applied after the training phase to denoise the memory network so that derived relations are formed in the memory network and retrieved during the testing phase. During the NE phase, indirect relations are enhanced, and the structure of episodic memory changes. This approach can also be interpreted as the agent's replay after the training phase, which is in line with recent findings in behavioral and neuroscience studies. In comparison with EPS, our model is able to model the formation of derived relations and other features such as the nodal effect in a more intrinsic manner. Decision making in the test phase is not an ad hoc computational method, but rather a retrieval and update process of the cached relations from the memory network based on the test trial. In order to study the role of parameters on agent performance, the proposed model is simulated and the results discussed through various experimental settings."}], "ArticleTitle": "Enhanced Equivalence Projective Simulation: A Framework for Modeling Formation of Stimulus Equivalence Classes."}, "32069173": {"mesh": ["Algorithms", "Electromyography", "Humans", "Muscle, Skeletal", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Surface electromyography (sEMG) is an electrophysiological reflection of skeletal muscle contractile activity that can directly reflect neuromuscular activity. It has been a matter of research to investigate feature extraction methods of sEMG signals. In this letter, we propose a feature extraction method of sEMG signals based on the improved small-world leaky echo state network (ISWLESN). The reservoir of leaky echo state network (LESN) is connected by a random network. First, we improved the reservoir of the echo state network (ESN) by these networks and used edge-added probability to improve these networks. That idea enhances the adaptability of the reservoir, the generalization ability, and the stability of ESN. Then we obtained the output weight of the network through training and used it as features. We recorded the sEMG signals during different activities: falling, walking, sitting, squatting, going upstairs, and going downstairs. Afterward, we extracted corresponding features by ISWLESN and used principal component analysis for dimension reduction. At the end, scatter plot, the class separability index, and the Davies-Bouldin index were used to assess the performance of features. The results showed that the ISWLESN clustering performance was better than those of LESN and ESN. By support vector machine, it was also revealed that the performance of ISWLESN for classifying the activities was better than those of ESN and LESN."}], "ArticleTitle": "Feature Extraction of Surface Electromyography Based on Improved Small-World Leaky Echo State Network."}, "30183509": {"mesh": ["Central Nervous System Diseases", "Disease Outbreaks", "Female", "Humans", "Male", "Microwaves", "Sensation Disorders"], "AbstractText": [{"section": null, "text": "Importance: A mystery illness striking U.S. and Canadian diplomats to Cuba (and now China) \"has confounded the FBI, the State Department and US intelligence agencies\" (Lederman, Weissenstein, & Lee, 2017). Sonic explanations for the so-called health attacks have long dominated media reports, propelled by peculiar sounds heard and auditory symptoms experienced. Sonic mediation was justly rejected by experts. We assessed whether pulsed radiofrequency/microwave radiation (RF/MW) exposure can accommodate reported facts in diplomats, including unusual ones. Observations: (1) Noises: Many diplomats heard chirping, ringing or grinding noises at night during episodes reportedly triggering health problems. Some reported that noises were localized with laser-like precision or said the sounds seemed to follow them (within the territory in which they were perceived). Pulsed RF/MW engenders just these apparent \"sounds\" via the Frey effect. Perceived \"sounds\" differ by head dimensions and pulse characteristics and can be perceived as located behind in or above the head. Ability to hear the \"sounds\" depends on high-frequency hearing and low ambient noise. (2) Signs/symptoms: Hearing loss and tinnitus are prominent in affected diplomats and in RF/MW-affected individuals. Each of the protean symptoms that diplomats report also affect persons reporting symptoms from RF/MW: sleep problems, headaches, and cognitive problems dominate in both groups. Sensations of pressure or vibration figure in each. Both encompass vision, balance, and speech problems and nosebleeds. Brain injury and brain swelling are reported in both. (3) Mechanisms: Oxidative stress provides a documented mechanism of RF/MW injury compatible with reported signs and symptoms; sequelae of endothelial dysfunction (yielding blood flow compromise), membrane damage, blood-brain barrier disruption, mitochondrial injury, apoptosis, and autoimmune triggering afford downstream mechanisms, of varying persistence, that merit investigation. (4) Of note, microwaving of the U.S. embassy in Moscow is historically documented. Conclusions and relevance: Reported facts appear consistent with pulsed RF/MW as the source of injury in affected diplomats. Nondiplomats citing symptoms from RF/MW, often with an inciting pulsed-RF/MW exposure, report compatible health conditions. Under the RF/MW hypothesis, lessons learned for diplomats and for RF/MW-affected civilians may each aid the other."}], "ArticleTitle": "Diplomats' Mystery Illness and Pulsed Radiofrequency/Microwave Radiation."}, "32521215": {"mesh": [], "AbstractText": [{"section": null, "text": "It is known that any target function is realized in a sufficiently small neighborhood of any randomly connected deep network, provided the width (the number of neurons in a layer) is sufficiently large. There are sophisticated analytical theories and discussions concerning this striking fact, but rigorous theories are very complicated. We give an elementary geometrical proof by using a simple model for the purpose of elucidating its structure. We show that high-dimensional geometry plays a magical role. When we project a high-dimensional sphere of radius 1 to a low-dimensional subspace, the uniform distribution over the sphere shrinks to a gaussian distribution with negligibly small variances and covariances."}], "ArticleTitle": "Any Target Function Exists in a Neighborhood of Any Sufficiently Wide Random Network: A Geometrical Perspective."}, "32343645": {"mesh": ["Data Interpretation, Statistical", "Humans", "Neural Networks, Computer", "Neurons", "Place Cells", "Space Perception"], "AbstractText": [{"section": null, "text": "Continuous attractors have been used to understand recent neuroscience experiments where persistent activity patterns encode internal representations of external attributes like head direction or spatial location. However, the conditions under which the emergent bump of neural activity in such networks can be manipulated by space and time-dependent external sensory or motor signals are not understood. Here, we find fundamental limits on how rapidly internal representations encoded along continuous attractors can be updated by an external signal. We apply these results to place cell networks to derive a velocity-dependent nonequilibrium memory capacity in neural networks."}], "ArticleTitle": "Nonequilibrium Statistical Mechanics of Continuous Attractors."}, "31703177": {"mesh": [], "AbstractText": [{"section": null, "text": "We analyze the effect of synchronization on distributed stochastic gradient algorithms. By exploiting an analogy with dynamical models of biological quorum sensing, where synchronization between agents is induced through communication with a common signal, we quantify how synchronization can significantly reduce the magnitude of the noise felt by the individual distributed agents and their spatial mean. This noise reduction is in turn associated with a reduction in the smoothing of the loss function imposed by the stochastic gradient approximation. Through simulations on model nonconvex objectives, we demonstrate that coupling can stabilize higher noise levels and improve convergence. We provide a convergence analysis for strongly convex functions by deriving a bound on the expected deviation of the spatial mean of the agents from the global minimizer for an algorithm based on quorum sensing, the same algorithm with momentum, and the elastic averaging SGD (EASGD) algorithm. We discuss extensions to new algorithms that allow each agent to broadcast its current measure of success and shape the collective computation accordingly. We supplement our theoretical analysis with numerical experiments on convolutional neural networks trained on the CIFAR-10 data set, where we note a surprising regularizing property of EASGD even when applied to the non-distributed case. This observation suggests alternative second-order in time algorithms for nondistributed optimization that are competitive with momentum methods."}], "ArticleTitle": "A Continuous-Time Analysis of Distributed Stochastic Gradient."}, "31525313": {"mesh": ["Brain", "Humans", "Learning", "Models, Neurological", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Humans are able to master a variety of knowledge and skills with ongoing learning. By contrast, dramatic performance degradation is observed when new tasks are added to an existing neural network model. This phenomenon, termed catastrophic forgetting, is one of the major roadblocks that prevent deep neural networks from achieving human-level artificial intelligence. Several research efforts (e.g., lifelong or continual learning algorithms) have proposed to tackle this problem. However, they either suffer from an accumulating drop in performance as the task sequence grows longer, or require storing an excessive number of model parameters for historical memory, or cannot obtain competitive performance on the new tasks. In this letter, we focus on the incremental multitask image classification scenario. Inspired by the learning process of students, who usually decompose complex tasks into easier goals, we propose an adversarial feature alignment method to avoid catastrophic forgetting. In our design, both the low-level visual features and high-level semantic features serve as soft targets and guide the training process in multiple stages, which provide sufficient supervised information of the old tasks and help to reduce forgetting. Due to the knowledge distillation and regularization phenomena, the proposed method gains even better performance than fine-tuning on the new tasks, which makes it stand out from other methods. Extensive experiments in several typical lifelong learning scenarios demonstrate that our method outperforms the state-of-the-art methods in both accuracy on new tasks and performance preservation on old tasks."}], "ArticleTitle": "Adversarial Feature Alignment: Avoid Catastrophic Forgetting in Incremental Task Lifelong Learning."}, "30979355": {"mesh": ["Animals", "Brain-Computer Interfaces", "Macaca mulatta", "Memory, Short-Term", "Motor Cortex", "Movement", "Neural Networks, Computer", "Somatosensory Cortex"], "AbstractText": [{"section": null, "text": "Although many real-time neural decoding algorithms have been proposed for brain-machine interface (BMI) applications over the years, an optimal, consensual approach remains elusive. Recent advances in deep learning algorithms provide new opportunities for improving the design of BMI decoders, including the use of recurrent artificial neural networks to decode neuronal ensemble activity in real time. Here, we developed a long-short term memory (LSTM) decoder for extracting movement kinematics from the activity of large (N = 134-402) populations of neurons, sampled simultaneously from multiple cortical areas, in rhesus monkeys performing motor tasks. Recorded regions included primary motor, dorsal premotor, supplementary motor, and primary somatosensory cortical areas. The LSTM's capacity to retain information for extended periods of time enabled accurate decoding for tasks that required both movements and periods of immobility. Our LSTM algorithm significantly outperformed the state-of-the-art unscented Kalman filter when applied to three tasks: center-out arm reaching, bimanual reaching, and bipedal walking on a treadmill. Notably, LSTM units exhibited a variety of well-known physiological features of cortical neuronal activity, such as directional tuning and neuronal dynamics across task epochs. LSTM modeled several key physiological attributes of cortical circuits involved in motor tasks. These findings suggest that LSTM-based approaches could yield a better algorithm strategy for neuroprostheses that employ BMIs to restore movement in severely disabled patients."}], "ArticleTitle": "Decoding Movements from Cortical Ensemble Activity Using a Long Short-Term Memory Recurrent Network."}, "31113298": {"mesh": ["Adult", "Brain", "Brain Mapping", "Electrocorticography", "Electroencephalography", "Epilepsy", "Female", "Humans", "Male", "Middle Aged", "Seizures"], "AbstractText": [{"section": null, "text": "Epilepsy is a neurological disorder characterized by the sudden occurrence of unprovoked seizures. There is extensive evidence of significantly altered brain connectivity during seizure periods in the human brain. Research on analyzing human brain functional connectivity during epileptic seizures has been limited predominantly to the use of the correlation method. However, spurious connectivity can be measured between two brain regions without having direct connection or interaction between them. Correlations can be due to the apparent interactions of the two brain regions resulting from common input from a third region, which may or may not be observed. Hence, researchers have recently proposed a sparse-plus-latent-regularized precision matrix (SLRPM) when there are unobserved or latent regions interacting with the observed regions. The SLRPM method yields partial correlations of the conditional statistics of the observed regions given the latent regions, thus identifying observed regions that are conditionally independent of both the observed and latent regions. We evaluate the performance of the methods using a spring-mass artificial network and assuming that some nodes cannot be observed, thus constituting the latent variables in the example. Several cases have been considered, including both sparse and dense connections, short-range and long-range connections, and a varying number of latent variables. The SLRPM method is then applied to estimate brain connectivity during epileptic seizures from human ECoG recordings. Seventy-four clinical seizures from five patients, all having complex partial epilepsy, were analyzed using SLRPM, and brain connectivity was quantified using modularity index, clustering coefficient, and eigenvector centrality. Furthermore, using a measure of latent inputs estimated by the SLRPM method, it was possible to automatically detect 72 of the 74 seizures with four false positives and find six seizures that were not marked manually."}], "ArticleTitle": "Characterizing Brain Connectivity From Human Electrocorticography Recordings With Unobserved Inputs During Epileptic Seizures."}, "30883275": {"mesh": ["Brain", "Diagnosis, Computer-Assisted", "Humans", "Magnetic Resonance Imaging", "Male", "Neural Networks, Computer", "Pattern Recognition, Automated", "Schizophrenia"], "AbstractText": [{"section": null, "text": "Machine learning (ML) is a growing field that provides tools for automatic pattern recognition. The neuroimaging community currently tries to take advantage of ML in order to develop an auxiliary diagnostic tool for schizophrenia diagnostics. In this letter, we present a classification framework based on features extracted from magnetic resonance imaging (MRI) data using two automatic whole-brain morphometry methods: voxel-based (VBM) and deformation-based morphometry (DBM). The framework employs a random subspace ensemble-based artificial neural network classifier-in particular, a multilayer perceptron (MLP). The framework was tested on data from first-episode schizophrenia patients and healthy controls. The experiments differed in terms of feature extraction methods, using VBM, DBM, and a combination of both morphometry methods. Thus, features of different types were available for model adaptation. As we expected, the combination of features increased the MLP classification accuracy up to 73.12%-an improvement of 5% versus MLP-based only on VBM or DBM features. To further verify the findings, other comparisons using support vector machines in place of MLPs were made within the framework. However, it cannot be concluded that any classifier was better than another."}], "ArticleTitle": "Brain Morphometry Methods for Feature Extraction in Random Subspace Ensemble Neural Network Classification of First-Episode Schizophrenia."}, "33080165": {"mesh": ["Algorithms", "Machine Learning", "Regression Analysis"], "AbstractText": [{"section": null, "text": "Stemming from information-theoretic learning, the correntropy criterion and its applications to machine learning tasks have been extensively studied and explored. Its application to regression problems leads to the robustness-enhanced regression paradigm: correntropy-based regression. Having drawn a great variety of successful real-world applications, its theoretical properties have also been investigated recently in a series of studies from a statistical learning viewpoint. The resulting big picture is that correntropy-based regression regresses toward the conditional mode function or the conditional mean function robustly under certain conditions. Continuing this trend and going further, in this study, we report some new insights into this problem. First, we show that under the additive noise regression model, such a regression paradigm can be deduced from minimum distance estimation, implying that the resulting estimator is essentially a minimum distance estimator and thus possesses robustness properties. Second, we show that the regression paradigm in fact provides a unified approach to regression problems in that it approaches the conditional mean, the conditional mode, and the conditional median functions under certain conditions. Third, we present some new results when it is used to learn the conditional mean function by developing its error bounds and exponential convergence rates under conditional (1+&#949;)-moment assumptions. The saturation effect on the established convergence rates, which was observed under (1+&#949;)-moment assumptions, still occurs, indicating the inherent bias of the regression estimator. These novel insights deepen our understanding of correntropy-based regression, help cement the theoretic correntropy framework, and enable us to investigate learning schemes induced by general bounded nonconvex loss functions."}], "ArticleTitle": "New Insights Into Learning With Correntropy-Based Regression."}, "26942747": {"mesh": ["Gamma Rhythm", "Humans", "Image Enhancement", "Models, Neurological", "Nerve Net", "Neural Networks, Computer", "Neuronal Plasticity"], "AbstractText": [{"section": null, "text": "Inspired by gamma-band oscillations and other neurobiological discoveries, neural networks research shifts the emphasis toward temporal coding, which uses explicit times at which spikes occur as an essential dimension in neural representations. We present a feature-linking model (FLM) that uses the timing of spikes to encode information. The first spiking time of FLM is applied to image enhancement, and the processing mechanisms are consistent with the human visual system. The enhancement algorithm achieves boosting the details while preserving the information of the input image. Experiments are conducted to demonstrate the effectiveness of the proposed method. Results show that the proposed method is effective."}], "ArticleTitle": "Feature-Linking Model for Image Enhancement."}, "28777725": {"mesh": ["Association", "Brain", "Humans", "Memory", "Models, Neurological", "Neural Networks, Computer", "Neurons"], "AbstractText": [{"section": null, "text": "Human memory is capable of retrieving similar memories to a just retrieved one. This associative ability is at the base of our everyday processing of information. Current models of memory have not been able to underpin the mechanism that the brain could use in order to actively exploit similarities between memories. The current idea is that to induce transitions in attractor neural networks, it is necessary to extinguish the current memory. We introduce a novel mechanism capable of inducing transitions between memories where similarities between memories are actively exploited by the neural dynamics to retrieve a new memory. Populations of neurons that are selective for multiple memories play a crucial role in this mechanism by becoming attractors on their own. The mechanism is based on the ability of the neural network to control the excitation-inhibition balance."}], "ArticleTitle": "Memory States and Transitions between Them in Attractor Neural Networks."}, "29949461": {"mesh": ["Algorithms", "Computer Simulation", "Decision Making", "Entropy", "Environment", "Humans", "Models, Theoretical"], "AbstractText": [{"section": null, "text": "When modeling goal-directed behavior in the presence of various sources of uncertainty, planning can be described as an inference process. A solution to the problem of planning as inference was previously proposed in the active inference framework in the form of an approximate inference scheme based on variational free energy. However, this approximate scheme was based on the mean-field approximation, which assumes statistical independence of hidden variables and is known to show overconfidence and may converge to local minima of the free energy. To better capture the spatiotemporal properties of an environment, we reformulated the approximate inference process using the so-called Bethe approximation. Importantly, the Bethe approximation allows for representation of pairwise statistical dependencies. Under these assumptions, the minimizer of the variational free energy corresponds to the belief propagation algorithm, commonly used in machine learning. To illustrate the differences between the mean-field approximation and the Bethe approximation, we have simulated agent behavior in a simple goal-reaching task with different types of uncertainties. Overall, the Bethe agent achieves higher success rates in reaching goal states. We relate the better performance of the Bethe agent to more accurate predictions about the consequences of its own actions. Consequently, active inference based on the Bethe approximation extends the application range of active inference to more complex behavioral tasks."}], "ArticleTitle": "Active Inference, Belief Propagation, and the Bethe Approximation."}, "27137671": {"mesh": ["Animals", "Brain", "Brain-Computer Interfaces", "Electroencephalography", "Humans", "Imagery, Psychotherapy", "Imagination", "Motor Skills", "Nerve Net", "Psychomotor Performance"], "AbstractText": [{"section": null, "text": "Recent research has reached a consensus on the feasibility of motor imagery brain-computer interface (MI-BCI) for different applications, especially in stroke rehabilitation. Most MI-BCI systems rely on temporal, spectral, and spatial features of single channels to distinguish different MI patterns. However, no successful communication has been established for a completely locked-in subject. To provide more useful and informative features, it has been recommended to take into account the relationships among electroencephalographic (EEG) sensor/source signals in the form of brain connectivity as an efficient tool of neuroscience. In this review, we briefly report the challenges and limitations of conventional MI-BCIs. Brain connectivity analysis, particularly functional and effective, has been described as one of the most promising approaches for improving MI-BCI performance. An extensive literature on EEG-based MI brain connectivity analysis of healthy subjects is reviewed. We subsequently discuss the brain connectomes during left and right hand, feet, and tongue MI movements. Moreover, key components involved in brain connectivity analysis that considerably affect the results are explained. Finally, possible technical shortcomings that may have influenced the results in previous research are addressed and suggestions are provided."}], "ArticleTitle": "Electroencephalographic Motor Imagery Brain Connectivity Analysis for BCI: A Review."}, "31525309": {"mesh": [], "AbstractText": [{"section": null, "text": "This study introduces PV-RNN, a novel variational RNN inspired by predictive-coding ideas. The model learns to extract the probabilistic structures hidden in fluctuating temporal patterns by dynamically changing the stochasticity of its latent states. Its architecture attempts to address two major concerns of variational Bayes RNNs: how latent variables can learn meaningful representations and how the inference model can transfer future observations to the latent variables. PV-RNN does both by introducing adaptive vectors mirroring the training data, whose values can then be adapted differently during evaluation. Moreover, prediction errors during backpropagation-rather than external inputs during the forward computation-are used to convey information to the network about the external data. For testing, we introduce error regression for predicting unseen sequences as inspired by predictive coding that leverages those mechanisms. As in other variational Bayes RNNs, our model learns by maximizing a lower bound on the marginal likelihood of the sequential data, which is composed of two terms: the negative of the expectation of prediction errors and the negative of the Kullback-Leibler divergence between the prior and the approximate posterior distributions. The model introduces a weighting parameter, the meta-prior, to balance the optimization pressure placed on those two terms. We test the model on two data sets with probabilistic structures and show that with high values of the meta-prior, the network develops deterministic chaos through which the randomness of the data is imitated. For low values, the model behaves as a random process. The network performs best on intermediate values and is able to capture the latent probabilistic structure with good generalization. Analyzing the meta-prior's impact on the network allows us to precisely study the theoretical value and practical benefits of incorporating stochastic dynamics in our model. We demonstrate better prediction performance on a robot imitation task with our model using error regression compared to a standard variational Bayes model lacking such a procedure."}], "ArticleTitle": "A Novel Predictive-Coding-Inspired Variational RNN Model for Online Prediction and Recognition."}, "32186999": {"mesh": [], "AbstractText": [{"section": null, "text": "Stimulus equivalence (SE) and projective simulation (PS) study complex behavior, the former in human subjects and the latter in artificial agents. We apply the PS learning framework for modeling the formation of equivalence classes. For this purpose, we first modify the PS model to accommodate imitating the emergence of equivalence relations. Later, we formulate the SE formation through the matching-to-sample (MTS) procedure. The proposed version of PS model, called the equivalence projective simulation (EPS) model, is able to act within a varying action set and derive new relations without receiving feedback from the environment. To the best of our knowledge, it is the first time that the field of equivalence theory in behavior analysis has been linked to an artificial agent in a machine learning context. This model has many advantages over existing neural network models. Briefly, our EPS model is not a black box model, but rather a model with the capability of easy interpretation and flexibility for further modifications. To validate the model, some experimental results performed by prominent behavior analysts are simulated. The results confirm that the EPS model is able to reliably simulate and replicate the same behavior as real experiments in various settings, including formation of equivalence relations in typical participants, nonformation of equivalence relations in language-disabled children, and nodal effect in a linear series with nodal distance five. Moreover, through a hypothetical experiment, we discuss the possibility of applying EPS in further equivalence theory research."}], "ArticleTitle": "Equivalence Projective Simulation as a Framework for Modeling Formation of Stimulus Equivalence Classes."}, "32795233": {"mesh": ["Bayes Theorem", "Confidence Intervals", "Humans", "Linear Models", "Supervised Machine Learning"], "AbstractText": [{"section": null, "text": "In this letter, we study an active learning problem for maximizing an unknown linear function with high-dimensional binary features. This problem is notoriously complex but arises in many important contexts. When the sampling budget, that is, the number of possible function evaluations, is smaller than the number of dimensions, it tends to be impossible to identify all of the optimal binary features. Therefore, in practice, only a small number of such features are considered, with the majority kept fixed at certain default values, which we call the working set heuristic. The main contribution of this letter is to formally study the working set heuristic and present a suite of theoretically robust algorithms for more efficient use of the sampling budget. Technically, we introduce a novel method for estimating the confidence regions of model parameters that is tailored to active learning with high-dimensional binary features. We provide a rigorous theoretical analysis of these algorithms and prove that a commonly used working set heuristic can identify optimal binary features with favorable sample complexity. We explore the performance of the proposed approach through numerical simulations and an application to a functional protein design problem."}], "ArticleTitle": "Active Learning of Bayesian Linear Models with High-Dimensional Binary Features by Parameter Confidence-Region Estimation."}, "33080162": {"mesh": ["Animals", "Brain", "Cognition", "Humans", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "The ability to encode and manipulate data structures with distributed neural representations could qualitatively enhance the capabilities of traditional neural networks by supporting rule-based symbolic reasoning, a central property of cognition. Here we show how this may be accomplished within the framework of Vector Symbolic Architectures (VSAs) (Plate, 1991; Gayler, 1998; Kanerva, 1996), whereby data structures are encoded by combining high-dimensional vectors with operations that together form an algebra on the space of distributed representations. In particular, we propose an efficient solution to a hard combinatorial search problem that arises when decoding elements of a VSA data structure: the factorization of products of multiple codevectors. Our proposed algorithm, called a resonator network, is a new type of recurrent neural network that interleaves VSA multiplication operations and pattern completion. We show in two examples-parsing of a tree-like data structure and parsing of a visual scene-how the factorization problem arises and how the resonator network can solve it. More broadly, resonator networks open the possibility of applying VSAs to myriad artificial intelligence problems in real-world domains. The companion article in this issue (Kent, Frady, Sommer, & Olshausen, 2020) presents a rigorous analysis and evaluation of the performance of resonator networks, showing it outperforms alternative approaches."}], "ArticleTitle": "Resonator Networks, 1: An Efficient Solution for Factoring High-Dimensional, Distributed Representations of Data Structures."}, "30314425": {"mesh": ["Brain", "Humans", "Neural Networks, Computer", "Pattern Recognition, Visual"], "AbstractText": [{"section": null, "text": "Deep neural networks (DNNs) trained in a supervised way suffer from two known problems. First, the minima of the objective function used in learning correspond to data points (also known as rubbish examples or fooling images) that lack semantic similarity with the training data. Second, a clean input can be changed by a small, and often imperceptible for human vision, perturbation so that the resulting deformed input is misclassified by the network. These findings emphasize the differences between the ways DNNs and humans classify patterns and raise a question of designing learning algorithms that more accurately mimic human perception compared to the existing methods. Our article examines these questions within the framework of dense associative memory (DAM) models. These models are defined by the energy function, with higher-order (higher than quadratic) interactions between the neurons. We show that in the limit when the power of the interaction vertex in the energy function is sufficiently large, these models have the following three properties. First, the minima of the objective function are free from rubbish images, so that each minimum is a semantically meaningful pattern. Second, artificial patterns poised precisely at the decision boundary look ambiguous to human subjects and share aspects of both classes that are separated by that decision boundary. Third, adversarial images constructed by models with small power of the interaction vertex, which are equivalent to DNN with rectified linear units, fail to transfer to and fool the models with higher-order interactions. This opens up the possibility of using higher-order models for detecting and stopping malicious adversarial attacks. The results we present suggest that DAMs with higher-order energy functions are more robust to adversarial and rubbish inputs than DNNs with rectified linear units."}], "ArticleTitle": "Dense Associative Memory Is Robust to Adversarial Inputs."}, "31614107": {"mesh": ["Algorithms", "Brain", "Face", "Image Processing, Computer-Assisted", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "A recent Cell paper (Chang & Tsao, 2017) reports an interesting discovery. For the face stimuli generated by a pretrained active appearance model (AAM), the responses of neurons in the areas of the primate brain that are responsible for face recognition exhibit a strong linear relationship with the shape variables and appearance variables of the AAM that generates the face stimuli. In this letter, we show that this behavior can be replicated by a deep generative model, the generator network, that assumes that the observed signals are generated by latent random variables via a top-down convolutional neural network. Specifically, we learn the generator network from the face images generated by a pretrained AAM model using a variational autoencoder, and we show that the inferred latent variables of the learned generator network have a strong linear relationship with the shape and appearance variables of the AAM model that generates the face images. Unlike the AAM model, which has an explicit shape model where the shape variables generate the control points or landmarks, the generator network has no such shape model and shape variables. Yet it can learn the shape knowledge in the sense that some of the latent variables of the learned generator network capture the shape variations in the face images generated by AAM."}], "ArticleTitle": "Replicating Neuroscience Observations on ML/MF and AM Face Patches by Deep Generative Model."}, "29652584": {"mesh": [], "AbstractText": [{"section": null, "text": "The neural correlates of decision making have been extensively studied with tasks involving a choice between two alternatives that is guided by visual cues. While a large body of work argues for a role of the lateral intraparietal (LIP) region of cortex in these tasks, this role may be confounded by the interaction between LIP and other regions, including medial temporal (MT) cortex. Here, we describe a simplified linear model of decision making that is adapted to two tasks: a motion discrimination and a categorization task. We show that the distinct contribution of MT and LIP may indeed be confounded in these tasks. In particular, we argue that the motion discrimination task relies on a straightforward visuomotor mapping, which leads to redundant information between MT and LIP. The categorization task requires a more complex mapping between visual information and decision behavior, and therefore does not lead to redundancy between MT and LIP. Going further, the model predicts that noise correlations within LIP should be greater in the categorization compared to the motion discrimination task due to the presence of shared inputs from MT. The impact of these correlations on task performance is examined by analytically deriving error estimates of an optimal linear readout for shared and unique inputs. Taken together, results clarify the contribution of MT and LIP to decision making and help characterize the role of noise correlations in these regions."}], "ArticleTitle": "Optimal Readout of Correlated Neural Activity in a Decision-Making Circuit."}, "27870615": {"mesh": ["Animals", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "On the basis of the neurophysiological strength-duration (amplitude-duration) curve of neuron activation (which relates the threshold amplitude of a rectangular current pulse of neuron activation to the pulse duration), as well as with the use of activation energy constraint (the threshold curve corresponds to the energy threshold of neuron activation by a rectangular current pulse), an energy model of neuron activation by a single current pulse has been constructed. The constructed model of activation, which determines its spectral properties, is a bandpass filter. Under the condition of minimum-phase feature of the neuron activation model, on the basis of Hilbert transform, the possibilities of phase-frequency response calculation from its amplitude-frequency response have been considered. Approximation to the amplitude-frequency response by the response of the Butterworth filter of the first order, as well as obtaining the pulse response corresponding to this approximation, give us the possibility of analyzing the efficiency of activating current pulses of various shapes, including analysis in accordance with the energy constraint."}], "ArticleTitle": "Energy Model of Neuron Activation."}, "31113304": {"mesh": ["Algorithms", "Artificial Intelligence", "Computers", "Humans", "Nerve Net", "Pattern Recognition, Automated"], "AbstractText": [{"section": null, "text": "With the development of neural recording technology, it has become possible to collect activities from hundreds or even thousands of neurons simultaneously. Visualization of neural population dynamics can help neuroscientists analyze large-scale neural activities efficiently. In this letter, Laplacian eigenmaps is applied to this task for the first time, and the experimental results show that the proposed method significantly outperforms the commonly used methods. This finding was confirmed by the systematic evaluation using nonhuman primate data, which contained the complex dynamics well suited for testing. According to our results, Laplacian eigenmaps is better than the other methods in various ways and can clearly visualize interesting biological phenomena related to neural dynamics."}], "ArticleTitle": "Effective Dimensionality Reduction for Visualizing Neural Dynamics by Laplacian Eigenmaps."}, "30021082": {"mesh": ["Action Potentials", "Adaptation, Physiological", "Algorithms", "Environment", "Humans", "Models, Biological", "Motion", "Motion Perception", "Neurons", "Optic Flow", "Retina", "Signal Detection, Psychological", "Synapses"], "AbstractText": [{"section": null, "text": "Apparent motion of the surroundings on an agent's retina can be used to navigate through cluttered environments, avoid collisions with obstacles, or track targets of interest. The pattern of apparent motion of objects, (i.e., the optic flow), contains spatial information about the surrounding environment. For a small, fast-moving agent, as used in search and rescue missions, it is crucial to estimate the distance to close-by objects to avoid collisions quickly. This estimation cannot be done by conventional methods, such as frame-based optic flow estimation, given the size, power, and latency constraints of the necessary hardware. A practical alternative makes use of event-based vision sensors. Contrary to the frame-based approach, they produce so-called events only when there are changes in the visual scene. We propose a novel asynchronous circuit, the spiking elementary motion detector (sEMD), composed of a single silicon neuron and synapse, to detect elementary motion from an event-based vision sensor. The sEMD encodes the time an object's image needs to travel across the retina into a burst of spikes. The number of spikes within the burst is proportional to the speed of events across the retina. A fast but imprecise estimate of the time-to-travel can already be obtained from the first two spikes of a burst and refined by subsequent interspike intervals. The latter encoding scheme is possible due to an adaptive nonlinear synaptic efficacy scaling. We show that the sEMD can be used to compute a collision avoidance direction in the context of robotic navigation in a cluttered outdoor environment and compared the collision avoidance direction to a frame-based algorithm. The proposed computational principle constitutes a generic spiking temporal correlation detector that can be applied to other sensory modalities (e.g., sound localization), and it provides a novel perspective to gating information in spiking neural networks."}], "ArticleTitle": "Spiking Elementary Motion Detector in Neuromorphic Systems."}, "28777718": {"mesh": ["Animals", "Computer Simulation", "Models, Biological", "Neural Networks, Computer", "Signal Processing, Computer-Assisted"], "AbstractText": [{"section": null, "text": "Blind source separation-the extraction of independent sources from a mixture-is an important problem for both artificial and natural signal processing. Here, we address a special case of this problem when sources (but not the mixing matrix) are known to be nonnegative-for example, due to the physical nature of the sources. We search for the solution to this problem that can be implemented using biologically plausible neural networks. Specifically, we consider the online setting where the data set is streamed to a neural network. The novelty of our approach is that we formulate blind nonnegative source separation as a similarity matching problem and derive neural networks from the similarity matching objective. Importantly, synaptic weights in our networks are updated according to biologically plausible local learning rules."}], "ArticleTitle": "Blind Nonnegative Source Separation Using Biological Neural Networks."}, "28562220": {"mesh": ["Action Potentials", "Animals", "Biophysics", "Cerebellum", "Computer Simulation", "Electric Stimulation", "Interneurons", "Models, Neurological", "Nerve Net", "Patch-Clamp Techniques", "Synapses"], "AbstractText": [{"section": null, "text": "Knowledge of synaptic input is crucial for understanding synaptic integration and ultimately neural function. However, in vivo, the rates at which synaptic inputs arrive are high, so that it is typically impossible to detect single events. We show here that it is nevertheless possible to extract the properties of the events and, in particular, to extract the event rate, the synaptic time constants, and the properties of the event size distribution from in vivo voltage-clamp recordings. Applied to cerebellar interneurons, our method reveals that the synaptic input rate increases from 600 Hz during rest to 1000 Hz during locomotion, while the amplitude and shape of the synaptic events are unaffected by this state change. This method thus complements existing methods to measure neural function in vivo."}], "ArticleTitle": "Extraction of Synaptic Input Properties in Vivo."}, "32946715": {"mesh": ["Animals", "Computer Simulation", "Humans", "Models, Neurological", "Models, Statistical", "Neurons"], "AbstractText": [{"section": null, "text": "Recent remarkable advances in experimental techniques have provided a background for inferring neuronal couplings from point process data that include a great number of neurons. Here, we propose a systematic procedure for pre- and postprocessing generic point process data in an objective manner to handle data in the framework of a binary simple statistical model, the Ising or generalized McCulloch-Pitts model. The procedure has two steps: (1) determining time bin size for transforming the point process data into discrete-time binary data and (2) screening relevant couplings from the estimated couplings. For the first step, we decide the optimal time bin size by introducing the null hypothesis that all neurons would fire independently, then choosing a time bin size so that the null hypothesis is rejected with the strict criteria. The likelihood associated with the null hypothesis is analytically evaluated and used for the rejection process. For the second postprocessing step, after a certain estimator of coupling is obtained based on the preprocessed data set (any estimator can be used with the proposed procedure), the estimate is compared with many other estimates derived from data sets obtained by randomizing the original data set in the time direction. We accept the original estimate as relevant only if its absolute value is sufficiently larger than those of randomized data sets. These manipulations suppress false positive couplings induced by statistical noise. We apply this inference procedure to spiking data from synthetic and in vitro neuronal networks. The results show that the proposed procedure identifies the presence or absence of synaptic couplings fairly well, including their signs, for the synthetic and experimental data. In particular, the results support that we can infer the physical connections of underlying systems in favorable situations, even when using a simple statistical model."}], "ArticleTitle": "Inferring Neuronal Couplings From Spiking Data Using a Systematic Procedure With a Statistical Criterion."}, "31525310": {"mesh": ["Adult", "Brain", "Brain Mapping", "Electroencephalography", "Female", "Fractals", "Humans", "Male"], "AbstractText": [{"section": null, "text": "The brain is known to be active even when not performing any overt cognitive tasks, and often it engages in involuntary mind wandering. This resting state has been extensively characterized in terms of fMRI-derived brain networks. However, an alternate method has recently gained popularity: EEG microstate analysis. Proponents of microstates postulate that the brain discontinuously switches between four quasi-stable states defined by specific EEG scalp topologies at peaks in the global field potential (GFP). These microstates are thought to be \"atoms of thought,\" involved with visual, auditory, salience, and attention processing. However, this method makes some major assumptions by excluding EEG data outside the GFP peaks and then clustering the EEG scalp topologies at the GFP peaks, assuming that only one microstate is active at any given time. This study explores the evidence surrounding these assumptions by studying the temporal dynamics of microstates and its clustering space using tools from dynamical systems analysis, fractal, and chaos theory to highlight the shortcomings in microstate analysis. The results show evidence of complex and chaotic EEG dynamics outside the GFP peaks, which is being missed by microstate analysis. Furthermore, the winner-takes-all approach of only one microstate being active at a time is found to be inadequate since the dynamic EEG scalp topology does not always resemble that of the assigned microstate, and there is competition among the different microstate classes. Finally, clustering space analysis shows that the four microstates do not cluster into four distinct and separable clusters. Taken collectively, these results show that the discontinuous description of EEG microstates is inadequate when looking at nonstationary short-scale EEG dynamics."}], "ArticleTitle": "Capturing the Forest but Missing the Trees: Microstates Inadequate for Characterizing Shorter-Scale EEG Dynamics."}, "32187001": {"mesh": ["Algorithms", "Bayes Theorem", "Deep Learning", "Humans", "Image Processing, Computer-Assisted", "Machine Learning", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Multilayer neural networks have led to remarkable performance on many kinds of benchmark tasks in text, speech, and image processing. Nonlinear parameter estimation in hierarchical models is known to be subject to overfitting and misspecification. One approach to these estimation and related problems (e.g., saddle points, colinearity, feature discovery) is called Dropout. The Dropout algorithm removes hidden units according to a binomial random variable with probability p prior to each update, creating random \"shocks\" to the network that are averaged over updates (thus creating weight sharing). In this letter, we reestablish an older parameter search method and show that Dropout is a special case of this more general model, stochastic delta rule (SDR), published originally in 1990. Unlike Dropout, SDR redefines each weight in the network as a random variable with mean &#956;wij and standard deviation &#963;wij. Each weight random variable is sampled on each forward activation, consequently creating an exponential number of potential networks with shared weights (accumulated in the mean values). Both parameters are updated according to prediction error, thus resulting in weight noise injections that reflect a local history of prediction error and local model averaging. SDR therefore implements a more sensitive local gradient-dependent simulated annealing per weight converging in the limit to a Bayes optimal network. We run tests on standard benchmarks (CIFAR and ImageNet) using a modified version of DenseNet and show that SDR outperforms standard Dropout in top-5 validation error by approximately 13% with DenseNet-BC 121 on ImageNet and find various validation error improvements in smaller networks. We also show that SDR reaches the same accuracy that Dropout attains in 100 epochs in as few as 40 epochs, as well as improvements in training error by as much as 80%."}], "ArticleTitle": "The Stochastic Delta Rule: Faster and More Accurate Deep Learning Through Adaptive Weight Noise."}, "32521216": {"mesh": [], "AbstractText": [{"section": null, "text": "Overfitting and treatment of small data are among the most challenging problems in machine learning (ML), when a relatively small data statistics size T is not enough to provide a robust ML fit for a relatively large data feature dimension D. Deploying a massively parallel ML analysis of generic classification problems for different D and T, we demonstrate the existence of statistically significant linear overfitting barriers for common ML methods. The results reveal that for a robust classification of bioinformatics-motivated generic problems with the long short-term memory deep learning classifier (LSTM), one needs in the best case a statistics T that is at least 13.8 times larger than the feature dimension D. We show that this overfitting barrier can be breached at a 10-12 fraction of the computational cost by means of the entropy-optimal scalable probabilistic approximations algorithm (eSPA), performing a joint solution of the entropy-optimal Bayesian network inference and feature space segmentation problems. Application of eSPA to experimental single cell RNA sequencing data exhibits a 30-fold classification performance boost when compared to standard bioinformatics tools and a 7-fold boost when compared to the deep learning LSTM classifier."}], "ArticleTitle": "On a Scalable Entropic Breaching of the Overfitting Barrier for Small Data Problems in Machine Learning."}, "27870614": {"mesh": [], "AbstractText": [{"section": null, "text": "This article describes a process theory based on active inference and belief propagation. Starting from the premise that all neuronal processing (and action selection) can be explained by maximizing Bayesian model evidence-or minimizing variational free energy-we ask whether neuronal responses can be described as a gradient descent on variational free energy. Using a standard (Markov decision process) generative model, we derive the neuronal dynamics implicit in this description and reproduce a remarkable range of well-characterized neuronal phenomena. These include repetition suppression, mismatch negativity, violation responses, place-cell activity, phase precession, theta sequences, theta-gamma coupling, evidence accumulation, race-to-bound dynamics, and transfer of dopamine responses. Furthermore, the (approximately Bayes' optimal) behavior prescribed by these dynamics has a degree of face validity, providing a formal explanation for reward seeking, context learning, and epistemic foraging. Technically, the fact that a gradient descent appears to be a valid description of neuronal activity means that variational free energy is a Lyapunov function for neuronal dynamics, which therefore conform to Hamilton's principle of least action."}], "ArticleTitle": "Active Inference: A Process Theory."}, "32069175": {"mesh": ["Action Potentials", "Bayes Theorem", "Humans", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Optimality principles have been useful in explaining many aspects of biological systems. In the context of neural encoding in sensory areas, optimality is naturally formulated in a Bayesian setting as neural tuning which minimizes mean decoding error. Many works optimize Fisher information, which approximates the minimum mean square error (MMSE) of the optimal decoder for long encoding time but may be misleading for short encoding times. We study MMSE-optimal neural encoding of a multivariate stimulus by uniform populations of spiking neurons, under firing rate constraints for each neuron as well as for the entire population. We show that the population-level constraint is essential for the formulation of a well-posed problem having finite optimal tuning widths and optimal tuning aligns with the principal components of the prior distribution. Numerical evaluation of the two-dimensional case shows that encoding only the dimension with higher variance is optimal for short encoding times. We also compare direct MMSE optimization to optimization of several proxies to MMSE: Fisher information, maximum likelihood estimation error, and the Bayesian Cram&#233;r-Rao bound. We find that optimization of these measures yields qualitatively misleading results regarding MMSE-optimal tuning and its dependence on encoding time and energy constraints."}], "ArticleTitle": "Optimal Multivariate Tuning with Neuron-Level and Population-Level Energy Constraints."}, "28181876": {"mesh": ["Animals", "Computer Simulation", "Humans", "Models, Neurological", "Neurons", "Normal Distribution", "Presynaptic Terminals", "Synapses"], "AbstractText": [{"section": null, "text": "In a sensory neural network, where a population of presynaptic neurons sends information to a downstream neuron, maximizing information transmission depends on utilizing the full operating range of the output of the postsynaptic neuron. Because the convergence of presynaptic inputs naturally biases higher outputs, a sparse input distribution would counter such bias and optimize information transmission."}], "ArticleTitle": "Information Maximization Explains the Sparseness of Presynaptic Neural Response."}, "29566356": {"mesh": ["Algorithms", "Brain", "Computer Simulation", "Electric Stimulation Therapy", "Electroencephalography", "Hippocampus", "Humans", "Models, Neurological", "Neurons", "Nonlinear Dynamics", "Seizures"], "AbstractText": [{"section": null, "text": "Neurostimulation is a promising therapy for abating epileptic seizures. However, it is extremely difficult to identify optimal stimulation patterns experimentally. In this study, human recordings are used to develop a functional 24 neuron network statistical model of hippocampal connectivity and dynamics. Spontaneous seizure-like activity is induced in silico in this reconstructed neuronal network. The network is then used as a testbed to design and validate a wide range of neurostimulation patterns. Commonly used periodic trains were not able to permanently abate seizures at any frequency. A simulated annealing global optimization algorithm was then used to identify an optimal stimulation pattern, which successfully abated 92% of seizures. Finally, in a fully responsive, or closed-loop, neurostimulation paradigm, the optimal stimulation successfully prevented the network from entering the seizure state. We propose that the framework presented here for algorithmically identifying patient-specific neurostimulation patterns can greatly increase the efficacy of neurostimulation devices for seizures."}], "ArticleTitle": "Designing Patient-Specific Optimal Neurostimulation Patterns for Seizure Suppression."}, "29566352": {"mesh": [], "AbstractText": [{"section": null, "text": "Due to the difficulty of collecting labeled images for hundreds of thousands of visual categories, zero-shot learning, where unseen categories do not have any labeled images in training stage, has attracted more attention. In the past, many studies focused on transferring knowledge from seen to unseen categories by projecting all category labels into a semantic space. However, the label embeddings could not adequately express the semantics of categories. Furthermore, the common semantics of seen and unseen instances cannot be captured accurately because the distribution of these instances may be quite different. For these issues, we propose a novel deep semisupervised method by jointly considering the heterogeneity gap between different modalities and the correlation among unimodal instances. This method replaces the original labels with the corresponding textual descriptions to better capture the category semantics. This method also overcomes the problem of distribution difference by minimizing the maximum mean discrepancy between seen and unseen instance distributions. Extensive experimental results on two benchmark data sets, CU200-Birds and Oxford Flowers-102, indicate that our method achieves significant improvements over previous methods."}], "ArticleTitle": "Deep Semisupervised Zero-Shot Learning with Maximum Mean Discrepancy."}, "30883278": {"mesh": ["Brain", "Brain-Computer Interfaces", "Electroencephalography", "Humans", "Imagination", "Machine Learning", "Motor Activity", "Pattern Recognition, Automated", "Signal Processing, Computer-Assisted", "Software"], "AbstractText": [{"section": null, "text": "Practical motor imagery electroencephalogram (EEG) data-based applications are limited by the waste of unlabeled samples in supervised learning and excessive time consumption in the pretraining period. A semisupervised deep stacking network with an adaptive learning rate strategy (SADSN) is proposed to solve the sample loss caused by supervised learning of EEG data and the extraction of manual features. The SADSN adopts the idea of an adaptive learning rate into a contrastive divergence (CD) algorithm to accelerate its convergence. Prior knowledge is introduced into the intermediary layer of the deep stacking network, and a restricted Boltzmann machine is trained by a semisupervised method in which the adjusting scope of the coefficient in learning rate is determined by performance analysis. Several EEG data sets are carried out to evaluate the performance of the proposed method. The results show that the recognition accuracy of SADSN is advanced with a more significant convergence rate and successfully classifies motor imagery."}], "ArticleTitle": "Semisupervised Deep Stacking Network with Adaptive Learning Rate Strategy for Motor Imagery EEG Recognition."}, "27626961": {"mesh": ["Action Potentials", "Hippocampus", "Humans", "Models, Neurological", "Neurons", "Theta Rhythm"], "AbstractText": [{"section": null, "text": "Predicting the timing and order of future events is an essential feature of cognition in higher life forms. We propose a neural mechanism to nondestructively translate the current state of spatiotemporal memory into the future, so as to construct an ordered set of future predictions almost instantaneously. We hypothesize that within each cycle of hippocampal theta oscillations, the memory state is swept through a range of translations to yield an ordered set of future predictions through modulations in synaptic connections. Theoretically, we operationalize critical neurobiological findings from hippocampal physiology in terms of neural network equations representing spatiotemporal memory. Combined with constraints based on physical principles requiring scale invariance and coherence in translation across memory nodes, the proposition results in Weber-Fechner spacing for the representation of both past (memory) and future (prediction) timelines. We show that the phenomenon of phase precession of neurons in the hippocampus and ventral striatum correspond to the cognitive act of future prediction."}], "ArticleTitle": "Neural Mechanism to Simulate a Scale-Invariant Future."}, "28562218": {"mesh": [], "AbstractText": [{"section": null, "text": "Graph-based clustering methods perform clustering on a fixed input data graph. Thus such clustering results are sensitive to the particular graph construction. If this initial construction is of low quality, the resulting clustering may also be of low quality. We address this drawback by allowing the data graph itself to be adaptively adjusted in the clustering procedure. In particular, our proposed weight adaptive Laplacian (WAL) method learns a new data similarity matrix that can adaptively adjust the initial graph according to the similarity weight in the input data graph. We develop three versions of these methods based on the L2-norm, fuzzy entropy regularizer, and another exponential-based weight strategy, that yield three new graph-based clustering objectives. We derive optimization algorithms to solve these objectives. Experimental results on synthetic data sets and real-world benchmark data sets exhibit the effectiveness of these new graph-based clustering methods."}], "ArticleTitle": "A Weight-Adaptive Laplacian Embedding for Graph-Based Clustering."}, "30148705": {"mesh": [], "AbstractText": [{"section": null, "text": "Grid cells of the rodent entorhinal cortex are essential for spatial navigation. Although their function is commonly believed to be either path integration or localization, the origin or purpose of their hexagonal firing fields remains disputed. Here they are proposed to arise as an optimal encoding of transitions in sequences. First, storage requirements for transitions in general episodic sequences are examined using propositional logic and graph theory. Subsequently, transitions in complete metric spaces are considered under the assumption of an ideal sampling of an input space. It is shown that memory capacity of neurons that have to encode multiple feasible spatial transitions is maximized by a hexagonal pattern. Grid cells are proposed to encode spatial transitions in spatiotemporal sequences, with the entorhinal-hippocampal loop forming a multitransition system."}], "ArticleTitle": "Hexagonal Grid Fields Optimally Encode Transitions in Spatiotemporal Sequences."}, "31260391": {"mesh": ["Amino Acid Sequence", "Computer Simulation", "Models, Molecular", "Models, Statistical", "Principal Component Analysis", "Probability", "Proteins", "Sequence Alignment", "Static Electricity", "Stochastic Processes", "Unsupervised Machine Learning"], "AbstractText": [{"section": null, "text": "A restricted Boltzmann machine (RBM) is an unsupervised machine learning bipartite graphical model that jointly learns a probability distribution over data and extracts their relevant statistical features. RBMs were recently proposed for characterizing the patterns of coevolution between amino acids in protein sequences and for designing new sequences. Here, we study how the nature of the features learned by RBM changes with its defining parameters, such as the dimensionality of the representations (size of the hidden layer) and the sparsity of the features. We show that for adequate values of these parameters, RBMs operate in a so-called compositional phase in which visible configurations sampled from the RBM are obtained by recombining these features. We then compare the performance of RBM with other standard representation learning algorithms, including principal or independent component analysis (PCA, ICA), autoencoders (AE), variational autoencoders (VAE), and their sparse variants. We show that RBMs, due to the stochastic mapping between data configurations and representations, better capture the underlying interactions in the system and are significantly more robust with respect to sample size than deterministic methods such as PCA or ICA. In addition, this stochastic mapping is not prescribed a priori as in VAE, but learned from data, which allows RBMs to show good performance even with shallow architectures. All numerical results are illustrated on synthetic lattice protein data that share similar statistical features with real protein sequences and for which ground-truth interactions are known."}], "ArticleTitle": "Learning Compositional Representations of Interacting Systems with Restricted Boltzmann Machines: Comparative Study of Lattice Proteins."}, "31525308": {"mesh": ["Animals", "Brain", "Humans", "Memory, Long-Term", "Models, Neurological", "Neuronal Plasticity"], "AbstractText": [{"section": null, "text": "Repeated stimuli that are spaced apart in time promote the transition from short- to long-term memory, while massing repetitions together does not. Previously, we showed that a model of integrative synaptic plasticity, in which plasticity induction signals are integrated by a low-pass filter before plasticity is expressed, gives rise to a natural timescale at which to repeat stimuli, hinting at a partial account of this spacing effect. The account was only partial because the important role of neuromodulation was not considered. We now show that by extending the model to allow dynamic integrative synaptic plasticity, the model permits synapses to robustly discriminate between spaced and massed repetition protocols, suppressing the response to massed stimuli while maintaining that to spaced stimuli. This is achieved by dynamically coupling the filter decay rate to neuromodulatory signaling in a very simple model of the signaling cascades downstream from cAMP production. In particular, the model's parameters may be interpreted as corresponding to the duration and amplitude of the waves of activity in the MAPK pathway. We identify choices of parameters and repetition times for stimuli in this model that optimize the ability of synapses to discriminate between spaced and massed repetition protocols. The model is very robust to reasonable changes around these optimal parameters and times, but for large changes in parameters, the model predicts that massed and spaced stimuli cannot be distinguished or that the responses to both patterns are suppressed. A model of dynamic integrative synaptic plasticity therefore explains the spacing effect under normal conditions and also predicts its breakdown under abnormal conditions."}], "ArticleTitle": "Dynamic Integrative Synaptic Plasticity Explains the Spacing Effect in the Transition from Short- to Long-Term Memory."}, "31393827": {"mesh": ["Brain-Computer Interfaces", "Electroencephalography", "Female", "Humans", "Imagination", "Internet of Things", "Machine Learning", "Male", "Signal Processing, Computer-Assisted", "Young Adult"], "AbstractText": [{"section": null, "text": "In this letter, we propose two novel methods for four-class motor imagery (MI) classification using electroencephalography (EEG). Also, we developed a real-time health 4.0 (H4.0) architecture for brain-controlled internet of things (IoT) enabled environments (BCE), which uses the classified MI task to assist disabled persons in controlling IoT-enabled environments such as lighting and heating, ventilation, and air-conditioning (HVAC). The first method for classification involves a simple and low-complex classification framework using a combination of regularized Riemannian mean (RRM) and linear SVM. Although this method performs better compared to state-of-the-art techniques, it still suffers from a nonnegligible misclassification rate. Hence, to overcome this, the second method offers a persistent decision engine (PDE) for the MI classification, which improves classification accuracy (CA) significantly. The proposed methods are validated using an in-house recorded four-class MI data set (data set I, collected over 14 subjects), and a four-class MI data set 2a of BCI competition IV (data set II, collected over 9 subjects). The proposed RRM architecture obtained average CAs of 74.30% and 67.60% when validated using datasets I and II, respectively. When analyzed along with the proposed PDE classification framework, an average CA of 92.25% on 12 subjects of data set I and 82.54% on 7 subjects of data set II is obtained. The results show that the PDE algorithm is more reliable for the classification of four-class MI and is also feasible for BCE applications. The proposed low-complex BCE architecture is implemented in real time using Raspberry Pi 3 Model B+ along with the Virgo EEG data acquisition system. The hardware implementation results show that the proposed system architecture is well suited for body-wearable devices in the scenario of Health 4.0. We strongly feel that this study can aid in driving the future scope of BCE research."}], "ArticleTitle": "A Real-Time Health 4.0 Framework with Novel Feature Extraction and Classification for Brain-Controlled IoT-Enabled Environments."}, "32946713": {"mesh": ["Arm", "Computer Simulation", "Humans", "Models, Biological", "Movement"], "AbstractText": [{"section": null, "text": "According to the neuromuscular model of virtual trajectory control, the postures and movements of limbs are performed by shifting the equilibrium positions determined by agonist and antagonist muscle activities. In this study, we develop virtual trajectory control for the reaching movements of a multi-joint arm, introducing a proportional-derivative feedback control scheme. In virtual trajectory control, it is crucial to design a suitable virtual trajectory such that the desired trajectory can be realized. To this end, we propose an algorithm for updating virtual trajectories in repetitive control, which can be regarded as a Newton-like method in a function space. In our repetitive control, the virtual trajectory is corrected without explicit calculation of the arm dynamics, and the actual trajectory converges to the desired trajectory. Using computer simulations, we assessed the proposed repetitive control for the trajectory tracking of a two-link arm. Our results confirmed that when the feedback gains were reasonably high and the sampling time was sufficiently small, the virtual trajectory was adequately updated, and the desired trajectory was almost achieved within approximately 10 iterative trials. We also propose a method for modifying the virtual trajectory to ensure that the formation of the actual trajectory is identical even when the feedback gains are changed. This modification method makes it possible to execute flexible control, in which the feedback gains are effectively altered according to motion tasks."}], "ArticleTitle": "Repetitive Control for Multi-Joint Arm Movements Based on Virtual Trajectories."}, "32795236": {"mesh": [], "AbstractText": [{"section": null, "text": "In this letter, we study a class of the regularized regression algorithms when the sampling process is unbounded. By choosing different loss functions, the learning algorithms can include a wide range of commonly used algorithms for regression. Unlike the prior work on theoretical analysis of unbounded sampling, no constraint on the output variables is specified in our setting. By an elegant error analysis, we prove consistency and finite sample bounds on the excess risk of the proposed algorithms under regular conditions."}], "ArticleTitle": "Analysis of Regression Algorithms with Unbounded Sampling."}, "32343647": {"mesh": ["Animals", "Association Learning", "Humans", "Memory", "Models, Neurological", "Neuronal Plasticity", "Synapses"], "AbstractText": [{"section": null, "text": "Models of associative memory with discrete state synapses learn new memories by forgetting old ones. In contrast to non-integrative models of synaptic plasticity, models with integrative, filter-based synapses exhibit an initial rise in the fidelity of recall of stored memories. This rise to a peak is driven by a transient process and is then followed by a return to equilibrium. In a series of papers, we have employed a first passage time (FPT) approach to define and study memory lifetimes, incrementally developing our methods, from both simple and complex binary-strength synapses to simple multistate synapses. Here, we complete this work by analyzing FPT memory lifetimes in multistate, filter-based synapses. To achieve this, we integrate out the internal filter states so that we can work with transitions only in synaptic strength. We then generalize results on polysynaptic generating functions from binary strength to multistate synapses, allowing us to examine the dynamics of synaptic strength changes in an ensemble of synapses rather than just a single synapse. To derive analytical results for FPT memory lifetimes, we partition the synaptic dynamics into two distinct phases: the first, pre-peak phase studied with a drift-only approximation, and the second, post-peak phase studied with approximations to the full strength transition probabilities. These approximations capture the underlying dynamics very well, as demonstrated by the extremely good agreement between results obtained by simulating our model and results obtained from the Fokker-Planck or integral equation approaches to FPT processes."}], "ArticleTitle": "First Passage Time Memory Lifetimes for Multistate, Filter-Based Synapses."}, "28410056": {"mesh": [], "AbstractText": [{"section": null, "text": "Recently, a new framework, Fredholm learning, was proposed for semisupervised learning problems based on solving a regularized Fredholm integral equation. It allows a natural way to incorporate unlabeled data into learning algorithms to improve their prediction performance. Despite rapid progress on implementable algorithms with theoretical guarantees, the generalization ability of Fredholm kernel learning has not been studied. In this letter, we focus on investigating the generalization performance of a family of classification algorithms, referred to as Fredholm kernel regularized classifiers. We prove that the corresponding learning rate can achieve [Formula: see text] ([Formula: see text] is the number of labeled samples) in a limiting case. In addition, a representer theorem is provided for the proposed regularized scheme, which underlies its applications."}], "ArticleTitle": "Generalization Analysis of Fredholm Kernel Regularized Classifiers."}, "30645182": {"mesh": ["Algorithms", "Bone and Bones", "Computer Simulation", "Electromyography", "Gait", "Humans", "Lower Extremity", "Models, Biological", "Muscle, Skeletal", "Musculoskeletal Physiological Phenomena", "Nervous System Physiological Phenomena", "Neurons"], "AbstractText": [{"section": null, "text": "The high computational cost (CC) of neuromusculoskeletal modeling is usually considered a serious barrier in clinical applications. Different approaches have been developed to lessen CC and amplify the accuracy of muscle activation prediction based on forward and inverse analyses by applying different optimization algorithms. This study is aimed at proposing two novel approaches, inverse muscular dynamics with inequality constraints (IMDIC) and inverse-forward muscular dynamics with inequality constraints (IFMDIC), not only to reduce CC but also to amend the computational errors compared to the well-known approach of extended inverse dynamics (EID). To do that, the equality constraints of optimization problem, which are computationally tough to satisfy, are replaced by inequality constraints, which are easier to satisfy. To verify the practical application of the proposed approaches, the muscle activations of the lower limbs during the half of a gait cycle are quantified. The simulation results of the optimal muscle activations are then compared to the experimental ones. The results reveal that IMDIC requires less CC (87.5%) compared to EID. In addition, CC of IMDIC was about 33.3% improved by the application of IFMDIC. The findings of this study suggest that although the novel approach of IFMDIC decreases CC compared to IMDIC, the convergence of its results is very sensitive to the primary guess of the optimization variables."}], "ArticleTitle": "A Novel Optimization Framework to Improve the Computational Cost of Muscle Activation Prediction for a Neuromusculoskeletal System."}, "32187002": {"mesh": ["Action Potentials", "Animals", "Bicuculline", "Computer Simulation", "Microelectrodes", "Models, Neurological", "Neurons", "Signal Processing, Computer-Assisted"], "AbstractText": [{"section": null, "text": "As synchronized activity is associated with basic brain functions and pathological states, spike train synchrony has become an important measure to analyze experimental neuronal data. Many measures of spike train synchrony have been proposed, but there is no gold standard allowing for comparison of results from different experiments. This work aims to provide guidance on which synchrony measure is best suited to quantify the effect of epileptiform-inducing substances (e.g., bicuculline, BIC) in in vitro neuronal spike train data. Spike train data from recordings are likely to suffer from erroneous spike detection, such as missed spikes (false negative) or noise (false positive). Therefore, different timescale-dependent (cross-correlation, mutual information, spike time tiling coefficient) and timescale-independent (Spike-contrast, phase synchronization (PS), A-SPIKE-synchronization, A-ISI-distance, ARI-SPIKE-distance) synchrony measures were compared in terms of their robustness to erroneous spike trains. For this purpose, erroneous spike trains were generated by randomly adding (false positive) or deleting (false negative) spikes (in silico manipulated data) from experimental data. In addition, experimental data were analyzed using different spike detection threshold factors in order to confirm the robustness of the synchrony measures. All experimental data were recorded from cortical neuronal networks on microelectrode array chips, which show epileptiform activity induced by the substance BIC. As a result of the in silico manipulated data, Spike-contrast was the only measure that was robust to false-negative as well as false-positive spikes. Analyzing the experimental data set revealed that all measures were able to capture the effect of BIC in a statistically significant way, with Spike-contrast showing the highest statistical significance even at low spike detection thresholds. In summary, we suggest using Spike-contrast to complement established synchrony measures because it is timescale independent and robust to erroneous spike trains."}], "ArticleTitle": "Comparison of Different Spike Train Synchrony Measures Regarding Their Robustness to Erroneous Data From Bicuculline-Induced Epileptiform Activity."}, "33253030": {"mesh": ["Brain", "Brain Concussion", "Humans", "Memory Disorders", "Mental Recall", "Nerve Net", "Neural Networks, Computer", "Neuronal Plasticity", "Receptors, N-Methyl-D-Aspartate", "Unsupervised Machine Learning"], "AbstractText": [{"section": null, "text": "Mild traumatic brain injury (mTBI) presents a significant health concern with potential persisting deficits that can last decades. Although a growing body of literature improves our understanding of the brain network response and corresponding underlying cellular alterations after injury, the effects of cellular disruptions on local circuitry after mTBI are poorly understood. Our group recently reported how mTBI in neuronal networks affects the functional wiring of neural circuits and how neuronal inactivation influences the synchrony of coupled microcircuits. Here, we utilized a computational neural network model to investigate the circuit-level effects of N-methyl D-aspartate receptor dysfunction. The initial increase in activity in injured neurons spreads to downstream neurons, but this increase was partially reduced by restructuring the network with spike-timing-dependent plasticity. As a model of network-based learning, we also investigated how injury alters pattern acquisition, recall, and maintenance of a conditioned response to stimulus. Although pattern acquisition and maintenance were impaired in injured networks, the greatest deficits arose in recall of previously trained patterns. These results demonstrate how one specific mechanism of cellular-level damage in mTBI affects the overall function of a neural network and point to the importance of reversing cellular-level changes to recover important properties of learning and memory in a microcircuit."}], "ArticleTitle": "NMDA Receptor Alterations After Mild Traumatic Brain Injury Induce Deficits in Memory Acquisition and Recall."}, "33080164": {"mesh": ["Arm", "Humans", "Learning", "Male", "Models, Biological", "Muscle Tonus", "Posture", "Reinforcement, Psychology"], "AbstractText": [{"section": null, "text": "This letter proposes a new idea to improve learning efficiency in reinforcement learning (RL) with the actor-critic method used as a muscle controller for posture stabilization of the human arm. Actor-critic RL (ACRL) is used for simulations to realize posture controls in humans or robots using muscle tension control. However, it requires very high computational costs to acquire a better muscle control policy for desirable postures. For efficient ACRL, we focused on embodiment that is supposed to potentially achieve efficient controls in research fields of artificial intelligence or robotics. According to the neurophysiology of motion control obtained from experimental studies using animals or humans, the pedunculopontine tegmental nucleus (PPTn) induces muscle tone suppression, and the midbrain locomotor region (MLR) induces muscle tone promotion. PPTn and MLR modulate the activation levels of mutually antagonizing muscles such as flexors and extensors in a process through which control signals are translated from the substantia nigra reticulata to the brain stem. Therefore, we hypothesized that the PPTn and MLR could control muscle tone, that is, the maximum values of activation levels of mutually antagonizing muscles using different sigmoidal functions for each muscle; then we introduced antagonism function models (AFMs) of PPTn and MLR for individual muscles, incorporating the hypothesis into the process to determine the activation level of each muscle based on the output of the actor in ACRL. ACRL with AFMs representing the embodiment of muscle tone successfully achieved posture stabilization in five joint motions of the right arm of a human adult male under gravity in predetermined target angles at an earlier period of learning than the learning methods without AFMs. The results obtained from this study suggest that the introduction of embodiment of muscle tone can enhance learning efficiency in posture stabilization disorders of humans or humanoid robots."}], "ArticleTitle": "Efficient Actor-Critic Reinforcement Learning With Embodiment of Muscle Tone for Posture Stabilization of the Human Arm."}, "33400897": {"mesh": [], "AbstractText": [{"section": null, "text": "Hopfield neural networks have been extended using hypercomplex numbers. The algebra of bicomplex numbers, also referred to as commutative quaternions, is a number system of dimension 4. Since the multiplication is commutative, many notions and theories of linear algebra, such as determinant, are available, unlike quaternions. A bicomplex-valued Hopfield neural network (BHNN) has been proposed as a multistate neural associative memory. However, the stability conditions have been insufficient for the projection rule. In this work, the stability conditions are extended and applied to improvement of the projection rule. The computer simulations suggest improved noise tolerance."}], "ArticleTitle": "Stability Conditions of Bicomplex-Valued Hopfield Neural Networks."}, "28095197": {"mesh": [], "AbstractText": [{"section": null, "text": "Since the work of Minsky and Papert ( 1969 ), it has been understood that single-layer neural networks cannot solve nonlinearly separable classifications (i.e., XOR). We describe and test a novel divergent autoassociative architecture capable of solving nonlinearly separable classifications with a single layer of weights. The proposed network consists of class-specific linear autoassociators. The power of the model comes from treating classification problems as within-class feature prediction rather than directly optimizing a discriminant function. We show unprecedented learning capabilities for a simple, single-layer network (i.e., solving XOR) and demonstrate that the famous limitation in acquiring nonlinearly separable problems is not just about the need for a hidden layer; it is about the choice between directly predicting classes or learning to classify indirectly by predicting features."}], "ArticleTitle": "Solving Nonlinearly Separable Classifications in a Single-Layer Neural Network."}, "32370605": {"mesh": [], "AbstractText": [{"section": null, "text": "As a training and analysis strategy for convolutional neural networks (CNNs), we slice images into tiled segments and use, for training and prediction, segments that both satisfy an information criterion and contain sufficient content to support classification. In particular, we use image entropy as the information criterion. This ensures that each tile carries as much information diversity as the original image and, for many applications, serves as an indicator of usefulness in classification. To make predictions, a probability aggregation framework is applied to probabilities assigned by the CNN to the input image tiles. This technique, which we call Salient Slices, facilitates the use of large, high-resolution images that would be impractical to analyze unmodified; provides data augmentation for training, which is particularly valuable when image availability is limited; and the ensemble nature of the input for prediction enhances its accuracy."}], "ArticleTitle": "Salient Slices: Improved Neural Network Training and Performance with Image Entropy."}, "30314424": {"mesh": ["Animals", "Bayes Theorem", "Brain", "Humans", "Models, Neurological", "Motion Perception"], "AbstractText": [{"section": null, "text": "A common practice to account for psychophysical biases in vision is to frame them as consequences of a dynamic process relying on optimal inference with respect to a generative model. The study presented here details the complete formulation of such a generative model intended to probe visual motion perception with a dynamic texture model. It is derived in a set of axiomatic steps constrained by biological plausibility. We extend previous contributions by detailing three equivalent formulations of this texture model. First, the composite dynamic textures are constructed by the random aggregation of warped patterns, which can be viewed as three-dimensional gaussian fields. Second, these textures are cast as solutions to a stochastic partial differential equation (sPDE). This essential step enables real-time, on-the-fly texture synthesis using time-discretized autoregressive processes. It also allows for the derivation of a local motion-energy model, which corresponds to the log likelihood of the probability density. The log likelihoods are essential for the construction of a Bayesian inference framework. We use the dynamic texture model to psychophysically probe speed perception in humans using zoom-like changes in the spatial frequency content of the stimulus. The human data replicate previous findings showing perceived speed to be positively biased by spatial frequency increments. A Bayesian observer who combines a gaussian likelihood centered at the true speed and a spatial frequency dependent width with a \"slow-speed prior\" successfully accounts for the perceptual bias. More precisely, the bias arises from a decrease in the observer's likelihood width estimated from the experiments as the spatial frequency increases. Such a trend is compatible with the trend of the dynamic texture likelihood width."}], "ArticleTitle": "Bayesian Modeling of Motion Perception Using Dynamical Stochastic Textures."}, "29894650": {"mesh": [], "AbstractText": [{"section": null, "text": "We formulate an equivalence between machine learning and the formulation of statistical data assimilation as used widely in physical and biological sciences. The correspondence is that layer number in a feedforward artificial network setting is the analog of time in the data assimilation setting. This connection has been noted in the machine learning literature. We add a perspective that expands on how methods from statistical physics and aspects of Lagrangian and Hamiltonian dynamics play a role in how networks can be trained and designed. Within the discussion of this equivalence, we show that adding more layers (making the network deeper) is analogous to adding temporal resolution in a data assimilation framework. Extending this equivalence to recurrent networks is also discussed. We explore how one can find a candidate for the global minimum of the cost functions in the machine learning context using a method from data assimilation. Calculations on simple models from both sides of the equivalence are reported. Also discussed is a framework in which the time or layer label is taken to be continuous, providing a differential equation, the Euler-Lagrange equation and its boundary conditions, as a necessary condition for a minimum of the cost function. This shows that the problem being solved is a two-point boundary value problem familiar in the discussion of variational methods. The use of continuous layers is denoted \"deepest learning.\" These problems respect a symplectic symmetry in continuous layer phase space. Both Lagrangian versions and Hamiltonian versions of these problems are presented. Their well-studied implementation in a discrete time/layer, while respecting the symplectic structure, is addressed. The Hamiltonian version provides a direct rationale for backpropagation as a solution method for a certain two-point boundary value problem."}], "ArticleTitle": "Machine Learning: Deepest Learning as Statistical Data Assimilation Problems."}, "30021085": {"mesh": ["Algorithms", "Animals", "Brain", "Computer Simulation", "Models, Neurological", "Neurons", "Pattern Recognition, Physiological", "Recognition, Psychology"], "AbstractText": [{"section": null, "text": "We formulate the computational processes of perception in the framework of the principle of least action by postulating the theoretical action as a time integral of the variational free energy in the neurosciences. The free energy principle is accordingly rephrased, on autopoetic grounds, as follows: all viable organisms attempt to minimize their sensory uncertainty about an unpredictable environment over a temporal horizon. By taking the variation of informational action, we derive neural recognition dynamics (RD), which by construction reduces to the Bayesian filtering of external states from noisy sensory inputs. Consequently, we effectively cast the gradient-descent scheme of minimizing the free energy into Hamiltonian mechanics by addressing only the positions and momenta of the organisms' representations of the causal environment. To demonstrate the utility of our theory, we show how the RD may be implemented in a neuronally based biophysical model at a single-cell level and subsequently in a coarse-grained, hierarchical architecture of the brain. We also present numerical solutions to the RD for a model brain and analyze the perceptual trajectories around attractors in neural state space."}], "ArticleTitle": "Recognition Dynamics in the Brain under the Free Energy Principle."}, "33400903": {"mesh": [], "AbstractText": [{"section": null, "text": "Active inference is a first principle account of how autonomous agents operate in dynamic, nonstationary environments. This problem is also considered in reinforcement learning, but limited work exists on comparing the two approaches on the same discrete-state environments. In this letter, we provide (1) an accessible overview of the discrete-state formulation of active inference, highlighting natural behaviors in active inference that are generally engineered in reinforcement learning, and (2) an explicit discrete-state comparison between active inference and reinforcement learning on an OpenAI gym baseline. We begin by providing a condensed overview of the active inference literature, in particular viewing the various natural behaviors of active inference agents through the lens of reinforcement learning. We show that by operating in a pure belief-based setting, active inference agents can carry out epistemic exploration-and account for uncertainty about their environment-in a Bayes-optimal fashion. Furthermore, we show that the reliance on an explicit reward signal in reinforcement learning is removed in active inference, where reward can simply be treated as another observation we have a preference over; even in the total absence of rewards, agent behaviors are learned through preference learning. We make these properties explicit by showing two scenarios in which active inference agents can infer behaviors in reward-free environments compared to both Q-learning and Bayesian model-based reinforcement learning agents and by placing zero prior preferences over rewards and learning the prior preferences over the observations corresponding to reward. We conclude by noting that this formalism can be applied to more complex settings (e.g., robotic arm movement, Atari games) if appropriate generative models can be formulated. In short, we aim to demystify the behavior of active inference agents by presenting an accessible discrete state-space and time formulation and demonstrate these behaviors in a OpenAI gym environment, alongside reinforcement learning agents."}], "ArticleTitle": "Active Inference: Demystified and Compared."}, "28333588": {"mesh": [], "AbstractText": [{"section": null, "text": "The connection density of nearby neurons in the cortex has been observed to be around 0.1, whereas the longer-range connections are present with much sparser density (Kalisman, Silberberg, & Markram, 2005 ). We propose a memory association model that qualitatively explains these empirical observations. The model we consider is a multiassociative, sparse, Willshaw-like model consisting of binary threshold neurons and binary synapses. It uses recurrent synapses for iterative retrieval of stored memories. We quantify the usefulness of recurrent synapses by simulating the model for small network sizes and by doing a precise mathematical analysis for large network sizes. Given the network parameters, we can determine the precise values of recurrent and afferent synapse densities that optimize the storage capacity of the network. If the network size is like that of a cortical column, then the predicted optimal recurrent density lies in a range that is compatible with biological measurements. Furthermore, we show that our model is able to surpass the standard Willshaw model in the multiassociative case if the information capacity is normalized per strong synapse or per bits required to store the model, as considered in Knoblauch, Palm, and Sommer ( 2010 )."}], "ArticleTitle": "Multiassociative Memory: Recurrent Synapses Increase Storage Capacity."}, "28562213": {"mesh": [], "AbstractText": [{"section": null, "text": "Many time series are considered to be a superposition of several oscillation components. We have proposed a method for decomposing univariate time series into oscillation components and estimating their phases (Matsuda & Komaki, 2017 ). In this study, we extend that method to multivariate time series. We assume that several oscillators underlie the given multivariate time series and that each variable corresponds to a superposition of the projections of the oscillators. Thus, the oscillators superpose on each variable with amplitude and phase modulation. Based on this idea, we develop gaussian linear state-space models and use them to decompose the given multivariate time series. The model parameters are estimated from data using the empirical Bayes method, and the number of oscillators is determined using the Akaike information criterion. Therefore, the proposed method extracts underlying oscillators in a data-driven manner and enables investigation of phase dynamics in a given multivariate time series. Numerical results show the effectiveness of the proposed method. From monthly mean north-south sunspot number data, the proposed method reveals an interesting phase relationship."}], "ArticleTitle": "Multivariate Time Series Decomposition into Oscillation Components."}, "28957021": {"mesh": [], "AbstractText": [{"section": null, "text": "Spectral clustering is a key research topic in the field of machine learning and data mining. Most of the existing spectral clustering algorithms are built on gaussian Laplacian matrices, which is sensitive to parameters. We propose a novel parameter-free distance-consistent locally linear embedding. The proposed distance-consistent LLE can promise that edges between closer data points are heavier. We also propose a novel improved spectral clustering via embedded label propagation. Our algorithm is built on two advancements of the state of the art. First is label propagation, which propagates a node's labels to neighboring nodes according to their proximity. We perform standard spectral clustering on original data and assign each cluster with [Formula: see text]-nearest data points and then we propagate labels through dense unlabeled data regions. Second is manifold learning, which has been widely used for its capacity to leverage the manifold structure of data points. Extensive experiments on various data sets validate the superiority of the proposed algorithm compared to state-of-the-art spectral algorithms."}], "ArticleTitle": "Refined Spectral Clustering via Embedded Label Propagation."}, "28095196": {"mesh": [], "AbstractText": [{"section": null, "text": "We study the convergence of the online composite mirror descent algorithm, which involves a mirror map to reflect the geometry of the data and a convex objective function consisting of a loss and a regularizer possibly inducing sparsity. Our error analysis provides convergence rates in terms of properties of the strongly convex differentiable mirror map and the objective function. For a class of objective functions with H&#246;lder continuous gradients, the convergence rates of the excess (regularized) risk under polynomially decaying step sizes have the order [Formula: see text] after [Formula: see text] iterates. Our results improve the existing error analysis for the online composite mirror descent algorithm by avoiding averaging and removing boundedness assumptions, and they sharpen the existing convergence rates of the last iterate for online gradient descent without any boundedness assumptions. Our methodology mainly depends on a novel error decomposition in terms of an excess Bregman distance, refined analysis of self-bounding properties of the objective function, and the resulting one-step progress bounds."}], "ArticleTitle": "Analysis of Online Composite Mirror Descent Algorithm."}, "27348736": {"mesh": ["Humans", "Memory", "Nerve Net", "Neurons"], "AbstractText": [{"section": null, "text": "Techniques from coding theory are able to improve the efficiency of neuroinspired and neural associative memories by forcing some construction and constraints on the network. In this letter, the approach is to embed coding techniques into neural associative memory in order to increase their performance in the presence of partial erasures. The motivation comes from recent work by Gripon, Berrou, and coauthors, which revisited Willshaw networks and presented a neural network with interacting neurons that partitioned into clusters. The model introduced stores patterns as small-size cliques that can be retrieved in spite of partial error. We focus on improving the success of retrieval by applying two techniques: doing a local coding in each cluster and then applying a precoding step. We use a slightly different decoding scheme, which is appropriate for partial erasures and converges faster. Although the ideas of local coding and precoding are not new, the way we apply them is different. Simulations show an increase in the pattern retrieval capacity for both techniques. Moreover, we use self-dual additive codes over field [Formula: see text], which have very interesting properties and a simple-graph representation."}], "ArticleTitle": "Clique-Based Neural Associative Memories with Local Coding and Precoding."}, "28599112": {"mesh": [], "AbstractText": [{"section": null, "text": "Convolutional neural networks (CNNs) have been applied to visual tasks since the late 1980s. However, despite a few scattered applications, they were dormant until the mid-2000s when developments in computing power and the advent of large amounts of labeled data, supplemented by improved algorithms, contributed to their advancement and brought them to the forefront of a neural network renaissance that has seen rapid progression since 2012. In this review, which focuses on the application of CNNs to image classification tasks, we cover their development, from their predecessors up to recent state-of-the-art deep learning systems. Along the way, we analyze (1) their early successes, (2) their role in the deep learning renaissance, (3) selected symbolic works that have contributed to their recent popularity, and (4) several improvement attempts by reviewing contributions and challenges of over 300 publications. We also introduce some of their current trends and remaining challenges."}], "ArticleTitle": "Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review."}, "31951795": {"mesh": ["Action Potentials", "Cerebellum", "Humans", "Models, Neurological", "Neurons", "Synapses"], "AbstractText": [{"section": null, "text": "Cerebellar stellate cells form inhibitory synapses with Purkinje cells, the sole output of the cerebellum. Upon stimulation by a pair of varying inhibitory and fixed excitatory presynaptic inputs, these cells do not respond to excitation (i.e., do not generate an action potential) when the magnitude of the inhibition is within a given range, but they do respond outside this range. We previously used a revised Hodgkin-Huxley type of model to study the nonmonotonic first-spike latency of these cells and their temporal increase in excitability in whole cell configuration (termed run-up). Here, we recompute these latency profiles using the same model by adapting an efficient computational technique, the two-point boundary value problem, that is combined with the continuation method. We then extend the study to investigate how switching in responsiveness, upon stimulation with presynaptic inputs, manifests itself in the context of run-up. A three-dimensional reduced model is initially derived from the original six-dimensional model and then analyzed to demonstrate that both models exhibit type 1 excitability possessing a saddle-node on an invariant cycle (SNIC) bifurcation when varying the amplitude of Iapp. Using slow-fast analysis, we show that the original model possesses three equilibria lying at the intersection of the critical manifold of the fast subsystem and the nullcline of the slow variable hA (the inactivation of the A-type K+ channel), the middle equilibrium is of saddle type with two-dimensional stable manifold (computed from the reduced model) acting as a boundary between the responsive and non-responsive regimes, and the (ghost of) SNIC is formed when the hA-nullcline is (nearly) tangential to the critical manifold. We also show that the slow dynamics associated with (the ghost of) the SNIC and the lower stable branch of the critical manifold are responsible for generating the nonmonotonic first-spike latency. These results thus provide important insight into the complex dynamics of stellate cells."}], "ArticleTitle": "Switching in Cerebellar Stellate Cell Excitability in Response to a Pair of Inhibitory/Excitatory Presynaptic Inputs: A Dynamical System Perspective."}, "29566357": {"mesh": ["Animals", "Computer Simulation", "Humans", "Models, Neurological", "Neocortex", "Nerve Net", "Neural Networks, Computer", "Neurons", "Nonlinear Dynamics", "Problem Solving"], "AbstractText": [{"section": null, "text": "Finding actions that satisfy the constraints imposed by both external inputs and internal representations is central to decision making. We demonstrate that some important classes of constraint satisfaction problems (CSPs) can be solved by networks composed of homogeneous cooperative-competitive modules that have connectivity similar to motifs observed in the superficial layers of neocortex. The winner-take-all modules are sparsely coupled by programming neurons that embed the constraints onto the otherwise homogeneous modular computational substrate. We show rules that embed any instance of the CSP's planar four-color graph coloring, maximum independent set, and sudoku on this substrate and provide mathematical proofs that guarantee these graph coloring problems will convergence to a solution. The network is composed of nonsaturating linear threshold neurons. Their lack of right saturation allows the overall network to explore the problem space driven through the unstable dynamics generated by recurrent excitation. The direction of exploration is steered by the constraint neurons. While many problems can be solved using only linear inhibitory constraints, network performance on hard problems benefits significantly when these negative constraints are implemented by nonlinear multiplicative inhibition. Overall, our results demonstrate the importance of instability rather than stability in network computation and offer insight into the computational role of dual inhibitory mechanisms in neural circuits."}], "ArticleTitle": "Solving Constraint-Satisfaction Problems with Distributed Neocortical-Like Neuronal Networks."}, "30576619": {"mesh": ["Animals", "Axons", "Connectome", "Humans", "Models, Neurological", "Neural Pathways", "Neurons", "Spinal Cord"], "AbstractText": [{"section": null, "text": "Connectomes abound, but few for the human spinal cord. Using anatomical data in the literature, we constructed a draft connectivity map of the human spinal cord connectome, providing a template for the many calibrations of specialized behavior to be overlaid on it and the basis for an initial computational model. A thorough literature review gleaned cell types, connectivity, and connection strength indications. Where human data were not available, we selected species that have been studied. Cadaveric spinal cord measurements, cross-sectional histology images, and cytoarchitectural data regarding cell size and density served as the starting point for estimating numbers of neurons. Simulations were run using neural circuitry simulation software. The model contains the neural circuitry in all ten Rexed laminae with intralaminar, interlaminar, and intersegmental connections, as well as ascending and descending brain connections and estimated neuron counts for various cell types in every lamina of all 31 segments. We noted the presence of highly interconnected complex networks exhibiting several orders of recurrence. The model was used to perform a detailed study of spinal cord stimulation for analgesia. This model is a starting point for workers to develop and test hypotheses across an array of biomedical applications focused on the spinal cord. Each such model requires additional calibrations to constrain its output to verifiable predictions. Future work will include simulating additional segments and expanding the research uses of the model."}], "ArticleTitle": "Dynamic Computational Model of the Human Spinal Cord Connectome."}, "28410055": {"mesh": ["Adult", "Algorithms", "Brain", "Electroencephalography", "Female", "Gamma Rhythm", "Humans", "Male", "Pattern Recognition, Physiological", "Smell", "Support Vector Machine", "Wavelet Analysis"], "AbstractText": [{"section": null, "text": "There are various kinds of brain monitoring techniques, including local field potential, near-infrared spectroscopy, magnetic resonance imaging (MRI), positron emission tomography, functional MRI, electroencephalography (EEG), and magnetoencephalography. Among those techniques, EEG is the most widely used one due to its portability, low setup cost, and noninvasiveness. Apart from other advantages, EEG signals also help to evaluate the ability of the smelling organ. In such studies, EEG signals, which are recorded during smelling, are analyzed to determine the subject lacks any smelling ability or to measure the response of the brain. The main idea of this study is to show the emotional difference in EEG signals during perception of valerian, lotus flower, cheese, and rosewater odors by the EEG gamma wave. The proposed method was applied to the EEG signals, which were taken from five healthy subjects in the conditions of eyes open and eyes closed at the Swiss Federal Institute of Technology. In order to represent the signals, we extracted features from the gamma band of the EEG trials by continuous wavelet transform with the selection of Morlet as a wavelet function. Then the [Formula: see text]-nearest neighbor algorithm was implemented as the classifier for recognizing the EEG trials as valerian, lotus flower, cheese, and rosewater. We achieved an average classification accuracy rate of 87.50% with the 4.3 standard deviation value for the subjects in eyes-open condition and an average classification accuracy rate of 94.12% with the 2.9 standard deviation value for the subjects in eyes-closed condition. The results prove that the proposed continuous wavelet transform-based feature extraction method has great potential to classify the EEG signals recorded during smelling of the present odors. It has been also established that gamma-band activity of the brain is highly associated with olfaction."}], "ArticleTitle": "Olfactory Recognition Based on EEG Gamma-Band Activity."}, "28095193": {"mesh": [], "AbstractText": [{"section": null, "text": "Many classification tasks require both labeling objects and determining label associations for parts of each object. Example applications include labeling segments of images or determining relevant parts of a text document when the training labels are available only at the image or document level. This task is usually referred to as multi-instance (MI) learning, where the learner typically receives a collection of labeled (or sometimes unlabeled) bags, each containing several segments (instances). We propose a semisupervised MI learning method for multilabel classification. Most MI learning methods treat instances in each bag as independent and identically distributed samples. However, in many practical applications, instances are related to each other and should not be considered independent. Our model discovers a latent low-dimensional space that captures structure within each bag. Further, unlike many other MI learning methods, which are primarily developed for binary classification, we model multiple classes jointly, thus also capturing possible dependencies between different classes. We develop our model within a semisupervised framework, which leverages both labeled and, typically, a larger set of unlabeled bags for training. We develop several efficient inference methods for our model. We first introduce a Markov chain Monte Carlo method for inference, which can handle arbitrary relations between bag labels and instance labels, including the standard hard-max MI assumption. We also develop an extension of our model that uses stochastic variational Bayes methods for inference, and thus scales better to massive data sets. Experiments show that our approach outperforms several MI learning and standard classification methods on both bag-level and instance-level label prediction. All code for replicating our experiments is available from https://github.com/hsoleimani/MLTM ."}], "ArticleTitle": "Semisupervised, Multilabel, Multi-Instance Learning for Structured Data."}, "31835001": {"mesh": ["Brain", "Connectome", "Humans", "Models, Neurological", "Nerve Net", "Neural Networks, Computer", "Neurons"], "AbstractText": [{"section": null, "text": "The study of neuronal interactions is at the center of several big collaborative neuroscience projects (including the Human Connectome Project, the Blue Brain Project, and the Brainome) that attempt to obtain a detailed map of the entire brain. Under certain constraints, mathematical theory can advance predictions of the expected neural dynamics based solely on the statistical properties of the synaptic interaction matrix. This work explores the application of free random variables to the study of large synaptic interaction matrices. Besides recovering in a straightforward way known results on eigenspectra in types of models of neural networks proposed by Rajan and Abbott (2006), we extend them to heavy-tailed distributions of interactions. More important, we analytically derive the behavior of eigenvector overlaps, which determine the stability of the spectra. We observe that on imposing the neuronal excitation/inhibition balance, despite the eigenvalues remaining unchanged, their stability dramatically decreases due to the strong nonorthogonality of associated eigenvectors. This leads us to the conclusion that understanding the temporal evolution of asymmetric neural networks requires considering the entangled dynamics of both eigenvectors and eigenvalues, which might bear consequences for learning and memory processes in these models. Considering the success of free random variables theory in a wide variety of disciplines, we hope that the results presented here foster the additional application of these ideas in the area of brain sciences."}], "ArticleTitle": "From Synaptic Interactions to Collective Dynamics in Random Neuronal Networks Models: Critical Role of Eigenvectors and Transient Behavior."}, "29220304": {"mesh": [], "AbstractText": [{"section": null, "text": "The experimental evidence on the interrelation between episodic memory and semantic memory is inconclusive. Are they independent systems, different aspects of a single system, or separate but strongly interacting systems? Here, we propose a computational role for the interaction between the semantic and episodic systems that might help resolve this debate. We hypothesize that episodic memories are represented as sequences of activation patterns. These patterns are the output of a semantic representational network that compresses the high-dimensional sensory input. We show quantitatively that the accuracy of episodic memory crucially depends on the quality of the semantic representation. We compare two types of semantic representations: appropriate representations, which means that the representation is used to store input sequences that are of the same type as those that it was trained on, and inappropriate representations, which means that stored inputs differ from the training data. Retrieval accuracy is higher for appropriate representations because the encoded sequences are less divergent than those encoded with inappropriate representations. Consistent with our model prediction, we found that human subjects remember some aspects of episodes significantly more accurately if they had previously been familiarized with the objects occurring in the episode, as compared to episodes involving unfamiliar objects. We thus conclude that the interaction with the semantic system plays an important role for episodic memory."}], "ArticleTitle": "The Interaction between Semantic Representation and Episodic Memory."}, "29162005": {"mesh": ["Action Potentials", "Animals", "Models, Neurological", "Neural Inhibition", "Neural Pathways", "Neurons", "Stochastic Processes", "Synapses"], "AbstractText": [{"section": null, "text": "We investigate rhythms in networks of neurons with recurrent excitation, that is, with excitatory cells exciting each other. Recurrent excitation can sustain activity even when the cells in the network are driven below threshold, too weak to fire on their own. This sort of \"reverberating\" activity is often thought to be the basis of working memory. Recurrent excitation can also lead to \"runaway\" transitions, sudden transitions to high-frequency firing; this may be related to epileptic seizures. Not all fundamental questions about these phenomena have been answered with clarity in the literature. We focus on three questions here: (1) How much recurrent excitation is needed to sustain reverberating activity? How does the answer depend on parameters? (2) Is there a positive minimum frequency of reverberating activity, a positive \"onset frequency\"? How does it depend on parameters? (3) When do runaway transitions occur? For reduced models, we give mathematical answers to these questions. We also examine computationally to which extent our findings are reflected in the behavior of biophysically more realistic model networks. Our main results can be summarized as follows. (1) Reverberating activity can be fueled by extremely weak slow recurrent excitation, but only by sufficiently strong fast recurrent excitation. (2) The onset of reverberating activity, as recurrent excitation is strengthened or external drive is raised, occurs at a positive frequency. It is faster when the external drive is weaker (and the recurrent excitation stronger). It is slower when the recurrent excitation has a longer decay time constant. (3) Runaway transitions occur only with fast, not with slow, recurrent excitation. We also demonstrate that the relation between reverberating activity fueled by recurrent excitation and runaway transitions can be visualized in an instructive way by a (generalized) cusp catastrophe surface."}], "ArticleTitle": "On Rhythms in Neuronal Networks with Recurrent Excitation."}, "29342395": {"mesh": [], "AbstractText": [{"section": null, "text": "Neurons integrate information from many neighbors when they process information. Inputs to a given neuron are thus indistinguishable from one another. Under the assumption that neurons maximize their information storage, indistinguishability is shown to place a strong constraint on the distribution of strengths between neurons. The distribution of individual synapse strengths is found to follow a modified Boltzmann distribution with strength proportional to [Formula: see text]. The model is shown to be consistent with experimental data from Caenorhabditis elegans connectivity and in vivo synaptic strength measurements. The [Formula: see text] dependence helps account for the observation of many zero or weak connections between neurons or sparsity of the neural network."}], "ArticleTitle": "Indistinguishable Synapses Lead to Sparse Networks."}, "30021084": {"mesh": [], "AbstractText": [{"section": null, "text": "In recent years, the development of algorithms to detect neuronal spiking activity from two-photon calcium imaging data has received much attention, yet few researchers have examined the metrics used to assess the similarity of detected spike trains with the ground truth. We highlight the limitations of the two most commonly used metrics, the spike train correlation and success rate, and propose an alternative, which we refer to as CosMIC. Rather than operating on the true and estimated spike trains directly, the proposed metric assesses the similarity of the pulse trains obtained from convolution of the spike trains with a smoothing pulse. The pulse width, which is derived from the statistics of the imaging data, reflects the temporal tolerance of the metric. The final metric score is the size of the commonalities of the pulse trains as a fraction of their average size. Viewed through the lens of set theory, CosMIC resembles a continuous S&#248;rensen-Dice coefficient-an index commonly used to assess the similarity of discrete, presence/absence data. We demonstrate the ability of the proposed metric to discriminate the precision and recall of spike train estimates. Unlike the spike train correlation, which appears to reward overestimation, the proposed metric score is maximized when the correct number of spikes have been detected. Furthermore, we show that CosMIC is more sensitive to the temporal precision of estimates than the success rate."}], "ArticleTitle": "CosMIC: A Consistent Metric for Spike Inference from Calcium Imaging."}, "29949460": {"mesh": ["Action Potentials", "Algorithms", "Animals", "Electronics, Medical", "Hippocampus", "Humans", "Models, Neurological", "Nerve Net", "Neural Networks, Computer", "Neural Prostheses", "Neurons", "Nonlinear Dynamics"], "AbstractText": [{"section": null, "text": "A hippocampal prosthesis is a very large scale integration (VLSI) biochip that needs to be implanted in the biological brain to solve a cognitive dysfunction. In this letter, we propose a novel low-complexity, small-area, and low-power programmable hippocampal neural network application-specific integrated circuit (ASIC) for a hippocampal prosthesis. It is based on the nonlinear dynamical model of the hippocampus: namely multi-input, multi-output (MIMO)-generalized Laguerre-Volterra model (GLVM). It can realize the real-time prediction of hippocampal neural activity. New hardware architecture, a storage space configuration scheme, low-power convolution, and gaussian random number generator modules are proposed. The ASIC is fabricated in 40 nm technology with a core area of 0.122 mm[Formula: see text] and test power of 84.4 [Formula: see text]W. Compared with the design based on the traditional architecture, experimental results show that the core area of the chip is reduced by 84.94% and the core power is reduced by 24.30%."}], "ArticleTitle": "ASIC Implementation of a Nonlinear Dynamical Model for Hippocampal Prosthesis."}, "28181874": {"mesh": ["Algorithms", "Brain", "Brain Mapping", "Electroencephalography", "Epilepsy", "Humans", "Neural Networks, Computer", "Nonlinear Dynamics", "Time Factors", "Wavelet Analysis"], "AbstractText": [{"section": null, "text": "In many realistic networks, the edges representing the interactions between nodes are time varying. Evidence is growing that the complex network that models the dynamics of the human brain has time-varying interconnections, that is, the network is evolving. Based on this evidence, we construct a patient- and data-specific evolving network model (comprising discrete-time dynamical systems) in which epileptic seizures or their terminations in the brain are also determined by the nature of the time-varying interconnections between the nodes. A novel and unique feature of our methodology is that the evolving network model remembers the data from which it was conceived from, in the sense that it evolves to almost regenerate the patient data even on presenting an arbitrary initial condition to it. We illustrate a potential utility of our methodology by constructing an evolving network from clinical data that aids in identifying an approximate seizure focus; nodes in such a theoretically determined seizure focus are outgoing hubs that apparently act as spreaders of seizures. We also point out the efficacy of removal of such spreaders in limiting seizures."}], "ArticleTitle": "Evolving Network Model That Almost Regenerates Epileptic Data."}, "27870611": {"mesh": ["Animals", "Brain", "Humans", "Learning", "Models, Neurological", "Neural Networks, Computer", "Neural Pathways", "Neurons"], "AbstractText": [{"section": null, "text": "We show that Hopfield neural networks with synchronous dynamics and asymmetric weights admit stable orbits that form sequences of maximal length. For [Formula: see text] units, these sequences have length [Formula: see text]; that is, they cover the full state space. We present a mathematical proof that maximal-length orbits exist for all [Formula: see text], and we provide a method to construct both the sequence and the weight matrix that allow its production. The orbit is relatively robust to dynamical noise, and perturbations of the optimal weights reveal other periodic orbits that are not maximal but typically still very long. We discuss how the resulting dynamics on slow time-scales can be used to generate desired output sequences."}], "ArticleTitle": "Exponentially Long Orbits in Hopfield Neural Networks."}, "27391681": {"mesh": ["Bayes Theorem", "Blinking", "Cerebellum", "Conditioning, Classical", "Humans", "Learning", "Models, Theoretical"], "AbstractText": [{"section": null, "text": "This letter offers a computational account of Pavlovian conditioning in the cerebellum based on active inference and predictive coding. Using eyeblink conditioning as a canonical paradigm, we formulate a minimal generative model that can account for spontaneous blinking, startle responses, and (delay or trace) conditioning. We then establish the face validity of the model using simulated responses to unconditioned and conditioned stimuli to reproduce the sorts of behavior that are observed empirically. The scheme's anatomical validity is then addressed by associating variables in the predictive coding scheme with nuclei and neuronal populations to match the (extrinsic and intrinsic) connectivity of the cerebellar (eyeblink conditioning) system. Finally, we try to establish predictive validity by reproducing selective failures of delay conditioning, trace conditioning, and extinction using (simulated and reversible) focal lesions. Although rather metaphorical, the ensuing scheme can account for a remarkable range of anatomical and neurophysiological aspects of cerebellar circuitry-and the specificity of lesion-deficit mappings that have been established experimentally. From a computational perspective, this work shows how conditioning or learning can be formulated in terms of minimizing variational free energy (or maximizing Bayesian model evidence) using exactly the same principles that underlie predictive coding in perception."}], "ArticleTitle": "Active Inference and Learning in the Cerebellum."}, "28777722": {"mesh": ["Brain", "Brain-Computer Interfaces", "Electroencephalography", "Equipment Design", "Female", "Humans", "Male", "Mental Processes", "Neurofeedback", "Signal Processing, Computer-Assisted"], "AbstractText": [{"section": null, "text": "Brain-computer interfaces (BCIs) allow users to control a device by interpreting their brain activity. For simplicity, these devices are designed to be operated by purposefully modulating specific predetermined neurophysiological signals, such as the sensorimotor rhythm. However, the ability to modulate a given neurophysiological signal is highly variable across individuals, contributing to the inconsistent performance of BCIs for different users. These differences suggest that individuals who experience poor BCI performance with one class of brain signals might have good results with another. In order to take advantage of individual abilities as they relate to BCI control, we need to move beyond the current approaches. In this letter, we explore a new BCI design aimed at a more individualized and user-focused experience, which we call open-ended BCI. Individual users were given the freedom to discover their own mental strategies as opposed to being trained to modulate a given brain signal. They then underwent multiple coadaptive training sessions with the BCI. Our first open-ended BCI performed similarly to comparable BCIs while accommodating a wider variety of mental strategies without a priori knowledge of the specific brain signals any individual might use. Post hoc analysis revealed individual differences in terms of which sensory modality yielded optimal performance. We found a large and significant effect of individual differences in background training and expertise, such as in musical training, on BCI performance. Future research should be focused on finding more generalized solutions to user training and brain state decoding methods to fully utilize the abilities of different individuals in an open-ended BCI. Accounting for each individual's areas of expertise could have important implications on BCI training and BCI application design."}], "ArticleTitle": "Toward an Open-Ended BCI: A User-Centered Coadaptive Design."}, "31335293": {"mesh": ["Action Potentials", "Brain", "Humans", "Models, Neurological", "Neural Networks, Computer", "Neurons", "Nonlinear Dynamics", "Principal Component Analysis"], "AbstractText": [{"section": null, "text": "It is known that brain can create a sparse representation of the environment in both sensory and mnemonic forms (Olshausen & Field, 2004). Such sparse representation can be combined in downstream areas to create rich multisensory responses to support various cognitive and motor functions. Determining the components present in neuronal responses in a given region is key to deciphering its functional role and connection with upstream areas. One approach for parsing out various sources of information in a single neuron is by using linear blind source separation (BSS) techniques. However, applying linear techniques to neuronal spiking activity is likely to be suboptimal due to inherent and unknown nonlinearity of neuronal responses to inputs. This letter proposes a nonlinear sparse component analysis (SCA) method to separate jointly sparse inputs to neurons with post summation nonlinearity, or SCA for post-nonlinear neurons (SCAPL). Specifically, a linear clustering approach followed by principal curve regression (PCR) and a nonlinear curve fitting are used to separate sources. Analysis using simulated data shows that SCAPL accuracy outperforms ones obtained by linear SCA, as well as other separating methods, including linear independent and principal component analyses. In SCAPL, the number of derived sparse components is not limited by the number of neurons, unlike most BSS methods. Furthermore, this method allows for a broad range of post-summation nonlinearities that could differ among neurons. The sensitivity of our method to noise, joint sparseness, degree, and shape of nonlinearity and mixing ill conditions is discussed and compared to existing methods. Our results show that the proposed method can successfully separate input components in a population of neurons provided that they are temporally sparse to some degree. Application of SCAPL should facilitate comparison of functional roles across regions by parsing various elements present in a region."}], "ArticleTitle": "A New Nonlinear Sparse Component Analysis for a Biologically Plausible Model of Neurons."}, "28095194": {"mesh": [], "AbstractText": [{"section": null, "text": "Many previous proposals for adversarial training of deep neural nets have included directly modifying the gradient, training on a mix of original and adversarial examples, using contractive penalties, and approximately optimizing constrained adversarial objective functions. In this article, we show that these proposals are actually all instances of optimizing a general, regularized objective we call DataGrad. Our proposed DataGrad framework, which can be viewed as a deep extension of the layerwise contractive autoencoder penalty, cleanly simplifies prior work and easily allows extensions such as adversarial training with multitask cues. In our experiments, we find that the deep gradient regularization of DataGrad (which also has L1 and L2 flavors of regularization) outperforms alternative forms of regularization, including classical L1, L2, and multitask, on both the original data set and adversarial sets. Furthermore, we find that combining multitask optimization with DataGrad adversarial training results in the most robust performance."}], "ArticleTitle": "Unifying Adversarial Training Algorithms with Data Gradient Regularization."}, "29342396": {"mesh": [], "AbstractText": [{"section": null, "text": "Learning an appropriate distance metric plays a substantial role in the success of many learning machines. Conventional metric learning algorithms have limited utility when the training and test samples are drawn from related but different domains (i.e., source domain and target domain). In this letter, we propose two novel metric learning algorithms for domain adaptation in an information-theoretic setting, allowing for discriminating power transfer and standard learning machine propagation across two domains. In the first one, a cross-domain Mahalanobis distance is learned by combining three goals: reducing the distribution difference between different domains, preserving the geometry of target domain data, and aligning the geometry of source domain data with label information. Furthermore, we devote our efforts to solving complex domain adaptation problems and go beyond linear cross-domain metric learning by extending the first method to a multiple kernel learning framework. A convex combination of multiple kernels and a linear transformation are adaptively learned in a single optimization, which greatly benefits the exploration of prior knowledge and the description of data characteristics. Comprehensive experiments in three real-world applications (face recognition, text classification, and object categorization) verify that the proposed methods outperform state-of-the-art metric learning and domain adaptation methods."}], "ArticleTitle": "Cross-Domain Metric and Multiple Kernel Learning Based on Information Theory."}, "29566349": {"mesh": [], "AbstractText": [{"section": null, "text": "Recent advances in engineering and signal processing have renewed the interest in invasive and surface brain recordings, yet many features of cortical field potentials remain incompletely understood. In the computational study that follows, we show that a model circuit of interneurons, coupled via both GABAA receptor synapses and electrical synapses, reproduces many essential features of the power spectrum of local field potential (LFP) recordings, such as 1/ f power scaling at low frequency (below 10 Hz), power accumulation in the &#947;-frequency band (30-100 Hz), and a robust &#945; rhythm in the absence of stimulation. The low-frequency 1/ f power scaling depends on strong reciprocal inhibition, whereas the &#945; rhythm is generated by electrical coupling of intrinsically active neurons. As in previous studies, the &#947; power arises through the amplification of single-neuron spectral properties, owing to the refractory period, by parameters that favor neuronal synchrony, such as delayed inhibition. This study also confirms that both synaptic and voltage-gated membrane currents contribute substantially to the LFP and that high-frequency signals such as action potentials quickly taper off with distance. Given the ubiquity of electrically coupled interneuron circuits in the mammalian brain, they may be major determinants of the recorded potentials."}], "ArticleTitle": "An Interneuron Circuit Reproducing Essential Spectral Features of Field Potentials."}, "28181880": {"mesh": [], "AbstractText": [{"section": null, "text": "This review examines the relevance of parameter identifiability for statistical models used in machine learning. In addition to defining main concepts, we address several issues of identifiability closely related to machine learning, showing the advantages and disadvantages of state-of-the-art research and demonstrating recent progress. First, we review criteria for determining the parameter structure of models from the literature. This has three related issues: parameter identifiability, parameter redundancy, and reparameterization. Second, we review the deep influence of identifiability on various aspects of machine learning from theoretical and application viewpoints. In addition to illustrating the utility and influence of identifiability, we emphasize the interplay among identifiability theory, machine learning, mathematical statistics, information theory, optimization theory, information geometry, Riemann geometry, symbolic computation, Bayesian inference, algebraic geometry, and others. Finally, we present a new perspective together with the associated challenges."}], "ArticleTitle": "Parameter Identifiability in Statistical Machine Learning: A Review."}, "29342400": {"mesh": [], "AbstractText": [{"section": null, "text": "A neuronal population is a computational unit that receives a multivariate, time-varying input signal and creates a related multivariate output. These neural signals are modeled as stochastic processes that transmit information in real time, subject to stochastic noise. In a stationary environment, where the input signals can be characterized by constant statistical properties, the systematic relationship between its input and output processes determines the computation carried out by a population. When these statistical characteristics unexpectedly change, the population needs to adapt to its new environment if it is to maintain stable operation. Based on the general concept of homeostatic plasticity, we propose a simple compositional model of adaptive networks that achieve invariance with regard to undesired changes in the statistical properties of their input signals and maintain outputs with well-defined joint statistics. To achieve such invariance, the network model combines two functionally distinct types of plasticity. An abstract stochastic process neuron model implements a generalized form of intrinsic plasticity that adapts marginal statistics, relying only on mechanisms locally confined within each neuron and operating continuously in time, while a simple form of Hebbian synaptic plasticity operates on synaptic connections, thus shaping the interrelation between neurons as captured by a copula function. The combined effect of both mechanisms allows a neuron population to discover invariant representations of its inputs that remain stable under a wide range of transformations (e.g., shifting, scaling and (affine linear) mixing). The probabilistic model of homeostatic adaptation on a population level as presented here allows us to isolate and study the individual and the interaction dynamics of both mechanisms of plasticity and could guide the future search for computationally beneficial types of adaptation."}], "ArticleTitle": "A Unifying Framework of Synaptic and Intrinsic Plasticity in Neural Populations."}, "29162008": {"mesh": [], "AbstractText": [{"section": null, "text": "Humans possess a remarkable ability to rapidly form coarse estimations of numerical averages. This ability is important for making decisions that are based on streams of numerical or value-based information, as well as for preference formation. Nonetheless, the mechanism underlying rapid approximate numerical averaging remains unknown, and several competing mechanism may account for it. Here, we tested the hypothesis that approximate numerical averaging relies on perceptual-like processes, instantiated by population coding. Participants were presented with rapid sequences of numerical values (four items per second) and were asked to convey the sequence average. We manipulated the sequences' length, variance, and mean magnitude and found that similar to perceptual averaging, the precision of the estimations improves with the length and deteriorates with (higher) variance or (higher) magnitude. To account for the results, we developed a biologically plausible population-coding model and showed that it is mathematically equivalent to a population vector. Using both quantitative and qualitative model comparison methods, we compared the population-coding model to several competing models, such as a step-by-step running average (based on leaky integration) and a midrange model. We found that the data support the population-coding model. We conclude that humans' ability to rapidly form estimations of numerical averages has many properties of the perceptual (intuitive) system rather than the arithmetic, linguistic-based (analytic) system and that population coding is likely to be its underlying mechanism."}], "ArticleTitle": "A Perceptual-Like Population-Coding Mechanism of Approximate Numerical Averaging."}, "31703172": {"mesh": [], "AbstractText": [{"section": null, "text": "We propose a novel family of connectionist models based on kernel machines and consider the problem of learning layer by layer a compositional hypothesis class (i.e., a feedforward, multilayer architecture) in a supervised setting. In terms of the models, we present a principled method to \"kernelize\" (partly or completely) any neural network (NN). With this method, we obtain a counterpart of any given NN that is powered by kernel machines instead of neurons. In terms of learning, when learning a feedforward deep architecture in a supervised setting, one needs to train all the components simultaneously using backpropagation (BP) since there are no explicit targets for the hidden layers (Rumelhart, Hinton, & Williams, 1986). We consider without loss of generality the two-layer case and present a general framework that explicitly characterizes a target for the hidden layer that is optimal for minimizing the objective function of the network. This characterization then makes possible a purely greedy training scheme that learns one layer at a time, starting from the input layer. We provide instantiations of the abstract framework under certain architectures and objective functions. Based on these instantiations, we present a layer-wise training algorithm for an l-layer feedforward network for classification, where l&#8805;2 can be arbitrary. This algorithm can be given an intuitive geometric interpretation that makes the learning dynamics transparent. Empirical results are provided to complement our theory. We show that the kernelized networks, trained layer-wise, compare favorably with classical kernel machines as well as other connectionist models trained by BP. We also visualize the inner workings of the greedy kernelized models to validate our claim on the transparency of the layer-wise algorithm."}], "ArticleTitle": "On Kernel Method-Based Connectionist Models and Supervised Deep Learning Without Backpropagation."}, "28599113": {"mesh": [], "AbstractText": [{"section": null, "text": "In order to interact intelligently with objects in the world, animals must first transform neural population responses into estimates of the dynamic, unknown stimuli that caused them. The Bayesian solution to this problem is known as a Bayes filter, which applies Bayes' rule to combine population responses with the predictions of an internal model. The internal model of the Bayes filter is based on the true stimulus dynamics, and in this note, we present a method for training a theoretical neural circuit to approximately implement a Bayes filter when the stimulus dynamics are unknown. To do this we use the inferential properties of linear probabilistic population codes to compute Bayes' rule and train a neural network to compute approximate predictions by the method of maximum likelihood. In particular, we perform stochastic gradient descent on the negative log-likelihood of the neural network parameters with a novel approximation of the gradient. We demonstrate our methods on a finite-state, a linear, and a nonlinear filtering problem and show how the hidden layer of the neural network develops tuning curves consistent with findings in experimental neuroscience."}], "ArticleTitle": "Implementing a Bayes Filter in a Neural Circuit: The Case of Unknown Stimulus Dynamics."}, "27870612": {"mesh": ["Action Potentials", "Animals", "Calcium", "Entropy", "Macaca", "Models, Neurological", "Models, Statistical", "Neurons", "Photic Stimulation", "Visual Cortex", "Voltage-Sensitive Dye Imaging"], "AbstractText": [{"section": null, "text": "Our understanding of neural population coding has been limited by a lack of analysis methods to characterize spiking data from large populations. The biggest challenge comes from the fact that the number of possible network activity patterns scales exponentially with the number of neurons recorded ([Formula: see text]). Here we introduce a new statistical method for characterizing neural population activity that requires semi-independent fitting of only as many parameters as the square of the number of neurons, requiring drastically smaller data sets and minimal computation time. The model works by matching the population rate (the number of neurons synchronously active) and the probability that each individual neuron fires given the population rate. We found that this model can accurately fit synthetic data from up to 1000 neurons. We also found that the model could rapidly decode visual stimuli from neural population data from macaque primary visual cortex about 65&#160;ms after stimulus onset. Finally, we used the model to estimate the entropy of neural population activity in developing mouse somatosensory cortex and, surprisingly, found that it first increases, and then decreases during development. This statistical model opens new options for interrogating neural population data and can bolster the use of modern large-scale in vivo Ca[Formula: see text] and voltage imaging tools."}], "ArticleTitle": "The Population Tracking Model: A Simple, Scalable Statistical Model for Neural Population Data."}, "28957017": {"mesh": ["Algorithms", "Game Theory", "Humans", "Learning", "Models, Neurological", "Neural Pathways", "Neurons", "Synapses"], "AbstractText": [{"section": null, "text": "Modeling self-organization of neural networks for unsupervised learning using Hebbian and anti-Hebbian plasticity has a long history in neuroscience. Yet derivations of single-layer networks with such local learning rules from principled optimization objectives became possible only recently, with the introduction of similarity matching objectives. What explains the success of similarity matching objectives in deriving neural networks with local learning rules? Here, using dimensionality reduction as an example, we introduce several variable substitutions that illuminate the success of similarity matching. We show that the full network objective may be optimized separately for each synapse using local learning rules in both the offline and online settings. We formalize the long-standing intuition of the rivalry between Hebbian and anti-Hebbian rules by formulating a min-max optimization problem. We introduce a novel dimensionality reduction objective using fractional matrix exponents. To illustrate the generality of our approach, we apply it to a novel formulation of dimensionality reduction combined with whitening. We confirm numerically that the networks with learning rules derived from principled objectives perform better than those with heuristic learning rules."}], "ArticleTitle": "Why Do Similarity Matching Objectives Lead to Hebbian/Anti-Hebbian Networks?"}, "31614104": {"mesh": [], "AbstractText": [{"section": null, "text": "Willshaw networks are single-layered neural networks that store associations between binary vectors. Using only binary weights, these networks can be implemented efficiently to store large numbers of patterns and allow for fault-tolerant recovery of those patterns from noisy cues. However, this is only the case when the involved codes are sparse and randomly generated. In this letter, we use a recently proposed approach that maps visual patterns into informative binary features. By doing so, we manage to transform MNIST handwritten digits into well-distributed codes that we then store in a Willshaw network in autoassociation. We perform experiments with both noisy and noiseless cues and verify a tenuous impact on the recovered pattern's relevant information. More specifically, we were able to perform retrieval after filling the memory to several factors of its number of units while preserving the information of the class to which the pattern belongs."}], "ArticleTitle": "Storing Object-Dependent Sparse Codes in a Willshaw Associative Network."}, "31260390": {"mesh": ["Computer Security", "Humans", "Image Processing, Computer-Assisted", "Machine Learning", "Neural Networks, Computer", "Pattern Recognition, Automated"], "AbstractText": [{"section": null, "text": "A significant threat to the recent, wide deployment of machine learning-based systems, including deep neural networks (DNNs), is adversarial learning attacks. The main focus here is on evasion attacks against DNN-based classifiers at test time. While much work has focused on devising attacks that make small perturbations to a test pattern (e.g., an image) that induce a change in the classifier's decision, until recently there has been a relative paucity of work defending against such attacks. Some works robustify the classifier to make correct decisions on perturbed patterns. This is an important objective for some applications and for natural adversary scenarios. However, we analyze the possible digital evasion attack mechanisms and show that in some important cases, when the pattern (image) has been attacked, correctly classifying it has no utility---when the image to be attacked is (even arbitrarily) selected from the attacker's cache and when the sole recipient of the classifier's decision is the attacker. Moreover, in some application domains and scenarios, it is highly actionable to detect the attack irrespective of correctly classifying in the face of it (with classification still performed if no attack is detected). We hypothesize that adversarial perturbations are machine detectable even if they are small. We propose a purely unsupervised anomaly detector (AD) that, unlike previous works, (1) models the joint density of a deep layer using highly suitable null hypothesis density models (matched in particular to the nonnegative support for rectified linear unit (ReLU) layers); (2) exploits multiple DNN layers; and (3) leverages a source and destination class concept, source class uncertainty, the class confusion matrix, and DNN weight information in constructing a novel decision statistic grounded in the Kullback-Leibler divergence. Tested on MNIST and CIFAR image databases under three prominent attack strategies, our approach outperforms previous detection methods, achieving strong receiver operating characteristic area under the curve detection accuracy on two attacks and better accuracy than recently reported for a variety of methods on the strongest (CW) attack. We also evaluate a fully white box attack on our system and demonstrate that our method can be leveraged to strong effect in detecting reverse engineering attacks. Finally, we evaluate other important performance measures such as classification accuracy versus true detection rate and multiple measures versus attack strength."}], "ArticleTitle": "When Not to Classify: Anomaly Detection of Attacks (ADA) on DNN Classifiers at Test Time."}, "29566350": {"mesh": ["Animals", "Brain", "Caenorhabditis elegans", "Cell Membrane", "Entropy", "Fractals", "Humans", "Ion Channels", "Ions", "Kinetics", "Models, Neurological", "Neurons", "Quantum Theory", "Stochastic Processes"], "AbstractText": [{"section": null, "text": "In this letter, we perform a complete and in-depth analysis of Lorentzian noises, such as those arising from [Formula: see text] and [Formula: see text] channel kinetics, in order to identify the source of [Formula: see text]-type noise in neurological membranes. We prove that the autocovariance of Lorentzian noise depends solely on the eigenvalues (time constants) of the kinetic matrix but that the Lorentzian weighting coefficients depend entirely on the eigenvectors of this matrix. We then show that there are rotations of the kinetic eigenvectors that send any initial weights to any target weights without altering the time constants. In particular, we show there are target weights for which the resulting Lorenztian noise has an approximately [Formula: see text]-type spectrum. We justify these kinetic rotations by introducing a quantum mechanical formulation of membrane stochastics, called hidden quantum activated-measurement models, and prove that these quantum models are probabilistically indistinguishable from the classical hidden Markov models typically used for ion channel stochastics. The quantum dividend obtained by replacing classical with quantum membranes is that rotations of the Lorentzian weights become simple readjustments of the quantum state without any change to the laboratory-determined kinetic and conductance parameters. Moreover, the quantum formalism allows us to model the activation energy of a membrane, and we show that maximizing entropy under constrained activation energy yields the previous [Formula: see text]-type Lorentzian weights, in which the spectral exponent [Formula: see text] is a Lagrange multiplier for the energy constraint. Thus, we provide a plausible neurophysical mechanism by which channel and membrane kinetics can give rise to [Formula: see text]-type noise (something that has been occasionally denied in the literature), as well as a realistic and experimentally testable explanation for the numerical values of the spectral exponents. We also discuss applications of quantum membranes beyond [Formula: see text]-type -noise, including applications to animal models and possible impact on quantum foundations."}], "ArticleTitle": "Hidden Quantum Processes, Quantum Ion Channels, and 1/ f&#952;-Type Noise."}, "28599117": {"mesh": [], "AbstractText": [{"section": null, "text": "Spike synchrony, which occurs in various cortical areas in response to specific perception, action, and memory tasks, has sparked a long-standing debate on the nature of temporal organization in cortex. One prominent view is that this type of synchrony facilitates the binding or grouping of separate stimulus components. We argue instead for a more general function: a measure of the prior probability of incoming stimuli, implemented by long-range, horizontal, intracortical connections. We show that networks of this kind-pulse-coupled excitatory spiking networks in a noisy environment-can provide a sufficient substrate for stimulus-dependent spike synchrony. This allows for a quick (few spikes) estimate of the match between inputs and the input history as encoded in the network structure. Given the ubiquity of small, strongly excitatory subnetworks in cortex, we thus propose that many experimental observations of spike synchrony can be viewed as signs of input patterns that resemble long-term experience-that is, of patterns with high prior probability."}], "ArticleTitle": "Cortical Spike Synchrony as a Measure of Input Familiarity."}, "30576615": {"mesh": [], "AbstractText": [{"section": null, "text": "This work lays the foundation for a framework of cortical learning based on the idea of a competitive column, which is inspired by the functional organization of neurons in the cortex. A column describes a prototypical organization for neurons that gives rise to an ability to learn scale, rotation, and translation-invariant features. This is empowered by a recently developed learning rule, conflict learning, which enables the network to learn over both driving and modulatory feedforward, feedback, and lateral inputs. The framework is further supported by introducing both a notion of neural ambiguity and an adaptive threshold scheme. Ambiguity, which captures the idea that too many decisions lead to indecision, gives the network a dynamic way to resolve locally ambiguous decisions. The adaptive threshold operates over multiple timescales to regulate neural activity under the varied arrival timings of input in a highly interconnected multilayer network with feedforward and feedback. The competitive column architecture is demonstrated on a large-scale (54,000 neurons and 18 million synapses), invariant model of border ownership. The model is trained on four simple, fixed-scale shapes: two squares, one rectangle, and one symmetric L-shape. Tested on 1899 synthetic shapes of varying scale and complexity, the model correctly assigned border ownership with 74% accuracy. The model's abilities were also illustrated on contours of objects taken from natural images. Combined with conflict learning, the competitive column and ambiguity give a better intuitive understanding of how feedback, modulation, and inhibition may interact in the brain to influence activation and learning."}], "ArticleTitle": "Learning Invariant Features in Modulatory Networks through Conflict and Ambiguity."}, "28030778": {"mesh": ["Brain", "Humans", "Models, Neurological", "Nervous System Diseases", "Neural Inhibition", "Neural Pathways"], "AbstractText": [{"section": null, "text": "The development of control technology for the brain is of potential significance to the prevention and treatment of neuropsychiatric disorders and the improvement of humans' mental health. A controllability analysis of the brain is necessary to ensure the feasibility of the brain control. In this letter, we investigate the influences of dynamical parameters on the controllability in the neural mass model by using controllability indices as quantitative indicators. The indices are obtained by computing Lie brackets and condition numbers of the system model. We show how controllability changes with important parameters of our dynamical (neuronal) model. Our results suggest that the underlying dynamical parameters have certain ranges with better controllability. We hope it can play potential roles in therapy for brain nervous disorder disease."}], "ArticleTitle": "Controllability Analysis of the Neural Mass Model with Dynamic Parameters."}, "29162009": {"mesh": [], "AbstractText": [{"section": null, "text": "While existing logistic regression suffers from overfitting and often fails in considering structural information, we propose a novel matrix-based logistic regression to overcome the weakness. In the proposed method, 2D matrices are directly used to learn two groups of parameter vectors along each dimension without vectorization, which allows the proposed method to fully exploit the underlying structural information embedded inside the 2D matrices. Further, we add a joint [Formula: see text]-norm on two parameter matrices, which are organized by aligning each group of parameter vectors in columns. This added co-regularization term has two roles-enhancing the effect of regularization and optimizing the rank during the learning process. With our proposed fast iterative solution, we carried out extensive experiments. The results show that in comparison to both the traditional tensor-based methods and the vector-based regression methods, our proposed solution achieves better performance for matrix data classifications."}], "ArticleTitle": "Rank-Optimized Logistic Matrix Regression toward Improved Matrix Data Classification."}, "27626966": {"mesh": [], "AbstractText": [{"section": null, "text": "We present a quantitative statistical analysis of pairwise crossings for all fibers obtained from whole brain tractography that confirms with high confidence that the brain grid theory (Wedeen et&#160;al., 2012a ) is not supported by the evidence. The overall fiber tracts structure appears to be more consistent with small angle treelike branching of tracts rather than with near-orthogonal gridlike crossing of fiber sheets. The analysis uses our new method for high-resolution whole brain tractography that is capable of resolving fibers crossing of less than 10 degrees and correctly following a continuous angular distribution of fibers even when the individual fiber directions are not resolved. This analysis also allows us to demonstrate that the whole brain fiber pathway system is very well approximated by a lamellar vector field, providing a concise and quantitative mathematical characterization of the structural connectivity of the human brain."}], "ArticleTitle": "The Lamellar Structure of the Brain Fiber Pathways."}, "27557102": {"mesh": [], "AbstractText": [{"section": null, "text": "Error backpropagation in networks of spiking neurons (SpikeProp) shows promise for the supervised learning of temporal patterns. However, its widespread use is hindered by its computational load and occasional convergence failures. In this letter, we show that the neuronal firing time equation at the core of SpikeProp can be solved analytically using the Lambert W function, offering a marked reduction in execution time over the step-based method used in the literature. Applying this analytical method to SpikeProp, we find that training time per epoch can be reduced by 12% to 56% under different experimental conditions. Finally, this work opens the way for further investigations of SpikeProp's convergence behavior."}], "ArticleTitle": "On the Analytical Solution of Firing Time for SpikeProp."}, "27557106": {"mesh": [], "AbstractText": [{"section": null, "text": "In many areas of neural computation, like learning, optimization, estimation, and inference, suitable divergences play a key role. In this note, we study the conjecture presented by Amari ( 2009 ) and find a counterexample to show that the conjecture does not hold generally. Moreover, we investigate two classes of [Formula: see text]-divergence (Zhang, 2004 ), weighted f-divergence and weighted [Formula: see text]-divergence, and prove that if a divergence is a weighted f-divergence, as well as a Bregman divergence, then it is a weighted [Formula: see text]-divergence. This result reduces in form to the main theorem established by Amari ( 2009 ) when [Formula: see text] [Formula: see text]."}], "ArticleTitle": "A Note on Divergences."}, "29894658": {"mesh": ["Animals", "Brain", "Computer Simulation", "Decision Making", "Eye Movements", "Humans", "Markov Chains", "Models, Biological", "Neural Pathways", "Neurons"], "AbstractText": [{"section": null, "text": "To act upon the world, creatures must change continuous variables such as muscle length or chemical concentration. In contrast, decision making is an inherently discrete process, involving the selection among alternative courses of action. In this article, we consider the interface between the discrete and continuous processes that translate our decisions into movement in a Newtonian world-and how movement informs our decisions. We do so by appealing to active inference, with a special focus on the oculomotor system. Within this exemplar system, we argue that the superior colliculus is well placed to act as a discrete-continuous interface. Interestingly, when the neuronal computations within the superior colliculus are formulated in terms of active inference, we find that many aspects of its neuroanatomy emerge from the computations it must perform in this role."}], "ArticleTitle": "The Discrete and Continuous Brain: From Decisions to Movement-And Back Again."}, "28095200": {"mesh": [], "AbstractText": [{"section": null, "text": "We show that Langevin Markov chain Monte Carlo inference in an energy-based model with latent variables has the property that the early steps of inference, starting from a stationary point, correspond to propagating error gradients into internal layers, similar to backpropagation. The backpropagated error is with respect to output units that have received an outside driving force pushing them away from the stationary point. Backpropagated error gradients correspond to temporal derivatives with respect to the activation of hidden units. These lead to a weight update proportional to the product of the presynaptic firing rate and the temporal rate of change of the postsynaptic firing rate. Simulations and a theoretical argument suggest that this rate-based update rule is consistent with those associated with spike-timing-dependent plasticity. The ideas presented in this article could be an element of a theory for explaining how brains perform credit assignment in deep hierarchies as efficiently as backpropagation does, with neural computation corresponding to both approximate inference in continuous-valued latent variables and error backpropagation, at the same time."}], "ArticleTitle": "STDP-Compatible Approximation of Backpropagation in an Energy-Based Model."}, "29220308": {"mesh": ["Animals", "Auditory Cortex", "Auditory Pathways", "Auditory Perception", "Cats", "Learning", "Models, Neurological", "Models, Statistical", "Neural Networks, Computer", "Neurons", "Noise", "Pattern Recognition, Physiological", "Sound Spectrography"], "AbstractText": [{"section": null, "text": "Interaction with the world requires an organism to transform sensory signals into representations in which behaviorally meaningful properties of the environment are made explicit. These representations are derived through cascades of neuronal processing stages in which neurons at each stage recode the output of preceding stages. Explanations of sensory coding may thus involve understanding how low-level patterns are combined into more complex structures. To gain insight into such midlevel representations for sound, we designed a hierarchical generative model of natural sounds that learns combinations of spectrotemporal features from natural stimulus statistics. In the first layer, the model forms a sparse convolutional code of spectrograms using a dictionary of learned spectrotemporal kernels. To generalize from specific kernel activation patterns, the second layer encodes patterns of time-varying magnitude of multiple first-layer coefficients. When trained on corpora of speech and environmental sounds, some second-layer units learned to group similar spectrotemporal features. Others instantiate opponency between distinct sets of features. Such groupings might be instantiated by neurons in the auditory cortex, providing a hypothesis for midlevel neuronal computation."}], "ArticleTitle": "Learning Midlevel Auditory Codes from Natural Sound Statistics."}, "30645179": {"mesh": [], "AbstractText": [{"section": null, "text": "We analyze algorithms for approximating a function <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>&#934;</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:math> mapping <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msup><mml:mi>&#8476;</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math> to <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msup><mml:mi>&#8476;</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math> using deep linear neural networks, that is, that learn a function <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>h</mml:mi></mml:math> parameterized by matrices <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:msub><mml:mi>&#920;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#920;</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math> and defined by <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>&#920;</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:msub><mml:mi>&#920;</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8230;</mml:mo><mml:msub><mml:mi>&#920;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>x</mml:mi></mml:mrow></mml:math> . We focus on algorithms that learn through gradient descent on the population quadratic loss in the case that the distribution over the inputs is isotropic. We provide polynomial bounds on the number of iterations for gradient descent to approximate the least-squares matrix <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>&#934;</mml:mi></mml:math> , in the case where the initial hypothesis <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:msub><mml:mi>&#920;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>&#920;</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math> has excess loss bounded by a small enough constant. We also show that gradient descent fails to converge for <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>&#934;</mml:mi></mml:math> whose distance from the identity is a larger constant, and we show that some forms of regularization toward the identity in each layer do not help. If <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>&#934;</mml:mi></mml:math> is symmetric positive definite, we show that an algorithm that initializes <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:msub><mml:mi>&#920;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math> learns an <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>&#949;</mml:mi></mml:math> -approximation of <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>f</mml:mi></mml:math> using a number of updates polynomial in <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>L</mml:mi></mml:math> , the condition number of <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>&#934;</mml:mi></mml:math> , and <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mo>log</mml:mo><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>/</mml:mo><mml:mi>&#949;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math> . In contrast, we show that if the least-squares matrix <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>&#934;</mml:mi></mml:math> is symmetric and has a negative eigenvalue, then all members of a class of algorithms that perform gradient descent with identity initialization, and optionally regularize toward the identity in each layer, fail to converge. We analyze an algorithm for the case that <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>&#934;</mml:mi></mml:math> satisfies <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mi>&#8868;</mml:mi></mml:msup><mml:mi>&#934;</mml:mi><mml:mi>u</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math> for all <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>u</mml:mi></mml:math> but may not be symmetric. This algorithm uses two regularizers: one that maintains the invariant <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mi>&#8868;</mml:mi></mml:msup><mml:msub><mml:mi>&#920;</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:msub><mml:mi>&#920;</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8230;</mml:mo><mml:msub><mml:mi>&#920;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>u</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math> for all <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>u</mml:mi></mml:math> and the other that \"balances\" <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:msub><mml:mi>&#920;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#920;</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math> so that they have the same singular values."}], "ArticleTitle": "Gradient Descent with Identity Initialization Efficiently Learns Positive-Definite Linear Transformations by Deep Residual Networks."}, "29652589": {"mesh": [], "AbstractText": [{"section": null, "text": "Deep learning involves a difficult nonconvex optimization problem with a large number of weights between any two adjacent layers of a deep structure. To handle large data sets or complicated networks, distributed training is needed, but the calculation of function, gradient, and Hessian is expensive. In particular, the communication and the synchronization cost may become a bottleneck. In this letter, we focus on situations where the model is distributedly stored and propose a novel distributed Newton method for training deep neural networks. By variable and feature-wise data partitions and some careful designs, we are able to explicitly use the Jacobian matrix for matrix-vector products in the Newton method. Some techniques are incorporated to reduce the running time as well as memory consumption. First, to reduce the communication cost, we propose a diagonalization method such that an approximate Newton direction can be obtained without communication between machines. Second, we consider subsampled Gauss-Newton matrices for reducing the running time as well as the communication cost. Third, to reduce the synchronization cost, we terminate the process of finding an approximate Newton direction even though some nodes have not finished their tasks. Details of some implementation issues in distributed environments are thoroughly investigated. Experiments demonstrate that the proposed method is effective for the distributed training of deep neural networks. Compared with stochastic gradient methods, it is more robust and may give better test accuracy."}], "ArticleTitle": "Distributed Newton Methods for Deep Neural Networks."}, "28777720": {"mesh": ["Algorithms", "Brain Mapping", "Brain Waves", "Computer Simulation", "Electrocorticography", "Electrodes, Implanted", "Epilepsy", "Female", "Humans", "Male", "Nonlinear Dynamics", "Signal Processing, Computer-Assisted", "Time Factors"], "AbstractText": [{"section": null, "text": "High-density electrocorticogram (ECoG) electrodes are capable of recording neurophysiological data with high temporal resolution with wide spatial coverage. These recordings are a window to understanding how the human brain processes information and subsequently behaves in healthy and pathologic states. Here, we describe and implement delay differential analysis (DDA) for the characterization of ECoG data obtained from human patients with intractable epilepsy. DDA is a time-domain analysis framework based on embedding theory in nonlinear dynamics that reveals the nonlinear invariant properties of an unknown dynamical system. The DDA embedding serves as a low-dimensional nonlinear dynamical basis onto which the data are mapped. This greatly reduces the risk of overfitting and improves the method's ability to fit classes of data. Since the basis is built on the dynamical structure of the data, preprocessing of the data (e.g., filtering) is not necessary. We performed a large-scale search for a DDA model that best fit ECoG recordings using a genetic algorithm to qualitatively discriminate between different cortical states and epileptic events for a set of 13 patients. A single DDA model with only three polynomial terms was identified. Singular value decomposition across the feature space of the model revealed both global and local dynamics that could differentiate electrographic and electroclinical seizures and provided insights into highly localized seizure onsets and diffuse seizure terminations. Other common ECoG features such as interictal periods, artifacts, and exogenous stimuli were also analyzed with DDA. This novel framework for signal processing of seizure information demonstrates an ability to reveal unique characteristics of the underlying dynamics of the seizure and may be useful in better understanding, detecting, and maybe even predicting seizures."}], "ArticleTitle": "Delay Differential Analysis of Seizures in Multichannel Electrocorticography Data."}, "28599119": {"mesh": ["Action Potentials", "Animals", "Biophysics", "Computer Simulation", "Dendrites", "Electric Stimulation", "Humans", "Models, Neurological", "Neurons", "Nonlinear Dynamics", "Synapses"], "AbstractText": [{"section": null, "text": "Hearing, vision, touch: underlying all of these senses is stimulus selectivity, a robust information processing operation in which cortical neurons respond more to some stimuli than to others. Previous models assume that these neurons receive the highest weighted input from an ensemble encoding the preferred stimulus, but dendrites enable other possibilities. Nonlinear dendritic processing can produce stimulus selectivity based on the spatial distribution of synapses, even if the total preferred stimulus weight does not exceed that of nonpreferred stimuli. Using a multi-subunit nonlinear model, we demonstrate that stimulus selectivity can arise from the spatial distribution of synapses. We propose this as a general mechanism for information processing by neurons possessing dendritic trees. Moreover, we show that this implementation of stimulus selectivity increases the neuron's robustness to synaptic and dendritic failure. Importantly, our model can maintain stimulus selectivity for a larger range of loss of synapses or dendrites than an equivalent linear model. We then use a layer 2/3 biophysical neuron model to show that our implementation is consistent with two recent experimental observations: (1) one can observe a mixture of selectivities in dendrites that can differ from the somatic selectivity, and (2) hyperpolarization can broaden somatic tuning without affecting dendritic tuning. Our model predicts that an initially nonselective neuron can become selective when depolarized. In addition to motivating new experiments, the model's increased robustness to synapses and dendrites loss provides a starting point for fault-resistant neuromorphic chip development."}], "ArticleTitle": "Dendrites Enable a Robust Mechanism for Neuronal Stimulus Selectivity."}, "27626969": {"mesh": [], "AbstractText": [{"section": null, "text": "This study considers the common situation in data analysis when there are few observations of the distribution of interest or the target distribution, while abundant observations are available from auxiliary distributions. In this situation, it is natural to compensate for the lack of data from the target distribution by using data sets from these auxiliary distributions-in other words, approximating the target distribution in a subspace spanned by a set of auxiliary distributions. Mixture modeling is one of the simplest ways to integrate information from the target and auxiliary distributions in order to express the target distribution as accurately as possible. There are two typical mixtures in the context of information geometry: the [Formula: see text]- and [Formula: see text]-mixtures. The [Formula: see text]-mixture is applied in a variety of research fields because of the presence of the well-known expectation-maximazation algorithm for parameter estimation, whereas the [Formula: see text]-mixture is rarely used because of its difficulty of estimation, particularly for nonparametric models. The [Formula: see text]-mixture, however, is a well-tempered distribution that satisfies the principle of maximum entropy. To model a target distribution with scarce observations accurately, this letter proposes a novel framework for a nonparametric modeling of the [Formula: see text]-mixture and a geometrically inspired estimation algorithm. As numerical examples of the proposed framework, a transfer learning setup is considered. The experimental results show that this framework works well for three types of synthetic data sets, as well as an EEG real-world data set."}], "ArticleTitle": "Nonparametric e-Mixture Estimation."}, "30148707": {"mesh": ["Commerce", "Models, Economic", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "This study focuses on predicting stock closing prices by using recurrent neural networks (RNNs). A long short-term memory (LSTM) model, a type of RNN coupled with stock basic trading data and technical indicators, is introduced as a novel method to predict the closing price of the stock market. We realize dimension reduction for the technical indicators by conducting principal component analysis (PCA). To train the model, some optimization strategies are followed, including adaptive moment estimation (Adam) and Glorot uniform initialization. Case studies are conducted on Standard & Poor's 500, NASDAQ, and Apple (AAPL). Plenty of comparison experiments are performed using a series of evaluation criteria to evaluate this model. Accurate prediction of stock market is considered an extremely challenging task because of the noisy environment and high volatility associated with the external factors. We hope the methodology we propose advances the research for analyzing and predicting stock time series. As the results of experiments suggest, the proposed model achieves a good level of fitness."}], "ArticleTitle": "Improving Stock Closing Price Prediction Using Recurrent Neural Network and Technical Indicators."}, "28957028": {"mesh": [], "AbstractText": [{"section": null, "text": "Memory models based on synapses with discrete and bounded strengths store new memories by forgetting old ones. Memory lifetimes in such memory systems may be defined in a variety of ways. A mean first passage time (MFPT) definition overcomes much of the arbitrariness and many of the problems associated with the more usual signal-to-noise ratio (SNR) definition. We have previously computed MFPT lifetimes for simple, binary-strength synapses that lack internal, plasticity-related states. In simulation we have also seen that for multistate synapses, optimality conditions based on SNR lifetimes are absent with MFPT lifetimes, suggesting that such conditions may be artifactual. Here we extend our earlier work by computing the entire first passage time (FPT) distribution for simple, multistate synapses, from which all statistics, including the MFPT lifetime, may be extracted. For this, we develop a Fokker-Planck equation using the jump moments for perceptron activation. Two models are considered that satisfy a particular eigenvector condition that this approach requires. In these models, MFPT lifetimes do not exhibit optimality conditions, while in one but not the other, SNR lifetimes do exhibit optimality. Thus, not only are such optimality conditions artifacts of the SNR approach, but they are also strongly model dependent. By examining the variance in the FPT distribution, we may identify regions in which memory storage is subject to high variability, although MFPT lifetimes are nevertheless robustly positive. In such regions, SNR lifetimes are typically (defined to be) zero. FPT-defined memory lifetimes therefore provide an analytically superior approach and also have the virtue of being directly related to a neuron's firing properties."}], "ArticleTitle": "First Passage Time Memory Lifetimes for Simple, Multistate Synapses."}, "30314427": {"mesh": ["Algorithms", "Animals", "Brain", "Humans", "Models, Neurological", "Neurons", "Nonlinear Dynamics"], "AbstractText": [{"section": null, "text": "Neurons communicate nonlinearly through spike activities. Generalized linear models (GLMs) describe spike activities with a cascade of a linear combination across inputs, a static nonlinear function, and an inhomogeneous Bernoulli or Poisson process, or Cox process if a self-history term is considered. This structure considers the output nonlinearity in spike generation but excludes the nonlinear interaction among input neurons. Recent studies extend GLMs by modeling the interaction among input neurons with a quadratic function, which considers the interaction between every pair of input spikes. However, quadratic effects may not fully capture the nonlinear nature of input interaction. We therefore propose a staged point-process model to describe the nonlinear interaction among inputs using a few hidden units, which follows the idea of artificial neural networks. The output firing probability conditioned on inputs is formed as a cascade of two linear-nonlinear (a linear combination plus a static nonlinear function) stages and an inhomogeneous Bernoulli process. Parameters of this model are estimated by maximizing the log likelihood on output spike trains. Unlike the iterative reweighted least squares algorithm used in GLMs, where the performance is guaranteed by the concave condition, we propose a modified Levenberg-Marquardt (L-M) algorithm, which directly calculates the Hessian matrix of the log likelihood, for the nonlinear optimization in our model. The proposed model is tested on both synthetic data and real spike train data recorded from the dorsal premotor cortex and primary motor cortex of a monkey performing a center-out task. Performances are evaluated by discrete-time rescaled Kolmogorov-Smirnov tests, where our model statistically outperforms a GLM and its quadratic extension, with a higher goodness-of-fit in the prediction results. In addition, the staged point-process model describes nonlinear interaction among input neurons with fewer parameters than quadratic models, and the modified L-M algorithm also demonstrates fast convergence."}], "ArticleTitle": "Nonlinear Modeling of Neural Interaction for Spike Prediction Using the Staged Point-Process Model."}, "30021081": {"mesh": [], "AbstractText": [{"section": null, "text": "Rule extraction from black box models is critical in domains that require model validation before implementation, as can be the case in credit scoring and medical diagnosis. Though already a challenging problem in statistical learning in general, the difficulty is even greater when highly nonlinear, recursive models, such as recurrent neural networks (RNNs), are fit to data. Here, we study the extraction of rules from second-order RNNs trained to recognize the Tomita grammars. We show that production rules can be stably extracted from trained RNNs and that in certain cases, the rules outperform the trained RNNs."}], "ArticleTitle": "An Empirical Evaluation of Rule Extraction from Recurrent Neural Networks."}, "29381442": {"mesh": ["Action Potentials", "Algorithms", "Brain", "Humans", "Models, Neurological", "Neurons", "Normal Distribution", "Stochastic Processes"], "AbstractText": [{"section": null, "text": "Neurons in many brain areas exhibit high trial-to-trial variability, with spike counts that are overdispersed relative to a Poisson distribution. Recent work&#160;(Goris, Movshon, & Simoncelli, 2014 ) has proposed to explain this variability in terms of a multiplicative interaction between a stochastic gain variable and a stimulus-dependent Poisson firing rate, which produces quadratic relationships between spike count mean and variance. Here we examine this quadratic assumption and propose a more flexible family of models that can account for a more diverse set of mean-variance relationships. Our model contains additive gaussian noise that is transformed nonlinearly to produce a Poisson spike rate. Different choices of the nonlinear function can give rise to qualitatively different mean-variance relationships, ranging from sublinear to linear to quadratic. Intriguingly, a rectified squaring nonlinearity produces a linear mean-variance function, corresponding to responses with a constant Fano factor. We describe a computationally efficient method for fitting this model to data and demonstrate that a majority of neurons in a V1 population are better described by a model with a nonquadratic relationship between mean and variance. Finally, we demonstrate a practical use of our model via an application to Bayesian adaptive stimulus selection in closed-loop neurophysiology experiments, which shows that accounting for overdispersion can lead to dramatic improvements in adaptive tuning curve estimation."}], "ArticleTitle": "Dethroning the Fano Factor: A Flexible, Model-Based Approach to Partitioning Neural Variability."}, "30216143": {"mesh": ["Brain", "Humans", "Memory"], "AbstractText": [{"section": null, "text": "Human brains seem to represent categories of objects and actions as locations in a continuous semantic space across the cortical surface that reflects the similarity among categories. This vision of the semantic organization of information in the brain, suggested by recent experimental findings, is in harmony with the well-known topographically organized somatotopic, retinotopic, and tonotopic maps in the cerebral cortex. Here we show that these topographies can be operationally represented with context-dependent associative memories. In these models, the input vectors and, eventually also, the associated output vectors are multiplied by context vectors via the Kronecker tensor product, which allows a spatial organization of memories. Input and output tensor contexts localize matrices of semantic categories into a neural layer or slice and, at the same time, direct the flow of information arriving at the layer to a specific address, and then forward the output information toward the corresponding targets. Given a neural topographic pattern, the tensor representation will place a set of associative matrix memories within a topographic regionalized host matrix in such way that they reproduce the empirical pattern of patches in the actual neural layer. Progressive approximations to this goal are accomplished by avoiding excessive overlap of memories or the existence of empty regions within the host matrix."}], "ArticleTitle": "Tensor Representation of Topographically Organized Semantic Spaces."}, "27391683": {"mesh": ["Animals", "Bayes Theorem", "Choice Behavior", "Decision Making", "Humans", "Nerve Net", "Probability"], "AbstractText": [{"section": null, "text": "Decision confidence is a forecast about the probability that a decision will be correct. From a statistical perspective, decision confidence can be defined as the Bayesian posterior probability that the chosen option is correct based on the evidence contributing to it. Here, we used this formal definition as a starting point to develop a normative statistical framework for decision confidence. Our goal was to make general predictions that do not depend on the structure of the noise or a specific algorithm for estimating confidence. We analytically proved several interrelations between statistical decision confidence and observable decision measures, such as evidence discriminability, choice, and accuracy. These interrelationships specify necessary signatures of decision confidence in terms of externally quantifiable variables that can be empirically tested. Our results lay the foundations for a mathematically rigorous treatment of decision confidence that can lead to a common framework for understanding confidence across different research domains, from human and animal behavior to neural representations."}], "ArticleTitle": "A Mathematical Framework for Statistical Decision Confidence."}, "32433899": {"mesh": ["Action Potentials", "Animals", "Humans", "Models, Neurological", "Neurons", "Time Factors", "Visual Cortex"], "AbstractText": [{"section": null, "text": "Precise timing of spikes between different neurons has been found to convey reliable information beyond the spike count. In contrast, the role of small and variable spiking delays, as reported, for example, in the visual cortex, remains largely unclear. This issue becomes particularly important considering the high speed of neuronal information processing, which is assumed to be based on only a few milliseconds within each processing step. We investigate the role of small and variable spiking delays with a parsimonious stochastic spiking model that is strongly motivated by experimental observations. The model contains only two parameters for the response of a neuron to one stimulus, describing directly the rate and the delay, or phase. Within the theoretical model, we specifically investigate two quantities, the probability of correct stimulus detection and the probability of correct change point detection, as a function of these parameters and within short periods of time. Optimal combinations of the two parameters across stimuli are derived that maximize these probabilities and enable comparison of pure rate, pure phase, and combined codes. In particular, the gain in correct detection probability when adding small and variable spiking delays to pure rate coding increases with the number of stimuli. More interesting, small and variable spiking delays can considerably improve the process of detecting changes in the stimulus, while also decreasing the probability of false alarms and thus increasing robustness and speed of change point detection. The results are compared to empirical spike train recordings of neurons in the visual cortex reported earlier in response to a number of visual stimuli. The results suggest that near-optimal combinations of rate and phase parameters may be implemented in the brain and that adding phase information could particularly increase the quality of change point detection in cases of highly similar stimuli."}], "ArticleTitle": "A Model for the Study of the Increase in Stimulus and Change Point Detection with Small and Variable Spiking Delays."}, "29902113": {"mesh": [], "AbstractText": [{"section": null, "text": "Nearest-neighbor estimators for the Kullback-Leiber (KL) divergence that are asymptotically unbiased have recently been proposed and demonstrated in a number of applications. However, with a small number of samples, nonparametric methods typically suffer from large estimation bias due to the nonlocality of information derived from nearest-neighbor statistics. In this letter, we show that this estimation bias can be mitigated by modifying the metric function, and we propose a novel method for learning a locally optimal Mahalanobis distance function from parametric generative models of the underlying density distributions. Using both simulations and experiments on a variety of data sets, we demonstrate that this interplay between approximate generative models and nonparametric techniques can significantly improve the accuracy of nearest-neighbor-based estimation of the KL divergence."}], "ArticleTitle": "Bias Reduction and Metric Learning for Nearest-Neighbor Estimation of Kullback-Leibler Divergence."}, "29381441": {"mesh": [], "AbstractText": [{"section": null, "text": "Self-organized criticality (SOC) and stochastic oscillations (SOs) are two theoretically contradictory phenomena that are suggested to coexist in the brain. Recently it has been shown that an accumulation-release process like sandpile dynamics can generate SOC and SOs simultaneously. We considered the effect of the network structure on this coexistence and showed that the sandpile dynamics on a small-world network can produce two power law regimes along with two groups of SOs-two peaks in the power spectrum of the generated signal simultaneously. We also showed that external stimuli in the sandpile dynamics do not affect the coexistence of SOC and SOs but increase the frequency of SOs, which is consistent with our knowledge of the brain."}], "ArticleTitle": "Coexistence of Stochastic Oscillations and Self-Organized Criticality in a Neuronal Network: Sandpile Model Application."}, "28562212": {"mesh": [], "AbstractText": [{"section": null, "text": "Nonnegative matrix factorization (NMF) is primarily a linear dimensionality reduction technique that factorizes a nonnegative data matrix into two smaller nonnegative matrices: one that represents the basis of the new subspace and the second that holds the coefficients of all the data points in that new space. In principle, the nonnegativity constraint forces the representation to be sparse and parts based. Instead of extracting holistic features from the data, real parts are extracted that should be significantly easier to interpret and analyze. The size of the new subspace selects how many features will be extracted from the data. An effective choice should minimize the noise while extracting the key features. We propose a mechanism for selecting the subspace size by using a minimum description length technique. We demonstrate that our technique provides plausible estimates for real data as well as accurately predicting the known size of synthetic data. We provide an implementation of our code in a Matlab format."}], "ArticleTitle": "Rank Selection in Nonnegative Matrix Factorization using Minimum Description Length."}, "29342397": {"mesh": ["Animals", "Auditory Perception", "Brain", "Cognition", "Evoked Potentials", "Feasibility Studies", "Hand", "Haplorhini", "Humans", "Magnetoencephalography", "Male", "Microelectrodes", "Motor Activity", "Music", "Neurons", "Periodicity", "Signal Processing, Computer-Assisted", "Time Factors"], "AbstractText": [{"section": null, "text": "This letter presents a noninvasive imaging technique that captures the exact timing and locations of cortical activity sequences that are specific to a cognitive process. These precise spatiotemporal sequences can be detected in the human brain as specific time-position pattern associated with a cognitive task. They are consistent with direct measurements of population activity recorded in nonhuman primates, thus suggesting that specific time-position patterns associated with a cognitive task can be identified. This imaging technique is based on estimating the amplitude of cortical current dipoles from MEG recordings. Although the spatial resolution of these estimations is poor (approximately 2 cm), the temporal resolution is high (milliseconds). We show that within these cortical current dipoles, time points of cortical activation can be identified as brief amplitude undulations and that sequences of these transients repeat with millisecond accuracy, hence making it possible to treat the timing of these transients as point processes. We illustrate the feasibility of finding spatiotemporal templates specific to the cognitive processes associated with following the rhythm of drumbeats that involve the activation at multiple cortical and cerebellar loci. These templates evolve at an accuracy of a few milliseconds. This approach can thus pave the way for new perspectives on the relationships between brain dynamics and cognition."}], "ArticleTitle": "Imaging the Spatiotemporal Dynamics of Cognitive Processes at High Temporal Resolution."}, "27626963": {"mesh": [], "AbstractText": [{"section": null, "text": "The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods-autoregressive integrated moving average; feedforward neural networks-time delay neural network and online sequential extreme learning machine; and recurrent neural networks-long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams."}], "ArticleTitle": "Continuous Online Sequence Learning with an Unsupervised Neural Network Model."}, "29566355": {"mesh": ["Action Potentials", "Algorithms", "Animals", "Discrimination Learning", "Macaca mulatta", "Models, Neurological", "Neurons", "Nonlinear Dynamics", "Photic Stimulation", "Prefrontal Cortex", "Probability", "Visual Cortex", "Visual Pathways", "Visual Perception"], "AbstractText": [{"section": null, "text": "The primate visual system has an exquisite ability to discriminate partially occluded shapes. Recent electrophysiological recordings suggest that response dynamics in intermediate visual cortical area V4, shaped by feedback from prefrontal cortex (PFC), may play a key role. To probe the algorithms that may underlie these findings, we build and test a model of V4 and PFC interactions based on a hierarchical predictive coding framework. We propose that probabilistic inference occurs in two steps. Initially, V4 responses are driven solely by bottom-up sensory input and are thus strongly influenced by the level of occlusion. After a delay, V4 responses combine both feedforward input and feedback signals from the PFC; the latter reflect predictions made by PFC about the visual stimulus underlying V4 activity. We find that this model captures key features of V4 and PFC dynamics observed in experiments. Specifically, PFC responses are strongest for occluded stimuli and delayed responses in V4 are less sensitive to occlusion, supporting our hypothesis that the feedback signals from PFC underlie robust discrimination of occluded shapes. Thus, our study proposes that area V4 and PFC participate in hierarchical inference, with feedback signals encoding top-down predictions about occluded shapes."}], "ArticleTitle": "Predictive Coding in Area V4: Dynamic Shape Discrimination under Partial Occlusion."}, "33080166": {"mesh": ["Algorithms", "Association Learning", "Deep Learning", "Humans", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Backpropagation (BP) is the cornerstone of today's deep learning algorithms, but it is inefficient partially because of backward locking, which means updating the weights of one layer locks the weight updates in the other layers. Consequently, it is challenging to apply parallel computing or a pipeline structure to update the weights in different layers simultaneously. In this letter, we introduce a novel learning structure, associated learning (AL), that modularizes the network into smaller components, each of which has a local objective. Because the objectives are mutually independent, AL can learn the parameters in different layers independently and simultaneously, so it is feasible to apply a pipeline structure to improve the training throughput. Specifically, this pipeline structure improves the complexity of the training time from O(n&#8467;), which is the time complexity when using BP and stochastic gradient descent (SGD) for training, to O(n+&#8467;), where n is the number of training instances and &#8467; is the number of hidden layers. Surprisingly, even though most of the parameters in AL do not directly interact with the target variable, training deep models by this method yields accuracies comparable to those from models trained using typical BP methods, in which all parameters are used to predict the target variable. Consequently, because of the scalability and the predictive power demonstrated in the experiments, AL deserves further study to determine the better hyperparameter settings, such as activation function selection, learning rate scheduling, and weight initialization, to accumulate experience, as we have done over the years with the typical BP method. In addition, perhaps our design can also inspire new network designs for deep learning. Our implementation is available at https://github.com/SamYWK/Associated_Learning."}], "ArticleTitle": "Associated Learning: Decomposing End-to-End Backpropagation Based on Autoencoders and Target Propagation."}, "29064781": {"mesh": [], "AbstractText": [{"section": null, "text": "The dynamics of supervised learning play a main role in deep learning, which takes place in the parameter space of a multilayer perceptron (MLP). We review the history of supervised stochastic gradient learning, focusing on its singular structure and natural gradient. The parameter space includes singular regions in which parameters are not identifiable. One of our results is a full exploration of the dynamical behaviors of stochastic gradient learning in an elementary singular network. The bad news is its pathological nature, in which part of the singular region becomes an attractor and another part a repulser at the same time, forming a Milnor attractor. A learning trajectory is attracted by the attractor region, staying in it for a long time, before it escapes the singular region through the repulser region. This is typical of plateau phenomena in learning. We demonstrate the strange topology of a singular region by introducing blow-down coordinates, which are useful for analyzing the natural gradient dynamics. We confirm that the natural gradient dynamics are free of critical slowdown. The second main result is the good news: the interactions of elementary singular networks eliminate the attractor part and the Milnor-type attractors disappear. This explains why large-scale networks do not suffer from serious critical slowdowns due to singularities. We finally show that the unit-wise natural gradient is effective for learning in spite of its low computational cost."}], "ArticleTitle": "Dynamics of Learning in MLP: Natural Gradient and Singularity Revisited."}, "28957026": {"mesh": [], "AbstractText": [{"section": null, "text": "The traditional [Formula: see text]-means algorithm has been widely used as a simple and efficient clustering method. However, the performance of this algorithm is highly dependent on the selection of initial cluster centers. Therefore, the method adopted for choosing initial cluster centers is extremely important. In this letter, we redefine the density of points according to the number of its neighbors, as well as the distance between points and their neighbors. In addition, we define a new distance measure that considers both Euclidean distance and density. Based on that, we propose an algorithm for selecting initial cluster centers that can dynamically adjust the weighting parameter. Furthermore, we propose a new internal clustering validation measure, the clustering validation index based on the neighbors (CVN), which can be exploited to select the optimal result among multiple clustering results. Experimental results show that the proposed algorithm outperforms existing initialization methods on real-world data sets and demonstrates the adaptability of the proposed algorithm to data sets with various characteristics."}], "ArticleTitle": "An Initialization Method Based on Hybrid Distance for k-Means Algorithm."}, "30462587": {"mesh": [], "AbstractText": [{"section": null, "text": "The Wilkie, Stonham, and Aleksander recognition device (WiSARD) <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>n</mml:mi></mml:math> -tuple classifier is a multiclass weightless neural network capable of learning a given pattern in a single step. Its architecture is determined by the number of classes it should discriminate. A target class is represented by a structure called a discriminator, which is composed of <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>N</mml:mi></mml:math> RAM nodes, each of them addressed by an <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>n</mml:mi></mml:math> -tuple. Previous studies were carried out in order to mitigate an important problem of the WiSARD <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>n</mml:mi></mml:math> -tuple classifier: having its RAM nodes saturated when trained by a large data set. Finding the VC dimension of the WiSARD <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>n</mml:mi></mml:math> -tuple classifier was one of those studies. Although no exact value was found, tight bounds were discovered. Later, the bleaching technique was proposed as a means to avoid saturation. Recent empirical results with the bleaching extension showed that the WiSARD <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>n</mml:mi></mml:math> -tuple classifier can achieve high accuracies with low variance in a great range of tasks. Theoretical studies had not been conducted with that extension previously. This work presents the exact VC dimension of the basic two-class WiSARD <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>n</mml:mi></mml:math> -tuple classifier, which is linearly proportional to the number of RAM nodes belonging to a discriminator, and exponentially to their addressing tuple length, precisely <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math> . The exact VC dimension of the bleaching extension to the WiSARD <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>n</mml:mi></mml:math> -tuple classifier, whose value is the same as that of the basic model, is also produced. Such a result confirms that the bleaching technique is indeed an enhancement to the basic WiSARD <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>n</mml:mi></mml:math> -tuple classifier as it does no harm to the generalization capability of the original paradigm."}], "ArticleTitle": "The Exact VC Dimension of the WiSARD <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>n</mml:mi></mml:math> -Tuple Classifier."}, "31120383": {"mesh": ["Algorithms", "Deep Learning", "Humans", "Machine Learning", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "In this paper, we analyze the effects of depth and width on the quality of local minima, without strong overparameterization and simplification assumptions in the literature. Without any simplification assumption, for deep nonlinear neural networks with the squared loss, we theoretically show that the quality of local minima tends to improve toward the global minimum value as depth and width increase. Furthermore, with a locally induced structure on deep nonlinear neural networks, the values of local minima of neural networks are theoretically proven to be no worse than the globally optimal values of corresponding classical machine learning models. We empirically support our theoretical observation with a synthetic data set, as well as MNIST, CIFAR-10, and SVHN data sets. When compared to previous studies with strong overparameterization assumptions, the results in this letter do not require overparameterization and instead show the gradual effects of overparameterization as consequences of general results."}], "ArticleTitle": "Effect of Depth and Width on Local Minima in Deep Learning."}, "29894652": {"mesh": [], "AbstractText": [{"section": null, "text": "Many machine learning problems can be formulated as predicting labels for a pair of objects. Problems of that kind are often referred to as pairwise learning, dyadic prediction, or network inference problems. During the past decade, kernel methods have played a dominant role in pairwise learning. They still obtain a state-of-the-art predictive performance, but a theoretical analysis of their behavior has been underexplored in the machine learning literature. In this work we review and unify kernel-based algorithms that are commonly used in different pairwise learning settings, ranging from matrix filtering to zero-shot learning. To this end, we focus on closed-form efficient instantiations of Kronecker kernel ridge regression. We show that independent task kernel ridge regression, two-step kernel ridge regression, and a linear matrix filter arise naturally as a special case of Kronecker kernel ridge regression, implying that all these methods implicitly minimize a squared loss. In addition, we analyze universality, consistency, and spectral filtering properties. Our theoretical results provide valuable insights into assessing the advantages and limitations of existing pairwise learning methods."}], "ArticleTitle": "A Comparative Study of Pairwise Learning Methods Based on Kernel Ridge Regression."}, "29342394": {"mesh": ["Action Potentials", "Algorithms", "Animals", "Computer Simulation", "Mice", "Models, Neurological", "Neural Pathways", "Neurons", "Nonlinear Dynamics", "Perception", "Sensation", "Signal Processing, Computer-Assisted", "Tissue Culture Techniques", "Visual Cortex"], "AbstractText": [{"section": null, "text": "Inferring mathematical models of sensory processing systems directly from input-output observations, while making the fewest assumptions about the model equations and the types of measurements available, is still a major issue in computational neuroscience. This letter introduces two new approaches for identifying sensory circuit models consisting of linear and nonlinear filters in series with spiking neuron models, based only on the sampled analog input to the filter and the recorded spike train output of the spiking neuron. For an ideal integrate-and-fire neuron model, the first algorithm can identify the spiking neuron parameters as well as the structure and parameters of an arbitrary nonlinear filter connected to it. The second algorithm can identify the parameters of the more general leaky integrate-and-fire spiking neuron model, as well as the parameters of an arbitrary linear filter connected to it. Numerical studies involving simulated and real experimental recordings are used to demonstrate the applicability and evaluate the performance of the proposed algorithms."}], "ArticleTitle": "Identification of Linear and Nonlinear Sensory Processing Circuits from Spiking Neuron Data."}, "29894655": {"mesh": [], "AbstractText": [{"section": null, "text": "In this letter, we study the confounder detection problem in the linear model, where the target variable [Formula: see text] is predicted using its [Formula: see text] potential causes [Formula: see text]. Based on an assumption of a rotation-invariant generating process of the model, recent study shows that the spectral measure induced by the regression coefficient vector with respect to the covariance matrix of [Formula: see text] is close to a uniform measure in purely causal cases, but it differs from a uniform measure characteristically in the presence of a scalar confounder. Analyzing spectral measure patterns could help to detect confounding. In this letter, we propose to use the first moment of the spectral measure for confounder detection. We calculate the first moment of the regression vector-induced spectral measure and compare it with the first moment of a uniform spectral measure, both defined with respect to the covariance matrix of [Formula: see text]. The two moments coincide in nonconfounding cases and differ from each other in the presence of confounding. This statistical causal-confounding asymmetry can be used for confounder detection. Without the need to analyze the spectral measure pattern, our method avoids the difficulty of metric choice and multiple parameter optimization. Experiments on synthetic and real data show the performance of this method."}], "ArticleTitle": "Confounder Detection in High-Dimensional Linear Models Using First Moments of Spectral Measures."}, "28957022": {"mesh": ["Action Potentials", "Animals", "Biophysical Phenomena", "Cell Communication", "Larva", "Markov Chains", "Models, Neurological", "Photic Stimulation", "Retina", "Retinal Ganglion Cells", "Urodela"], "AbstractText": [{"section": null, "text": "An appealing new principle for neural population codes is that correlations among neurons organize neural activity patterns into a discrete set of clusters, which can each be viewed as a noise-robust population codeword. Previous studies assumed that these codewords corresponded geometrically with local peaks in the probability landscape of neural population responses. Here, we analyze multiple data sets of the responses of approximately 150 retinal ganglion cells and show that local probability peaks are absent under broad, nonrepeated stimulus ensembles, which are characteristic of natural behavior. However, we find that neural activity still forms noise-robust clusters in this regime, albeit clusters with a different geometry. We start by defining a soft local maximum, which is a local probability maximum when constrained to a fixed spike count. Next, we show that soft local maxima are robustly present and can, moreover, be linked across different spike count levels in the probability landscape to form a ridge. We found that these ridges comprise combinations of spiking and silence in the neural population such that all of the spiking neurons are members of the same neuronal community, a notion from network theory. We argue that a neuronal community shares many of the properties of Donald Hebb's classic cell assembly and show that a simple, biologically plausible decoding algorithm can recognize the presence of a specific neuronal community."}], "ArticleTitle": "Noise-Robust Modes of the Retinal Population Code Have the Geometry of \"Ridges\" and Correspond to Neuronal Communities."}, "28777726": {"mesh": [], "AbstractText": [{"section": null, "text": "Policy search is a class of reinforcement learning algorithms for finding optimal policies in control problems with limited feedback. These methods have been shown to be successful in high-dimensional problems such as robotics control. Though successful, current methods can lead to unsafe policy parameters that potentially could damage hardware units. Motivated by such constraints, we propose projection-based methods for safe policies. These methods, however, can handle only convex policy constraints. In this letter, we propose the first safe policy search reinforcement learner capable of operating under nonconvex policy constraints. This is achieved by observing, for the first time, a connection between nonconvex variational inequalities and policy search problems. We provide two algorithms, Mann and two-step iteration, to solve the above problems and prove convergence in the nonconvex stochastic setting. Finally, we demonstrate the performance of the algorithms on six benchmark dynamical systems and show that our new method is capable of outperforming previous methods under a variety of settings."}], "ArticleTitle": "Nonconvex Policy Search Using Variational Inequalities."}, "33080160": {"mesh": ["Animals", "Brain", "Cognition", "Humans", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "We develop theoretical foundations of resonator networks, a new type of recurrent neural network introduced in Frady, Kent, Olshausen, and Sommer (2020), a companion article in this issue, to solve a high-dimensional vector factorization problem arising in Vector Symbolic Architectures. Given a composite vector formed by the Hadamard product between a discrete set of high-dimensional vectors, a resonator network can efficiently decompose the composite into these factors. We compare the performance of resonator networks against optimization-based methods, including Alternating Least Squares and several gradient-based algorithms, showing that resonator networks are superior in several important ways. This advantage is achieved by leveraging a combination of nonlinear dynamics and searching in superposition, by which estimates of the correct solution are formed from a weighted superposition of all possible solutions. While the alternative methods also search in superposition, the dynamics of resonator networks allow them to strike a more effective balance between exploring the solution space and exploiting local information to drive the network toward probable solutions. Resonator networks are not guaranteed to converge, but within a particular regime they almost always do. In exchange for relaxing the guarantee of global convergence, resonator networks are dramatically more effective at finding factorizations than all alternative approaches considered."}], "ArticleTitle": "Resonator Networks, 2: Factorization Performance and Capacity Compared to Optimization-Based Methods."}, "28095195": {"mesh": [], "AbstractText": [{"section": null, "text": "Recent work in computer science has shown the power of deep learning driven by the backpropagation algorithm in networks of artificial neurons. But real neurons in the brain are different from most of these artificial ones in at least three crucial ways: they emit spikes rather than graded outputs, their inputs and outputs are related dynamically rather than by piecewise-smooth functions, and they have no known way to coordinate arrays of synapses in separate forward and feedback pathways so that they change simultaneously and identically, as they do in backpropagation. Given these differences, it is unlikely that current deep learning algorithms can operate in the brain, but we that show these problems can be solved by two simple devices: learning rules can approximate dynamic input-output relations with piecewise-smooth functions, and a variation on the feedback alignment algorithm can train deep networks without having to coordinate forward and feedback synapses. Our results also show that deep spiking networks learn much better if each neuron computes an intracellular teaching signal that reflects that cell's nonlinearity. With this mechanism, networks of spiking neurons show useful learning in synapses at least nine layers upstream from the output cells and perform well compared to other spiking networks in the literature on the MNIST digit recognition task."}], "ArticleTitle": "Deep Learning with Dynamic Spiking Neurons and Fixed Feedback Weights."}, "27764589": {"mesh": [], "AbstractText": [{"section": null, "text": "The dentate gyrus forms a critical link between the entorhinal cortex and CA3 by providing a sparse version of the signal. Concurrent with this increase in sparsity, a widely accepted theory suggests the dentate gyrus performs pattern separation-similar inputs yield decorrelated outputs. Although an active region of study and theory, few logically rigorous arguments detail the dentate gyrus's (DG) coding. We suggest a theoretically tractable, combinatorial model for this action. The model provides formal methods for a highly redundant, arbitrarily sparse, and decorrelated output signal.To explore the value of this model framework, we assess how suitable it is for two notable aspects of DG coding: how it can handle the highly structured grid cell representation in the input entorhinal cortex region and the presence of adult neurogenesis, which has been proposed to produce a heterogeneous code in the DG. We find tailoring the model to grid cell input yields expansion parameters consistent with the literature. In addition, the heterogeneous coding reflects activity gradation observed experimentally. Finally, we connect this approach with more conventional binary threshold neural circuit models via a formal embedding."}], "ArticleTitle": "A Combinatorial Model for Dentate Gyrus Sparse Coding."}, "27870610": {"mesh": [], "AbstractText": [{"section": null, "text": "Much experimental evidence suggests that during decision making, neural circuits accumulate evidence supporting alternative options. A computational model well describing this accumulation for choices between two options assumes that the brain integrates the log ratios of the likelihoods of the sensory inputs given the two options. Several models have been proposed for how neural circuits can learn these log-likelihood ratios from experience, but all of these models introduced novel and specially dedicated synaptic plasticity rules. Here we show that for a certain wide class of tasks, the log-likelihood ratios are approximately linearly proportional to the expected rewards for selecting actions. Therefore, a simple model based on standard reinforcement learning rules is able to estimate the log-likelihood ratios from experience and on each trial accumulate the log-likelihood ratios associated with presented stimuli while selecting an action. The simulations of the model replicate experimental data on both behavior and neural activity in tasks requiring accumulation of probabilistic cues. Our results suggest that there is no need for the brain to support dedicated plasticity rules, as the standard mechanisms proposed to describe reinforcement learning can enable the neural circuits to perform efficient probabilistic inference."}], "ArticleTitle": "Neural Circuits Trained with Standard Reinforcement Learning Can Accumulate Probabilistic Information during Decision Making."}, "27348304": {"mesh": ["Action Potentials", "Bayes Theorem", "Humans", "Learning", "Models, Neurological", "Nerve Net", "Neuronal Plasticity", "Neurons"], "AbstractText": [{"section": null, "text": "Motivated by the growing evidence for Bayesian computation in the brain, we show how a two-layer recurrent network of Poisson neurons can perform both approximate Bayesian inference and learning for any hidden Markov model. The lower-layer sensory neurons receive noisy measurements of hidden world states. The higher-layer neurons infer a posterior distribution over world states via Bayesian inference from inputs generated by sensory neurons. We demonstrate how such a neuronal network with synaptic plasticity can implement a form of Bayesian inference similar to Monte Carlo methods such as particle filtering. Each spike in a higher-layer neuron represents a sample of a particular hidden world state. The spiking activity across the neural population approximates the posterior distribution over hidden states. In this model, variability in spiking is regarded not as a nuisance but as an integral feature that provides the variability necessary for sampling during inference. We demonstrate how the network can learn the likelihood model, as well as the transition probabilities underlying the dynamics, using a Hebbian learning rule. We present results illustrating the ability of the network to perform inference and learning for arbitrary hidden Markov models."}], "ArticleTitle": "Bayesian Inference and Online Learning in Poisson Neuronal Networks."}, "29949459": {"mesh": [], "AbstractText": [{"section": null, "text": "Computer vision algorithms are often limited in their application by the large amount of data that must be processed. Mammalian vision systems mitigate this high bandwidth requirement by prioritizing certain regions of the visual field with neural circuits that select the most salient regions. This work introduces a novel and computationally efficient visual saliency algorithm for performing this neuromorphic attention-based data reduction. The proposed algorithm has the added advantage that it is compatible with an analog CMOS design while still achieving comparable performance to existing state-of-the-art saliency algorithms. This compatibility allows for direct integration with the analog-to-digital conversion circuitry present in CMOS image sensors. This integration leads to power savings in the converter by quantizing only the salient pixels. Further system-level power savings are gained by reducing the amount of data that must be transmitted and processed in the digital domain. The analog CMOS compatible formulation relies on a pulse width (i.e., time mode) encoding of the pixel data that is compatible with pulse-mode imagers and slope based converters often used in imager designs. This letter begins by discussing this time-mode encoding for implementing neuromorphic architectures. Next, the proposed algorithm is derived. Hardware-oriented optimizations and modifications to this algorithm are proposed and discussed. Next, a metric for quantifying saliency accuracy is proposed, and simulation results of this metric are presented. Finally, an analog synthesis approach for a time-mode architecture is outlined, and postsynthesis transistor-level simulations that demonstrate functionality of an implementation in a modern CMOS process are discussed."}], "ArticleTitle": "A Computationally Efficient Visual Saliency Algorithm Suitable for an Analog CMOS Implementation."}, "29162002": {"mesh": ["Animals", "Behavior", "Brain", "Models, Neurological", "Neural Inhibition", "Neural Pathways", "Neurons", "Synapses", "Time Factors"], "AbstractText": [{"section": null, "text": "Brain activity evolves through time, creating trajectories of activity that underlie sensorimotor processing, behavior, and learning and memory. Therefore, understanding the temporal nature of neural dynamics is essential to understanding brain function and behavior. In vivo studies have demonstrated that sequential transient activation of neurons can encode time. However, it remains unclear whether these patterns emerge from feedforward network architectures or from recurrent networks and, furthermore, what role network structure plays in timing. We address these issues using a recurrent neural network (RNN) model with distinct populations of excitatory and inhibitory units. Consistent with experimental data, a single RNN could autonomously produce multiple functionally feedforward trajectories, thus potentially encoding multiple timed motor patterns lasting up to several seconds. Importantly, the model accounted for Weber's law, a hallmark of timing behavior. Analysis of network connectivity revealed that efficiency-a measure of network interconnectedness-decreased as the number of stored trajectories increased. Additionally, the balance of excitation (E) and inhibition (I) shifted toward excitation during each unit's activation time, generating the prediction that observed sequential activity relies on dynamic control of the E/I balance. Our results establish for the first time that the same RNN can generate multiple functionally feedforward patterns of activity as a result of dynamic shifts in the E/I balance imposed by the connectome of the RNN. We conclude that recurrent network architectures account for sequential neural activity, as well as for a fundamental signature of timing behavior: Weber's law."}], "ArticleTitle": "Encoding Time in Feedforward Trajectories of a Recurrent Neural Network Model."}, "28562222": {"mesh": [], "AbstractText": [{"section": null, "text": "Since combining features from heterogeneous data sources can significantly boost classification performance in many applications, it has attracted much research attention over the past few years. Most of the existing multiview feature analysis approaches separately learn features in each view, ignoring knowledge shared by multiple views. Different views of features may have some intrinsic correlations that might be beneficial to feature learning. Therefore, it is assumed that multiviews share subspaces from which common knowledge can be discovered. In this letter, we propose a new multiview feature learning algorithm, aiming to exploit common features shared by different views. To achieve this goal, we propose a feature learning algorithm in a batch mode, by which the correlations among different views are taken into account. Multiple transformation matrices for different views are simultaneously learned in a joint framework. In this way, our algorithm can exploit potential correlations among views as supplementary information that further improves the performance result. Since the proposed objective function is nonsmooth and difficult to solve directly, we propose an iterative algorithm for effective optimization. Extensive experiments have been conducted on a number of real-world data sets. Experimental results demonstrate superior performance in terms of classification against all the compared approaches. Also, the convergence guarantee has been validated in the experiment."}], "ArticleTitle": "Multiview Feature Analysis via Structured Sparsity and Shared Subspace Discovery."}, "33253031": {"mesh": ["Algorithms", "Neural Networks, Computer", "Stochastic Processes", "Time Factors"], "AbstractText": [{"section": null, "text": "We discuss stability analysis for uncertain stochastic neural networks (SNNs) with time delay in this letter. By constructing a suitable Lyapunov-Krasovskii functional (LKF) and utilizing Wirtinger inequalities for estimating the integral inequalities, the delay-dependent stochastic stability conditions are derived in terms of linear matrix inequalities (LMIs). We discuss the parameter uncertainties in terms of norm-bounded conditions in the given interval with constant delay. The derived conditions ensure that the global, asymptotic stability of the states for the proposed SNNs. We verify the effectiveness and applicability of the proposed criteria with numerical examples."}], "ArticleTitle": "Robust Stability Analysis of Delayed Stochastic Neural Networks via Wirtinger-Based Integral Inequality."}, "30148703": {"mesh": ["Cerebral Cortex", "Computer Simulation", "Humans", "Microcomputers", "Neural Networks, Computer", "Software"], "AbstractText": [{"section": null, "text": "Simulation of the cerebral cortex requires a combination of extensive domain-specific knowledge and efficient software. However, when the complexity of the biological system is combined with that of the software, the likelihood of coding errors increases, which slows model adjustments. Moreover, few life scientists are familiar with software engineering and would benefit from simplicity in form of a high-level abstraction of the biological model. Our primary aim was to build a scalable cortical simulation framework for personal computers. We isolated an adjustable part of the domain-specific knowledge from the software. Next, we designed a framework that reads the model parameters from comma-separated value files and creates the necessary code for Brian2 model simulation. This separation allows rapid exploration of complex cortical circuits while decreasing the likelihood of coding errors and automatically using efficient hardware devices. Next, we tested the system on a simplified version of the neocortical microcircuit proposed by Markram and colleagues ( 2015 ). Our results indicate that the framework can efficiently perform simulations using Python, C <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:math> , and GPU devices. The most efficient device varied with computer hardware and the duration and scale of the simulated system. The speed of Brian2 was retained despite an overlying layer of software. However, the Python and C <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:math> devices inherited the single core limitation of Brian2. The CxSystem framework supports exploration of complex models on personal computers and thus has the potential to facilitate research on cortical networks and systems."}], "ArticleTitle": "Controlling Complexity of Cerebral Cortex Simulations-I: CxSystem, a Flexible Cortical Simulation Framework."}, "32433900": {"mesh": ["Animals", "Computer Simulation", "Humans", "Mathematical Concepts", "Memory", "Neural Networks, Computer", "Neuronal Plasticity"], "AbstractText": [{"section": null, "text": "We study the learning of an external signal by a neural network and the time to forget it when this network is submitted to noise. The presentation of an external stimulus to the recurrent network of binary neurons may change the state of the synapses. Multiple presentations of a unique signal lead to its learning. Then, during the forgetting time, the presentation of other signals (noise) may also modify the synaptic weights. We construct an estimator of the initial signal using the synaptic currents and in this way define a probability of error. In our model, these synaptic currents evolve as Markov chains. We study the dynamics of these Markov chains and obtain a lower bound on the number of external stimuli that the network can receive before the initial signal is considered forgotten (probability of error above a given threshold). Our results are based on a finite-time analysis rather than large-time asymptotic. We finally present numerical illustrations of our results."}], "ArticleTitle": "A Mathematical Analysis of Memory Lifetime in a Simple Network Model of Memory."}, "28777721": {"mesh": ["Bayes Theorem", "Brain", "Brain Mapping", "Cluster Analysis", "Computer Simulation", "Humans", "Magnetic Resonance Imaging", "Markov Chains", "Models, Theoretical", "Monte Carlo Method", "Rest"], "AbstractText": [{"section": null, "text": "Cluster analysis of functional magnetic resonance imaging (fMRI) data is often performed using gaussian mixture models, but when the time series are standardized such that the data reside on a hypersphere, this modeling assumption is questionable. The consequences of ignoring the underlying spherical manifold are rarely analyzed, in part due to the computational challenges imposed by directional statistics. In this letter, we discuss a Bayesian von Mises-Fisher (vMF) mixture model for data on the unit hypersphere and present an efficient inference procedure based on collapsed Markov chain Monte Carlo sampling. Comparing the vMF and gaussian mixture models on synthetic data, we demonstrate that the vMF model has a slight advantage inferring the true underlying clustering when compared to gaussian-based models on data generated from both a mixture of vMFs and a mixture of gaussians subsequently normalized. Thus, when performing model selection, the two models are not in agreement. Analyzing multisubject whole brain resting-state fMRI data from healthy adult subjects, we find that the vMF mixture model is considerably more reliable than the gaussian mixture model when comparing solutions across models trained on different groups of subjects, and again we find that the two models disagree on the optimal number of components. The analysis indicates that the fMRI data support more than a thousand clusters, and we confirm this is not a result of overfitting by demonstrating better prediction on data from held-out subjects. Our results highlight the utility of using directional statistics to model standardized fMRI data and demonstrate that whole brain segmentation of fMRI data requires a very large number of functional units in order to adequately account for the discernible statistical patterns in the data."}], "ArticleTitle": "Infinite von Mises-Fisher Mixture Modeling of Whole Brain fMRI Data."}, "29894651": {"mesh": ["Algorithms", "Animals", "Biological Evolution", "Gene Regulatory Networks", "Humans", "Information Services", "Models, Biological", "Nerve Net", "Neural Networks, Computer", "Protein Interaction Maps", "Social Networking"], "AbstractText": [{"section": null, "text": "Biological networks have long been known to be modular, containing sets of nodes that are highly connected internally. Less emphasis, however, has been placed on understanding how intermodule connections are distributed within a network. Here, we borrow ideas from engineered circuit design and study Rentian scaling, which states that the number of external connections between nodes in different modules is related to the number of nodes inside the modules by a power-law relationship. We tested this property in a broad class of molecular networks, including protein interaction networks for six species and gene regulatory networks for 41 human and 25 mouse cell types. Using evolutionarily defined modules corresponding to known biological processes in the cell, we found that all networks displayed Rentian scaling with a broad range of exponents. We also found evidence for Rentian scaling in functional modules in the Caenorhabditis elegans neural network, but, interestingly, not in three different social networks, suggesting that this property does not inevitably emerge. To understand how such scaling may have arisen evolutionarily, we derived a new graph model that can generate Rentian networks given a target Rent exponent and a module decomposition as inputs. Overall, our work uncovers a new principle shared by engineered circuits and biological networks."}], "ArticleTitle": "Evidence of Rentian Scaling of Functional Modules in Diverse Biological Networks."}, "29949462": {"mesh": ["Acoustic Stimulation", "Adult", "Animals", "Attention", "Brain Mapping", "Brain Waves", "Choice Behavior", "Cues", "Electroencephalography", "Female", "Humans", "Male", "Perception", "Photic Stimulation", "Principal Component Analysis", "Psychomotor Performance", "Reaction Time", "Signal Detection, Psychological", "Statistics, Nonparametric", "Young Adult"], "AbstractText": [{"section": null, "text": "This letter makes scientific and methodological contributions. Scientifically, it demonstrates a new and behaviorally relevant effect of temporal expectation on the phase coherence of the electroencephalogram (EEG). Methodologically, it introduces novel methods to characterize EEG recordings at the single-trial level. Expecting events in time can lead to more efficient behavior. A remarkable finding in the study of temporal expectation is the foreperiod effect on reaction time, that is, the influence on reaction time of the delay between a warning signal and a succeeding imperative stimulus to which subjects are instructed to respond as quickly as possible. Here we study a new foreperiod effect in an audiovisual attention-shifting oddball task in which attention-shift cues directed the attention of subjects to impendent deviant stimuli of a given modality and therefore acted as warning signals for these deviants. Standard stimuli, to which subjects did not respond, were interspersed between warning signals and deviants. We hypothesized that foreperiod durations modulated intertrial phase coherence (ITPC, the degree of phase alignment across multiple trials) evoked by behaviorally irrelevant standards and that these modulations are behaviorally meaningful. Using averaged data, we first observed that ITPC evoked by standards closer to the warning signal was significantly different from that evoked by standards further away from it, establishing a new foreperiod effect on ITPC evoked by standards. We call this effect the standard foreperiod (SFP) effect on ITPC. We reasoned that if the SFP influences ITPC evoked by standards, it should be possible to decode the former from the latter on a trial-by-trial basis. We were able to do so showing that this effect can be observed in single trials. We demonstrated the behavioral relevance of the SFP effect on ITPC by showing significant correlations between its strength and subjects' behavioral performance."}], "ArticleTitle": "A New Foreperiod Effect on Intertrial Phase Coherence. Part I: Existence and Behavioral Relevance."}, "30979351": {"mesh": ["Computer Simulation", "Humans", "Models, Biological", "Neural Networks, Computer", "Pattern Recognition, Automated", "Photic Stimulation"], "AbstractText": [{"section": null, "text": "Although deep neural networks (DNNs) have led to many remarkable results in cognitive tasks, they are still far from catching up with human-level cognition in antinoise capability. New research indicates how brittle and susceptible current models are to small variations in data distribution. In this letter, we study the stochasticity-resistance character of biological neurons by simulating the input-output response process of a leaky integrate-and-fire (LIF) neuron model and proposed a novel activation function, rand softplus (RSP), to model the response process. In RSP, a scale factor <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>&#951;</mml:mi></mml:math> is employed to mimic the stochasticity-adaptability of biological neurons, thereby enabling the antinoise capability of a DNN to be improved by the novel activation function. We validated the performance of RSP with a 19-layer residual network (ResNet) and a 19-layer visual geometry group (VGG) on facial expression recognition data sets and compared it with other popular activation functions, such as rectified linear units (ReLU), softplus, leaky ReLU (LReLU), exponential linear unit (ELU), and noisy softplus (NSP). The experimental results show that RSP is applied to VGG-19 or ResNet-19, and the average recognition accuracy under five different noise levels exceeds the other functions on both of the two facial expression data sets; in other words, RSP outperforms the other activation functions in noise resistance. Compared with the application in ResNet-19, the application of RSP in VGG-19 can improve a network's antinoise performance to a greater extent. In addition, RSP is easier to train compared to NSP because it has only one parameter to be calculated automatically according to the input data. Therefore, this work provides the deep learning community with a novel activation function that can better deal with overfitting problems."}], "ArticleTitle": "Improving the Antinoise Ability of DNNs via a Bio-Inspired Noise Adaptive Activation Function Rand Softplus."}, "33400901": {"mesh": ["Animals", "Bayes Theorem", "Linear Models", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "A central theme in computational neuroscience is determining the neural correlates of efficient and accurate coding of sensory signals. Diversity, or heterogeneity, of intrinsic neural attributes is known to exist in many brain areas and is thought to significantly affect neural coding. Recent theoretical and experimental work has argued that in uncoupled networks, coding is most accurate at intermediate levels of heterogeneity. Here we consider this question with data from in vivo recordings of neurons in the electrosensory system of weakly electric fish subject to the same realization of noisy stimuli; we use a generalized linear model (GLM) to assess the accuracy of (Bayesian) decoding of stimulus given a population spiking response. The long recordings enable us to consider many uncoupled networks and a relatively wide range of heterogeneity, as well as many instances of the stimuli, thus enabling us to address this question with statistical power. The GLM decoding is performed on a single long time series of data to mimic realistic conditions rather than using trial-averaged data for better model fits. For a variety of fixed network sizes, we generally find that the optimal levels of heterogeneity are at intermediate values, and this holds in all core components of GLM. These results are robust to several measures of decoding performance, including the absolute value of the error, error weighted by the uncertainty of the estimated stimulus, and the correlation between the actual and estimated stimulus. Although a quadratic fit to decoding performance as a function of heterogeneity is statistically significant, the result is highly variable with low R2 values. Taken together, intermediate levels of neural heterogeneity are indeed a prominent attribute for efficient coding even within a single time series, but the performance is highly variable."}], "ArticleTitle": "Statistical Analysis of Decoding Performances of Diverse Populations of Neurons."}, "27391680": {"mesh": [], "AbstractText": [{"section": null, "text": "The free-energy principle is a candidate unified theory for learning and memory in the brain that predicts that neurons, synapses, and neuromodulators work in a manner that minimizes free energy. However, electrophysiological data elucidating the neural and synaptic bases for this theory are lacking. Here, we propose a novel theory bridging the information-theoretical principle with the biological phenomenon of spike-timing dependent plasticity (STDP) regulated by neuromodulators, which we term mSTDP. We propose that by integrating an mSTDP equation, we can obtain a form of Friston's free energy (an information-theoretical function). Then we analytically and numerically show that dopamine (DA) and noradrenaline (NA) influence the accuracy of a principal component analysis (PCA) performed using the mSTDP algorithm. From the perspective of free-energy minimization, these neuromodulatory changes alter the relative weighting or precision of accuracy and prior terms, which induces a switch from pattern completion to separation. These results are consistent with electrophysiological findings and validate the free-energy principle and mSTDP. Moreover, our scheme can potentially be applied in computational psychiatry to build models of the faulty neural networks that underlie the positive symptoms of schizophrenia, which involve abnormal DA levels, as well as models of the NA contribution to memory triage and posttraumatic stress disorder."}], "ArticleTitle": "Linking Neuromodulated Spike-Timing Dependent Plasticity with the Free-Energy Principle."}, "29652588": {"mesh": ["Brain", "Brain Mapping", "Diffusion Tensor Imaging", "Electroencephalography", "Humans", "Magnetic Resonance Imaging", "Magnetoencephalography", "Mental Processes", "Models, Biological", "Multimodal Imaging", "Neural Pathways", "Rest", "Time Factors"], "AbstractText": [{"section": null, "text": "In this letter, we present a new method for integration of sensor-based multifrequency bands of electroencephalography and magnetoencephalography data sets into a voxel-based structural-temporal magnetic resonance imaging analysis by utilizing the general joint estimation using entropy regularization (JESTER) framework. This allows enhancement of the spatial-temporal localization of brain function and the ability to relate it to morphological features and structural connectivity. This method has broad implications for both basic neuroscience research and clinical neuroscience focused on identifying disease-relevant biomarkers by enhancing the spatial-temporal resolution of the estimates derived from current neuroimaging modalities, thereby providing a better picture of the normal human brain in basic neuroimaging experiments and variations associated with disease states."}], "ArticleTitle": "Joint Estimation of Effective Brain Wave Activation Modes Using EEG/MEG Sensor Arrays and Multimodal MRI Volumes."}, "29220307": {"mesh": [], "AbstractText": [{"section": null, "text": "In this letter, we have implemented and compared two neural coding algorithms in the networks of spiking neurons: Winner-takes-all (WTA) and winners-share-all (WSA). Winners-Share-All exploits the code space provided by the temporal code by training a different combination of [Formula: see text] out of [Formula: see text] neurons to fire together in response to different patterns, while WTA uses a one-hot-coding to respond to distinguished patterns. Using WSA, the maximum value of [Formula: see text] in order to maximize information capacity using [Formula: see text] output neurons was theoretically determined and utilized. A small proof-of-concept classification problem was applied to a spiking neural network using both algorithms to classify 14 letters of English alphabet with an image size of 15 [Formula: see text] 15 pixels. For both schemes, a modified spike-timing-dependent-plasticity (STDP) learning rule has been used to train the spiking neurons in an unsupervised fashion. The performance and the number of neurons required to perform this computation are compared between the two algorithms. We show that by tolerating a small drop in performance accuracy (84% in WSA versus 91% in WTA), we are able to reduce the number of output neurons by more than a factor of two. We show how the reduction in the number of neurons will increase as the number of patterns increases. The reduction in the number of output neurons would then proportionally reduce the number of training parameters, which requires less memory and hence speeds up the computation, and in the case of neuromorphic implementation on silicon, would take up much less area."}], "ArticleTitle": "From Winner-Takes-All to Winners-Share-All: Exploiting the Information Capacity in Temporal Codes."}, "31835004": {"mesh": ["Algorithms", "Computer User Training", "Data Analysis", "Humans", "Machine Learning", "Pattern Recognition, Automated"], "AbstractText": [{"section": null, "text": "Zero-shot learning (ZSL) aims to recognize unseen objects (test classes) given some other seen objects (training classes) by sharing information of attributes between different objects. Attributes are artificially annotated for objects and treated equally in recent ZSL tasks. However, some inferior attributes with poor predictability or poor discriminability may have negative impacts on the ZSL system performance. This letter first derives a generalization error bound for ZSL tasks. Our theoretical analysis verifies that selecting the subset of key attributes can improve the generalization performance of the original ZSL model, which uses all the attributes. Unfortunately, previous attribute selection methods have been conducted based on the seen data, and their selected attributes have poor generalization capability to the unseen data, which is unavailable in the training stage of ZSL tasks. Inspired by learning from pseudo-relevance feedback, this letter introduces out-of-the-box data-pseudo-data generated by an attribute-guided generative model-to mimic the unseen data. We then present an iterative attribute selection (IAS) strategy that iteratively selects key attributes based on the out-of-the-box data. Since the distribution of the generated out-of-the-box data is similar to that of the test data, the key attributes selected by IAS can be effectively generalized to test data. Extensive experiments demonstrate that IAS can significantly improve existing attribute-based ZSL methods and achieve state-of-the-art performance."}], "ArticleTitle": "Improving Generalization via Attribute Selection on Out-of-the-Box Data."}, "32069176": {"mesh": [], "AbstractText": [{"section": null, "text": "A hierarchical neural network usually has many singular regions in the parameter space due to the degeneration of hidden units. Here, we focus on a three-layer perceptron, which has one-dimensional singular regions comprising both attractive and repulsive parts. Such a singular region is often called a Milnor-like attractor. It is empirically known that in the vicinity of a Milnor-like attractor, several parameters converge much faster than the rest and that the dynamics can be reduced to smaller-dimensional ones. Here we give a rigorous proof for this phenomenon based on a center manifold theory. As an application, we analyze the reduced dynamics near the Milnor-like attractor and study the stochastic effects of the online learning."}], "ArticleTitle": "Center Manifold Analysis of Plateau Phenomena Caused by Degeneration of Three-Layer Perceptron."}, "27140943": {"mesh": [], "AbstractText": [{"section": null, "text": "Estimating the derivatives of probability density functions is an essential step in statistical data analysis. A naive approach to estimate the derivatives is to first perform density estimation and then compute its derivatives. However, this approach can be unreliable because a good density estimator does not necessarily mean a good density derivative estimator. To cope with this problem, in this letter, we propose a novel method that directly estimates density derivatives without going through density estimation. The proposed method provides computationally efficient estimation for the derivatives of any order on multidimensional data with a hyperparameter tuning method and achieves the optimal parametric convergence rate. We further discuss an extension of the proposed method by applying regularized multitask learning and a general framework for density derivative estimation based on Bregman divergences. Applications of the proposed method to nonparametric Kullback-Leibler divergence approximation and bandwidth matrix selection in kernel density estimation are also explored."}], "ArticleTitle": "Direct Density Derivative Estimation."}, "27870609": {"mesh": [], "AbstractText": [{"section": null, "text": "Many time series are naturally considered as a superposition of several oscillation components. For example, electroencephalogram (EEG) time series include oscillation components such as alpha, beta, and gamma. We propose a method for decomposing time series into such oscillation components using state-space models. Based on the concept of random frequency modulation, gaussian linear state-space models for oscillation components are developed. In this model, the frequency of an oscillator fluctuates by noise. Time series decomposition is accomplished by this model like the Bayesian seasonal adjustment method. Since the model parameters are estimated from data by the empirical Bayes' method, the amplitudes and the frequencies of oscillation components are determined in a data-driven manner. Also, the appropriate number of oscillation components is determined with the Akaike information criterion (AIC). In this way, the proposed method provides a natural decomposition of the given time series into oscillation components. In neuroscience, the phase of neural time series plays an important role in neural information processing. The proposed method can be used to estimate the phase of each oscillation component and has several advantages over a conventional method based on the Hilbert transform. Thus, the proposed method enables an investigation of the phase dynamics of time series. Numerical results show that the proposed method succeeds in extracting intermittent oscillations like ripples and detecting the phase reset phenomena. We apply the proposed method to real data from various fields such as astronomy, ecology, tidology, and neuroscience."}], "ArticleTitle": "Time Series Decomposition into Oscillation Components and Phase Estimation."}, "28030774": {"mesh": ["Animals", "Computer Simulation", "Models, Theoretical", "Pattern Recognition, Visual", "Stochastic Processes", "Visual Cortex", "Visual Pathways"], "AbstractText": [{"section": null, "text": "This letter presents a mathematical model of figure-ground articulation that takes into account both local and global gestalt laws and is compatible with the functional architecture of the primary visual cortex (V1). The local gestalt law of good continuation is described by means of suitable connectivity kernels that are derived from Lie group theory and quantitatively compared with long-range connectivity in V1. Global gestalt constraints are then introduced in terms of spectral analysis of a connectivity matrix derived from these kernels. This analysis performs grouping of local features and individuates perceptual units with the highest salience. Numerical simulations are performed, and results are obtained by applying the technique to a number of stimuli."}], "ArticleTitle": "Local and Global Gestalt Laws: A Neurally Based Spectral Approach."}, "30764744": {"mesh": [], "AbstractText": [{"section": null, "text": "In this letter, we propose a method to decrease the number of hidden units of the restricted Boltzmann machine while avoiding a decrease in the performance quantified by the Kullback-Leibler divergence. Our algorithm is then demonstrated by numerical simulations."}], "ArticleTitle": "Decreasing the Size of the Restricted Boltzmann Machine."}, "27171856": {"mesh": [], "AbstractText": [{"section": null, "text": "In visual modeling, invariance properties of visual cells are often explained by a pooling mechanism, in which outputs of neurons with similar selectivities to some stimulus parameters are integrated so as to gain some extent of invariance to other parameters. For example, the classical energy model of phase-invariant V1 complex cells pools model simple cells preferring similar orientation but different phases. Prior studies, such as independent subspace analysis, have shown that phase-invariance properties of V1 complex cells can be learned from spatial statistics of natural inputs. However, those previous approaches assumed a squaring nonlinearity on the neural outputs to capture energy correlation; such nonlinearity is arguably unnatural from a neurobiological viewpoint but hard to change due to its tight integration into their formalisms. Moreover, they used somewhat complicated objective functions requiring expensive computations for optimization. In this study, we show that visual spatial pooling can be learned in a much simpler way using strong dimension reduction based on principal component analysis. This approach learns to ignore a large part of detailed spatial structure of the input and thereby estimates a linear pooling matrix. Using this framework, we demonstrate that pooling of model V1 simple cells learned in this way, even with nonlinearities other than squaring, can reproduce standard tuning properties of V1 complex cells. For further understanding, we analyze several variants of the pooling model and argue that a reasonable pooling can generally be obtained from any kind of linear transformation that retains several of the first principal components and suppresses the remaining ones. In particular, we show how the classic Wiener filtering theory leads to one such variant."}], "ArticleTitle": "Learning Visual Spatial Pooling by Strong PCA Dimension Reduction."}, "29342398": {"mesh": [], "AbstractText": [{"section": null, "text": "Most existing multiview clustering methods require that graph matrices in different views are computed beforehand and that each graph is obtained independently. However, this requirement ignores the correlation between multiple views. In this letter, we tackle the problem of multiview clustering by jointly optimizing the graph matrix to make full use of the data correlation between views. With the interview correlation, a concept factorization-based multiview clustering method is developed for data integration, and the adaptive method correlates the affinity weights of all views. This method differs from nonnegative matrix factorization-based clustering methods in that it can be applicable to data sets containing negative values. Experiments are conducted to demonstrate the effectiveness of the proposed method in comparison with state-of-the-art approaches in terms of accuracy, normalized mutual information, and purity."}], "ArticleTitle": "Adaptive Structure Concept Factorization for Multiview Clustering."}, "30314421": {"mesh": ["Datasets as Topic", "Humans", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "We describe a simple method to transfer from weights in deep neural networks (NNs) trained by a deep belief network (DBN) to weights in a backpropagation NN (BPNN) in the recursive-rule eXtraction (Re-RX) algorithm with J48graft (Re-RX with J48graft) and propose a new method to extract accurate and interpretable classification rules for rating category data sets. We apply this method to the Wisconsin Breast Cancer Data Set (WBCD), the Mammographic Mass Data Set, and the Dermatology Dataset, which are small, high-abstraction data sets with prior knowledge. After training these three data sets, our proposed rule extraction method was able to extract accurate and concise rules for deep NNs trained by a DBN. These results suggest that our proposed method could help fill the gap between the very high learning capability of DBNs and the very high interpretability of rule extraction algorithms such as Re-RX with J48graft."}], "ArticleTitle": "Use of a Deep Belief Network for Small High-Level Abstraction Data Sets Using Artificial Intelligence with Rule Extraction."}, "33400898": {"mesh": ["Algorithms", "Animals", "Bayes Theorem", "Behavior", "Computer Simulation", "Humans", "Learning", "Reinforcement, Psychology"], "AbstractText": [{"section": null, "text": "Surprise-based learning allows agents to rapidly adapt to nonstationary stochastic environments characterized by sudden changes. We show that exact Bayesian inference in a hierarchical model gives rise to a surprise-modulated trade-off between forgetting old observations and integrating them with the new ones. The modulation depends on a probability ratio, which we call the Bayes Factor Surprise, that tests the prior belief against the current belief. We demonstrate that in several existing approximate algorithms, the Bayes Factor Surprise modulates the rate of adaptation to new observations. We derive three novel surprise-based algorithms, one in the family of particle filters, one in the family of variational learning, and one in the family of message passing, that have constant scaling in observation sequence length and particularly simple update dynamics for any distribution in the exponential family. Empirical results show that these surprise-based algorithms estimate parameters better than alternative approximate approaches and reach levels of performance comparable to computationally more expensive algorithms. The Bayes Factor Surprise is related to but different from the Shannon Surprise. In two hypothetical experiments, we make testable predictions for physiological indicators that dissociate the Bayes Factor Surprise from the Shannon Surprise. The theoretical insight of casting various approaches as surprise-based learning, as well as the proposed online algorithms, may be applied to the analysis of animal and human behavior and to reinforcement learning in nonstationary environments."}], "ArticleTitle": "Learning in Volatile Environments With the Bayes Factor Surprise."}, "29064784": {"mesh": ["Adaptation, Psychological", "Algorithms", "Decision Making", "Environment", "Female", "Humans", "Learning", "Male", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Surprise describes a range of phenomena from unexpected events to behavioral responses. We propose a novel measure of surprise and use it for surprise-driven learning. Our surprise measure takes into account data likelihood as well as the degree of commitment to a belief via the entropy of the belief distribution. We find that surprise-minimizing learning dynamically adjusts the balance between new and old information without the need of knowledge about the temporal statistics of the environment. We apply our framework to a dynamic decision-making task and a maze exploration task. Our surprise-minimizing framework is suitable for learning in complex environments, even if the environment undergoes gradual or sudden changes, and it could eventually provide a framework to study the behavior of humans and animals as they encounter surprising events."}], "ArticleTitle": "Balancing New against Old Information: The Role of Puzzlement Surprise in Learning."}, "27391685": {"mesh": ["Algorithms", "Computer Simulation", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Two complex Zhang neural network (ZNN) models for computing the Drazin inverse of arbitrary time-varying complex square matrix are presented. The design of these neural networks is based on corresponding matrix-valued error functions arising from the limit representations of the Drazin inverse. Two types of activation functions, appropriate for handling complex matrices, are exploited to develop each of these networks. Theoretical results of convergence analysis are presented to show the desirable properties of the proposed complex-valued ZNN models. Numerical results further demonstrate the effectiveness of the proposed models."}], "ArticleTitle": "Complex Neural Network Models for Time-Varying Drazin Inverse."}, "29220309": {"mesh": ["Action Potentials", "Animals", "Bayes Theorem", "Brain", "Feedback, Physiological", "Models, Neurological", "Neural Inhibition", "Neural Pathways", "Neurons", "Nonlinear Dynamics", "Periodicity", "Stochastic Processes", "Synapses"], "AbstractText": [{"section": null, "text": "In this study, I considered quantifying the strength of chaos in the population firing rate of a pulse-coupled neural network. In particular, I considered the dynamics where the population firing rate is chaotic and the firing of each neuron is stochastic. I calculated a time histogram of firings to show the variation in the population firing rate over time. To smooth this histogram, I used Bayesian adaptive regression splines and a gaussian filter. The nonlinear prediction method, based on reconstruction, was applied to a sequence of interpeak intervals in the smoothed time histogram of firings. I propose the use of the sum of nonlinearity as a quantifier of the strength of chaos. When applying this method to the firings of a pulse-coupled neural network, the sum of nonlinearity was seen to satisfy three properties for quantifying the strength of chaos. First, it can be calculated from spiking data alone. Second, it takes large values when applied to firings that are confirmed, theoretically or numerically, to be chaotic. Third, it reflects the strength of chaos of the original dynamics."}], "ArticleTitle": "Quantifying Strength of Chaos in the Population Firing Rate of Neurons."}, "29566351": {"mesh": [], "AbstractText": [{"section": null, "text": "Despite their biological plausibility, neural network models with asymmetric weights are rarely solved analytically, and closed-form solutions are available only in some limiting cases or in some mean-field approximations. We found exact analytical solutions of an asymmetric spin model of neural networks with arbitrary size without resorting to any approximation, and we comprehensively studied its dynamical and statistical properties. The network had discrete time evolution equations and binary firing rates, and it could be driven by noise with any distribution. We found analytical expressions of the conditional and stationary joint probability distributions of the membrane potentials and the firing rates. By manipulating the conditional probability distribution of the firing rates, we extend to stochastic networks the associating learning rule previously introduced by Personnaz and coworkers. The new learning rule allowed the safe storage, under the presence of noise, of point and cyclic attractors, with useful implications for content-addressable memories. Furthermore, we studied the bifurcation structure of the network dynamics in the zero-noise limit. We analytically derived examples of the codimension 1 and codimension 2 bifurcation diagrams of the network, which describe how the neuronal dynamics changes with the external stimuli. This showed that the network may undergo transitions among multistable regimes, oscillatory behavior elicited by asymmetric synaptic connections, and various forms of spontaneous symmetry breaking. We also calculated analytically groupwise correlations of neural activity in the network in the stationary regime. This revealed neuronal regimes where, statistically, the membrane potentials and the firing rates are either synchronous or asynchronous. Our results are valid for networks with any number of neurons, although our equations can be realistically solved only for small networks. For completeness, we also derived the network equations in the thermodynamic limit of infinite network size and we analytically studied their local bifurcations. All the analytical results were extensively validated by numerical simulations."}], "ArticleTitle": "Pattern Storage, Bifurcations, and Groupwise Correlation Structure of an Exactly Solvable Asymmetric Neural Network Model."}, "33253032": {"mesh": ["Action Potentials", "Algorithms", "Humans", "Learning", "Models, Neurological", "Neural Networks, Computer", "Neuronal Plasticity", "Neurons"], "AbstractText": [{"section": null, "text": "We propose a novel neural model with lateral interaction for learning tasks. The model consists of two functional fields: an elementary field to extract features and a high-level field to store and recognize patterns. Each field is composed of some neurons with lateral interaction, and the neurons in different fields are connected by the rules of synaptic plasticity. The model is established on the current research of cognition and neuroscience, making it more transparent and biologically explainable. Our proposed model is applied to data classification and clustering. The corresponding algorithms share similar processes without requiring any parameter tuning and optimization processes. Numerical experiments validate that the proposed model is feasible in different learning tasks and superior to some state-of-the-art methods, especially in small sample learning, one-shot learning, and clustering."}], "ArticleTitle": "A Novel Neural Model With Lateral Interaction for Learning Tasks."}, "27870613": {"mesh": [], "AbstractText": [{"section": null, "text": "Contour is a critical feature for image description and object recognition in many computer vision tasks. However, detection of object contour remains a challenging problem because of disturbances from texture edges. This letter proposes a scheme to handle texture edges by implementing contour integration. The proposed scheme integrates structural segments into contours while inhibiting texture edges with the help of the orientation histogram-based center-surround interaction model. In the model, local edges within surroundings exert a modulatory effect on central contour cues based on the co-occurrence statistics of local edges described by the divergence of orientation histograms in the local region. We evaluate the proposed scheme on two well-known challenging boundary detection data sets (RuG and BSDS500). The experiments demonstrate that our scheme achieves a high [Formula: see text]-measure of up to 0.74. Results show that our scheme achieves integrating accurate contour while eliminating most of texture edges, a novel approach to long-range feature analysis."}], "ArticleTitle": "Orientation Histogram-Based Center-Surround Interaction: An Integration Approach for Contour Detection."}, "27348738": {"mesh": [], "AbstractText": [{"section": null, "text": "Hermite polynomial-based functional link artificial neural network (FLANN) is proposed here to solve the Van der Pol-Duffing oscillator equation. A single-layer hermite neural network (HeNN) model is used, where a hidden layer is replaced by expansion block of input pattern using Hermite orthogonal polynomials. A feedforward neural network model with the unsupervised error backpropagation principle is used for modifying the network parameters and minimizing the computed error function. The Van der Pol-Duffing and Duffing oscillator equations may not be solved exactly. Here, approximate solutions of these types of equations have been obtained by applying the HeNN model for the first time. Three mathematical example problems and two real-life application problems of Van der Pol-Duffing oscillator equation, extracting the features of early mechanical failure signal and weak signal detection problems, are solved using the proposed HeNN method. HeNN approximate solutions have been compared with results obtained by the well known Runge-Kutta method. Computed results are depicted in term of graphs. After training the HeNN model, we may use it as a black box to get numerical results at any arbitrary point in the domain. Thus, the proposed HeNN method is efficient. The results reveal that this method is reliable and can be applied to other nonlinear problems too."}], "ArticleTitle": "Hermite Functional Link Neural Network for Solving the Van der Pol-Duffing Oscillator Equation."}, "28957024": {"mesh": [], "AbstractText": [{"section": null, "text": "This letter proposes a novel approach using the [Formula: see text]-norm regularization for the sparse covariance matrix estimation (SCME) problem. The objective function of SCME problem is composed of a nonconvex part and the [Formula: see text] term, which is discontinuous and difficult to tackle. Appropriate DC (difference of convex functions) approximations of [Formula: see text]-norm are used that result in approximation SCME problems that are still nonconvex. DC programming and DCA (DC algorithm), powerful tools in nonconvex programming framework, are investigated. Two DC formulations are proposed and corresponding DCA schemes developed. Two applications of the SCME problem that are considered are classification via sparse quadratic discriminant analysis and portfolio optimization. A careful empirical experiment is performed through simulated and real data sets to study the performance of the proposed algorithms. Numerical results showed their efficiency and their superiority compared with seven state-of-the-art methods."}], "ArticleTitle": "Sparse Covariance Matrix Estimation by DCA-Based Algorithms."}, "28410054": {"mesh": [], "AbstractText": [{"section": null, "text": "We propose a pulse neural network that exhibits chaotic pattern alternations among stored patterns as a model of multistable perception, which is reflected in phenomena such as binocular rivalry and perceptual ambiguity. When we regard the mixed state of patterns as a part of each pattern, the durations of the retrieved pattern obey unimodal distributions. We confirmed that no chaotic properties are observed in the time series of durations, consistent with the findings of previous psychological studies. Moreover, it is shown that our model also reproduces two properties of multistable perception that characterize the relationship between the contrast of inputs and the durations."}], "ArticleTitle": "Chaotic Pattern Alternations Can Reproduce Properties of Dominance Durations in Multistable Perception."}, "33626312": {"mesh": [], "AbstractText": [{"section": null, "text": "Active inference offers a first principle account of sentient behavior, from which special and important cases-for example, reinforcement learning, active learning, Bayes optimal inference, Bayes optimal design-can be derived. Active inference finesses the exploitation-exploration dilemma in relation to prior preferences by placing information gain on the same footing as reward or value. In brief, active inference replaces value functions with functionals of (Bayesian) beliefs, in the form of an expected (variational) free energy. In this letter, we consider a sophisticated kind of active inference using a recursive form of expected free energy. Sophistication describes the degree to which an agent has beliefs about beliefs. We consider agents with beliefs about the counterfactual consequences of action for states of affairs and beliefs about those latent states. In other words, we move from simply considering beliefs about \"what would happen if I did that\" to \"what I would believe about what would happen if I did that.\" The recursive form of the free energy functional effectively implements a deep tree search over actions and outcomes in the future. Crucially, this search is over sequences of belief states as opposed to states per se. We illustrate the competence of this scheme using numerical simulations of deep decision problems."}], "ArticleTitle": "Sophisticated Inference."}, "29220305": {"mesh": ["Action Potentials", "Animals", "Biomimetic Materials", "Computers", "Dendrites", "Electrical Equipment and Supplies", "Metals", "Models, Neurological", "Neural Networks, Computer", "Nonlinear Dynamics", "Oxides", "Pattern Recognition, Automated", "Semiconductors", "Synapses"], "AbstractText": [{"section": null, "text": "We present a neuromorphic current mode implementation of a spiking neural classifier with lumped square law dendritic nonlinearity. It has been shown previously in software simulations that such a system with binary synapses can be trained with structural plasticity algorithms to achieve comparable classification accuracy with fewer synaptic resources than conventional algorithms. We show that even in real analog systems with manufacturing imperfections (CV of 23.5% and 14.4% for dendritic branch gains and leaks respectively), this network is able to produce comparable results with fewer synaptic resources. The chip fabricated in [Formula: see text]m complementary metal oxide semiconductor has eight dendrites per cell and uses two opposing cells per class to cancel common-mode inputs. The chip can operate down to a [Formula: see text] V and dissipates 19 nW of static power per neuronal cell and [Formula: see text] 125 pJ/spike. For two-class classification problems of high-dimensional rate encoded binary patterns, the hardware achieves comparable performance as software implementation of the same with only about a 0.5% reduction in accuracy. On two UCI data sets, the IC integrated circuit has classification accuracy comparable to standard machine learners like support vector machines and extreme learning machines while using two to five times binary synapses. We also show that the system can operate on mean rate encoded spike patterns, as well as short bursts of spikes. To the best of our knowledge, this is the first attempt in hardware to perform classification exploiting dendritic properties and binary synapses."}], "ArticleTitle": "Spiking Neural Classifier with Lumped Dendritic Nonlinearity and Binary Synapses: A Current Mode VLSI Implementation and Analysis."}, "30216139": {"mesh": ["Animals", "Brain", "Humans", "Memory", "Models, Neurological", "Nerve Net", "Neuronal Plasticity"], "AbstractText": [{"section": null, "text": "Experimental constraints have traditionally implied separate studies of different cortical functions, such as memory and sensory-motor control. Yet certain cortical modalities, while repeatedly observed and reported, have not been clearly identified with one cortical function or another. Specifically, while neuronal membrane and synapse polarities with respect to a certain potential value have been attracting considerable interest in recent years, the purposes of such polarities have largely remained a subject for speculation and debate. Formally identifying these polarities as on-off neuronal polarity gates, we analytically show that cortical circuit structure, behavior, and memory are all governed by the combined potent effect of these gates, which we collectively term circuit polarity. Employing widely accepted and biologically validated firing rate and plasticity paradigms, we show that circuit polarity is mathematically embedded in the corresponding models. Moreover, we show that the firing rate dynamics implied by these models are driven by ongoing circuit polarity gating dynamics. Furthermore, circuit polarity is shown to segregate cortical circuits into internally synchronous, externally asynchronous subcircuits, defining their firing rate modes in accordance with different cortical tasks. In contrast to the Hebbian paradigm, which is shown to be susceptible to mutual neuronal interference in the face of asynchrony, circuit polarity is shown to block such interference. Noting convergence of synaptic weights, we show that circuit polarity holds the key to cortical memory, having a segregated capacity linear in the number of neurons. While memory concealment is implied by complete neuronal silencing, memory is restored by reactivating the original circuit polarity. Finally, we show that incomplete deterioration or restoration of circuit polarity results in memory modification, which may be associated with partial or false recall, or novel innovation."}], "ArticleTitle": "Circuit Polarity Effect of Cortical Connectivity, Activity, and Memory."}, "28777719": {"mesh": ["Animals", "Calcium", "Cerebral Cortex", "Computer Simulation", "Membrane Potentials", "Models, Neurological", "Multivariate Analysis", "Neural Pathways", "Neurons", "Patch-Clamp Techniques", "Signal Processing, Computer-Assisted", "Synapses", "Thalamus", "Voltage-Sensitive Dye Imaging"], "AbstractText": [{"section": null, "text": "With our ability to record more neurons simultaneously, making sense of these data is a challenge. Functional connectivity is one popular way to study the relationship of multiple neural signals. Correlation-based methods are a set of currently well-used techniques for functional connectivity estimation. However, due to explaining away and unobserved common inputs (Stevenson, Rebesco, Miller, & K&#246;rding, 2008 ), they produce spurious connections. The general linear model (GLM), which models spike trains as Poisson processes (Okatan, Wilson, & Brown, 2005 ; Truccolo, Eden, Fellows, Donoghue, & Brown, 2005 ; Pillow et&#160;al., 2008 ), avoids these confounds. We develop here a new class of methods by using differential signals based on simulated intracellular voltage recordings. It is equivalent to a regularized AR(2) model. We also expand the method to simulated local field potential recordings and calcium imaging. In all of our simulated data, the differential covariance-based methods achieved performance better than or similar to the GLM method and required fewer data samples. This new class of methods provides alternative ways to analyze neural signals."}], "ArticleTitle": "Differential Covariance: A New Class of Methods to Estimate Sparse Connectivity from Neural Recordings."}, "28777730": {"mesh": [], "AbstractText": [{"section": null, "text": "The statistical dependencies that independent component analysis (ICA) cannot remove often provide rich information beyond the linear independent components. It would thus be very useful to estimate the dependency structure from data. While such models have been proposed, they have usually concentrated on higher-order correlations such as energy (square) correlations. Yet linear correlations are a fundamental and informative form of dependency in many real data sets. Linear correlations are usually completely removed by ICA and related methods so they can only be analyzed by developing new methods that explicitly allow for linearly correlated components. In this article, we propose a probabilistic model of linear nongaussian components that are allowed to have both linear and energy correlations. The precision matrix of the linear components is assumed to be randomly generated by a higher-order process and explicitly parameterized by a parameter matrix. The estimation of the parameter matrix is shown to be particularly simple because using score-matching (Hyv&#228;rinen, 2005 ), the objective function is a quadratic form. Using simulations with artificial data, we demonstrate that the proposed method improves the identifiability of nongaussian components by simultaneously learning their correlation structure. Applications on simulated complex cells with natural image input, as well as spectrograms of natural audio data, show that the method finds new kinds of dependencies between the components."}], "ArticleTitle": "Simultaneous Estimation of Nongaussian Components and Their Correlation Structure."}, "28095201": {"mesh": ["Acoustic Stimulation", "Afferent Pathways", "Animals", "Bayes Theorem", "Brain", "Computer Simulation", "Humans", "Learning", "Models, Neurological", "Neurons", "Photic Stimulation", "Synapses"], "AbstractText": [{"section": null, "text": "Recent theoretical and experimental studies suggest that in multisensory conditions, the brain performs a near-optimal Bayesian estimate of external events, giving more weight to the more reliable stimuli. However, the neural mechanisms responsible for this behavior, and its progressive maturation in a multisensory environment, are still insufficiently understood. The aim of this letter is to analyze this problem with a neural network model of audiovisual integration, based on probabilistic population coding-the idea that a population of neurons can encode probability functions to perform Bayesian inference. The model consists of two chains of unisensory neurons (auditory and visual) topologically organized. They receive the corresponding input through a plastic receptive field and reciprocally exchange plastic cross-modal synapses, which encode the spatial co-occurrence of visual-auditory inputs. A third chain of multisensory neurons performs a simple sum of auditory and visual excitations. The work includes a theoretical part and a computer simulation study. We show how a simple rule for synapse learning (consisting of Hebbian reinforcement and a decay term) can be used during training to shrink the receptive fields and encode the unisensory likelihood functions. Hence, after training, each unisensory area realizes a maximum likelihood estimate of stimulus position (auditory or visual). In cross-modal conditions, the same learning rule can encode information on prior probability into the cross-modal synapses. Computer simulations confirm the theoretical results and show that the proposed network can realize a maximum likelihood estimate of auditory (or visual) positions in unimodal conditions and a Bayesian estimate, with moderate deviations from optimality, in cross-modal conditions. Furthermore, the model explains the ventriloquism illusion and, looking at the activity in the multimodal neurons, explains the automatic reweighting of auditory and visual inputs on a trial-by-trial basis, according to the reliability of the individual cues."}], "ArticleTitle": "Multisensory Bayesian Inference Depends on Synapse Maturation during Training: Theoretical Analysis and Neural Modeling Implementation."}, "28410049": {"mesh": [], "AbstractText": [{"section": null, "text": "Distant supervision, a widely applied approach in the field of relation extraction can automatically generate large amounts of labeled training corpus with minimal manual effort. However, the labeled training corpus may have many false-positive data, which would hurt the performance of relation extraction. Moreover, in traditional feature-based distant supervised approaches, extraction models adopt human design features with natural language processing. It may also cause poor performance. To address these two shortcomings, we propose a customized attention-based long short-term memory network. Our approach adopts word-level attention to achieve better data representation for relation extraction without manually designed features to perform distant supervision instead of fully supervised relation extraction, and it utilizes instance-level attention to tackle the problem of false-positive data. Experimental results demonstrate that our proposed approach is effective and achieves better performance than traditional methods."}], "ArticleTitle": "A Customized Attention-Based Long Short-Term Memory Network for Distant Supervised Relation Extraction."}, "29652586": {"mesh": ["Action Potentials", "Animals", "Auditory Pathways", "Cochlear Nerve", "Cochlear Nucleus", "Computer Simulation", "Haplorhini", "Humans", "Inferior Colliculi", "Learning", "Models, Neurological", "Neural Networks, Computer", "Neuronal Plasticity", "Neurons", "Pattern Recognition, Physiological", "Rats", "Synapses", "Time Factors"], "AbstractText": [{"section": null, "text": "It is well known that auditory nerve (AN) fibers overcome bandwidth limitations through the volley principle, a form of multiplexing. What is less well known is that the volley principle introduces a degree of unpredictability into AN neural firing patterns that may be affecting even simple stimulus categorization learning. We use a physiologically grounded, unsupervised spiking neural network model of the auditory brain with spike time dependent plasticity learning to demonstrate that plastic auditory cortex is unable to learn even simple auditory object categories when exposed to the raw AN firing input without subcortical preprocessing. We then demonstrate the importance of nonplastic subcortical preprocessing within the cochlear nucleus and the inferior colliculus for stabilizing and denoising AN responses. Such preprocessing enables the plastic auditory cortex to learn efficient robust representations of the auditory object categories. The biological realism of our model makes it suitable for generating neurophysiologically testable hypotheses."}], "ArticleTitle": "A Computational Account of the Role of Cochlear Nucleus and Inferior Colliculus in Stabilizing Auditory Nerve Firing for Auditory Category Learning."}, "27348420": {"mesh": ["Animals", "Brain", "Brain Mapping", "Humans", "Neurons"], "AbstractText": [{"section": null, "text": "Large-scale data collection efforts to map the brain are underway at multiple spatial and temporal scales, but all face fundamental problems posed by high-dimensional data and intersubject variability. Even seemingly simple problems, such as identifying a neuron/brain region across animals/subjects, become exponentially more difficult in high dimensions, such as recognizing dozens of neurons/brain regions simultaneously. We present a framework and tools for functional neurocartography-the large-scale mapping of neural activity during behavioral states. Using a voltage-sensitive dye (VSD), we imaged the multifunctional responses of hundreds of leech neurons during several behaviors to identify and functionally map homologous neurons. We extracted simple features from each of these behaviors and combined them with anatomical features to create a rich medium-dimensional feature space. This enabled us to use machine learning techniques and visualizations to characterize and account for intersubject variability, piece together a canonical atlas of neural activity, and identify two behavioral networks. We identified 39 neurons (18 pairs, 3 unpaired) as part of a canonical swim network and 17 neurons (8 pairs, 1 unpaired) involved in a partially overlapping preparatory network. All neurons in the preparatory network rapidly depolarized at the onsets of each behavior, suggesting that it is part of a dedicated rapid-response network. This network is likely mediated by the S cell, and we referenced VSD recordings to an activity atlas to identify multiple cells of interest simultaneously in real time for further experiments. We targeted and electrophysiologically verified several neurons in the swim network and further showed that the S cell is presynaptic to multiple neurons in the preparatory network. This study illustrates the basic framework to map neural activity in high dimensions with large-scale recordings and how to extract the rich information necessary to perform analyses in light of intersubject variability."}], "ArticleTitle": "Scalable Semisupervised Functional Neurocartography Reveals Canonical Neurons in Behavioral Networks."}, "28562216": {"mesh": [], "AbstractText": [{"section": null, "text": "The area under the ROC curve (AUC) is a widely used performance measure in machine learning. Increasingly, however, in several applications, ranging from ranking to biometric screening to medicine, performance is measured not in terms of the full area under the ROC curve but in terms of the partial area under the ROC curve between two false-positive rates. In this letter, we develop support vector algorithms for directly optimizing the partial AUC between any two false-positive rates. Our methods are based on minimizing a suitable proxy or surrogate objective for the partial AUC error. In the case of the full AUC, one can readily construct and optimize convex surrogates by expressing the performance measure as a summation of pairwise terms. The partial AUC, on the other hand, does not admit such a simple decomposable structure, making it more challenging to design and optimize (tight) convex surrogates for this measure. Our approach builds on the structural SVM framework of Joachims ( 2005 ) to design convex surrogates for partial AUC and solves the resulting optimization problem using a cutting plane solver. Unlike the full AUC, where the combinatorial optimization needed in each iteration of the cutting plane solver can be decomposed and solved efficiently, the corresponding problem for the partial AUC is harder to decompose. One of our main contributions is a polynomial time algorithm for solving the combinatorial optimization problem associated with partial AUC. We also develop an approach for optimizing a tighter nonconvex hinge loss-based surrogate for the partial AUC using difference-of-convex programming. Our experiments on a variety of real-world and benchmark tasks confirm the efficacy of the proposed methods."}], "ArticleTitle": "Support Vector Algorithms for Optimizing the Partial Area under the ROC Curve."}, "29220306": {"mesh": ["Action Potentials", "Animals", "Biomimetics", "Brain", "Computer Simulation", "Models, Neurological", "Neural Networks, Computer", "Neurons", "Nonlinear Dynamics", "Synapses", "Time Factors"], "AbstractText": [{"section": null, "text": "Researchers building spiking neural networks face the challenge of improving the biological plausibility of their model networks while maintaining the ability to quantitatively characterize network behavior. In this work, we extend the theory behind the neural engineering framework (NEF), a method of building spiking dynamical networks, to permit the use of a broad class of synapse models while maintaining prescribed dynamics up to a given order. This theory improves our understanding of how low-level synaptic properties alter the accuracy of high-level computations in spiking dynamical networks. For completeness, we provide characterizations for both continuous-time (i.e., analog) and discrete-time (i.e., digital) simulations. We demonstrate the utility of these extensions by mapping an optimal delay line onto various spiking dynamical networks using higher-order models of the synapse. We show that these networks nonlinearly encode rolling windows of input history, using a scale invariant representation, with accuracy depending on the frequency content of the input signal. Finally, we reveal that these methods provide a novel explanation of time cell responses during a delay task, which have been observed throughout hippocampus, striatum, and cortex."}], "ArticleTitle": "Improving Spiking Dynamical Networks: Accurate Delays, Higher-Order Synapses, and Time Cells."}, "28777728": {"mesh": ["Animals", "Computer Simulation", "Decision Making", "Functional Laterality", "GABA-A Receptor Agonists", "Memory", "Models, Neurological", "Models, Psychological", "Muscimol", "Neural Networks, Computer", "Prefrontal Cortex", "Psychometrics", "Rats", "Time Factors"], "AbstractText": [{"section": null, "text": "Two-node attractor networks are flexible models for neural activity during decision making. Depending on the network configuration, these networks can model distinct aspects of decisions including evidence integration, evidence categorization, and decision memory. Here, we use attractor networks to model recent causal perturbations of the frontal orienting fields (FOF) in rat cortex during a perceptual decision-making task (Erlich, Brunton, Duan, Hanks, & Brody, 2015 ). We focus on a striking feature of the perturbation results. Pharmacological silencing of the FOF resulted in a stimulus-independent bias. We fit several models to test whether integration, categorization, or decision memory could account for this bias and found that only the memory configuration successfully accounts for it. This memory model naturally accounts for optogenetic perturbations of FOF in the same task and correctly predicts a memory-duration-dependent deficit caused by silencing FOF in a different task. Our results provide mechanistic support for a \"postcategorization\" memory role of the FOF in upcoming choices."}], "ArticleTitle": "Rat Prefrontal Cortex Inactivations during Decision Making Are Explained by Bistable Attractor Dynamics."}, "28410052": {"mesh": ["Algorithms", "Brain", "Brain Mapping", "Brain-Computer Interfaces", "Electroencephalography", "Humans", "Imagination", "Machine Learning", "Motor Activity", "Signal Processing, Computer-Assisted"], "AbstractText": [{"section": null, "text": "The estimation of covariance matrices is of prime importance to analyze the distribution of multivariate signals. In motor imagery-based brain-computer interfaces (MI-BCI), covariance matrices play a central role in the extraction of features from recorded electroencephalograms (EEGs); therefore, correctly estimating covariance is crucial for EEG classification. This letter discusses algorithms to average sample covariance matrices (SCMs) for the selection of the reference matrix in tangent space mapping (TSM)-based MI-BCI. Tangent space mapping is a powerful method of feature extraction and strongly depends on the selection of a reference covariance matrix. In general, the observed signals may include outliers; therefore, taking the geometric mean of SCMs as the reference matrix may not be the best choice. In order to deal with the effects of outliers, robust estimators have to be used. In particular, we discuss and test the use of geometric medians and trimmed averages (defined on the basis of several metrics) as robust estimators. The main idea behind trimmed averages is to eliminate data that exhibit the largest distance from the average covariance calculated on the basis of all available data. The results of the experiments show that while the geometric medians show little differences from conventional methods in terms of classification accuracy in the classification of electroencephalographic recordings, the trimmed averages show significant improvement for all subjects."}], "ArticleTitle": "Robust Averaging of Covariances for EEG Recordings Classification in Motor Imagery Brain-Computer Interfaces."}, "29064786": {"mesh": ["Animals", "Astrocytes", "Extracellular Fluid", "GABA Plasma Membrane Transport Proteins", "Gap Junctions", "Learning", "Membrane Potentials", "Models, Neurological", "Nerve Net", "Neural Networks, Computer", "Neurons", "Nonlinear Dynamics", "Perception", "gamma-Aminobutyric Acid"], "AbstractText": [{"section": null, "text": "Learning of sensory cues is believed to rely on synchronous pre- and postsynaptic neuronal firing. Evidence is mounting that such synchronicity is not merely caused by properties of the underlying neuronal network but could also depend on the integrity of gap junctions that connect neurons and astrocytes in networks too. In this perspective, we set out to investigate the effect of astrocytic gap junctions on perceptual learning, introducing a model for coupled neuron-astrocyte networks. In particular, we focus on the fact that astrocytes are rich of GABA transporters (GATs) which can either uptake or release GABA depending on the astrocyte membrane potential, which is a function of local neural activity. We show that GABAergic signaling is a crucial component of intracolumnar neuronal synchronization, thereby promoting learning by neurons in the same cell assembly that are activated by a shared sensory cue. At the same time, we show that this effect can critically depend on astrocytic gap junctions insofar as these latter could synchronize extracellular GABA levels around many neurons and throughout entire cell assemblies. These results are supported by extensive computational arguments and predict that astrocytic gap junctions could improve perceptual learning by controlling extracellular GABA."}], "ArticleTitle": "Improved Perceptual Learning by Control of Extracellular GABA Concentration by Astrocytic Gap Junctions."}, "29064782": {"mesh": [], "AbstractText": [{"section": null, "text": "To understand neural activity, two broad categories of models exist: statistical and dynamical. While statistical models possess rigorous methods for parameter estimation and goodness-of-fit assessment, dynamical models provide mechanistic insight. In general, these two categories of models are separately applied; understanding the relationships between these modeling approaches remains an area of active research. In this letter, we examine this relationship using simulation. To do so, we first generate spike train data from a well-known dynamical model, the Izhikevich neuron, with a noisy input current. We then fit these spike train data with a statistical model (a generalized linear model, GLM, with multiplicative influences of past spiking). For different levels of noise, we show how the GLM captures both the deterministic features of the Izhikevich neuron and the variability driven by the noise. We conclude that the GLM captures essential features of the simulated spike trains, but for near-deterministic spike trains, goodness-of-fit analyses reveal that the model does not fit very well in a statistical sense; the essential random part of the GLM is not captured."}], "ArticleTitle": "Capturing Spike Variability in Noisy Izhikevich Neurons Using Point Process Generalized Linear Models."}, "28333589": {"mesh": ["Adaptation, Physiological", "Brain", "Brain Mapping", "Computer Simulation", "Humans", "Image Processing, Computer-Assisted", "Magnetic Resonance Imaging", "Models, Neurological", "Nerve Net", "Neural Pathways", "Neurons"], "AbstractText": [{"section": null, "text": "A primary goal of many neuroimaging studies that use magnetic resonance imaging (MRI) is to deduce the structure-function relationships in the human brain using data from the three major neuro-MRI modalities: high-resolution anatomical, diffusion tensor imaging, and functional MRI. To date, the general procedure for analyzing these data is to combine the results derived independently from each of these modalities. In this article, we develop a new theoretical and computational approach for combining these different MRI modalities into a powerful and versatile framework that combines our recently developed methods for morphological shape analysis and segmentation, simultaneous local diffusion estimation and global tractography, and nonlinear and nongaussian spatial-temporal activation pattern classification and ranking, as well as our fast and accurate approach for nonlinear registration between modalities. This joint analysis method is capable of extracting new levels of information that is not achievable from any of those single modalities alone. A theoretical probabilistic framework based on a reformulation of prior information and available interdependencies between modalities through a joint coupling matrix and an efficient computational implementation allows construction of quantitative functional, structural, and effective brain connectivity modes and parcellation. This new method provides an overall increase of resolution, accuracy, level of detail, and information content and has the potential to be instrumental in the clinical adaptation of neuro-MRI modalities, which, when jointly analyzed, provide a more comprehensive view of a subject's structure-function relations, while the current standard, wherein single-modality methods are analyzed separately, leaves a critical gap in an integrated view of a subject's neuorphysiological state. As one example of this increased sensitivity, we demonstrate that the jointly estimated structural and functional dependencies of mode power follow the same power law decay with the same exponent."}], "ArticleTitle": "A Unified Theory of Neuro-MRI Data Shows Scale-Free Nature of Connectivity Modes."}, "27391688": {"mesh": ["Action Potentials", "Algorithms", "Animals", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Threshold-linear networks are a common class of firing rate models that describe recurrent interactions among neurons. Unlike their linear counterparts, these networks generically possess multiple stable fixed points (steady states), making them viable candidates for memory encoding and retrieval. In this work, we characterize stable fixed points of general threshold-linear networks with constant external drive and discover constraints on the coexistence of fixed points involving different subsets of active neurons. In the case of symmetric networks, we prove the following antichain property: if a set of neurons [Formula: see text] is the support of a stable fixed point, then no proper subset or superset of [Formula: see text] can support a stable fixed point. Symmetric threshold-linear networks thus appear to be well suited for pattern completion, since the dynamics are guaranteed not to get stuck in a subset or superset of a stored pattern. We also show that for any graph G, we can construct a network whose stable fixed points correspond precisely to the maximal cliques of G. As an application, we design network decoders for place field codes and demonstrate their efficacy for error correction and pattern completion. The proofs of our main results build on the theory of permitted sets in threshold-linear networks, including recently developed connections to classical distance geometry."}], "ArticleTitle": "Pattern Completion in Symmetric Threshold-Linear Networks."}, "28181877": {"mesh": ["Acoustic Stimulation", "Antipsychotic Agents", "Biomarkers", "Brain Waves", "Computer Simulation", "Hippocampus", "Humans", "Neurons", "Schizophrenia"], "AbstractText": [{"section": null, "text": "The recent explosion in neuroscience research has markedly increased our understanding of the neurobiological correlates of many psychiatric illnesses, but this has unfortunately not translated into more effective pharmacologic treatments for these conditions. At the same time, researchers have increasingly sought out biological markers, or biomarkers, as a way to categorize psychiatric illness, as these are felt to be closer to underlying genetic and neurobiological vulnerabilities. While biomarker-based drug discovery approaches have tended to employ in vivo (e.g., rodent) or in vitro test systems, relatively little attention has been paid to the potential of computational, or in silico, methodologies. Here we describe such a methodology, using as an example a biophysically detailed computational model of hippocampus that is made to generate putative schizophrenia biomarkers by the inclusion of a number of neuropathological changes that have been associated with the illness (NMDA system deficit, decreased neural connectivity, hyperdopaminergia). We use the specific inability to attune to gamma band (40&#160;Hz) auditory stimulus as our illness biomarker. We expose this system to a large number of virtual medications, defined by systematic variation of model parameters corresponding to five cellular-level effects. The potential efficacy of virtual medications is determined by a wellness metric (WM) that we have developed. We identify a number of virtual agents that consist of combinations of mechanisms, which are not simply reversals of the causative lesions. The manner in which this methodology could be extended to other neuropsychiatric conditions, such as Alzheimer's disease, autism, and fragile X syndrome, is discussed."}], "ArticleTitle": "An in Silico, Biomarker-Based Method for the Evaluation of Virtual Neuropsychiatric Drug Effects."}, "28957020": {"mesh": [], "AbstractText": [{"section": null, "text": "A key problem in computational neuroscience is to find simple, tractable models that are nevertheless flexible enough to capture the response properties of real neurons. Here we examine the capabilities of recurrent point process models known as Poisson generalized linear models (GLMs). These models are defined by a set of linear filters and a point nonlinearity and are conditionally Poisson spiking. They have desirable statistical properties for fitting and have been widely used to analyze spike trains from electrophysiological recordings. However, the dynamical repertoire of GLMs has not been systematically compared to that of real neurons. Here we show that GLMs can reproduce a comprehensive suite of canonical neural response behaviors, including tonic and phasic spiking, bursting, spike rate adaptation, type I and type II excitation, and two forms of bistability. GLMs can also capture stimulus-dependent changes in spike timing precision and reliability that mimic those observed in real neurons, and can exhibit varying degrees of stochasticity, from virtually deterministic responses to greater-than-Poisson variability. These results show that Poisson GLMs can exhibit a wide range of dynamic spiking behaviors found in real neurons, making them well suited for qualitative dynamical as well as quantitative statistical studies of single-neuron and population response properties."}], "ArticleTitle": "Capturing the Dynamical Repertoire of Single Neurons with Generalized Linear Models."}, "27137840": {"mesh": ["Animals", "Hippocampus", "Models, Neurological", "Nerve Net", "Place Cells", "Rats", "Spatial Behavior"], "AbstractText": [{"section": null, "text": "Place cells in the rat hippocampus play a key role in creating the animal's internal representation of the world. During active navigation, these cells spike only in discrete locations, together encoding a map of the environment. Electrophysiological recordings have shown that the animal can revisit this map mentally during both sleep and awake states, reactivating the place cells that fired during its exploration in the same sequence in which they were originally activated. Although consistency of place cell activity during active navigation is arguably enforced by sensory and proprioceptive inputs, it remains unclear how a consistent representation of space can be maintained during spontaneous replay. We propose a model that can account for this phenomenon and suggest that a spatially consistent replay requires a number of constraints on the hippocampal network that affect its synaptic architecture and the statistics of synaptic connection strengths."}], "ArticleTitle": "Maintaining Consistency of Spatial Information in the Hippocampal Network: A Combinatorial Geometry Model."}, "28030777": {"mesh": [], "AbstractText": [{"section": null, "text": "Networks have become instrumental in deciphering how information is processed and transferred within systems in almost every scientific field today. Nearly all network analyses, however, have relied on humans to devise structural features of networks believed to be most discriminative for an application. We present a framework for comparing and classifying networks without human-crafted features using deep learning. After training, autoencoders contain hidden units that encode a robust structural vocabulary for succinctly describing graphs. We use this feature vocabulary to tackle several network mining problems and find improved predictive performance versus many popular features used today. These problems include uncovering growth mechanisms driving the evolution of networks, predicting protein network fragility, and identifying environmental niches for metabolic networks. Deep learning offers a principled approach for mining complex networks and tackling graph-theoretic problems."}], "ArticleTitle": "Learning the Structural Vocabulary of a Network."}, "30462586": {"mesh": [], "AbstractText": [{"section": null, "text": "Modeling videos and image sets by linear subspaces has achieved great success in various visual recognition tasks. However, subspaces constructed from visual data are always notoriously embedded in a high-dimensional ambient space, which limits the applicability of existing techniques. This letter explores the possibility of proposing a geometry-aware framework for constructing lower-dimensional subspaces with maximum discriminative power from high-dimensional subspaces in the supervised scenario. In particular, we make use of Riemannian geometry and optimization techniques on matrix manifolds to learn an orthogonal projection, which shows that the learning process can be formulated as an unconstrained optimization problem on a Grassmann manifold. With this natural geometry, any metric on the Grassmann manifold can theoretically be used in our model. Experimental evaluations on several data sets show that our approach results in significantly higher accuracy than other state-of-the-art algorithms."}], "ArticleTitle": "Supervised Dimensionality Reduction on Grassmannian for Image Set Recognition."}, "30021086": {"mesh": [], "AbstractText": [{"section": null, "text": "Least squares regression (LSR) is a fundamental statistical analysis technique that has been widely applied to feature learning. However, limited by its simplicity, the local structure of data is easy to neglect, and many methods have considered using orthogonal constraint for preserving more local information. Another major drawback of LSR is that the loss function between soft regression results and hard target values cannot precisely reflect the classification ability; thus, the idea of the large margin constraint is put forward. As a consequence, we pay attention to the concepts of large margin and orthogonal constraint to propose a novel algorithm, orthogonal least squares regression with large margin (OLSLM), for multiclass classification in this letter. The core task of this algorithm is to learn regression targets from data and an orthogonal transformation matrix simultaneously such that the proposed model not only ensures every data point can be correctly classified with a large margin than conventional least squares regression, but also can preserve more local data structure information in the subspace. Our efficient optimization method for solving the large margin constraint and orthogonal constraint iteratively proved to be convergent in both theory and practice. We also apply the large margin constraint in the process of generating a sparse learning model for feature selection via joint [Formula: see text]-norm minimization on both loss function and regularization terms. Experimental results validate that our method performs better than state-of-the-art methods on various real-world data sets."}], "ArticleTitle": "Multiclass Classification and Feature Selection Based on Least Squares Regression with Large Margin."}, "30576617": {"mesh": [], "AbstractText": [{"section": null, "text": "Models of associative memory with discrete-strength synapses are palimpsests, learning new memories by forgetting old ones. Memory lifetimes can be defined by the mean first passage time (MFPT) for a perceptron's activation to fall below firing threshold. By imposing the condition that the vector of possible strengths available to a synapse is a left eigenvector of the stochastic matrix governing transitions in strength, we previously derived results for MFPTs and first passage time (FPT) distributions in models with simple, multistate synapses. This condition permits jump moments to be computed via a 1-dimensional Fokker-Planck approach. Here, we study memory lifetimes in the absence of this condition. To do so, we must introduce additional variables, including the perceptron activation, that parameterize synaptic configurations, permitting Markovian dynamics in these variables to be formulated. FPT problems in these variables require solving multidimensional partial differential or integral equations. However, the FPT dynamics can be analytically well approximated by focusing on the slowest eigenmode in this higher-dimensional space. We may also obtain a much better approximation by restricting to the two dominant variables in this space, the restriction making numerical methods tractable. Analytical and numerical methods are in excellent agreement with simulation data, validating our methods. These methods prepare the ground for the study of FPT memory lifetimes with complex rather than simple, multistate synapses."}], "ArticleTitle": "First Passage Time Memory Lifetimes for Simple, Multistate Synapses: Beyond the Eigenvector Requirement."}, "28562214": {"mesh": [], "AbstractText": [{"section": null, "text": "We propose a nonparametric procedure to achieve fast inference in generative graphical models when the number of latent states is very large. The approach is based on iterative latent variable preselection, where we alternate between learning a selection function to reveal the relevant latent variables and using this to obtain a compact approximation of the posterior distribution for EM. This can make inference possible where the number of possible latent states is, for example, exponential in the number of latent variables, whereas an exact approach would be computationally infeasible. We learn the selection function entirely from the observed data and current expectation-maximization state via gaussian process regression. This is in contrast to earlier approaches, where selection functions were manually designed for each problem setting. We show that our approach performs as well as these bespoke selection functions on a wide variety of inference problems. In particular, for the challenging case of a hierarchical model for object localization with occlusion, we achieve results that match a customized state-of-the-art selection method at a far lower computational cost."}], "ArticleTitle": "GP-Select: Accelerating EM Using Adaptive Subspace Preselection."}, "29566353": {"mesh": [], "AbstractText": [{"section": null, "text": "The computational principles of slowness and predictability have been proposed to describe aspects of information processing in the visual system. From the perspective of slowness being a limited special case of predictability we investigate the relationship between these two principles empirically. On a collection of real-world data sets we compare the features extracted by slow feature analysis (SFA) to the features of three recently proposed methods for predictable feature extraction: forecastable component analysis, predictable feature analysis, and graph-based predictable feature analysis. Our experiments show that the predictability of the learned features is highly correlated, and, thus, SFA appears to effectively implement a method for extracting predictable features according to different measures of predictability."}], "ArticleTitle": "Slowness as a Proxy for Temporal Predictability: An Empirical Comparison."}, "28410057": {"mesh": [], "AbstractText": [{"section": null, "text": "This letter aims at refined error analysis for binary classification using support vector machine (SVM) with gaussian kernel and convex loss. Our first result shows that for some loss functions, such as the truncated quadratic loss and quadratic loss, SVM with gaussian kernel can reach the almost optimal learning rate provided the regression function is smooth. Our second result shows that for a large number of loss functions, under some Tsybakov noise assumption, if the regression function is infinitely smooth, then SVM with gaussian kernel can achieve the learning rate of order [Formula: see text], where [Formula: see text] is the number of samples."}], "ArticleTitle": "Learning Rates for Classification with Gaussian Kernels."}, "28095198": {"mesh": ["Action Potentials", "Animals", "Cerebral Cortex", "Computer Simulation", "Electric Conductivity", "Humans", "Models, Neurological", "Nerve Net", "Neurons", "Probability", "Synapses", "Synaptic Potentials"], "AbstractText": [{"section": null, "text": "Physiological rhythms play a critical role in the functional development of living beings. Many biological functions are executed with an interaction of rhythms produced by internal characteristics of scores of cells. While synchronized oscillations may be associated with normal brain functions, anomalies in these oscillations may cause or relate the emergence of some neurological or neuropsychological pathologies. This study was designed to investigate the effects of topological structure and synaptic conductivity noise on the spatial synchronization and temporal rhythmicity of the waves generated by cells in the network. Because of holding the ability of clustering and randomizing with change of parameters, small-world (SW) network topology was chosen. The oscillatory activity of network was tried out by manipulating an insulated SW, cortical network model whose morphology is very close to real world. According to the obtained results, it was observed that at the optimal probabilistic rates of conductivity noise and rewiring of SW, powerful synchronized oscillatory small waves are generated in relation to the internal dynamics of cells, which are in line with the network's input. These two parameters were observed to be quite effective on the excitation-inhibition balance of the network. Accordingly, it may be suggested that the topological dynamics of SW and noisy synaptic conductivity may be associated with the normal and abnormal development of neurobiological structure."}], "ArticleTitle": "Effects of Small-World Rewiring Probability and Noisy Synaptic Conductivity on Slow Waves: Cortical Network."}, "28410051": {"mesh": [], "AbstractText": [{"section": null, "text": "Synapses are the communication channels for information transfer between neurons; these are the points at which pulse-like signals are converted into the stochastic release of quantized amounts of chemical neurotransmitter. At many synapses, prior neuronal activity depletes synaptic resources, depressing subsequent responses of both spontaneous and spike-evoked releases. We analytically compute the information transmission rate of a synaptic release site, which we model as a binary asymmetric channel. Short-term depression is incorporated by assigning the channel a memory of depth one. A successful release, whether spike evoked or spontaneous, decreases the probability of a subsequent release; if no release occurs on the following time step, the release probabilities recover back to their default values. We prove that synaptic depression can increase the release site's information rate if spontaneous release is more strongly depressed than spike-evoked release. When depression affects spontaneous and evoked release equally, the information rate must invariably decrease, even when the rate is normalized by the resources used for synaptic transmission. For identical depression levels, we analytically disprove the hypothesis, at least in this simplified model, that synaptic depression serves energy- and information-efficient encoding."}], "ArticleTitle": "Information Rate Analysis of a Synaptic Release Site Using a Two-State Model of Short-Term Depression."}, "28562217": {"mesh": [], "AbstractText": [{"section": null, "text": "The aim of this letter is to propose a theory of deep restricted kernel machines offering new foundations for deep learning with kernel machines. From the viewpoint of deep learning, it is partially related to restricted Boltzmann machines, which are characterized by visible and hidden units in a bipartite graph without hidden-to-hidden connections and deep learning extensions as deep belief networks and deep Boltzmann machines. From the viewpoint of kernel machines, it includes least squares support vector machines for classification and regression, kernel principal component analysis (PCA), matrix singular value decomposition, and Parzen-type models. A key element is to first characterize these kernel machines in terms of so-called conjugate feature duality, yielding a representation with visible and hidden units. It is shown how this is related to the energy form in restricted Boltzmann machines, with continuous variables in a nonprobabilistic setting. In this new framework of so-called restricted kernel machine (RKM) representations, the dual variables correspond to hidden features. Deep RKM are obtained by coupling the RKMs. The method is illustrated for deep RKM, consisting of three levels with a least squares support vector machine regression level and two kernel PCA levels. In its primal form also deep feedforward neural networks can be trained within this framework."}], "ArticleTitle": "Deep Restricted Kernel Machines Using Conjugate Feature Duality."}, "29162001": {"mesh": [], "AbstractText": [{"section": null, "text": "It has been debated whether kinematic features, such as the number of peaks or decomposed submovements in a velocity profile, indicate the number of discrete motor impulses or result from a continuous control process. The debate is particularly relevant for tasks involving target perturbation, which can alter movement kinematics. To simulate such tasks, finite-horizon models require two preset movement durations to compute two control policies before and after the perturbation. Another model employs infinite- and finite-horizon formulations to determine, respectively, movement durations and control policies, which are updated every time step. We adopted an infinite-horizon optimal feedback control model that, unlike previous approaches, does not preset movement durations or use multiple control policies. It contains both control-dependent and independent noises in system dynamics, state-dependent and independent noises in sensory feedbacks, and different delays and noise levels for visual and proprioceptive feedbacks. We analytically derived an optimal solution that can be applied continuously to move an effector toward a target regardless of whether, when, or where the target jumps. This single policy produces different numbers of peaks and \"submovements\" in velocity profiles for different conditions and trials. Movements that are slower or perturbed later appear to have more submovements. The model is also consistent with the observation that subjects can perform the perturbation task even without detecting the target jump or seeing their hands during reaching. Finally, because the model incorporates Weber's law via a state representation relative to the target, it explains why initial and terminal visual feedback are, respectively, less and more effective in improving end-point accuracy. Our work suggests that the number of peaks or submovements in a velocity profile does not necessarily reflect the number of motor impulses and that the difference between initial and terminal feedback does not necessarily imply a transition between open- and closed-loop strategies."}], "ArticleTitle": "A Single, Continuously Applied Control Policy for Modeling Reaching Movements with and without Perturbation."}, "28599115": {"mesh": [], "AbstractText": [{"section": null, "text": "We consider the problem of optimizing information-theoretic quantities in recurrent networks via synaptic learning. In contrast to feedforward networks, the recurrence presents a key challenge insofar as an optimal learning rule must aggregate the joint distribution of the whole network. This challenge, in particular, makes a local policy (i.e., one that depends on only pairwise interactions) difficult. Here, we report a local metaplastic learning rule that performs approximate optimization by estimating whole-network statistics through the use of several slow, nested dynamical variables. These dynamics provide the rule with both anti-Hebbian and Hebbian components, thus allowing for decorrelating and correlating learning regimes that can occur when either is favorable for optimality. We demonstrate the performance of the synthesized rule in comparison to classical BCM dynamics and use the networks to conduct history-dependent tasks that highlight the advantages of recurrence. Finally, we show the consistency of the resultant learned networks with notions of criticality, including balanced ratios of excitation and inhibition."}], "ArticleTitle": "Recurrent Information Optimization with Local, Metaplastic Synaptic Dynamics."}, "28333587": {"mesh": [], "AbstractText": [{"section": null, "text": "When governed by underlying low-dimensional dynamics, the interdependence of simultaneously recorded populations of neurons can be explained by a small number of shared factors, or a low-dimensional trajectory. Recovering these latent trajectories, particularly from single-trial population recordings, may help us understand the dynamics that drive neural computation. However, due to the biophysical constraints and noise in the spike trains, inferring trajectories from data is a challenging statistical problem in general. Here, we propose a practical and efficient inference method, the variational latent gaussian process (vLGP). The vLGP combines a generative model with a history-dependent point process observation, together with a smoothness prior on the latent trajectories. The vLGP improves on earlier methods for recovering latent trajectories, which assume either observation models inappropriate for point processes or linear dynamics. We compare and validate vLGP on both simulated data sets and population recordings from the primary visual cortex. In the V1 data set, we find that vLGP achieves substantially higher performance than previous methods for predicting omitted spike trains, as well as capturing both the toroidal topology of visual stimuli space and the noise correlation. These results show that vLGP is a robust method with the potential to reveal hidden neural dynamics from large-scale neural recordings."}], "ArticleTitle": "Variational Latent Gaussian Process for Recovering Single-Trial Dynamics from Population Spike Trains."}, "28562224": {"mesh": ["Animals", "Artifacts", "Fourier Analysis", "Humans", "Nonlinear Dynamics", "Signal Processing, Computer-Assisted"], "AbstractText": [{"section": null, "text": "In estimating the frequency spectrum of real-world time series data, we must violate the assumption of infinite-length, orthogonal components in the Fourier basis. While it is widely known that care must be taken with discretely sampled data to avoid aliasing of high frequencies, less attention is given to the influence of low frequencies with period below the sampling time window. Here, we derive an analytic expression for the side-lobe attenuation of signal components in the frequency domain representation. This expression allows us to detail the influence of individual frequency components throughout the spectrum. The first consequence is that the presence of low-frequency components introduces a 1/f[Formula: see text] component across the power spectrum, with a scaling exponent of [Formula: see text]. This scaling artifact could be composed of diffuse low-frequency components, which can render it difficult to detect a priori. Further, treatment of the signal with standard digital signal processing techniques cannot easily remove this scaling component. While several theoretical models have been introduced to explain the ubiquitous 1/f[Formula: see text] scaling component in neuroscientific data, we conjecture here that some experimental observations could be the result of such data analysis procedures."}], "ArticleTitle": "Analytical Derivation of Nonlinear Spectral Effects and 1/f Scaling Artifact in Signal Processing of Real-World Data."}, "31614106": {"mesh": [], "AbstractText": [{"section": null, "text": "We analyze the joint probability distribution on the lengths of the vectors of hidden variables in different layers of a fully connected deep network, when the weights and biases are chosen randomly according to gaussian distributions. We show that if the activation function &#966; satisfies a minimal set of assumptions, satisfied by all activation functions that we know that are used in practice, then, as the width of the network gets large, the \"length process\" converges in probability to a length map that is determined as a simple function of the variances of the random weights and biases and the activation function &#966;. We also show that this convergence may fail for &#966; that violate our assumptions. We show how to use this analysis to choose the variance of weight initialization, depending on the activation function, so that hidden variables maintain a consistent scale throughout the network."}], "ArticleTitle": "On the Effect of the Activation Function on the Distribution of Hidden Nodes in a Deep Network."}, "28333583": {"mesh": ["Algorithms", "Cerebral Cortex", "Feedback, Physiological", "Humans", "Learning", "Models, Neurological", "Neural Networks, Computer", "Neuronal Plasticity", "Neurons"], "AbstractText": [{"section": null, "text": "To efficiently learn from feedback, cortical networks need to update synaptic weights on multiple levels of cortical hierarchy. An effective and well-known algorithm for computing such changes in synaptic weights is the error backpropagation algorithm. However, in this algorithm, the change in synaptic weights is a complex function of weights and activities of neurons not directly connected with the synapse being modified, whereas the changes in biological synapses are determined only by the activity of presynaptic and postsynaptic neurons. Several models have been proposed that approximate the backpropagation algorithm with local synaptic plasticity, but these models require complex external control over the network or relatively complex plasticity rules. Here we show that a network developed in the predictive coding framework can efficiently perform supervised learning fully autonomously, employing only simple local Hebbian plasticity. Furthermore, for certain parameters, the weight change in the predictive coding model converges to that of the backpropagation algorithm. This suggests that it is possible for cortical networks with simple Hebbian synaptic plasticity to implement efficient learning algorithms in which synapses in areas on multiple levels of hierarchy are modified to minimize the error on the output."}], "ArticleTitle": "An Approximation of the Error Backpropagation Algorithm in a Predictive Coding Network with Local Hebbian Synaptic Plasticity."}, "28562223": {"mesh": ["Action Potentials", "Animals", "Biological Clocks", "Computer Simulation", "Models, Neurological", "Nerve Net", "Neurons", "Synapses"], "AbstractText": [{"section": null, "text": "The role of the phase response curve (PRC) shape on the synchrony of synaptically coupled oscillating neurons is examined. If the PRC is independent of the phase, because of the synaptic form of the coupling, synchrony is found to be stable for both excitatory and inhibitory coupling at all rates, whereas the antisynchrony becomes stable at low rates. A faster synaptic rise helps extend the stability of antisynchrony to higher rates. If the PRC is not constant but has a profile like that of a leaky integrate-and-fire model, then, in contrast to the earlier reports that did not include the voltage effects, mutual excitation could lead to stable synchrony provided the synaptic reversal potential is below the voltage level the neuron would have reached in the absence of the interaction and threshold reset. This level is controlled by the applied current and the leakage parameters. Such synchrony is contingent on significant phase response (that would result, for example, by a sharp PRC jump) occurring during the synaptic rising phase. The rising phase, however, does not contribute significantly if it occurs before the voltage spike reaches its peak. Then a stable near-synchronous state can still exist between type 1 PRC neurons if the PRC shows a left skewness in its shape. These results are examined comprehensively using perfect integrate-and-fire, leaky integrate-and-fire, and skewed PRC shapes under the assumption of the weakly coupled oscillator theory applied to synaptically coupled neuron models."}], "ArticleTitle": "Effect of Phase Response Curve Shape and Synaptic Driving Force on Synchronization of Coupled Neuronal Oscillators."}, "27764595": {"mesh": ["Animals", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "The efficient coding hypothesis assumes that biological sensory systems use neural codes that are optimized to best possibly represent the stimuli that occur in their environment. Most common models use information-theoretic measures, whereas alternative formulations propose incorporating downstream decoding performance. Here we provide a systematic evaluation of different optimality criteria using a parametric formulation of the efficient coding problem based on the [Formula: see text] reconstruction error of the maximum likelihood decoder. This parametric family includes both the information maximization criterion and squared decoding error as special cases. We analytically derived the optimal tuning curve of a single neuron encoding a one-dimensional stimulus with an arbitrary input distribution. We show how the result can be generalized to a class of neural populations by introducing the concept of a meta-tuning curve. The predictions of our framework are tested against previously measured characteristics of some early visual systems found in biology. We find solutions that correspond to low values of [Formula: see text], suggesting that across different animal models, neural representations in the early visual pathways optimize similar criteria about natural stimuli that are relatively close to the information maximization criterion."}], "ArticleTitle": "Efficient Neural Codes That Minimize Lp Reconstruction Error."}, "29064785": {"mesh": ["Algorithms", "Humans", "Learning", "Memory", "Movement", "Neural Networks, Computer", "Nonlinear Dynamics", "Predictive Value of Tests", "Visual Pathways", "Visual Perception"], "AbstractText": [{"section": null, "text": "This letter proposes a novel predictive coding type neural network model, the predictive multiple spatiotemporal scales recurrent neural network (P-MSTRNN). The P-MSTRNN learns to predict visually perceived human whole-body cyclic movement patterns by exploiting multiscale spatiotemporal constraints imposed on network dynamics by using differently sized receptive fields as well as different time constant values for each layer. After learning, the network can imitate target movement patterns by inferring or recognizing corresponding intentions by means of the regression of prediction error. Results show that the network can develop a functional hierarchy by developing a different type of dynamic structure at each layer. The letter examines how model performance during pattern generation, as well as predictive imitation, varies depending on the stage of learning. The number of limit cycle attractors corresponding to target movement patterns increases as learning proceeds. Transient dynamics developing early in the learning process successfully perform pattern generation and predictive imitation tasks. The letter concludes that exploitation of transient dynamics facilitates successful task performance during early learning periods."}], "ArticleTitle": "Predictive Coding for Dynamic Visual Processing: Development of Functional Hierarchy in a Multiple Spatiotemporal Scales RNN Model."}, "27557104": {"mesh": ["Action Potentials", "Brain", "Computer Simulation", "Humans", "Models, Neurological", "Neural Networks, Computer", "Neurons"], "AbstractText": [{"section": null, "text": "Large multiscale neuronal network simulations are of increasing value as more big data are gathered about brain wiring and organization under the auspices of a current major research initiative, such as Brain Research through Advancing Innovative Neurotechnologies. The development of these models requires new simulation technologies. We describe here the current use of the NEURON simulator with message passing interface (MPI) for simulation in the domain of moderately large networks on commonly available high-performance computers (HPCs). We discuss the basic layout of such simulations, including the methods of simulation setup, the run-time spike-passing paradigm, and postsimulation data storage and data management approaches. Using the Neuroscience Gateway, a portal for computational neuroscience that provides access to large HPCs, we benchmark simulations of neuronal networks of different sizes (500-100,000 cells), and using different numbers of nodes (1-256). We compare three types of networks, composed of either Izhikevich integrate-and-fire neurons (I&F), single-compartment Hodgkin-Huxley (HH) cells, or a hybrid network with half of each. Results show simulation run time increased approximately linearly with network size and decreased almost linearly with the number of nodes. Networks with I&F neurons were faster than HH networks, although differences were small since all tested cells were point neurons with a single compartment."}], "ArticleTitle": "Simulation Neurotechnologies for Advancing Brain Research: Parallelizing Large Networks in NEURON."}, "33080163": {"mesh": [], "AbstractText": [{"section": null, "text": "Testing under what conditions a product satisfies the desired properties is a fundamental problem in manufacturing industry. If the condition and the property are respectively regarded as the input and the output of a black-box function, this task can be interpreted as the problem called level set estimation (LSE): the problem of identifying input regions such that the function value is above (or below) a threshold. Although various methods for LSE problems have been developed, many issues remain to be solved for their practical use. As one of such issues, we consider the case where the input conditions cannot be controlled precisely-LSE problems under input uncertainty. We introduce a basic framework for handling input uncertainty in LSE problems and then propose efficient methods with proper theoretical guarantees. The proposed methods and theories can be generally applied to a variety of challenges related to LSE under input uncertainty such as cost-dependent input uncertainties and unknown input uncertainties. We apply the proposed methods to artificial and real data to demonstrate their applicability and effectiveness."}], "ArticleTitle": "Active Learning for Level Set Estimation Under Input Uncertainty and Its Extensions."}, "27764592": {"mesh": [], "AbstractText": [{"section": null, "text": "The communication-through-coherence (CTC) hypothesis states that a sending group of neurons will have a particularly strong effect on a receiving group if both groups oscillate in a phase-locked (\"coherent\") manner (Fries, 2005 , 2015 ). Here, we consider a situation with two visual stimuli, one in the focus of attention and the other distracting, resulting in two sites of excitation at an early cortical area that project to a common site in a next area. Taking a modeler's perspective, we confirm the workings of a mechanism that was proposed by Bosman et&#160;al. ( 2012 ) in the context of providing experimental evidence for the CTC hypothesis: a slightly higher gamma frequency of the attended sending site compared to the distracting site may cause selective interareal synchronization with the receiving site if combined with a slow-rhythm gamma phase reset. We&#160;also demonstrate the relevance of a slightly lower intrinsic frequency of the receiving site for this scenario. Moreover, we discuss conditions for a transition from bottom-up to top-down driven phase locking."}], "ArticleTitle": "Selective Interareal Synchronization through Gamma Frequency Differences and Slower-Rhythm Gamma Phase Reset."}, "27391678": {"mesh": ["Bayes Theorem", "Brain", "Brain Mapping", "Entropy", "Humans", "Magnetic Resonance Imaging", "Probability", "Rest"], "AbstractText": [{"section": null, "text": "The ability of functional magnetic resonance imaging (FMRI) to noninvasively measure fluctuations in brain activity in the absence of an applied stimulus offers the possibility of discerning functional networks in the resting state of the brain. However, the reconstruction of brain networks from these signal fluctuations poses a significant challenge because they are generally nonlinear and nongaussian and can overlap in both their spatial and temporal extent. Moreover, because there is no explicit input stimulus, there is no signal model with which to compare the brain responses. A variety of techniques have been devised to address this problem, but the predominant approaches are based on the presupposition of statistical properties of complex brain signal parameters, which are unprovable but facilitate the analysis. In this article, we address this problem with a new method, entropy field decomposition, for estimating structure within spatiotemporal data. This method is based on a general information field-theoretic formulation of Bayesian probability theory incorporating prior coupling information that allows the enumeration of the most probable parameter configurations without the need for unjustified statistical assumptions. This approach facilitates the construction of brain activation modes directly from the spatial-temporal correlation structure of the data. These modes and their associated spatial-temporal correlation structure can then be used to generate space-time activity probability trajectories, called functional connectivity pathways, which provide a characterization of functional brain networks."}], "ArticleTitle": "Dynamic Multiscale Modes of Resting State Brain Activity Detected by Entropy Field Decomposition."}, "28410058": {"mesh": [], "AbstractText": [{"section": null, "text": "We propose a method for intrinsic dimension estimation. By fitting the power of distance from an inspection point and the number of samples included inside a ball with a radius equal to the distance, to a regression model, we estimate the goodness of fit. Then, by using the maximum likelihood method, we estimate the local intrinsic dimension around the inspection point. The proposed method is shown to be comparable to conventional methods in global intrinsic dimension estimation experiments. Furthermore, we experimentally show that the proposed method outperforms a conventional local dimension estimation method."}], "ArticleTitle": "Local Intrinsic Dimension Estimation by Generalized Linear Modeling."}, "31614105": {"mesh": [], "AbstractText": [{"section": null, "text": "For nonconvex optimization in machine learning, this article proves that every local minimum achieves the globally optimal value of the perturbable gradient basis model at any differentiable point. As a result, nonconvex machine learning is theoretically as supported as convex machine learning with a handcrafted basis in terms of the loss at differentiable local minima, except in the case when a preference is given to the handcrafted basis over the perturbable gradient basis. The proofs of these results are derived under mild assumptions. Accordingly, the proven results are directly applicable to many machine learning models, including practical deep neural networks, without any modification of practical methods. Furthermore, as special cases of our general results, this article improves or complements several state-of-the-art theoretical results on deep neural networks, deep residual networks, and overparameterized deep neural networks with a unified proof technique and novel geometric insights. A special case of our results also contributes to the theoretical foundation of representation learning."}], "ArticleTitle": "Every Local Minimum Value Is the Global Minimum Value of Induced Model in Nonconvex Machine Learning."}, "28181879": {"mesh": [], "AbstractText": [{"section": null, "text": "In order to address with the problem of the traditional or improved cuckoo search (CS) algorithm, we propose a dynamic adaptive cuckoo search with crossover operator (DACS-CO) algorithm. Normally, the parameters of the CS algorithm are kept constant or adapted by empirical equation that may result in decreasing the efficiency of the algorithm. In order to solve the problem, a feedback control scheme of algorithm parameters is adopted in cuckoo search; Rechenberg's 1/5 criterion, combined with a learning strategy, is used to evaluate the evolution process. In addition, there are no information exchanges between individuals for cuckoo search algorithm. To promote the search progress and overcome premature convergence, the multiple-point random crossover operator is merged into the CS algorithm to exchange information between individuals and improve the diversification and intensification of the population. The performance of the proposed hybrid algorithm is investigated through different nonlinear systems, with the numerical results demonstrating that the method can estimate parameters accurately and efficiently. Finally, we compare the results with the standard CS algorithm, orthogonal learning cuckoo search algorithm (OLCS), an adaptive and simulated annealing operation with the cuckoo search algorithm (ACS-SA), a genetic algorithm (GA), a particle swarm optimization algorithm (PSO), and a genetic simulated annealing algorithm (GA-SA). Our simulation results demonstrate the effectiveness and superior performance of the proposed algorithm."}], "ArticleTitle": "Parameter Estimation of Nonlinear Systems by Dynamic Cuckoo Search."}, "27626964": {"mesh": [], "AbstractText": [{"section": null, "text": "It is well known that cerebellar motor control is fine-tuned by the learning process adjusted according to rich error signals from inferior olive (IO) neurons. Schweighofer and colleagues proposed that these signals can be produced by chaotic irregular firing in the IO neuron assembly; such chaotic resonance (CR) was replicated in their computer demonstration of a Hodgkin-Huxley (HH)-type compartment model. In this study, we examined the response of CR to a periodic signal in the IO neuron assembly comprising the Llin&#225;s approach IO neuron model. This system involves empirically observed dynamics of the IO membrane potential and is simpler than the HH-type compartment model. We then clarified its dependence on electrical coupling strength, input signal strength, and frequency. Furthermore, we compared the physiological validity for IO neurons such as low firing rate and sustaining subthreshold oscillation between CR and conventional stochastic resonance (SR) and examined the consistency with asynchronous firings indicated by the previous model-based studies in the cerebellar learning process. In addition, the signal response of CR and SR was investigated in a large neuron assembly. As the result, we confirmed that CR was consistent with the above IO neuron's characteristics, but it was not as easy for SR."}], "ArticleTitle": "Chaotic Resonance in Coupled Inferior Olive Neurons with the Llin&#225;s Approach Neuron Model."}, "27348737": {"mesh": [], "AbstractText": [{"section": null, "text": "In typical machine learning applications such as information retrieval, precision and recall are two commonly used measures for assessing an algorithm's performance. Symmetrical confidence intervals based on K-fold cross-validated t distributions are widely used for the inference of precision and recall measures. As we confirmed through simulated experiments, however, these confidence intervals often exhibit lower degrees of confidence, which may easily lead to liberal inference results. Thus, it is crucial to construct faithful confidence (credible) intervals for precision and recall with a high degree of confidence and a short interval length. In this study, we propose two posterior credible intervals for precision and recall based on K-fold cross-validated beta distributions. The first credible interval for precision (or recall) is constructed based on the beta posterior distribution inferred by all K data sets corresponding to K confusion matrices from a K-fold cross-validation. Second, considering that each data set corresponding to a confusion matrix from a K-fold cross-validation can be used to infer a beta posterior distribution of precision (or recall), the second proposed credible interval for precision (or recall) is constructed based on the average of K beta posterior distributions. Experimental results on simulated and real data sets demonstrate that the first credible interval proposed in this study almost always resulted in degrees of confidence greater than 95%. With an acceptable degree of confidence, both of our two proposed credible intervals have shorter interval lengths than those based on a corrected K-fold cross-validated t distribution. Meanwhile, the average ranks of these two credible intervals are superior to that of the confidence interval based on a K-fold cross-validated t distribution for the degree of confidence and are superior to that of the confidence interval based on a corrected K-fold cross-validated t distribution for the interval length in all 27 cases of simulated and real data experiments. However, the confidence intervals based on the K-fold and corrected K-fold cross-validated t distributions are in the two extremes. Thus, when focusing on the reliability of the inference for precision and recall, the proposed methods are preferable, especially for the first credible interval."}], "ArticleTitle": "Credible Intervals for Precision and Recall Based on a K-Fold Cross-Validated Beta Distribution."}, "27870616": {"mesh": [], "AbstractText": [{"section": null, "text": "Binary undirected graphs are well established, but when these graphs are constructed, often a threshold is applied to a parameter describing the connection between two nodes. Therefore, the use of weighted graphs is more appropriate. In this work, we focus on weighted undirected graphs. This implies that we have to incorporate edge weights in the graph measures, which require generalizations of common graph metrics. After reviewing existing generalizations of the clustering coefficient and the local efficiency, we proposed new generalizations for these graph measures. To be able to compare different generalizations, a number of essential and useful properties were defined that ideally should be satisfied. We applied the generalizations to two real-world networks of different sizes. As a result, we found that not all existing generalizations satisfy all essential properties. Furthermore, we determined the best generalization for the clustering coefficient and local efficiency based on their properties and the performance when applied to two networks. We found that the best generalization of the clustering coefficient is [Formula: see text], defined in Miyajima and Sakuragawa ( 2014 ), while the best generalization of the local efficiency is [Formula: see text], proposed in this letter. Depending on the application and the relative importance of sensitivity and robustness to noise, other generalizations may be selected on the basis of the properties investigated in this letter."}], "ArticleTitle": "Comparison of Different Generalizations of Clustering Coefficient and Local Efficiency for Weighted Undirected Graphs."}, "28777723": {"mesh": [], "AbstractText": [{"section": null, "text": "This work proposes a robust regression framework with nonconvex loss function. Two regression formulations are presented based on the Laplace kernel-induced loss (LK-loss). Moreover, we illustrate that the LK-loss function is a nice approximation for the zero-norm. However, nonconvexity of the LK-loss makes it difficult to optimize. A continuous optimization method is developed to solve the proposed framework. The problems are formulated as DC (difference of convex functions) programming. The corresponding DC algorithms (DCAs) converge linearly. Furthermore, the proposed algorithms are applied directly to determine the hardness of licorice seeds using near-infrared spectral data with noisy input. Experiments in eight spectral regions show that the proposed methods improve generalization compared with the traditional support vector regressions (SVR), especially in high-frequency regions. Experiments on several benchmark data sets demonstrate that the proposed methods achieve better results than the traditional regression methods in most of data sets we have considered."}], "ArticleTitle": "A Robust Regression Framework with Laplace Kernel-Induced Loss."}, "27557098": {"mesh": [], "AbstractText": [{"section": null, "text": "The time to the first spike after stimulus onset typically varies with the stimulation intensity. Experimental evidence suggests that neural systems use such response latency to encode information about the stimulus. We investigate the decoding accuracy of the latency code in relation to the level of noise in the form of presynaptic spontaneous activity. Paradoxically, the optimal performance is achieved at a nonzero level of noise and suprathreshold stimulus intensities. We argue that this phenomenon results from the influence of the spontaneous activity on the stabilization of the membrane potential in the absence of stimulation. The reported decoding accuracy improvement represents a novel manifestation of the noise-aided signal enhancement."}], "ArticleTitle": "Presynaptic Spontaneous Activity Enhances the Accuracy of Latency Coding."}, "29652590": {"mesh": [], "AbstractText": [{"section": null, "text": "We present a comprehensive framework of search methods, such as simulated annealing and batch training, for solving nonconvex optimization problems. These methods search a wider range by gradually decreasing the randomness added to the standard gradient descent method. The formulation that we define on the basis of this framework can be directly applied to neural network training. This produces an effective approach that gradually increases batch size during training. We also explain why large batch training degrades generalization performance, which previous studies have not clarified."}], "ArticleTitle": "Why Does Large Batch Training Result in Poor Generalization? A Comprehensive Explanation and a Better Strategy from the Viewpoint of Stochastic Optimization."}, "27764598": {"mesh": ["Color", "Color Perception", "Humans", "Neurons", "Photic Stimulation"], "AbstractText": [{"section": null, "text": "The accuracy with which humans detect chromatic differences varies throughout color space. For example, we are far more precise when discriminating two similar orange stimuli than two similar green stimuli. In order for two colors to be perceived as different, the neurons representing chromatic information must respond differently, and the difference must be larger than the trial-to-trial variability of the response to each separate color. Photoreceptors constitute the first stage in the processing of color information; many more stages are required before humans can consciously report whether two stimuli are perceived as chromatically distinguishable. Therefore, although photoreceptor absorption curves are expected to influence the accuracy of conscious discriminability, there is no reason to believe that they should suffice to explain it. Here we develop information-theoretical tools based on the Fisher metric that demonstrate that photoreceptor absorption properties explain about 87% of the variance of human color discrimination ability, as tested by previous behavioral experiments. In the context of this theory, the bottleneck in chromatic information processing is determined by photoreceptor absorption characteristics. Subsequent encoding stages modify only marginally the chromatic discriminability at the photoreceptor level."}], "ArticleTitle": "Derivation of Human Chromatic Discrimination Ability from an Information-Theoretical Notion of Distance in Color Space."}, "27764596": {"mesh": [], "AbstractText": [{"section": null, "text": "The techniques of random matrices have played an important role in many machine learning models. In this letter, we present a new method to study the tail inequalities for sums of random matrices. Different from other work (Ahlswede & Winter, 2002 ; Tropp, 2012 ; Hsu, Kakade, & Zhang, 2012 ), our tail results are based on the largest singular value (LSV) and independent of the matrix dimension. Since the LSV operation and the expectation are noncommutative, we introduce a diagonalization method to convert the LSV operation into the trace operation of an infinitely dimensional diagonal matrix. In this way, we obtain another version of Laplace-transform bounds and then achieve the LSV-based tail inequalities for sums of random matrices."}], "ArticleTitle": "LSV-Based Tail Inequalities for Sums of Random Matrices."}, "29064783": {"mesh": ["Action Potentials", "Animals", "CA1 Region, Hippocampal", "CA3 Region, Hippocampal", "Humans", "Models, Neurological", "Neural Inhibition", "Neural Networks, Computer", "Neurons", "Nonlinear Dynamics", "Receptors, GABA", "Receptors, N-Methyl-D-Aspartate", "Synapses"], "AbstractText": [{"section": null, "text": "This letter examines the results of input-output (nonparametric) modeling based on the analysis of data generated by a mechanism-based (parametric) model of CA3-CA1 neuronal connections in the hippocampus. The motivation is to obtain biological insight into the interpretation of such input-output (Volterra-equivalent) models estimated from synthetic data. The insights obtained may be subsequently used to interpretat input-output models extracted from actual experimental data. Specifically, we found that a simplified parametric model may serve as a useful tool to study the signal transformations in the hippocampal CA3-CA1 regions. Input-output modeling of model-based synthetic data show that GABAergic interneurons are responsible for regulating neuronal excitation, controlling the precision of spike timing, and maintaining network oscillations, in a manner consistent with previous studies. The input-output model obtained from real data exhibits intriguing similarities with its synthetic-data counterpart, demonstrating the importance of a dynamic resonance in the system/model response around 2 Hz to 3 Hz. Using the input-output model from real data as a guide, we may be able to amend the parametric model by incorporating more mechanisms in order to yield better-matching input-output model. The approach we present can also be applied to the study of other neural systems and pathways."}], "ArticleTitle": "Mechanism-Based and Input-Output Modeling of the Key Neuronal Connections and Signal Transformations in the CA3-CA1 Regions of the Hippocampus."}, "27626967": {"mesh": [], "AbstractText": [{"section": null, "text": "In this letter, we propose a novel neuro-inspired low-resolution online unsupervised learning rule to train the reservoir or liquid of liquid state machines. The liquid is a sparsely interconnected huge recurrent network of spiking neurons. The proposed learning rule is inspired from structural plasticity and trains the liquid through formating and eliminating synaptic connections. Hence, the learning involves rewiring of the reservoir connections similar to structural plasticity observed in biological neural networks. The network connections can be stored as a connection matrix and updated in memory by using address event representation (AER) protocols, which are generally employed in neuromorphic systems. On investigating the pairwise separation property, we find that trained liquids provide 1.36 0.18&#160;times more interclass separation while retaining similar intraclass separation as compared to random liquids. Moreover, analysis of the linear separation property reveals that trained liquids are 2.05 0.27&#160;times better than random liquids. Furthermore, we show that our liquids are able to retain the generalization ability and generality of random liquids. A memory analysis shows that trained liquids have 83.67 5.79&#160;ms longer fading memory than random liquids, which have shown 92.8 5.03&#160;ms fading memory for a particular type of spike train inputs. We also throw some light on the dynamics of the evolution of recurrent connections within the liquid. Moreover, compared to separation-driven synaptic modification', a recently proposed algorithm for iteratively refining reservoirs, our learning rule provides 9.30%, 15.21%, and 12.52% more liquid separations and 2.8%, 9.1%, and 7.9% better classification accuracies for 4, 8, and 12 class pattern recognition tasks, respectively."}], "ArticleTitle": "An Online Structural Plasticity Rule for Generating Better Reservoirs."}, "29162010": {"mesh": ["Action Potentials", "Animals", "Cerebral Cortex", "Image Processing, Computer-Assisted", "Macaca mulatta", "Models, Neurological", "Neural Networks, Computer", "Neurons", "Pattern Recognition, Automated", "Statistics as Topic", "Visual Perception"], "AbstractText": [{"section": null, "text": "Under the goal-driven paradigm, Yamins et&#160;al. ( 2014 ; Yamins & DiCarlo, 2016 ) have shown that by optimizing only the final eight-way categorization performance of a four-layer hierarchical network, not only can its top output layer quantitatively predict IT neuron responses but its penultimate layer can also automatically predict V4 neuron responses. Currently, deep neural networks (DNNs) in the field of computer vision have reached image object categorization performance comparable to that of human beings on ImageNet, a data set that contains 1.3 million training images of 1000 categories. We explore whether the DNN neurons (units in DNNs) possess image object representational statistics similar to monkey IT neurons, particularly when the network becomes deeper and the number of image categories becomes larger, using VGG19, a typical and widely used deep network of 19 layers in the computer vision field. Following Lehky, Kiani, Esteky, and Tanaka ( 2011 , 2014 ), where the response statistics of 674 IT neurons to 806 image stimuli are analyzed using three measures (kurtosis, Pareto tail index, and intrinsic dimensionality), we investigate the three issues in this letter using the same three measures: (1) the similarities and differences of the neural response statistics between VGG19 and primate IT cortex, (2) the variation trends of the response statistics of VGG19 neurons at different layers from low to high, and (3) the variation trends of the response statistics of VGG19 neurons when the numbers of stimuli and neurons increase. We find that the response statistics on both single-neuron selectivity and population sparseness of VGG19 neurons are fundamentally different from those of IT neurons in most cases; by increasing the number of neurons in different layers and the number of stimuli, the response statistics of neurons at different layers from low to high do not substantially change; and the estimated intrinsic dimensionality values at the low convolutional layers of VGG19 are considerably larger than the value of approximately 100 reported for IT neurons in Lehky et&#160;al. ( 2014 ), whereas those at the high fully connected layers are close to or lower than 100. To the best of our knowledge, this work is the first attempt to analyze the response statistics of DNN neurons with respect to primate IT neurons in image object representation."}], "ArticleTitle": "Statistics of Visual Responses to Image Object Stimuli from Primate AIT Neurons to DNN Neurons."}, "28333592": {"mesh": [], "AbstractText": [{"section": null, "text": "Nonconvex variants of support vector machines (SVMs) have been developed for various purposes. For example, robust SVMs attain robustness to outliers by using a nonconvex loss function, while extended [Formula: see text]-SVM (E[Formula: see text]-SVM) extends the range of the hyperparameter by introducing a nonconvex constraint. Here, we consider an extended robust support vector machine (ER-SVM), a robust variant of E[Formula: see text]-SVM. ER-SVM combines two types of nonconvexity from robust SVMs and E[Formula: see text]-SVM. Because of the two nonconvexities, the existing algorithm we proposed needs to be divided into two parts depending on whether the hyperparameter value is in the extended range or not. The algorithm also heuristically solves the nonconvex problem in the extended range. In this letter, we propose a new, efficient algorithm for ER-SVM. The algorithm deals with two types of nonconvexity while never entailing more computations than either E[Formula: see text]-SVM or robust SVM, and it finds a critical point of ER-SVM. Furthermore, we show that ER-SVM includes the existing robust SVMs as special cases. Numerical experiments confirm the effectiveness of integrating the two nonconvexities."}], "ArticleTitle": "DC Algorithm for Extended Robust Support Vector Machine."}, "27782778": {"mesh": [], "AbstractText": [{"section": null, "text": "We investigate several distribution-free dependence detection procedures, all based on a shuffling of the trials, from a statistical point of view. The mathematical justification of such procedures lies in the bootstrap principle and its approximation properties. In particular, we show that such a shuffling has mainly to be done on centered quantities-that is, quantities with zero mean under independence-to construct correct p-values, meaning that the corresponding tests control their false positive (FP) rate. Thanks to this study, we introduce a method, named permutation UE, which consists of a multiple testing procedure based on permutation of experimental trials and delayed coincidence count. Each involved single test of this procedure achieves the prescribed level, so that the corresponding multiple testing procedure controls the false discovery rate (FDR), and this with as few assumptions as possible on the underneath distribution, except independence and identical distribution across trials. The mathematical meaning of this assumption is discussed, and it is in particular argued that it does not mean what is commonly referred in neuroscience to as cross-trials stationarity. Some simulations show, moreover, that permutation UE outperforms the trial-shuffling of Pipa and Gr&#252;n ( 2003 ) and the MTGAUE method of Tuleau-Malot et&#160;al. ( 2014 ) in terms of single levels and FDR, for a comparable amount of false negatives. Application to real data is also provided."}], "ArticleTitle": "Surrogate Data Methods Based on a Shuffling of the Trials for Synchrony Detection: The Centering Issue."}, "28957018": {"mesh": ["Animals", "Computer Simulation", "Extracellular Space", "Finite Element Analysis", "Membrane Potentials", "Microelectrodes", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "Microelectrode arrays serve as an indispensable tool in electro-physiological research to study the electrical activity of neural cells, enabling measurements of single cell as well as network communication analysis. Recent experimental studies have reported that the neuronal geometry has an influence on electrical signaling and extracellular recordings. However, the corresponding mechanisms are not yet fully understood and require further investigation. Allowing systematic parameter studies, computational modeling provides the opportunity to examine the underlying effects that influence extracellular potentials. In this letter, we present an in silico single cell model to analyze the effect of geometrical variability on the extracellular electric potentials. We describe finite element models of a single neuron with varying geometric complexity in three-dimensional space. The electric potential generation of the neuron is modeled using Hodgkin-Huxley equations. The signal propagation is described with electro-quasi-static equations, and results are compared with corresponding cable equation descriptions. Our results show that both the geometric dimensions and the distribution of ion channels of a neuron are critical factors that significantly influence both the amplitude and shape of extracellular potentials."}], "ArticleTitle": "Effect of Morphologic Features of Neurons on the Extracellular Electric Potential: A Simulation Study Using Cable Theory and Electro-Quasi-Static Equations."}, "28333585": {"mesh": [], "AbstractText": [{"section": null, "text": "Many machine learning and data-related applications require the knowledge of approximate ranks of large data matrices at hand. This letter presents two computationally inexpensive techniques to estimate the approximate ranks of such matrices. These techniques exploit approximate spectral densities, popular in physics, which are probability density distributions that measure the likelihood of finding eigenvalues of the matrix at a given point on the real line. Integrating the spectral density over an interval gives the eigenvalue count of the matrix in that interval. Therefore, the rank can be approximated by integrating the spectral density over a carefully selected interval. Two different approaches are discussed to estimate the approximate rank, one based on Chebyshev polynomials and the other based on the Lanczos algorithm. In order to obtain the appropriate interval, it is necessary to locate a gap between the eigenvalues that correspond to noise and the relevant eigenvalues that contribute to the matrix rank. A method for locating this gap and selecting the interval of integration is proposed based on the plot of the spectral density. Numerical experiments illustrate the performance of these techniques on matrices from typical applications."}], "ArticleTitle": "Fast Estimation of Approximate Matrix Ranks Using Spectral Densities."}, "28599114": {"mesh": [], "AbstractText": [{"section": null, "text": "Much attention has been paid to the question of how Bayesian integration of information could be implemented by a simple neural mechanism. We show that population vectors based on point-process inputs combine evidence in a form that closely resembles Bayesian inference, with each input spike carrying information about the tuning of the input neuron. We also show that population vectors can combine information relatively accurately in the presence of noisy synaptic encoding of tuning curves."}], "ArticleTitle": "Population Vectors Can Provide Near Optimal Integration of Information."}, "28095202": {"mesh": ["Brain", "Brain Mapping", "Computer Simulation", "Electrocorticography", "Humans", "Models, Neurological", "Neural Pathways", "Nonlinear Dynamics", "Sleep", "Time Factors"], "AbstractText": [{"section": null, "text": "The correlation method from brain imaging has been used to estimate functional connectivity in the human brain. However, brain regions might show very high correlation even when the two regions are not directly connected due to the strong interaction of the two regions with common input from a third region. One previously proposed solution to this problem is to use a sparse regularized inverse covariance matrix or precision matrix (SRPM) assuming that the connectivity structure is sparse. This method yields partial correlations to measure strong direct interactions between pairs of regions while simultaneously removing the influence of the rest of the regions, thus identifying regions that are conditionally independent. To test our methods, we first demonstrated conditions under which the SRPM method could indeed find the true physical connection between a pair of nodes for a spring-mass example and an RC circuit example. The recovery of the connectivity structure using the SRPM method can be explained by energy models using the Boltzmann distribution. We then demonstrated the application of the SRPM method for estimating brain connectivity during stage 2 sleep spindles from human electrocorticography (ECoG) recordings using an [Formula: see text] electrode array. The ECoG recordings that we analyzed were from a 32-year-old male patient with long-standing pharmaco-resistant left temporal lobe complex partial epilepsy. Sleep spindles were automatically detected using delay differential analysis and then analyzed with SRPM and the Louvain method for community detection. We found spatially localized brain networks within and between neighboring cortical areas during spindles, in contrast to the case when sleep spindles were not present."}], "ArticleTitle": "Interpretation of the Precision Matrix and Its Application in Estimating Sparse Brain Connectivity during Sleep Spindles from Human Electrocorticography Recordings."}, "27764590": {"mesh": [], "AbstractText": [{"section": null, "text": "Function approximation in online, incremental, reinforcement learning needs to deal with two fundamental problems: biased sampling and nonstationarity. In this kind of task, biased sampling occurs because samples are obtained from specific trajectories dictated by the dynamics of the environment and are usually concentrated in particular convergence regions, which in the long term tend to dominate the approximation in the less sampled regions. The nonstationarity comes from the recursive nature of the estimations typical of temporal difference methods. This nonstationarity has a local profile, varying not only along the learning process but also along different regions of the state space. We propose to deal with these problems using an estimation of the probability density of samples represented with a gaussian mixture model. To deal with the nonstationarity problem, we use the common approach of introducing a forgetting factor in the updating formula. However, instead of using the same forgetting factor for the whole domain, we make it dependent on the local density of samples, which we use to estimate the nonstationarity of the function at any given input point. To address the biased sampling problem, the forgetting factor applied to each mixture component is modulated according to the new information provided in the updating, rather than forgetting depending only on time, thus avoiding undesired distortions of the approximation in less sampled regions."}], "ArticleTitle": "Online Reinforcement Learning Using a Probability Density Estimation."}, "27348735": {"mesh": [], "AbstractText": [{"section": null, "text": "Consider a self-motivated artificial agent who is exploring a complex environment. Part of the complexity is due to the raw high-dimensional sensory input streams, which the agent needs to make sense of. Such inputs can be compactly encoded through a variety of means; one of these is slow feature analysis (SFA). Slow features encode spatiotemporal regularities, which are information-rich explanatory factors (latent variables) underlying the high-dimensional input streams. In our previous work, we have shown how slow features can be learned incrementally, while the agent explores its world, and modularly, such that different sets of features are learned for different parts of the environment (since a single set of regularities does not explain everything). In what order should the agent explore the different parts of the environment? Following Schmidhuber's theory of artificial curiosity, the agent should always concentrate on the area where it can learn the easiest-to-learn set of features that it has not already learned. We formalize this learning problem and theoretically show that, using our model, called curiosity-driven modular incremental slow feature analysis, the agent on average will learn slow feature representations in order of increasing learning difficulty, under certain mild conditions. We provide experimental results to support the theoretical analysis."}], "ArticleTitle": "Optimal Curiosity-Driven Modular Incremental Slow Feature Analysis."}, "29162004": {"mesh": ["Algorithms", "Animals", "Classification", "Humans", "Likelihood Functions", "Markov Chains", "Models, Theoretical", "Time Factors"], "AbstractText": [{"section": null, "text": "Model-based classification of sequence data using a set of hidden Markov models is a well-known technique. The involved score function, which is often based on the class-conditional likelihood, can, however, be computationally demanding, especially for long data sequences. Inspired by recent theoretical advances in spectral learning of hidden Markov models, we propose a score function based on third-order moments. In particular, we propose to use the Kullback-Leibler divergence between theoretical and empirical third-order moments for classification of sequence data with discrete observations. The proposed method provides lower computational complexity at classification time than the usual likelihood-based methods. In order to demonstrate the properties of the proposed method, we perform classification of both simulated data and empirical data from a human activity recognition study."}], "ArticleTitle": "Sequence Classification Using Third-Order Moments."}, "27557103": {"mesh": ["Action Potentials", "Humans", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "In this letter, we propose a definition of the operational mode of a neuron, that is, whether a neuron integrates over its input or detects coincidences. We complete the range of possible operational modes by a new mode we call gap detection, which means that a neuron responds to gaps in its stimulus. We propose a measure consisting of two scalar values, both ranging from -1 to +1: the neural drive, which indicates whether its stimulus excites the neuron, serves as background noise, or inhibits it; the neural mode, which indicates whether the neuron's response is the result of integration over its input, of coincidence detection, or of gap detection; with all three modes possible for all neural drive values. This is a pure spike-based measure and can be applied to measure the influence of either all or subset of a neuron's stimulus. We derive the measure by decomposing the reverse correlation, test it in several artificial and biological settings, and compare it to other measures, finding little or no correlation between them. We relate the results of the measure to neural parameters and investigate the effect of time delay during spike generation. Our results suggest that a neuron can use several different modes simultaneously on different subsets of its stimulus to enable it to respond to its stimulus in a complex manner."}], "ArticleTitle": "Integrator or Coincidence Detector: A Novel Measure Based on the Discrete Reverse Correlation to Determine a Neuron's Operational Mode."}, "28957029": {"mesh": [], "AbstractText": [{"section": null, "text": "Learning useful information across long time lags is a critical and difficult problem for temporal neural models in tasks such as language modeling. Existing architectures that address the issue are often complex and costly to train. The differential state framework (DSF) is a simple and high-performing design that unifies previously introduced gated neural models. DSF models maintain longer-term memory by learning to interpolate between a fast-changing data-driven representation and a slowly changing, implicitly stable state. Within the DSF framework, a new architecture is presented, the delta-RNN. This model requires hardly any more parameters than a classical, simple recurrent network. In language modeling at the word and character levels, the delta-RNN outperforms popular complex architectures, such as the long short-term memory (LSTM) and the gated recurrent unit (GRU), and, when regularized, performs comparably to several state-of-the-art baselines. At the subword level, the delta-RNN's performance is comparable to that of complex gated architectures."}], "ArticleTitle": "Learning Simpler Language Models with the Differential State Framework."}, "28777717": {"mesh": [], "AbstractText": [{"section": null, "text": "Nonnegative matrix factorization (NMF) is well known to be an effective tool for dimensionality reduction in problems involving big data. For this reason, it frequently appears in many areas of scientific and engineering literature. This letter proposes a novel semisupervised NMF algorithm for overcoming a variety of problems associated with NMF algorithms, including poor use of prior information, negative impact on manifold structure of the sparse constraint, and inaccurate graph construction. Our proposed algorithm, nonnegative matrix factorization with rank regularization and hard constraint (NMFRC), incorporates label information into data representation as a hard constraint, which makes full use of prior information. NMFRC also measures pairwise similarity according to geodesic distance rather than Euclidean distance. This results in more accurate measurement of pairwise relationships, resulting in more effective manifold information. Furthermore, NMFRC adopts rank constraint instead of norm constraints for regularization to balance the sparseness and smoothness of data. In this way, the new data representation is more representative and has better interpretability. Experiments on real data sets suggest that NMFRC outperforms four other state-of-the-art algorithms in terms of clustering accuracy."}], "ArticleTitle": "Nonnegative Matrix Factorization with Rank Regularization and Hard Constraint."}, "27557105": {"mesh": [], "AbstractText": [{"section": null, "text": "The brain consists of specialized cortical regions that exchange information between each other, reflecting a combination of segregated (local) and integrated (distributed) processes that define brain function. Functional magnetic resonance imaging (fMRI) is widely used to characterize these functional relationships, although it is an ongoing challenge to develop robust, interpretable models for high-dimensional fMRI data. Gaussian mixture models (GMMs) are a powerful tool for parcellating the brain, based on the similarity of voxel time series. However, conventional GMMs have limited parametric flexibility: they only estimate segregated structure and do not model interregional functional connectivity, nor do they account for network variability across voxels or between subjects. To address these issues, this letter develops the functional segregation and integration model (FSIM). This extension of the GMM framework simultaneously estimates spatial clustering and the most consistent group functional connectivity structure. It also explicitly models network variability, based on voxel- and subject-specific network scaling profiles. We compared the FSIM to standard GMM in a predictive cross-validation framework and examined the importance of different model parameters, using both simulated and experimental resting-state data. The reliability of parcellations is not significantly altered by flexibility of the FSIM, whereas voxel- and subject-specific network scaling profiles significantly improve the ability to predict functional connectivity in independent test data. Moreover, the FSIM provides a set of interpretable parameters to characterize both consistent and variable aspects functional connectivity structure. As an example of its utility, we use subject-specific network profiles to identify brain regions where network expression predicts subject age in the experimental data. Thus, the FSIM is effective at summarizing functional connectivity structure in group-level fMRI, with applications in modeling the relationships between network variability and behavioral/demographic variables."}], "ArticleTitle": "The Functional Segregation and Integration Model: Mixture Model Representations of Consistent and Variable Group-Level Connectivity in fMRI."}, "29652581": {"mesh": [], "AbstractText": [{"section": null, "text": "Many recent generative models make use of neural networks to transform the probability distribution of a simple low-dimensional noise process into the complex distribution of the data. This raises the question of whether biological networks operate along similar principles to implement a probabilistic model of the environment through transformations of intrinsic noise processes. The intrinsic neural and synaptic noise processes in biological networks, however, are quite different from the noise processes used in current abstract generative networks. This, together with the discrete nature of spikes and local circuit interactions among the neurons, raises several difficulties when using recent generative modeling frameworks to train biologically motivated models. In this letter, we show that a biologically motivated model based on multilayer winner-take-all circuits and stochastic synapses admits an approximate analytical description. This allows us to use the proposed networks in a variational learning setting where stochastic backpropagation is used to optimize a lower bound on the data log likelihood, thereby learning a generative model of the data. We illustrate the generality of the proposed networks and learning technique by using them in a structured output prediction task and a semisupervised learning task. Our results extend the domain of application of modern stochastic network architectures to networks where synaptic transmission failure is the principal noise mechanism."}], "ArticleTitle": "A Learning Framework for Winner-Take-All Networks with Stochastic Synapses."}, "28095203": {"mesh": [], "AbstractText": [{"section": null, "text": "Robust principal component analysis (PCA) is one of the most important dimension-reduction techniques for handling high-dimensional data with outliers. However, most of the existing robust PCA presupposes that the mean of the data is zero and incorrectly utilizes the average of data as the optimal mean of robust PCA. In fact, this assumption holds only for the squared [Formula: see text]-norm-based traditional PCA. In this letter, we equivalently reformulate the objective of conventional PCA and learn the optimal projection directions by maximizing the sum of projected difference between each pair of instances based on [Formula: see text]-norm. The proposed method is robust to outliers and also invariant to rotation. More important, the reformulated objective not only automatically avoids the calculation of optimal mean and makes the assumption of centered data unnecessary, but also theoretically connects to the minimization of reconstruction error. To solve the proposed nonsmooth problem, we exploit an efficient optimization algorithm to soften the contributions from outliers by reweighting each data point iteratively. We theoretically analyze the convergence and computational complexity of the proposed algorithm. Extensive experimental results on several benchmark data sets illustrate the effectiveness and superiority of the proposed method."}], "ArticleTitle": "Avoiding Optimal Mean &#8467;2,1-Norm Maximization-Based Robust PCA for Reconstruction."}, "27348511": {"mesh": ["Algorithms", "Likelihood Functions", "Linear Models", "Models, Statistical", "Signal Transduction"], "AbstractText": [{"section": null, "text": "A unified approach to nonnegative matrix factorization based on the theory of generalized linear models is proposed. This approach embeds a variety of statistical models, including the exponential family, within a single theoretical framework and provides a unified view of such factorizations from the perspective of quasi-likelihood. Using this framework, a family of algorithms for handling signal-dependent noise is developed and its convergence proved using the expectation-maximization algorithm. In addition, a measure to evaluate the goodness of fit of the resulting factorization is described. The proposed methods allow modeling of nonlinear effects using appropriate link functions and are illustrated using an application in biomedical signal processing."}], "ArticleTitle": "A Quasi-Likelihood Approach to Nonnegative Matrix Factorization."}, "27391677": {"mesh": [], "AbstractText": [{"section": null, "text": "This letter investigates the supervised learning problem with observations drawn from certain general stationary stochastic processes. Here by general, we mean that many stationary stochastic processes can be included. We show that when the stochastic processes satisfy a generalized Bernstein-type inequality, a unified treatment on analyzing the learning schemes with various mixing processes can be conducted and a sharp oracle inequality for generic regularized empirical risk minimization schemes can be established. The obtained oracle inequality is then applied to derive convergence rates for several learning schemes such as empirical risk minimization (ERM), least squares support vector machines (LS-SVMs) using given generic kernels, and SVMs using gaussian kernels for both least squares and quantile regression. It turns out that for independent and identically distributed (i.i.d.) processes, our learning rates for ERM recover the optimal rates. For non-i.i.d. processes, including geometrically [Formula: see text]-mixing Markov processes, geometrically [Formula: see text]-mixing processes with restricted decay, [Formula: see text]-mixing processes, and (time-reversed) geometrically [Formula: see text]-mixing processes, our learning rates for SVMs with gaussian kernels match, up to some arbitrarily small extra term in the exponent, the optimal rates. For the remaining cases, our rates are at least close to the optimal rates. As a by-product, the assumed generalized Bernstein-type inequality also provides an interpretation of the so-called effective number of observations for various mixing processes."}], "ArticleTitle": "Learning Theory Estimates with Observations from General Stationary Stochastic Processes."}, "33400902": {"mesh": ["Bias", "Machine Learning", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Recent work suggests that changing convolutional neural network (CNN) architecture by introducing a bottleneck in the second layer can yield changes in learned function. To understand this relationship fully requires a way of quantitatively comparing trained networks. The fields of electrophysiology and psychophysics have developed a wealth of methods for characterizing visual systems that permit such comparisons. Inspired by these methods, we propose an approach to obtaining spatial and color tuning curves for convolutional neurons that can be used to classify cells in terms of their spatial and color opponency. We perform these classifications for a range of CNNs with different depths and bottleneck widths. Our key finding is that networks with a bottleneck show a strong functional organization: almost all cells in the bottleneck layer become both spatially and color opponent, and cells in the layer following the bottleneck become nonopponent. The color tuning data can further be used to form a rich understanding of how color a network encodes color. As a concrete demonstration, we show that shallower networks without a bottleneck learn a complex nonlinear color system, whereas deeper networks with tight bottlenecks learn a simple channel opponent code in the bottleneck layer. We develop a method of obtaining a hue sensitivity curve for a trained CNN that enables high-level insights that complement the low-level findings from the color tuning data. We go on to train a series of networks under different conditions to ascertain the robustness of the discussed results. Ultimately our methods and findings coalesce with prior art, strengthening our ability to interpret trained CNNs and furthering our understanding of the connection between architecture and learned representation. Trained models and code for all experiments are available at https://github.com/ecs-vlc/opponency."}], "ArticleTitle": "How Convolutional Neural Network Architecture Biases Learned Opponency and Color Tuning."}, "27557099": {"mesh": ["Action Potentials", "Animals", "Computer Simulation", "Models, Neurological", "Neurons", "Prefrontal Cortex", "Rats", "Stochastic Processes"], "AbstractText": [{"section": null, "text": "We consider a classical space-clamped Hodgkin-Huxley model neuron stimulated by synaptic excitation and inhibition with conductances represented by Ornstein-Uhlenbeck processes. Using numerical solutions of the stochastic model system obtained by an Euler method, it is found that with excitation only, there is a critical value of the steady-state excitatory conductance for repetitive spiking without noise, and for values of the conductance near the critical value, small noise has a powerfully inhibitory effect. For a given level of inhibition, there is also a critical value of the steady-state excitatory conductance for repetitive firing, and it is demonstrated that noise in either the excitatory or inhibitory processes or both can powerfully inhibit spiking. Furthermore, near the critical value, inverse stochastic resonance was observed when noise was present only in the inhibitory input process. The system of deterministic differential equations for the approximate first- and second-order moments of the model is derived. They are solved using Runge-Kutta methods, and the solutions are compared with the results obtained by simulation for various sets of parameters, including some with conductances obtained by experiment on pyramidal cells of rat prefrontal cortex. The mean and variance obtained from simulation are in good agreement when there is spiking induced by strong stimulation and relatively small noise or when the voltage is fluctuating at subthreshold levels. In the occasional spike mode sometimes exhibited by spinal motoneurons and cortical pyramidal cells, the assumptions underlying the moment equation approach are not satisfied. The simulation results show that noisy synaptic input of either an excitatory or inhibitory character or both may lead to the suppression of firing in neurons operating near a critical point and this has possible implications for cortical networks. Although suppression of firing is corroborated for the system of moment equations, there seem to be substantial differences between the dynamical properties of the original system of stochastic differential equations and the much larger system of moment equations."}], "ArticleTitle": "The Space-Clamped Hodgkin-Huxley System with Random Synaptic Input: Inhibition of Spiking by Weak Noise and Analysis with Moment Equations."}, "27391687": {"mesh": [], "AbstractText": [{"section": null, "text": "This work is part of an effort to understand the neural basis for our visual system's ability, or failure, to accurately track moving visual signals. We consider here a ring model of spiking neurons, intended as a simplified computational model of a single hypercolumn of the primary visual cortex of primates. Signals that consist of edges with time-varying orientations localized in space are considered. Our model is calibrated to produce spontaneous and driven firing rates roughly consistent with experiments, and our two main findings, for which we offer dynamical explanation on the level of neuronal interactions, are the following. First, we have documented consistent transient overshoots in signal perception following signal switches due to emergent interactions of the E- and I-populations. Second, for continuously moving signals, we have found that accuracy is considerably lower at reversals of orientation than when continuing in the same direction (as when the signal is a rotating bar). To measure performance, we use two metrics, called fidelity and reliability, to compare signals reconstructed by the system to the ones presented and assess trial-to-trial variability. We propose that the same population mechanisms responsible for orientation selectivity also impose constraints on dynamic signal tracking that manifest in perception failures consistent with psychophysical observations."}], "ArticleTitle": "Dynamic Signal Tracking in a Simple V1 Spiking Model."}, "30216145": {"mesh": [], "AbstractText": [{"section": null, "text": "We consider Bayesian inference problems with computationally intensive likelihood functions. We propose a Gaussian process (GP)-based method to approximate the joint distribution of the unknown parameters and the data, built on recent work (Kandasamy, Schneider, & P&#243;czos, 2015). In particular, we write the joint density approximately as a product of an approximate posterior density and an exponentiated GP surrogate. We then provide an adaptive algorithm to construct such an approximation, where an active learning method is used to choose the design points. With numerical examples, we illustrate that the proposed method has competitive performance against existing approaches for Bayesian computation."}], "ArticleTitle": "Adaptive Gaussian Process Approximation for Bayesian Inference with Expensive Likelihood Functions."}, "30576612": {"mesh": [], "AbstractText": [{"section": null, "text": "Specialization and hierarchical organization are important features of efficient collaboration in economical, artificial, and biological systems. Here, we investigate the hypothesis that both features can be explained by the fact that each entity of such a system is limited in a certain way. We propose an information-theoretic approach based on a free energy principle in order to computationally analyze systems of bounded rational agents that deal with such limitations optimally. We find that specialization allows a focus on fewer tasks, thus leading to a more efficient execution, but in turn, it requires coordination in hierarchical structures of specialized experts and coordinating units. Our results suggest that hierarchical architectures of specialized units at lower levels that are coordinated by units at higher levels are optimal, given that each unit's information-processing capability is limited and conforms to constraints on complexity costs."}], "ArticleTitle": "Systems of Bounded Rational Agents with Information-Theoretic Constraints."}, "29162007": {"mesh": [], "AbstractText": [{"section": null, "text": "In recent years, multilabel classification has attracted significant attention in multimedia annotation. However, most of the multilabel classification methods focus only on the inherent correlations existing among multiple labels and concepts and ignore the relevance between features and the target concepts. To obtain more robust multilabel classification results, we propose a new multilabel classification method aiming to capture the correlations among multiple concepts by leveraging hypergraph that is proved to be beneficial for relational learning. Moreover, we consider mining feature-concept relevance, which is often overlooked by many multilabel learning algorithms. To better show the feature-concept relevance, we impose a sparsity constraint on the proposed method. We compare the proposed method with several other multilabel classification methods and evaluate the classification performance by mean average precision on several data sets. The experimental results show that the proposed method outperforms the state-of-the-art methods."}], "ArticleTitle": "Joint Concept Correlation and Feature-Concept Relevance Learning for Multilabel Classification."}, "33513322": {"mesh": ["Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "Empirical estimates of the dimensionality of neural population activity are often much lower than the population size. Similar phenomena are also observed in trained and designed neural network models. These experimental and computational results suggest that mapping low-dimensional dynamics to high-dimensional neural space is a common feature of cortical computation. Despite the ubiquity of this observation, the constraints arising from such mapping are poorly understood. Here we consider a specific example of mapping low-dimensional dynamics to high-dimensional neural activity-the neural engineering framework. We analytically solve the framework for the classic ring model-a neural network encoding a static or dynamic angular variable. Our results provide a complete characterization of the success and failure modes for this model. Based on similarities between this and other frameworks, we speculate that these results could apply to more general scenarios."}], "ArticleTitle": "Mapping Low-Dimensional Dynamics to High-Dimensional Neural Activity: A Derivation of the Ring Model From the Neural Engineering Framework."}, "26942745": {"mesh": [], "AbstractText": [{"section": null, "text": "Contamination of scattered observations, which are either featureless or unlike the other observations, frequently degrades the performance of standard methods such as K-means and model-based clustering. In this letter, we propose a robust clustering method in the presence of scattered observations called Gamma-clust. Gamma-clust is based on a robust estimation for cluster centers using gamma-divergence. It provides a proper solution for clustering in which the distributions for clustered data are nonnormal, such as t-distributions with different variance-covariance matrices and degrees of freedom. As demonstrated in a simulation study and data analysis, Gamma-clust is more flexible and provides superior results compared to the robustified K-means and model-based clustering."}], "ArticleTitle": "Robust Clustering Method in the Presence of Scattered Observations."}, "29381444": {"mesh": [], "AbstractText": [{"section": null, "text": "Although nonstationary data are more common in the real world, most existing causal discovery methods do not take nonstationarity into consideration. In this letter, we propose a kernel embedding-based approach, ENCI, for nonstationary causal model inference where data are collected from multiple domains with varying distributions. In ENCI, we transform the complicated relation of a cause-effect pair into a linear model of variables of which observations correspond to the kernel embeddings of the cause-and-effect distributions in different domains. In this way, we are able to estimate the causal direction by exploiting the causal asymmetry of the transformed linear model. Furthermore, we extend ENCI to causal graph discovery for multiple variables by transforming the relations among them into a linear nongaussian acyclic model. We show that by exploiting the nonstationarity of distributions, both cause-effect pairs and two kinds of causal graphs are identifiable under mild conditions. Experiments on synthetic and real-world data are conducted to justify the efficacy of ENCI over major existing methods."}], "ArticleTitle": "A Kernel Embedding-Based Approach for Nonstationary Causal Model Inference."}, "29566347": {"mesh": [], "AbstractText": [{"section": null, "text": "Most community question answering (CQA) websites manage plenty of question-answer pairs (QAPs) through topic-based organizations, which may not satisfy users' fine-grained search demands. Facets of topics serve as a powerful tool to navigate, refine, and group the QAPs. In this work, we propose FACM, a model to annotate QAPs with facets by extending convolution neural networks (CNNs) with a matching strategy. First, phrase information is incorporated into text representation by CNNs with different kernel sizes. Then, through a matching strategy among QAPs and facet label texts (FaLTs) acquired from Wikipedia, we generate similarity matrices to deal with the facet heterogeneity. Finally, a three-channel CNN is trained for facet label assignment of QAPs. Experiments on three real-world data sets show that FACM outperforms the state-of-the-art methods."}], "ArticleTitle": "Facet Annotation by Extending CNN with a Matching Strategy."}, "29894656": {"mesh": [], "AbstractText": [{"section": null, "text": "We explore classifier training for data sets with very few labels. We investigate this task using a neural network for nonnegative data. The network is derived from a hierarchical normalized Poisson mixture model with one observed and two hidden layers. With the single objective of likelihood optimization, both labeled and unlabeled data are naturally incorporated into learning. The neural activation and learning equations resulting from our derivation are concise and local. As a consequence, the network can be scaled using standard deep learning tools for parallelized GPU implementation. Using standard benchmarks for nonnegative data, such as text document representations, MNIST, and NIST SD19, we study the classification performance when very few labels are used for training. In different settings, the network's performance is compared to standard and recently suggested semisupervised classifiers. While other recent approaches are more competitive for many labels or fully labeled data sets, we find that the network studied here can be applied to numbers of few labels where no other system has been reported to operate so far."}], "ArticleTitle": "Neural Simpletrons: Learning in the Limit of Few Labels with Directed Generative Networks."}, "30021080": {"mesh": [], "AbstractText": [{"section": null, "text": "Although the number of artificial neural network and machine learning architectures is growing at an exponential pace, more attention needs to be paid to theoretical guarantees of asymptotic convergence for novel, nonlinear, high-dimensional adaptive learning algorithms. When properly understood, such guarantees can guide the algorithm development and evaluation process and provide theoretical validation for a particular algorithm design. For many decades, the machine learning community has widely recognized the importance of stochastic approximation theory as a powerful tool for identifying explicit convergence conditions for adaptive learning machines. However, the verification of such conditions is challenging for multidisciplinary researchers not working in the area of stochastic approximation theory. For this reason, this letter presents a new stochastic approximation theorem for both passive and reactive learning environments with assumptions that are easily verifiable. The theorem is widely applicable to the analysis and design of important machine learning algorithms including deep learning algorithms with multiple strict local minimizers, Monte Carlo expectation-maximization algorithms, contrastive divergence learning in Markov fields, and policy gradient reinforcement learning."}], "ArticleTitle": "Adaptive Learning Algorithm Convergence in Passive and Reactive Environments."}, "27136704": {"mesh": [], "AbstractText": [{"section": null, "text": "In this letter, we consider the nonnegative matrix factorization (NMF) problem and several NMF variants. Two approaches based on DC (difference of convex functions) programming and DCA (DC algorithm) are developed. The first approach follows the alternating framework that requires solving, at each iteration, two nonnegativity-constrained least squares subproblems for which DCA-based schemes are investigated. The convergence property of the proposed algorithm is carefully studied. We show that with suitable DC decompositions, our algorithm generates most of the standard methods for the NMF problem. The second approach directly applies DCA on the whole NMF problem. Two algorithms-one computing all variables and one deploying a variable selection strategy-are proposed. The proposed methods are then adapted to solve various NMF variants, including the nonnegative factorization, the smooth regularization NMF, the sparse regularization NMF, the multilayer NMF, the convex/convex-hull NMF, and the symmetric NMF. We also show that our algorithms include several existing methods for these NMF variants as special versions. The efficiency of the proposed approaches is empirically demonstrated on both real-world and synthetic data sets. It turns out that our algorithms compete favorably with five state-of-the-art alternating nonnegative least squares algorithms."}], "ArticleTitle": "Efficient Nonnegative Matrix Factorization by DC Programming and DCA."}, "27557107": {"mesh": [], "AbstractText": [{"section": null, "text": "Polychronous neuronal group (PNG), a type of cell assembly, is one of the putative mechanisms for neural information representation. According to the reader-centric definition, some readout neurons can become selective to the information represented by polychronous neuronal groups under ongoing activity. Here, in computational models, we show that the frequently activated polychronous neuronal groups can be learned by readout neurons with joint weight-delay spike-timing-dependent plasticity. The identity of neurons in the group and their expected spike timing at millisecond scale can be recovered from the incoming weights and delays of the readout neurons. The detection performance can be further improved by two layers of readout neurons. In this way, the detection of polychronous neuronal groups becomes an intrinsic part of the network, and the readout neurons become differentiated members in the group to indicate whether subsets of the group have been activated according to their spike timing. The readout spikes representing this information can be used to analyze how PNGs interact with each other or propagate to downstream networks for higher-level processing."}], "ArticleTitle": "Learning Polychronous Neuronal Groups Using Joint Weight-Delay Spike-Timing-Dependent Plasticity."}, "27764591": {"mesh": [], "AbstractText": [{"section": null, "text": "This letter considers the problem of dictionary learning for sparse signal representation whose atoms have low mutual coherence. To learn such dictionaries, at each step, we first update the dictionary using the method of optimal directions (MOD) and then apply a dictionary rank shrinkage step to decrease its mutual coherence. In the rank shrinkage step, we first compute a rank 1 decomposition of the column-normalized least squares estimate of the dictionary obtained from the MOD step. We then shrink the rank of this learned dictionary by transforming the problem of reducing the rank to a nonnegative garrotte estimation problem and solving it using a path-wise coordinate descent approach. We establish theoretical results that show that the rank shrinkage step included will reduce the coherence of the dictionary, which is further validated by experimental results. Numerical experiments illustrating the performance of the proposed algorithm in comparison to various other well-known dictionary learning algorithms are also presented."}], "ArticleTitle": "Improving the Incoherence of a Learned Dictionary via Rank Shrinkage."}, "27171012": {"mesh": [], "AbstractText": [{"section": null, "text": "We present a mathematical construction for the restricted Boltzmann machine (RBM) that does not require specifying the number of hidden units. In fact, the hidden layer size is adaptive and can grow during training. This is obtained by first extending the RBM to be sensitive to the ordering of its hidden units. Then, with a carefully chosen definition of the energy function, we show that the limit of infinitely many hidden units is well defined. As with RBM, approximate maximum likelihood training can be performed, resulting in an algorithm that naturally and adaptively adds trained hidden units during learning. We empirically study the behavior of this infinite RBM, showing that its performance is competitive to that of the RBM, while not requiring the tuning of a hidden layer size."}], "ArticleTitle": "An Infinite Restricted Boltzmann Machine."}, "27626968": {"mesh": [], "AbstractText": [{"section": null, "text": "Linear submodular bandits has been proven to be effective in solving the diversification and feature-based exploration problem in information retrieval systems. Considering there is inevitably a budget constraint in many web-based applications, such as news article recommendations and online advertising, we study the problem of diversification under a budget constraint in a bandit setting. We first introduce a budget constraint to each exploration step of linear submodular bandits as a new problem, which we call per-round knapsack-constrained linear submodular bandits. We then define an [Formula: see text]-approximation unit-cost regret considering that the submodular function maximization is NP-hard. To solve this new problem, we propose two greedy algorithms based on a modified UCB rule. We prove these two algorithms with different regret bounds and computational complexities. Inspired by the lazy evaluation process in submodular function maximization, we also prove that a modified lazy evaluation process can be used to accelerate our algorithms without losing their theoretical guarantee. We conduct a number of experiments, and the experimental results confirm our theoretical analyses."}], "ArticleTitle": "Per-Round Knapsack-Constrained Linear Submodular Bandits."}, "29162003": {"mesh": [], "AbstractText": [{"section": null, "text": "Recurrent neural network architectures can have useful computational properties, with complex temporal dynamics and input-sensitive attractor states. However, evaluation of recurrent dynamic architectures requires solving systems of differential equations, and the number of evaluations required to determine their response to a given input can vary with the input or can be indeterminate altogether in the case of oscillations or instability. In feedforward networks, by contrast, only a single pass through the network is needed to determine the response to a given input. Modern machine learning systems are designed to operate efficiently on feedforward architectures. We hypothesized that two-layer feedforward architectures with simple, deterministic dynamics could approximate the responses of single-layer recurrent network architectures. By identifying the fixed-point responses of a given recurrent network, we trained two-layer networks to directly approximate the fixed-point response to a given input. These feedforward networks then embodied useful computations, including competitive interactions, information transformations, and noise rejection. Our approach was able to find useful approximations to recurrent networks, which can then be evaluated in linear and deterministic time complexity."}], "ArticleTitle": "Feedforward Approximations to Dynamic Recurrent Network Architectures."}, "28333584": {"mesh": [], "AbstractText": [{"section": null, "text": "In recent years, unsupervised two-dimensional (2D) dimensionality reduction methods for unlabeled large-scale data have made progress. However, performance of these degrades when the learning of similarity matrix is at the beginning of the dimensionality reduction process. A similarity matrix is used to reveal the underlying geometry structure of data in unsupervised dimensionality reduction methods. Because of noise data, it is difficult to learn the optimal similarity matrix. In this letter, we propose a new dimensionality reduction model for 2D image matrices: unsupervised 2D dimensionality reduction with adaptive structure learning (DRASL). Instead of using a predetermined similarity matrix to characterize the underlying geometry structure of the original 2D image space, our proposed approach involves the learning of a similarity matrix in the procedure of dimensionality reduction. To realize a desirable neighbors assignment after dimensionality reduction, we add a constraint to our model such that there are exact [Formula: see text] connected components in the final subspace. To accomplish these goals, we propose a unified objective function to integrate dimensionality reduction, the learning of the similarity matrix, and the adaptive learning of neighbors assignment into it. An iterative optimization algorithm is proposed to solve the objective function. We compare the proposed method with several 2D unsupervised dimensionality methods. K-means is used to evaluate the clustering performance. We conduct extensive experiments on Coil20, AT&T, FERET, USPS, and Yale data sets to verify the effectiveness of our proposed method."}], "ArticleTitle": "Unsupervised 2D Dimensionality Reduction with Adaptive Structure Learning."}, "28181875": {"mesh": [], "AbstractText": [{"section": null, "text": "Emotional disorders and psychological flourishing are the result of complex interactions between positive and negative affects that depend on external events and the subject's internal representations. Based on psychological data, we mathematically model the dynamical balance between positive and negative affects as a function of the response to external positive and negative events. This modeling allows the investigation of the relative impact of two leading forms of therapy on affect balance. The model uses a delay differential equation to analytically study the bifurcation diagram of the system. We compare the results of the model to psychological data on a single, recurrently depressed patient who was administered the two types of therapies considered (coping focused versus affect focused). The model leads to the prediction that stabilization at a normal state may rely on evaluating one's emotional state through a historical ongoing emotional state rather than in a narrow present window. The simple mathematical model proposed here offers a theoretical framework for investigating the temporal process of change and parameters of resilience to relapse."}], "ArticleTitle": "On the Dynamical Interplay of Positive and Negative Affects."}, "29894657": {"mesh": [], "AbstractText": [{"section": null, "text": "Estimation of a generating partition is critical for symbolization of measurements from discrete-time dynamical systems, where a sequence of symbols from a (finite-cardinality) alphabet may uniquely specify the underlying time series. Such symbolization is useful for computing measures (e.g., Kolmogorov-Sinai entropy) to identify or characterize the (possibly unknown) dynamical system. It is also useful for time series classification and anomaly detection. The seminal work of Hirata, Judd, and Kilminster ( 2004 ) derives a novel objective function, akin to a clustering objective, that measures the discrepancy between a set of reconstruction values and the points from the time series. They cast estimation of a generating partition via the minimization of their objective function. Unfortunately, their proposed algorithm is nonconvergent, with no guarantee of finding even locally optimal solutions with respect to their objective. The difficulty is a heuristic nearest neighbor symbol assignment step. Alternatively, we develop a novel, locally optimal algorithm for their objective. We apply iterative nearest-neighbor symbol assignments with guaranteed discrepancy descent, by which joint, locally optimal symbolization of the entire time series is achieved. While most previous approaches frame generating partition estimation as a state-space partitioning problem, we recognize that minimizing the Hirata et al. ( 2004 ) objective function does not induce an explicit partitioning of the state space, but rather the space consisting of the entire time series (effectively, clustering in a (countably) infinite-dimensional space). Our approach also amounts to a novel type of sliding block lossy source coding. Improvement, with respect to several measures, is demonstrated over popular methods for symbolizing chaotic maps. We also apply our approach to time-series anomaly detection, considering both chaotic maps and failure application in a polycrystalline alloy material."}], "ArticleTitle": "A Locally Optimal Algorithm for Estimating a Generating Partition from an Observed Time Series and Its Application to Anomaly Detection."}, "29652583": {"mesh": [], "AbstractText": [{"section": null, "text": "This letter suggests two new types of asymmetrical higher-order kernels (HOK) that are generated using the orthogonal polynomials Laguerre (positive or right skew) and Bessel (negative or left skew). These skewed HOK are implemented in the blind source separation/independent component analysis (BSS/ICA) algorithm. The tests for these proposed HOK are accomplished using three scenarios to simulate a real environment using actual sound sources, an environment of mixtures of multimodal fast-changing probability density function (pdf) sources that represent a challenge to the symmetrical HOK, and an environment of an adverse case (near gaussian). The separation is performed by minimizing the mutual information (MI) among the mixed sources. The performance of the skewed kernels is compared to the performance of the standard kernels such as Epanechnikov, bisquare, trisquare, and gaussian and the performance of the symmetrical HOK generated using the polynomials Chebyshev1, Chebyshev2, Gegenbauer, Jacobi, and Legendre to the tenth order. The gaussian HOK are generated using the Hermite polynomial and the Wand and Schucany procedure. The comparison among the 96 kernels is based on the average intersymbol interference ratio (AISIR) and the time needed to complete the separation. In terms of AISIR, the skewed kernels' performance is better than that of the standard kernels and rivals most of the symmetrical kernels' performance. The importance of these new skewed HOK is manifested in the environment of the multimodal pdf mixtures. In such an environment, the skewed HOK come in first place compared with the symmetrical HOK. These new families can substitute for symmetrical HOKs in such applications."}], "ArticleTitle": "New Families of Skewed Higher-Order Kernel Estimators to Solve the BSS/ICA Problem for Multimodal Sources Mixtures."}, "29949463": {"mesh": [], "AbstractText": [{"section": null, "text": "Neural decoding may be formulated as dynamic state estimation (filtering) based on point-process observations, a generally intractable problem. Numerical sampling techniques are often practically useful for the decoding of real neural data. However, they are less useful as theoretical tools for modeling and understanding sensory neural systems, since they lead to limited conceptual insight into optimal encoding and decoding strategies. We consider sensory neural populations characterized by a distribution over neuron parameters. We develop an analytically tractable Bayesian approximation to optimal filtering based on the observation of spiking activity that greatly facilitates the analysis of optimal encoding in situations deviating from common assumptions of uniform coding. Continuous distributions are used to approximate large populations with few parameters, resulting in a filter whose complexity does not grow with population size and allowing optimization of population parameters rather than individual tuning functions. Numerical comparison with particle filtering demonstrates the quality of the approximation. The analytic framework leads to insights that are difficult to obtain from numerical algorithms and is consistent with biological observations about the distribution of sensory cells' preferred stimuli."}], "ArticleTitle": "Optimal Decoding of Dynamic Stimuli by Heterogeneous Populations of Spiking Neurons: A Closed-Form Approximation."}, "27391682": {"mesh": [], "AbstractText": [{"section": null, "text": "We propose an approach for learning latent directed polytrees as long as there exists an appropriately defined discrepancy measure between the observed nodes. Specifically, we use our approach for learning directed information polytrees where samples are available from only a subset of processes. Directed information trees are a new type of probabilistic graphical models that represent the causal dynamics among a set of random processes in a stochastic system. We prove that the approach is consistent for learning minimal latent directed trees. We analyze the sample complexity of the learning task when the empirical estimator of mutual information is used as the discrepancy measure."}], "ArticleTitle": "Learning Minimal Latent Directed Information Polytrees."}, "30148704": {"mesh": [], "AbstractText": [{"section": null, "text": "Modeling and interpreting spike train data is a task of central importance in computational neuroscience, with significant translational implications. Two popular classes of data-driven models for this task are autoregressive point-process generalized linear models (PPGLM) and latent state-space models (SSM) with point-process observations. In this letter, we derive a mathematical connection between these two classes of models. By introducing an auxiliary history process, we represent exactly a PPGLM in terms of a latent, infinite-dimensional dynamical system, which can then be mapped onto an SSM by basis function projections and moment closure. This representation provides a new perspective on widely used methods for modeling spike data and also suggests novel algorithmic approaches to fitting such models. We illustrate our results on a phasic bursting neuron model, showing that our proposed approach provides an accurate and efficient way to capture neural dynamics."}], "ArticleTitle": "Autoregressive Point Processes as Latent State-Space Models: A Moment-Closure Approach to Fluctuations and Autocorrelations."}, "33617744": {"mesh": ["Algorithms", "Feedback", "Neural Networks, Computer", "Neurons", "Supervised Machine Learning"], "AbstractText": [{"section": null, "text": "We propose a novel biologically plausible solution to the credit assignment problem motivated by observations in the ventral visual pathway and trained deep neural networks. In both, representations of objects in the same category become progressively more similar, while objects belonging to different categories become less similar. We use this observation to motivate a layer-specific learning goal in a deep network: each layer aims to learn a representational similarity matrix that interpolates between previous and later layers. We formulate this idea using a contrastive similarity matching objective function and derive from it deep neural networks with feedforward, lateral, and feedback connections and neurons that exhibit biologically plausible Hebbian and anti-Hebbian plasticity. Contrastive similarity matching can be interpreted as an energy-based learning algorithm, but with significant differences from others in how a contrastive function is constructed."}], "ArticleTitle": "Contrastive Similarity Matching for Supervised Learning."}, "28030775": {"mesh": ["Computer Simulation", "Neural Networks, Computer"], "AbstractText": [{"section": null, "text": "This letter studies the multistability analysis of delayed recurrent neural networks with Mexican hat activation function. Some sufficient conditions are obtained to ensure that an [Formula: see text]-dimensional recurrent neural network can have [Formula: see text] equilibrium points with [Formula: see text], and [Formula: see text] of them are locally exponentially stable. Furthermore, the attraction basins of these stable equilibrium points are estimated. We show that the attraction basins of these stable equilibrium points can be larger than their originally partitioned subsets. The results of this letter improve and extend the existing stability results in the literature. Finally, a numerical example containing different cases is given to illustrate the theoretical results."}], "ArticleTitle": "Multistability of Delayed Recurrent Neural Networks with Mexican Hat Activation Functions."}, "27391686": {"mesh": ["Humans", "Learning", "Memory", "Models, Neurological", "Neuronal Plasticity", "Synapses"], "AbstractText": [{"section": null, "text": "Integrate-and-express models of synaptic plasticity propose that synapses may act as low-pass filters, integrating synaptic plasticity induction signals in order to discern trends before expressing synaptic plasticity. We have previously shown that synaptic filtering strongly controls destabilizing fluctuations in developmental models. When applied to palimpsest memory systems that learn new memories by forgetting old ones, we have also shown that with binary-strength synapses, integrative synapses lead to an initial memory signal rise before its fall back to equilibrium. Such an initial rise is in dramatic contrast to nonintegrative synapses, in which the memory signal falls monotonically. We now extend our earlier analysis of palimpsest memories with synaptic filters to consider the more general case of discrete state, multilevel synapses. We derive exact results for the memory signal dynamics and then consider various simplifying approximations. We show that multilevel synapses enhance the initial rise in the memory signal and then delay its subsequent fall by inducing a plateau-like region in the memory signal. Such dynamics significantly increase memory lifetimes, defined by a signal-to-noise ratio (SNR). We derive expressions for optimal choices of synaptic parameters (filter size, number of strength states, number of synapses) that maximize SNR memory lifetimes. However, we find that with memory lifetimes defined via mean-first-passage times, such optimality conditions do not exist, suggesting that optimality may be an artifact of SNRs."}], "ArticleTitle": "The Enhanced Rise and Delayed Fall of Memory in a Model of Synaptic Integration: Extension to Discrete State Synapses."}, "27137357": {"mesh": [], "AbstractText": [{"section": null, "text": "This letter addresses the robustness problem when learning a large margin classifier in the presence of label noise. In our study, we achieve this purpose by proposing robustified large margin support vector machines. The robustness of the proposed robust support vector classifiers (RSVC), which is interpreted from a weighted viewpoint in this work, is due to the use of nonconvex classification losses. Besides the robustness, we also show that the proposed RSCV is simultaneously smooth, which again benefits from using smooth classification losses. The idea of proposing RSVC comes from M-estimation in statistics since the proposed robust and smooth classification losses can be taken as one-sided cost functions in robust statistics. Its Fisher consistency property and generalization ability are also investigated. Besides the robustness and smoothness, another nice property of RSVC lies in the fact that its solution can be obtained by solving weighted squared hinge loss-based support vector machine problems iteratively. We further show that in each iteration, it is a quadratic programming problem in its dual space and can be solved by using state-of-the-art methods. We thus propose an iteratively reweighted type algorithm and provide a constructive proof of its convergence to a stationary point. Effectiveness of the proposed classifiers is verified on both artificial and real data sets."}], "ArticleTitle": "Robust Support Vector Machines for Classification with Nonconvex and Smooth Losses."}, "27172447": {"mesh": ["Action Potentials", "Animals", "Computer Simulation", "Hippocampus", "Models, Neurological", "Neurons", "Rats", "Rats, Long-Evans", "Statistics, Nonparametric"], "AbstractText": [{"section": null, "text": "Pyramidal neurons recorded from the rat hippocampus and entorhinal cortex, such as place and grid cells, have diverse receptive fields, which are either unimodal or multimodal. Spiking activity from these cells encodes information about the spatial position of a freely foraging rat. At fine timescales, a neuron's spike activity also depends significantly on its own spike history. However, due to limitations of current parametric modeling approaches, it remains a challenge to estimate complex, multimodal neuronal receptive fields while incorporating spike history dependence. Furthermore, efforts to decode the rat's trajectory in one- or two-dimensional space from hippocampal ensemble spiking activity have mainly focused on spike history-independent neuronal encoding models. In this letter, we address these two important issues by extending a recently introduced nonparametric neural encoding framework that allows modeling both complex spatial receptive fields and spike history dependencies. Using this extended nonparametric approach, we develop novel algorithms for decoding a rat's trajectory based on recordings of hippocampal place cells and entorhinal grid cells. Results show that both encoding and decoding models derived from our new method performed significantly better than state-of-the-art encoding and decoding models on 6 minutes of test data. In addition, our model's performance remains invariant to the apparent modality of the neuron's receptive field."}], "ArticleTitle": "A Novel Nonparametric Approach for Neural Encoding and Decoding Models of Multimodal Receptive Fields."}, "28777727": {"mesh": [], "AbstractText": [{"section": null, "text": "The two-dimensional Gabor function is adapted to natural image statistics, leading to a tractable probabilistic generative model that can be used to model simple cell receptive field profiles, or generate basis functions for sparse coding applications. Learning is found to be most pronounced in three Gabor function parameters representing the size and spatial frequency of the two-dimensional Gabor function and characterized by a nonuniform probability distribution with heavy tails. All three parameters are found to be strongly correlated, resulting in a basis of multiscale Gabor functions with similar aspect ratios and size-dependent spatial frequencies. A key finding is that the distribution of receptive-field sizes is scale invariant over a wide range of values, so there is no characteristic receptive field size selected by natural image statistics. The Gabor function aspect ratio is found to be approximately conserved by the learning rules and is therefore not well determined by natural image statistics. This allows for three distinct solutions: a basis of Gabor functions with sharp orientation resolution at the expense of spatial-frequency resolution, a basis of Gabor functions with sharp spatial-frequency resolution at the expense of orientation resolution, or a basis with unit aspect ratio. Arbitrary mixtures of all three cases are also possible. Two parameters controlling the shape of the marginal distributions in a probabilistic generative model fully account for all three solutions. The best-performing probabilistic generative model for sparse coding applications is found to be a gaussian copula with Pareto marginal probability density functions."}], "ArticleTitle": "The Two-Dimensional Gabor Function Adapted to Natural Image Statistics: A Model of Simple-Cell Receptive Fields and Sparse Structure in Images."}, "28562219": {"mesh": [], "AbstractText": [{"section": null, "text": "The possibility of using a quantum computer D-Wave 2X with more than 1000 qubits to determine the global minimum of the energy landscape of trained restricted Boltzmann machines is investigated. In order to overcome the problem of limited interconnectivity in the D-Wave architecture, the proposed RBM embedding combines multiple qubits to represent a particular RBM unit. The results for the lowest-energy (the ground state) and some of the higher-energy states found by the D-Wave 2X were compared with those of the classical simulated annealing (SA) algorithm. In many cases, the D-Wave machine successfully found the same RBM lowest-energy state as that found by SA. In some examples, the D-Wave machine returned a state corresponding to one of the higher-energy local minima found by SA. The inherently nonperfect embedding of the RBM into the Chimera lattice explored in this work (i.e., multiple qubits combined into a single RBM unit were found not to be guaranteed to be all aligned) and the existence of small, persistent biases in the D-Wave hardware may cause a discrepancy between the D-Wave and the SA results. In some of the investigated cases, introduction of a small bias field into the energy function or optimization of the chain-strength parameter in the D-Wave embedding successfully addressed difficulties of the particular RBM embedding. With further development of the D-Wave hardware, the approach will be suitable for much larger numbers of RBM units."}], "ArticleTitle": "Determination of the Lowest-Energy States for the Model Distribution of Trained Restricted Boltzmann Machines Using a 1000 Qubit D-Wave 2X Quantum Computer."}, "28957025": {"mesh": [], "AbstractText": [{"section": null, "text": "Support vector machines, which maximize the margin from patterns to the separation hyperplane subject to correct classification, have received remarkable success in machine learning. Margin error bounds based on Hilbert spaces have been introduced in the literature to justify the strategy of maximizing the margin in SVM. Recently, there has been much interest in developing Banach space methods for machine learning. Large margin classification in Banach spaces is a focus of such attempts. In this letter we establish a margin error bound for the SVM on reproducing kernel Banach spaces, thus supplying statistical justification for large-margin classification in Banach spaces."}], "ArticleTitle": "Margin Error Bounds for Support Vector Machines on Reproducing Kernel Banach Spaces."}, "27391679": {"mesh": [], "AbstractText": [{"section": null, "text": "The k-dimensional coding schemes refer to a collection of methods that attempt to represent data using a set of representative k-dimensional vectors and include nonnegative matrix factorization, dictionary learning, sparse coding, k-means clustering, and vector quantization as special cases. Previous generalization bounds for the reconstruction error of the k-dimensional coding schemes are mainly dimensionality-independent. A major advantage of these bounds is that they can be used to analyze the generalization error when data are mapped into an infinite- or high-dimensional feature space. However, many applications use finite-dimensional data features. Can we obtain dimensionality-dependent generalization bounds for k-dimensional coding schemes that are tighter than dimensionality-independent bounds when data are in a finite-dimensional feature space? Yes. In this letter, we address this problem and derive a dimensionality-dependent generalization bound for k-dimensional coding schemes by bounding the covering number of the loss function class induced by the reconstruction error. The bound is of order [Formula: see text], where m is the dimension of features, k is the number of the columns in the linear implementation of coding schemes, and n is the size of sample, [Formula: see text] when n is finite and [Formula: see text] when n is infinite. We show that our bound can be tighter than previous results because it avoids inducing the worst-case upper bound on k of the loss function. The proposed generalization bound is also applied to some specific coding schemes to demonstrate that the dimensionality-dependent bound is an indispensable complement to the dimensionality-independent generalization bounds."}], "ArticleTitle": "Dimensionality-Dependent Generalization Bounds for k-Dimensional Coding Schemes."}, "28095199": {"mesh": ["Acoustic Stimulation", "Adult", "Animals", "Brain", "Brain Mapping", "Brain Waves", "Electroencephalography", "Factor Analysis, Statistical", "Female", "Humans", "Image Processing, Computer-Assisted", "Magnetic Resonance Imaging", "Male", "Neural Pathways", "Oxygen", "Photic Stimulation", "Young Adult"], "AbstractText": [{"section": null, "text": "Multiway array decomposition methods have been shown to be promising statistical tools for identifying neural activity in the EEG spectrum. They blindly decompose the EEG spectrum into spatial-temporal-spectral patterns by taking into account inherent relationships among signals acquired at different frequencies and sensors. Our study evaluates the stability of spatial-temporal-spectral patterns derived by one particular method, parallel factor analysis (PARAFAC). We focused on patterns' stability over time and in population and divided the complete data set containing data from 50 healthy subjects into several subsets. Our results suggest that the patterns are highly stable in time, as well as among different subgroups of subjects. Further, we show with simultaneously acquired fMRI data that power fluctuations of some patterns have stable correspondence to hemodynamic fluctuations in large-scale brain networks. We did not find such correspondence for power fluctuations in standard frequency bands, the common way of dealing with EEG data. Altogether, our results suggest that PARAFAC is a suitable method for research in the field of large-scale brain networks and their manifestation in EEG signal."}], "ArticleTitle": "Multiway Array Decomposition of EEG Spectrum: Implications of Its Stability for the Exploration of Large-Scale Brain Networks."}, "27391684": {"mesh": [], "AbstractText": [{"section": null, "text": "A well-known phenomenon in sensory perception is desensitization, wherein behavioral responses to persistent stimuli become attenuated over time. In this letter, our focus is on studying mechanisms through which desensitization may be mediated at the network level and, specifically, how sensitivity changes arise as a function of long-term plasticity. Our principal object of study is a generic isoinhibitory motif: a small excitatory-inhibitory network with recurrent inhibition. Such a motif is of interest due to its overrepresentation in laminar sensory network architectures. Here, we introduce a sensitivity analysis derived from control theory in which we characterize the fixed-energy reachable set of the motif. This set describes the regions of the phase-space that are more easily (in terms of stimulus energy) accessed, thus providing a holistic assessment of sensitivity. We specifically focus on how the geometry of this set changes due to repetitive application of a persistent stimulus. We find that for certain motif dynamics, this geometry contracts along the stimulus orientation while expanding in orthogonal directions. In other words, the motif not only desensitizes to the persistent input, but heightens its responsiveness (sensitizes) to those that are orthogonal. We develop a perturbation analysis that links this sensitization to both plasticity-induced changes in synaptic weights and the intrinsic dynamics of the network, highlighting that the effect is not purely due to weight-dependent disinhibition. Instead, this effect depends on the relative neuronal time constants and the consequent stimulus-induced drift that arises in the motif phase-space. For tightly distributed (but random) parameter ranges, sensitization is quite generic and manifests in larger recurrent E-I networks within which the motif is embedded."}], "ArticleTitle": "The Geometry of Plasticity-Induced Sensitization in Isoinhibitory Rate Motifs."}, "27870617": {"mesh": [], "AbstractText": [{"section": null, "text": "Mutual information is a commonly used measure of communication between neurons, but little theory exists describing the relationship between mutual information and the parameters of the underlying neuronal interaction. Such a theory could help us understand how specific physiological changes affect the capacity of neurons to synaptically communicate, and, in particular, they could help us characterize the mechanisms by which neuronal dynamics gate the flow of information in the brain. Here we study a pair of linear-nonlinear-Poisson neurons coupled by a weak synapse. We derive an analytical expression describing the mutual information between their spike trains in terms of synapse strength, neuronal activation function, the time course of postsynaptic currents, and the time course of the background input received by the two neurons. This expression allows mutual information calculations that would otherwise be computationally intractable. We use this expression to analytically explore the interaction of excitation, information transmission, and the convexity of the activation function. Then, using this expression to quantify mutual information in simulations, we illustrate the information-gating effects of neural oscillations and oscillatory coherence, which may either increase or decrease the mutual information across the synapse depending on parameters. Finally, we show analytically that our results can quantitatively describe the selection of one information pathway over another when multiple sending neurons project weakly to a single receiving neuron."}], "ArticleTitle": "Analytical Calculation of Mutual Information between Weakly Coupled Poisson-Spiking Neurons in Models of Dynamically Gated Communication."}, "27557101": {"mesh": [], "AbstractText": [{"section": null, "text": "Characterization of long-term activity-dependent plasticity from behaviorally driven spiking activity is important for understanding the underlying mechanisms of learning and memory. In this letter, we present a computational framework for quantifying spike-timing-dependent plasticity (STDP) during behavior by identifying a functional plasticity rule solely from spiking activity. First, we formulate a flexible point-process spiking neuron model structure with STDP, which includes functions that characterize the stationary and plastic properties of the neuron. The STDP model includes a novel function for prolonged plasticity induction, as well as a more typical function for synaptic weight change based on the relative timing of input-output spike pairs. Consideration for system stability is incorporated with weight-dependent synaptic modification. Next, we formalize an estimation technique using a generalized multilinear model (GMLM) structure with basis function expansion. The weight-dependent synaptic modification adds a nonlinearity to the model, which is addressed with an iterative unconstrained optimization approach. Finally, we demonstrate successful model estimation on simulated spiking data and show that all model functions can be estimated accurately with this method across a variety of simulation parameters, such as number of inputs, output firing rate, input firing type, and simulation time. Since this approach requires only naturally generated spikes, it can be readily applied to behaving animal studies to characterize the underlying mechanisms of learning and memory."}], "ArticleTitle": "Identification of Stable Spike-Timing-Dependent Plasticity from Spiking Activity with Generalized Multilinear Modeling."}, "28181878": {"mesh": ["Algorithms", "Computer Simulation", "Humans", "Models, Neurological", "Nerve Net", "Neural Networks, Computer", "Neuronal Plasticity", "Neurons", "Signal Processing, Computer-Assisted"], "AbstractText": [{"section": null, "text": "Controlling the flow and routing of data is a fundamental problem in many distributed networks, including transportation systems, integrated circuits, and the Internet. In the brain, synaptic plasticity rules have been discovered that regulate network activity in response to environmental inputs, which enable circuits to be stable yet flexible. Here, we develop a new neuro-inspired model for network flow control that depends only on modifying edge weights in an activity-dependent manner. We show how two fundamental plasticity rules, long-term potentiation and long-term depression, can be cast as a distributed gradient descent algorithm for regulating traffic flow in engineered networks. We then characterize, both by simulation and analytically, how different forms of edge-weight-update rules affect network routing efficiency and robustness. We find a close correspondence between certain classes of synaptic weight update rules derived experimentally in the brain and rules commonly used in engineering, suggesting common principles to both."}], "ArticleTitle": "Using Inspiration from Synaptic Plasticity Rules to Optimize Traffic Flow in Distributed Engineered Networks."}, "33513324": {"mesh": [], "AbstractText": [{"section": null, "text": "Sustained attention is a cognitive ability to maintain task focus over extended periods of time (Mackworth, 1948; Chun, Golomb, & Turk-Browne, 2011). In this study, scalp electroencephalography (EEG) signals were processed in real time using a 32 dry-electrode system during a sustained visual attention task. An attention training paradigm was implemented, as designed in DeBettencourt, Cohen, Lee, Norman, and Turk-Browne (2015) in which the composition of a sequence of blended images is updated based on the participant's decoded attentional level to a primed image category. It was hypothesized that a single neurofeedback training session would improve sustained attention abilities. Twenty-two participants were trained on a single neurofeedback session with behavioral pretraining and posttraining sessions within three consecutive days. Half of the participants functioned as controls in a double-blinded design and received sham neurofeedback. During the neurofeedback session, attentional states to primed categories were decoded in real time and used to provide a continuous feedback signal customized to each participant in a closed-loop approach. We report a mean classifier decoding error rate of 34.3% (chance = 50%). Within the neurofeedback group, there was a greater level of task-relevant attentional information decoded in the participant's brain before making a correct behavioral response than before an incorrect response. This effect was not visible in the control group (interaction p=7.23e-4), which strongly indicates that we were able to achieve a meaningful measure of subjective attentional state in real time and control participants' behavior during the neurofeedback session. We do not provide conclusive evidence whether the single neurofeedback session per se provided lasting effects in sustained attention abilities. We developed a portable EEG neurofeedback system capable of decoding attentional states and predicting behavioral choices in the attention task at hand. The neurofeedback code framework is Python based and open source, and it allows users to actively engage in the development of neurofeedback tools for scientific and translational use."}], "ArticleTitle": "Real-Time Decoding of Attentional States Using Closed-Loop EEG Neurofeedback."}, "28333586": {"mesh": [], "AbstractText": [{"section": null, "text": "Recent experiments have shown that stereotypical spatiotemporal patterns occur during brief packets of spiking activity in the cortex, and it has been suggested that top-down inputs can modulate these patterns according to the context. We propose a simple model that may explain important features of these experimental observations and is analytically tractable. The key mechanism underlying this model is that context-dependent top-down inputs can modulate the effective connection strengths between neurons because of short-term synaptic depression. As a result, the degree of synchrony and, in turn, the spatiotemporal patterns of spiking activity that occur during packets are modulated by the top-down inputs. This is shown using an analytical framework, based on avalanche dynamics, that allows calculating the probability that a given neuron spikes during a packet and numerical simulations. Finally, we show that the spatiotemporal patterns that replay previously experienced sequential stimuli and their binding with their corresponding context can be learned because of spike-timing-dependent plasticity."}], "ArticleTitle": "Modulation of Context-Dependent Spatiotemporal Patterns within Packets of Spiking Activity."}, "33617742": {"mesh": [], "AbstractText": [{"section": null, "text": "This letter introduces a new framework for quantifying predictive uncertainty for both data and models that relies on projecting the data into a gaussian reproducing kernel Hilbert space (RKHS) and transforming the data probability density function (PDF) in a way that quantifies the flow of its gradient as a topological potential field (quantified at all points in the sample space). This enables the decomposition of the PDF gradient flow by formulating it as a moment decomposition problem using operators from quantum physics, specifically Schr&#246;dinger's formulation. We experimentally show that the higher-order moments systematically cluster the different tail regions of the PDF, thereby providing unprecedented discriminative resolution of data regions having high epistemic uncertainty. In essence, this approach decomposes local realizations of the data PDF in terms of uncertainty moments. We apply this framework as a surrogate tool for predictive uncertainty quantification of point-prediction neural network models, overcoming various limitations of conventional Bayesian-based uncertainty quantification methods. Experimental comparisons with some established methods illustrate performance advantages that our framework exhibits."}], "ArticleTitle": "Toward a Kernel-Based Uncertainty Decomposition Framework for Data and Models."}, "28095191": {"mesh": [], "AbstractText": [{"section": null, "text": "Mixture of autoregressions (MoAR) models provide a model-based approach to the clustering of time series data. The maximum likelihood (ML) estimation of MoAR models requires evaluating products of large numbers of densities of normal random variables. In practical scenarios, these products converge to zero as the length of the time series increases, and thus the ML estimation of MoAR models becomes infeasible without the use of numerical tricks. We propose a maximum pseudolikelihood (MPL) estimation approach as an alternative to the use of numerical tricks. The MPL estimator is proved to be consistent and can be computed with an EM (expectation-maximization) algorithm. Simulations are used to assess the performance of the MPL estimator against that of the ML estimator in cases where the latter was able to be calculated. An application to the clustering of time series data arising from a resting state fMRI experiment is presented as a demonstration of the methodology."}], "ArticleTitle": "Maximum Pseudolikelihood Estimation for Model-Based Clustering of Time Series Data."}, "33513329": {"mesh": [], "AbstractText": [{"section": null, "text": "This work addresses the problem of network pruning and proposes a novel joint training method based on a multiobjective optimization model. Most of the state-of-the-art pruning methods rely on user experience for selecting the sparsity ratio of the weight matrices or tensors, and thus suffer from severe performance reduction with inappropriate user-defined parameters. Moreover, networks might be inferior due to the inefficient connecting architecture search, especially when it is highly sparse. It is revealed in this work that the network model might maintain sparse characteristic in the early stage of the backpropagation (BP) training process, and evolutionary computation-based algorithms can accurately discover the connecting architecture with satisfying network performance. In particular, we establish a multiobjective sparse model for network pruning and propose an efficient approach that combines BP training and two modified multiobjective evolutionary algorithms (MOEAs). The BP algorithm converges quickly, and the two MOEAs can search for the optimal sparse structure and refine the weights, respectively. Experiments are also included to prove the benefits of the proposed algorithm. We show that the proposed method can obtain a desired Pareto front (PF), leading to a better pruning result comparing to the state-of-the-art methods, especially when the network structure is highly sparse."}], "ArticleTitle": "Joint Structure and Parameter Optimization of Multiobjective Sparse Neural Network."}, "27348739": {"mesh": [], "AbstractText": [{"section": null, "text": "The leaky integrator, the basis for many neuronal models, possesses a negative group delay when a time-delayed recurrent inhibition is added to it. By means of this delay, the leaky integrator becomes a predictor for some frequency components of the input signal. The prediction properties are derived analytically, and an application to a local field potential is provided."}], "ArticleTitle": "The Leaky Integrator with Recurrent Inhibition as a Predictor."}, "27172379": {"mesh": [], "AbstractText": [{"section": null, "text": "Estimation of current source density (CSD) from the low-frequency part of extracellular electric potential recordings is an unstable linear inverse problem. To make the estimation possible in an experimental setting where recordings are contaminated with noise, it is necessary to stabilize the inversion. Here we present a unified framework for zero- and higher-order singular-value-decomposition (SVD)-based spectral regularization of 1D (linear) CSD estimation from local field potentials. The framework is based on two general approaches commonly employed for solving inverse problems: quadrature and basis function expansion. We first show that both inverse CSD (iCSD) and kernel CSD (kCSD) fall into the category of basis function expansion methods. We then use these general categories to introduce two new estimation methods, quadrature CSD (qCSD), based on discretizing the CSD integral equation with a chosen quadrature rule, and representer CSD (rCSD), an even-determined basis function expansion method that uses the problem's data kernels (representers) as basis functions. To determine the best candidate methods to use in the analysis of experimental data, we compared the different methods on simulations under three regularization schemes (Tikhonov, tSVD, and dSVD), three regularization parameter selection methods (NCP, L-curve, and GCV), and seven different a priori spatial smoothness constraints on the CSD distribution. This resulted in a comparison of 531 estimation schemes. We evaluated the estimation schemes according to their source reconstruction accuracy by testing them using different simulated noise levels, lateral source diameters, and CSD depth profiles. We found that ranking schemes according to the average error over all tested conditions results in a reproducible ranking, where the top schemes are found to perform well in the majority of tested conditions. However, there is no single best estimation scheme that outperforms all others under all tested conditions. The unified framework we propose expands the set of available estimation methods, provides increased flexibility for 1D CSD estimation in noisy experimental conditions, and allows for a meaningful comparison between estimation schemes."}], "ArticleTitle": "1D Current Source Density (CSD) Estimation in Inverse Theory: A Unified Framework for Higher-Order Spectral Regularization of Quadrature and Expansion-Type CSD Methods."}, "28030776": {"mesh": [], "AbstractText": [{"section": null, "text": "A cross-validation method based on [Formula: see text] replications of two-fold cross validation is called an [Formula: see text] cross validation. An [Formula: see text] cross validation is used in estimating the generalization error and comparing of algorithms' performance in machine learning. However, the variance of the estimator of the generalization error in [Formula: see text] cross validation is easily affected by random partitions. Poor data partitioning may cause a large fluctuation in the number of overlapping samples between any two training (test) sets in [Formula: see text] cross validation. This fluctuation results in a large variance in the [Formula: see text] cross-validated estimator. The influence of the random partitions on variance becomes serious as [Formula: see text] increases. Thus, in this study, the partitions with a restricted number of overlapping samples between any two training (test) sets are defined as a block-regularized partition set. The corresponding cross validation is called block-regularized [Formula: see text] cross validation ([Formula: see text] BCV). It can effectively reduce the influence of random partitions. We prove that the variance of the [Formula: see text] BCV estimator of the generalization error is smaller than the variance of [Formula: see text] cross-validated estimator and reaches the minimum in a special situation. An analytical expression of the variance can also be derived in this special situation. This conclusion is validated through simulation experiments. Furthermore, a practical construction method of [Formula: see text] BCV by a two-level orthogonal array is provided. Finally, a conservative estimator is proposed for the variance of estimator of the generalization error."}], "ArticleTitle": "Block-Regularized m &#215; 2 Cross-Validated Estimator of the Generalization Error."}, "27764599": {"mesh": [], "AbstractText": [{"section": null, "text": "Cuing a location in space produces a short-lived advantage in reaction time to targets at that location. This early advantage, however, switches to a reaction time cost and has been termed inhibition of return (IOR). IOR behaves differently for different response modalities, suggesting that it may not be a unified effect. This letter presents new data from two experiments testing the gradient of IOR with random, continuous cue-target Euclidean distance and cue-target onset asynchrony. These data were then used to train multiple diffusion models of saccadic and manual reaction time for these cuing experiments. Diffusion models can generate accurate distributions of reaction time data by modeling a response as a buildup of evidence toward a response threshold. If saccadic and attentional IOR are based on similar processes, then differences in distribution will be best explained by adjusting parameter values such as signal and noise within the same model structure. Although experimental data show differences in the timing of IOR across modality, best-fit models are shown to have similar model parameters for the gradient of IOR, suggesting similar underlying mechanisms for saccadic and manual IOR."}], "ArticleTitle": "Multiple Diffusion Models to Compare Saccadic and Manual Responses for Inhibition of Return."}, "27348595": {"mesh": ["Hippocampus", "Humans", "Learning", "Memory", "Neuronal Plasticity", "Neurons"], "AbstractText": [{"section": null, "text": "Synaptic change is a costly resource, particularly for brain structures that have a high demand of synaptic plasticity. For example, building memories of object positions requires efficient use of plasticity resources since objects can easily change their location in space and yet we can memorize object locations. But how should a neural circuit ideally be set up to integrate two input streams (object location and identity) in case the overall synaptic changes should be minimized during ongoing learning? This letter provides a theoretical framework on how the two input pathways should ideally be specified. Generally the model predicts that the information-rich pathway should be plastic and encoded sparsely, whereas the pathway conveying less information should be encoded densely and undergo learning only if a neuronal representation of a novel object has to be established. As an example, we consider hippocampal area CA1, which combines place and object information. The model thereby provides a normative account of hippocampal rate remapping, that is, modulations of place field activity by changes of local cues. It may as well be applicable to other brain areas (such as neocortical layer V) that learn combinatorial codes from multiple input streams."}], "ArticleTitle": "Asymmetry of Neuronal Combinatorial Codes Arises from Minimizing Synaptic Weight Change."}, "27764593": {"mesh": [], "AbstractText": [{"section": null, "text": "We initiate a mathematical analysis of hidden effects induced by binning spike trains of neurons. Assuming that the original spike train has been generated by a discrete Markov process, we show that binning generates a stochastic process that is no longer Markov but is instead a variable-length Markov chain (VLMC) with unbounded memory. We also show that the law of the binned raster is a Gibbs measure in the DLR (Dobrushin-Lanford-Ruelle) sense coined in mathematical statistical mechanics. This allows the derivation of several important consequences on statistical properties of binned spike trains. In particular, we introduce the DLR framework as a natural setting to mathematically formalize anticipation, that is, to tell \"how good\" our nervous system is at making predictions. In a probabilistic sense, this corresponds to condition a process by its future, and we discuss how binning may affect our conclusions on this ability. We finally comment on the possible consequences of binning in the detection of spurious phase transitions or in the detection of incorrect evidence of criticality."}], "ArticleTitle": "On the Mathematical Consequences of Binning Spike Trains."}, "27626962": {"mesh": [], "AbstractText": [{"section": null, "text": "The mixture-of-experts (MoE) model is a popular neural network architecture for nonlinear regression and classification. The class of MoE mean functions is known to be uniformly convergent to any unknown target function, assuming that the target function is from a Sobolev space that is sufficiently differentiable and that the domain of estimation is a compact unit hypercube. We provide an alternative result, which shows that the class of MoE mean functions is dense in the class of all continuous functions over arbitrary compact domains of estimation. Our result can be viewed as a universal approximation theorem for MoE models. The theorem we present allows MoE users to be confident in applying such models for estimation when data arise from nonlinear and nondifferentiable generative processes."}], "ArticleTitle": "A Universal Approximation Theorem for Mixture-of-Experts Models."}, "28599116": {"mesh": [], "AbstractText": [{"section": null, "text": "A typical goal of linear-supervised dimension reduction is to find a low-dimensional subspace of the input space such that the projected input variables preserve maximal information about the output variables. The dependence-maximization approach solves the supervised dimension-reduction problem through maximizing a statistical dependence between projected input variables and output variables. A well-known statistical dependence measure is mutual information (MI), which is based on the Kullback-Leibler (KL) divergence. However, it is known that the KL divergence is sensitive to outliers. Quadratic MI (QMI) is a variant of MI based on the [Formula: see text] distance, which is more robust against outliers than the KL divergence, and a computationally efficient method to estimate QMI from data, least squares QMI (LSQMI), has been proposed recently. For these reasons, developing a supervised dimension-reduction method based on LSQMI seems promising. However, not QMI itself but the derivative of QMI is needed for subspace search in linear-supervised dimension reduction, and the derivative of an accurate QMI estimator is not necessarily a good estimator of the derivative of QMI. In this letter, we propose to directly estimate the derivative of QMI without estimating QMI itself. We show that the direct estimation of the derivative of QMI is more accurate than the derivative of the estimated QMI. Finally, we develop a linear-supervised dimension-reduction algorithm that efficiently uses the proposed derivative estimator and demonstrate through experiments that the proposed method is more robust against outliers than existing methods."}], "ArticleTitle": "Direct Estimation of the Derivative of Quadratic Mutual Information with Application in Supervised Dimension Reduction."}, "28410048": {"mesh": [], "AbstractText": [{"section": null, "text": "This letter focuses on lag synchronization control analysis for memristor-based coupled neural networks with parameter mismatches. Due to the parameter mismatches, lag complete synchronization in general cannot be achieved. First, based on the [Formula: see text]-measure method, generalized Halanay inequality, together with control algorithms, some sufficient conditions are obtained to ensure that coupled memristor-based neural networks are in a state of lag synchronization with an error. Moreover, the error level is estimated. Second, we show that memristor-based coupled neural networks with parameter mismatches can reach lag complete synchronization under a discontinuous controller. Finally, two examples are given to illustrate the effectiveness of the proposed criteria and well support theoretical results."}], "ArticleTitle": "Lag Synchronization Criteria for Memristor-Based Coupled Neural Networks via Parameter Mismatches Analysis Approach."}, "33513326": {"mesh": [], "AbstractText": [{"section": null, "text": "Spatial Monte Carlo integration (SMCI) is an extension of standard Monte Carlo integration and can approximate expectations on Markov random fields with high accuracy. SMCI was applied to pairwise Boltzmann machine (PBM) learning, achieving superior results over those of some existing methods. The approximation level of SMCI can be altered, and it was proved that a higher-order approximation of SMCI is statistically more accurate than a lower-order approximation. However, SMCI as proposed in previous studies suffers from a limitation that prevents the application of a higher-order method to dense systems. This study makes two contributions. First, a generalization of SMCI (called generalized SMCI (GSMCI)) is proposed, which allows a relaxation of the above-mentioned limitation; moreover, a statistical accuracy bound of GSMCI is proved. Second, a new PBM learning method based on SMCI is proposed, which is obtained by combining SMCI and persistent contrastive divergence. The proposed learning method significantly improves learning accuracy."}], "ArticleTitle": "A Generalization of Spatial Monte Carlo Integration."}, "33513328": {"mesh": ["Algorithms", "Brain", "Learning", "Neural Networks, Computer", "Neurons"], "AbstractText": [{"section": null, "text": "Brains process information in spiking neural networks. Their intricate connections shape the diverse functions these networks perform. Yet how network connectivity relates to function is poorly understood, and the functional capabilities of models of spiking networks are still rudimentary. The lack of both theoretical insight and practical algorithms to find the necessary connectivity poses a major impediment to both studying information processing in the brain and building efficient neuromorphic hardware systems. The training algorithms that solve this problem for artificial neural networks typically rely on gradient descent. But doing so in spiking networks has remained challenging due to the nondifferentiable nonlinearity of spikes. To avoid this issue, one can employ surrogate gradients to discover the required connectivity. However, the choice of a surrogate is not unique, raising the question of how its implementation influences the effectiveness of the method. Here, we use numerical simulations to systematically study how essential design parameters of surrogate gradients affect learning performance on a range of classification problems. We show that surrogate gradient learning is robust to different shapes of underlying surrogate derivatives, but the choice of the derivative's scale can substantially affect learning performance. When we combine surrogate gradients with suitable activity regularization techniques, spiking networks perform robust information processing at the sparse activity limit. Our study provides a systematic account of the remarkable robustness of surrogate gradient learning and serves as a practical guide to model functional spiking neural networks."}], "ArticleTitle": "The Remarkable Robustness of Surrogate Gradient Learning for Instilling Complex Function in Spiking Neural Networks."}, "33400896": {"mesh": ["Cognition", "Concept Formation", "Humans", "Learning", "Models, Psychological", "Pattern Recognition, Visual", "Predictive Value of Tests"], "AbstractText": [{"section": null, "text": "Our goal is to understand and optimize human concept learning by predicting the ease of learning of a particular exemplar or category. We propose a method for estimating ease values, quantitative measures of ease of learning, as an alternative to conducting costly empirical training studies. Our method combines a psychological embedding of domain exemplars with a pragmatic categorization model. The two components are integrated using a radial basis function network (RBFN) that predicts ease values. The free parameters of the RBFN are fit using human similarity judgments, circumventing the need to collect human training data to fit more complex models of human categorization. We conduct two category-training experiments to validate predictions of the RBFN. We demonstrate that an instance-based RBFN outperforms both a prototype-based RBFN and an empirical approach using the raw data. Although the human data were collected across diverse experimental conditions, the predicted ease values strongly correlate with human learning performance. Training can be sequenced by (predicted) ease, achieving what is known as fading in the psychology literature and curriculum learning in the machine-learning literature, both of which have been shown to facilitate learning."}], "ArticleTitle": "Predicting the Ease of Human Category Learning Using Radial Basis Function Networks."}, "28333590": {"mesh": ["Animals", "Computer Simulation", "Humans", "Memory", "Models, Neurological", "Neuronal Plasticity", "Nonlinear Dynamics", "Perception", "Probability", "Synapses", "Time Factors"], "AbstractText": [{"section": null, "text": "Memory models that store new memories by forgetting old ones have memory lifetimes that are rather short and grow only logarithmically in the number of synapses. Attempts to overcome these deficits include \"complex\" models of synaptic plasticity in which synapses possess internal states governing the expression of synaptic plasticity. Integrate-and-express, filter-based models of synaptic plasticity propose that synapses act as low-pass filters, integrating plasticity induction signals before expressing synaptic plasticity. Such mechanisms enhance memory lifetimes, leading to an initial rise in the memory signal that is in radical contrast to other related, but nonintegrative, memory models. Because of the complexity of models with internal synaptic states, however, their dynamics can be more difficult to extract compared to \"simple\" models that lack internal states. Here, we show that by focusing only on processes that lead to changes in synaptic strength, we can integrate out internal synaptic states and effectively reduce complex synapses to simple synapses. For binary-strength synapses, these simplified dynamics then allow us to work directly in the transitions in perceptron activation induced by memory storage rather than in the underlying transitions in synaptic configurations. This permits us to write down master and Fokker-Planck equations that may be simplified under certain, well-defined approximations. These methods allow us to see that memory based on synaptic filters can be viewed as an initial transient that leads to memory signal rise, followed by the emergence of Ornstein-Uhlenbeck-like dynamics that return the system to equilibrium. We may use this approach to compute mean first passage time-defined memory lifetimes for complex models of memory storage."}], "ArticleTitle": "Mean First Passage Memory Lifetimes by Reducing Complex Synapses to Simple Synapses."}, "33513323": {"mesh": [], "AbstractText": [{"section": null, "text": "In this note, I study how the precision of a binary classifier depends on the ratio r of positive to negative cases in the test set, as well as the classifier's true and false-positive rates. This relationship allows prediction of how the precision-recall curve will change with r, which seems not to be well known. It also allows prediction of how F&#946; and the precision gain and recall gain measures of Flach and Kull (2015) vary with r."}], "ArticleTitle": "The Effect of Class Imbalance on Precision-Recall Curves."}, "27764597": {"mesh": ["Algorithms", "Animals", "Brain", "Computer Simulation", "Grid Cells", "Humans", "Models, Neurological", "Nerve Net"], "AbstractText": [{"section": null, "text": "Neural systems are inherently noisy. One well-studied example of a noise reduction mechanism in the brain is the population code, where representing a variable with multiple neurons allows the encoded variable to be recovered with fewer errors. Studies have assumed ideal observer models for decoding population codes, and the manner in which information in the neural population can be retrieved remains elusive. This letter addresses a mechanism by which realistic neural circuits can recover encoded variables. Specifically, the decoding problem of recovering a spatial location from populations of grid cells is studied using belief propagation. We extend the belief propagation decoding algorithm in two aspects. First, beliefs are approximated rather than being calculated exactly. Second, decoding noises are introduced into the decoding circuits. Numerical simulations demonstrate that beliefs can be effectively approximated by combining polynomial nonlinearities with divisive normalization. This approximate belief propagation algorithm is tolerant to decoding noises. Thus, this letter presents a realistic model for decoding neural population codes and investigates fault-tolerant information retrieval mechanisms in the brain."}], "ArticleTitle": "On Decoding Grid Cell Population Codes Using Approximate Belief Propagation."}, "27171269": {"mesh": [], "AbstractText": [{"section": null, "text": "The possibility of approximating a continuous function on a compact subset of the real line by a feedforward single hidden layer neural network with a sigmoidal activation function has been studied in many papers. Such networks can approximate an arbitrary continuous function provided that an unlimited number of neurons in a hidden layer is permitted. In this note, we consider constructive approximation on any finite interval of [Formula: see text] by neural networks with only one neuron in the hidden layer. We construct algorithmically a smooth, sigmoidal, almost monotone activation function [Formula: see text] providing approximation to an arbitrary continuous function within any degree of accuracy. This algorithm is implemented in a computer program, which computes the value of [Formula: see text] at any reasonable point of the real axis."}], "ArticleTitle": "A Single Hidden Layer Feedforward Network with Only One Neuron in the Hidden Layer Can Approximate Any Univariate Function."}, "27171983": {"mesh": [], "AbstractText": [{"section": null, "text": "Log-density gradient estimation is a fundamental statistical problem and possesses various practical applications such as clustering and measuring nongaussianity. A naive two-step approach of first estimating the density and then taking its log gradient is unreliable because an accurate density estimate does not necessarily lead to an accurate log-density gradient estimate. To cope with this problem, a method to directly estimate the log-density gradient without density estimation has been explored and demonstrated to work much better than the two-step method. The objective of this letter is to improve the performance of this direct method in multidimensional cases. Our idea is to regard the problem of log-density gradient estimation in each dimension as a task and apply regularized multitask learning to the direct log-density gradient estimator. We experimentally demonstrate the usefulness of the proposed multitask method in log-density gradient estimation and mode-seeking clustering."}], "ArticleTitle": "Regularized Multitask Learning for Multidimensional Log-Density Gradient Estimation."}, "28562215": {"mesh": [], "AbstractText": [{"section": null, "text": "We used the phase-resetting method to study a biologically relevant three-neuron network in which one neuron receives multiple inputs per cycle. For this purpose, we first generalized the concept of phase resetting to accommodate multiple inputs per cycle. We explicitly showed how analytical conditions for the existence and the stability of phase-locked modes are derived. In particular, we solved newly derived recursive maps using as an example a biologically relevant driving-driven neural network with a dynamic feedback loop. We applied the generalized phase-resetting definition to predict the relative-phase and the stability of a phase-locked mode in open loop setup. We also compared the predicted phase-locked mode against numerical simulations of the fully connected network."}], "ArticleTitle": "Predicting the Existence and Stability of Phase-Locked Mode in Neural Networks Using Generalized Phase-Resetting Curve."}, "33513321": {"mesh": [], "AbstractText": [{"section": null, "text": "Stable concurrent learning and control of dynamical systems is the subject of adaptive control. Despite being an established field with many practical applications and a rich theory, much of the development in adaptive control for nonlinear systems revolves around a few key algorithms. By exploiting strong connections between classical adaptive nonlinear control techniques and recent progress in optimization and machine learning, we show that there exists considerable untapped potential in algorithm development for both adaptive nonlinear control and adaptive dynamics prediction. We begin by introducing first-order adaptation laws inspired by natural gradient descent and mirror descent. We prove that when there are multiple dynamics consistent with the data, these non-Euclidean adaptation laws implicitly regularize the learned model. Local geometry imposed during learning thus may be used to select parameter vectors-out of the many that will achieve perfect tracking or prediction-for desired properties such as sparsity. We apply this result to regularized dynamics predictor and observer design, and as concrete examples, we consider Hamiltonian systems, Lagrangian systems, and recurrent neural networks. We subsequently develop a variational formalism based on the Bregman Lagrangian. We show that its Euler Lagrange equations lead to natural gradient and mirror descent-like adaptation laws with momentum, and we recover their first-order analogues in the infinite friction limit. We illustrate our analyses with simulations demonstrating our theoretical results."}], "ArticleTitle": "Implicit Regularization and Momentum Algorithms in Nonlinearly Parameterized Adaptive Control and Prediction."}, "29162006": {"mesh": [], "AbstractText": [{"section": null, "text": "Sufficient dimension reduction (SDR) is aimed at obtaining the low-rank projection matrix in the input space such that information about output data is maximally preserved. Among various approaches to SDR, a promising method is based on the eigendecomposition of the outer product of the gradient of the conditional density of output given input. In this letter, we propose a novel estimator of the gradient of the logarithmic conditional density that directly fits a linear-in-parameter model to the true gradient under the squared loss. Thanks to this simple least-squares formulation, its solution can be computed efficiently in a closed form. Then we develop a new SDR method based on the proposed gradient estimator. We theoretically prove that the proposed gradient estimator, as well as the SDR solution obtained from it, achieves the optimal parametric convergence rate. Finally, we experimentally demonstrate that our SDR method compares favorably with existing approaches in both accuracy and computational efficiency on a variety of artificial and benchmark data sets."}], "ArticleTitle": "Sufficient Dimension Reduction via Direct Estimation of the Gradients of Logarithmic Conditional Densities."}, "33617741": {"mesh": ["Brain", "Brain Waves", "Models, Neurological", "Neurons"], "AbstractText": [{"section": null, "text": "The relationship between complex brain oscillations and the dynamics of individual neurons is poorly understood. Here we utilize maximum caliber, a dynamical inference principle, to build a minimal yet general model of the collective (mean field) dynamics of large populations of neurons. In agreement with previous experimental observations, we describe a simple, testable mechanism, involving only a single type of neuron, by which many of these complex oscillatory patterns may emerge. Our model predicts that the refractory period of neurons, which has often been neglected, is essential for these behaviors."}], "ArticleTitle": "The Refractory Period Matters: Unifying Mechanisms of Macroscopic Brain Waves."}, "33513327": {"mesh": [], "AbstractText": [{"section": null, "text": "We study the learning dynamics and the representations emerging in recurrent neural networks (RNNs) trained to integrate one or multiple temporal signals. Combining analytical and numerical investigations, we characterize the conditions under which an RNN with n neurons learns to integrate D(&#8810;n) scalar signals of arbitrary duration. We show, for linear, ReLU, and sigmoidal neurons, that the internal state lives close to a D-dimensional manifold, whose shape is related to the activation function. Each neuron therefore carries, to various degrees, information about the value of all integrals. We discuss the deep analogy between our results and the concept of mixed selectivity forged by computational neuroscientists to interpret cortical recordings."}], "ArticleTitle": "Low-Dimensional Manifolds Support Multiplexed Integrations in Recurrent Neural Networks."}, "27764594": {"mesh": ["Brain", "Brain Mapping", "Brain Waves", "Electroencephalography", "Humans", "Seizures", "Wavelet Analysis"], "AbstractText": [{"section": null, "text": "This letter describes the improvement of two methods of detecting high-frequency oscillations (HFOs) and their use to localize epileptic seizure onset zones (SOZs). The wavelet transform (WT) method was improved by combining the complex Morlet WT with Shannon entropy to enhance the temporal-frequency resolution during HFO detection. And the matching pursuit (MP) method was improved by combining it with an adaptive genetic algorithm to improve the speed and accuracy of the calculations for HFO detection. The HFOs detected by these two methods were used to localize SOZs in five patients. A comparison shows that the improved WT method provides high specificity and quick localization and that the improved MP method provides high sensitivity."}], "ArticleTitle": "Fast, Accurate Localization of Epileptic Seizure Onset Zones Based on Detection of High-Frequency Oscillations Using Improved Wavelet Transform and Matching Pursuit Methods."}, "28777729": {"mesh": [], "AbstractText": [{"section": null, "text": "Canonical correlation analysis (CCA) is a useful tool in detecting the latent relationship between two sets of multivariate variables. In theoretical analysis of CCA, a regularization technique is utilized to investigate the consistency of its analysis. This letter addresses the consistency property of CCA from a least squares view. We construct a constrained empirical risk minimization framework of CCA and apply a two-stage randomized Kaczmarz method to solve it. In the first stage, we remove the noise, and in the second stage, we compute the canonical weight vectors. Rigorous theoretical consistency is addressed. The statistical consistency of this novel scenario is extended to the kernel version of it. Moreover, experiments on both synthetic and real-world data sets demonstrate the effectiveness and efficiency of the proposed algorithms."}], "ArticleTitle": "Constrained ERM Learning of Canonical Correlation Analysis: A Least Squares Perspective."}, "33617746": {"mesh": [], "AbstractText": [{"section": null, "text": "Backdoor data poisoning attacks add mislabeled examples to the training set, with an embedded backdoor pattern, so that the classifier learns to classify to a target class whenever the backdoor pattern is present in a test sample. Here, we address posttraining detection of scene-plausible perceptible backdoors, a type of backdoor attack that can be relatively easily fashioned, particularly against DNN image classifiers. A post-training defender does not have access to the potentially poisoned training set, only to the trained classifier, as well as some unpoisoned examples that need not be training samples. Without the poisoned training set, the only information about a backdoor pattern is encoded in the DNN's trained weights. This detection scenario is of great import considering legacy and proprietary systems, cell phone apps, as well as training outsourcing, where the user of the classifier will not have access to the entire training set. We identify two important properties of scene-plausible perceptible backdoor patterns, spatial invariance and robustness, based on which we propose a novel detector using the maximum achievable misclassification fraction (MAMF) statistic. We detect whether the trained DNN has been backdoor-attacked and infer the source and target classes. Our detector outperforms existing detectors and, coupled with an imperceptible backdoor detector, helps achieve posttraining detection of most evasive backdoors of interest."}], "ArticleTitle": "Detecting Scene-Plausible Perceptible Backdoors in Trained DNNs Without Access to the Training Set."}, "33513320": {"mesh": [], "AbstractText": [{"section": null, "text": "Our work focuses on unsupervised and generative methods that address the following goals: (1) learning unsupervised generative representations that discover latent factors controlling image semantic attributes, (2) studying how this ability to control attributes formally relates to the issue of latent factor disentanglement, clarifying related but dissimilar concepts that had been confounded in the past, and (3) developing anomaly detection methods that leverage representations learned in the first goal. For goal 1, we propose a network architecture that exploits the combination of multiscale generative models with mutual information (MI) maximization. For goal 2, we derive an analytical result, lemma 1, that brings clarity to two related but distinct concepts: the ability of generative networks to control semantic attributes of images they generate, resulting from MI maximization, and the ability to disentangle latent space representations, obtained via total correlation minimization. More specifically, we demonstrate that maximizing semantic attribute control encourages disentanglement of latent factors. Using lemma 1 and adopting MI in our loss function, we then show empirically that for image generation tasks, the proposed approach exhibits superior performance as measured in the quality and disentanglement of the generated images when compared to other state-of-the-art methods, with quality assessed via the Fr&#233;chet inception distance (FID) and disentanglement via mutual information gap. For goal 3, we design several systems for anomaly detection exploiting representations learned in goal 1 and demonstrate their performance benefits when compared to state-of-the-art generative and discriminative algorithms. Our contributions in representation learning have potential applications in addressing other important problems in computer vision, such as bias and privacy in AI."}], "ArticleTitle": "Unsupervised Discovery, Control, and Disentanglement of Semantic Attributes With Applications to Anomaly Detection."}, "33617745": {"mesh": [], "AbstractText": [{"section": null, "text": "It is of great interest to characterize the spiking activity of individual neurons in a cell ensemble. Many different mechanisms, such as synaptic coupling and the spiking activity of itself and its neighbors, drive a cell's firing properties. Though this is a widely studied modeling problem, there is still room to develop modeling solutions by simplifications embedded in previous models. The first shortcut is that synaptic coupling mechanisms in previous models do not replicate the complex dynamics of the synaptic response. The second is that the number of synaptic connections in these models is an order of magnitude smaller than in an actual neuron. In this research, we push this barrier by incorporating a more accurate model of the synapse and propose a system identification solution that can scale to a network incorporating hundreds of synaptic connections. Although a neuron has hundreds of synaptic connections, only a subset of these connections significantly contributes to its spiking activity. As a result, we assume the synaptic connections are sparse, and to characterize these dynamics, we propose a Bayesian point-process state-space model that lets us incorporate the sparsity of synaptic connections within the regularization technique into our framework. We develop an extended expectation-maximization. algorithm to estimate the free parameters of the proposed model and demonstrate the application of this methodology to the problem of estimating the parameters of many dynamic synaptic connections. We then go through a simulation example consisting of the dynamic synapses across a range of parameter values and show that the model parameters can be estimated using our method. We also show the application of the proposed algorithm in the intracellular data that contains 96 presynaptic connections and assess the estimation accuracy of our method using a combination of goodness-of-fit measures."}], "ArticleTitle": "Parameter Estimation in Multiple Dynamic Synaptic Coupling Model Using Bayesian Point Process State-Space Modeling Framework."}, "33513330": {"mesh": [], "AbstractText": [{"section": null, "text": "Neuronal networks in rodent primary visual cortex (V1) can generate oscillations in different frequency bands depending on the network state and the level of visual stimulation. High-frequency gamma rhythms, for example, dominate the network's spontaneous activity in adult mice but are attenuated upon visual stimulation, during which the network switches to the beta band instead. The spontaneous local field potential (LFP) of juvenile mouse V1, however, mainly contains beta rhythms and presenting a stimulus does not elicit drastic changes in network oscillations. We study, in a spiking neuron network model, the mechanism in adult mice allowing for flexible switches between multiple frequency bands and contrast this to the network structure in juvenile mice that lack this flexibility. The model comprises excitatory pyramidal cells (PCs) and two types of interneurons: the parvalbumin-expressing (PV) and the somatostatinexpressing (SOM) interneuron. In accordance with experimental findings, the pyramidal-PV and pyramidal-SOM cell subnetworks are associated with gamma and beta oscillations, respectively. In our model, they are both generated via a pyramidal-interneuron gamma (PING) mechanism, wherein the PCs drive the oscillations. Furthermore, we demonstrate that large but not small visual stimulation activates SOM cells, which shift the frequency of resting-state gamma oscillations produced by the pyramidal-PV cell subnetwork so that beta rhythms emerge. Finally, we show that this behavior is obtained for only a subset of PV and SOM interneuron projection strengths, indicating that their influence on the PCs should be balanced so that they can compete for oscillatory control of the PCs. In sum, we propose a mechanism by which visual beta rhythms can emerge from spontaneous gamma oscillations in a network model of the mouse V1; for this mechanism to reproduce V1 dynamics in adult mice, balance between the effective strengths of PV and SOM cells is required."}], "ArticleTitle": "Flexible Frequency Switching in Adult Mouse Visual Cortex Is Mediated by Competition Between Parvalbumin and Somatostatin Expressing Interneurons."}, "27626965": {"mesh": ["Nerve Net", "Time Factors"], "AbstractText": [{"section": null, "text": "In this letter, we deal with a class of memristor-based neural networks with distributed leakage delays. By applying a new Lyapunov function method, we obtain some sufficient conditions that ensure the existence, uniqueness, and global exponential stability of almost periodic solutions of neural networks. We apply the results of this solution to prove the existence and stability of periodic solutions for this delayed neural network with periodic coefficients. We then provide an example to illustrate the effectiveness of the theoretical results. Our results are completely new and complement the previous studies Chen, Zeng, and Jiang ( 2014 ) and Jiang, Zeng, and Chen ( 2015 )."}], "ArticleTitle": "Exponential Stability of Almost Periodic Solutions for Memristor-Based Neural Networks with Distributed Leakage Delays."}, "27626970": {"mesh": [], "AbstractText": [{"section": null, "text": "Integrate-and-express models of synaptic plasticity propose that synapses integrate plasticity induction signals before expressing synaptic plasticity. By discerning trends in their induction signals, synapses can control destabilizing fluctuations in synaptic strength. In a feedforward perceptron framework with binary-strength synapses for associative memory storage, we have previously shown that such a filter-based model outperforms other, nonintegrative, \"cascade\"-type models of memory storage in most regions of biologically relevant parameter space. Here, we consider some natural extensions of our earlier filter model, including one specifically tailored to binary-strength synapses and one that demands a fixed, consecutive number of same-type induction signals rather than merely an excess before expressing synaptic plasticity. With these extensions, we show that filter-based models outperform nonintegrative models in all regions of biologically relevant parameter space except for a small sliver in which all models encode memories only weakly. In this sliver, which model is superior depends on the metric used to gauge memory lifetimes (whether a signal-to-noise ratio or a mean first passage time). After comparing and contrasting these various filter models, we discuss the multiple mechanisms and timescales that underlie both synaptic plasticity and memory phenomena and suggest that multiple, different filtering mechanisms may operate at single synapses."}], "ArticleTitle": "Variations on the Theme of Synaptic Filtering: A Comparison of Integrate-and-Express Models of Synaptic Plasticity for Memory Lifetimes."}, "33513325": {"mesh": [], "AbstractText": [{"section": null, "text": "A new network with super-approximation power is introduced. This network is built with Floor (&#8970;x&#8971;) or ReLU (max{0,x}) activation function in each neuron; hence, we call such networks Floor-ReLU networks. For any hyperparameters N&#8712;N+ and L&#8712;N+, we show that Floor-ReLU networks with width max{d,5N+13} and depth 64dL+3 can uniformly approximate a H&#246;lder function f on [0,1]d with an approximation error 3&#955;d&#945;/2N-&#945;L, where &#945;&#8712;(0,1] and &#955; are the H&#246;lder order and constant, respectively. More generally for an arbitrary continuous function f on [0,1]d with a modulus of continuity &#969;f(&#183;), the constructive approximation rate is &#969;f(dN-L)+2&#969;f(d)N-L. As a consequence, this new class of networks overcomes the curse of dimensionality in approximation power when the variation of &#969;f(r) as r&#8594;0 is moderate (e.g., &#969;f(r)&#8818;r&#945; for H&#246;lder continuous functions), since the major term to be considered in our approximation rate is essentially d times a function of N and L independent of d within the modulus of continuity."}], "ArticleTitle": "Deep Network With Approximation Error Being Reciprocal of Width to Power of Square Root of Depth."}, "27172266": {"mesh": [], "AbstractText": [{"section": null, "text": "This letter addresses the reservoir design problem in the context of delay-based reservoir computers for multidimensional input signals, parallel architectures, and real-time multitasking. First, an approximating reservoir model is presented in those frameworks that provides an explicit functional link between the reservoir architecture and its performance in the execution of a specific task. Second, the inference properties of the ridge regression estimator in the multivariate context are used to assess the impact of finite sample training on the decrease of the reservoir capacity. Finally, an empirical study is conducted that shows the adequacy of the theoretical results with the empirical performances exhibited by various reservoir architectures in the execution of several nonlinear tasks with multidimensional inputs. Our results confirm the robustness properties of the parallel reservoir architecture with respect to task misspecification and parameter choice already documented in the literature."}], "ArticleTitle": "Nonlinear Memory Capacity of Parallel Time-Delay Reservoir Computers in the Processing of Multidimensional Signals."}, "33617743": {"mesh": [], "AbstractText": [{"section": null, "text": "Pairwise similarities and dissimilarities between data points are often obtained more easily than full labels of data in real-world classification problems. To make use of such pairwise information, an empirical risk minimization approach has been proposed, where an unbiased estimator of the classification risk is computed from only pairwise similarities and unlabeled data. However, this approach has not yet been able to handle pairwise dissimilarities. Semisupervised clustering methods can incorporate both similarities and dissimilarities into their framework; however, they typically require strong geometrical assumptions on the data distribution such as the manifold assumption, which may cause severe performance deterioration. In this letter, we derive an unbiased estimator of the classification risk based on all of similarities and dissimilarities and unlabeled data. We theoretically establish an estimation error bound and experimentally demonstrate the practical usefulness of our empirical risk minimization method."}], "ArticleTitle": "Classification From Pairwise Similarities/Dissimilarities and Unlabeled Data via Empirical Risk Minimization."}}